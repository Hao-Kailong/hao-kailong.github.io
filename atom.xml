<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hold the Torch</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://haokailong.top/"/>
  <updated>2021-03-19T15:30:16.066Z</updated>
  <id>http://haokailong.top/</id>
  
  <author>
    <name>dinosaur</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ELECTRA预训练模型</title>
    <link href="http://haokailong.top/2021/03/19/ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>http://haokailong.top/2021/03/19/ELECTRA预训练模型/</id>
    <published>2021-03-19T13:13:12.000Z</published>
    <updated>2021-03-19T15:30:16.066Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ELECTRA详解：</p><p><a href="https://zhuanlan.zhihu.com/p/118135466" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/118135466</a></p></blockquote><p><img src="https://pic4.zhimg.com/80/v2-93eaff2a96a1b7d2991417ba10aab35f_720w.jpg" alt></p><h2 id="1-创新点"><a href="#1-创新点" class="headerlink" title="1. 创新点"></a>1. 创新点</h2><ul><li>提出了新的模型预训练的框架，采用generator和discriminator的结合方式，但又不同于GAN</li><li>将Masked Language Model的方式改为了replaced token detection</li><li>因为masked language model 能有效地学习到context的信息，所以能很好地学习embedding，所以使用了weight sharing的方式将generator的embedding的信息共享给discriminator</li><li>dicriminator 预测了generator输出的每个token是不是original的，从而高效地更新transformer的各个参数，使得模型的熟练速度加快</li><li>该模型采用了小的generator以及discriminator的方式共同训练，并且采用了两者loss相加，使得discriminator的学习难度逐渐地提升，学习到更难的token（plausible tokens）</li><li>模型在fine-tuning 的时候，丢弃generator，只使用discrinator</li></ul><a id="more"></a><h2 id="2-模型架构"><a href="#2-模型架构" class="headerlink" title="2. 模型架构"></a>2. 模型架构</h2><p>BERT存在预训练和fine-tuning的mismatch，因为在fine-tuning阶段，并不会有[MASK]的token。</p><p>ELECTRA由两部分组成，分别是generator以及discriminator，两个都是transformer的encoder结构，只是两者的size不同：</p><ul><li><p>generator：就是一个小的 masked language model（通常是 1/4 的discriminator的size），该模块的具体作用是他采用了经典的bert的MLM方式：</p></li><li><ul><li>首先随机选取15%的tokens，替代为[MASK]token，（取消了bert的80%[MASK],10%unchange, 10% random replaced 的操作，具体原因也是因为没必要，因为我们finetuning使用的discriminator)</li><li>使用generator去训练模型，使得模型预测masked token，得到corrupted tokens</li><li>generator的目标函数和bert一样，都是希望被masked的能够被还原成原本的original tokens<br>如上图， token，<code>the</code> 和 <code>cooked</code> 被随机选为被masked，然后generator预测得到corrupted tokens，变成了<code>the</code>和<code>ate</code></li></ul></li></ul><ul><li><p>discriminator：discriminator的接收被generator corrupt之后的输入，discriminator的作用是分辨输入的每一个token是original的还是replaced，注意：如果generator生成的token和原始token一致，那么这个token仍然是original的</p></li><li><ul><li>所以，对于每个token，discriminator都会进行一个二分类，最后获得loss</li></ul></li></ul><p>以上的方式被称为replaced token detection。</p><ul><li>learn from all tokens (instead of 15%)</li><li>compute efficient</li><li>paramter efficient</li><li>improves downstream task performance</li></ul><h3 id="2-1-个人理解"><a href="#2-1-个人理解" class="headerlink" title="2.1. 个人理解"></a>2.1. 个人理解</h3><p>可以把generator看作之前任意一种预训练语言模型，如BERT, RoBERTa等，借鉴了GAN的思想。但是否同样存在训练困难的问题？例如generator性能足够好，导致discriminator输出均为<code>original</code>?</p><h2 id="3-如何训练"><a href="#3-如何训练" class="headerlink" title="3. 如何训练"></a>3. 如何训练</h2><p>该模型采用了minimize the combined loss的方式进行训练：</p><script type="math/tex; mode=display">\min_{\theta_G, \theta_D} \sum_{\vec{x} \in X} L_{MLM}(\vec{x},\theta_{G}) + \lambda L_{Dis}(\vec{x}, \theta_{D})</script><p>其中</p><p>$L<em>{MLM}(\vec{x}, \theta</em>{G})$对于输入$\vec{x} = [x_1, …, x_n]$，经过generator之后得到编码了上下文信息的vector representation，$h(\vec{x}) = [h_1,…,h_n]$，对于位置t，其被替换为[MASK]，那么它的output probability（经过softmax）为：</p><script type="math/tex; mode=display">p_G(x_t|\vec{x}) = \frac{\exp(\mathrm{e}(x_t)^T \cdot \mathrm{h_g}(\vec{x})_t)}{\sum_{x^{'}} \exp(\mathrm{e}(x^{'})^T \cdot \mathrm{h_g} (\vec{x})_t)}</script><p>其中$\mathrm{e}(x_t)$为单词的embedding表示，而$\mathrm{h_g}(\vec{x})_t$为经过generator之后的隐藏层表示。</p><p>然后计算交叉熵损失。</p><p><br></p><p>对于discriminator：</p><script type="math/tex; mode=display">D(\vec{x},t) = \mathrm{sigmoid} (w^Th_D(\vec{x})_t)</script><p>然后计算BCE (binary cross entropy) 损失。</p><h3 id="3-1-反向传播"><a href="#3-1-反向传播" class="headerlink" title="3.1. 反向传播"></a>3.1. 反向传播</h3><p><strong><em>在训练过程中，discriminator的loss不会反向传播到generator</em></strong>（因为generator的sampling的步骤导致）</p><font color="red">但是如果采用这种做法，是否generator存在的必要性就没有了？完全可以采用手动替换的方式，首先进行较为明显的替换，例如用动词替换名词。随着训练的进行，逐渐改为用BERT等预训练模型的预测结果作为替换，增大检测难度。emmm</font><h3 id="3-2-其他训练方式"><a href="#3-2-其他训练方式" class="headerlink" title="3.2. 其他训练方式"></a>3.2. 其他训练方式</h3><p>作者显然也考虑到这一点，在论文中提出了其他训练方式：</p><ul><li>一种是GAN：ELECTRA以一种对抗学习的思想来训练。作者将生成器的目标函数由最小化MLM loss换成了最大化判别器在被替换token上的loss。用强化学习Policy Gradient的思想，将被替换token的交叉熵作为生成器的reward，然后进行梯度下降。强化方法优化下来生成器在MLM任务上可以达到54%的准确率，而之前MLE优化下可以达到65%。</li><li>一种是two-stage 训练，先训练 $L_{MLM}$ n steps，然后froze住 generator，再训练 discriminator n steps<br>但是效果都没有共同训练效果好</li></ul><h2 id="4-超参数"><a href="#4-超参数" class="headerlink" title="4. 超参数"></a>4. 超参数</h2><p>文章中提到了，如果generator过强，那么discriminator就无法成功训练，这其实也很好理解。因为generator非常强，那么预测出来的token都非常好，即都是original tokens，那么discrinator 并不需要如何学习就收敛，因为它只需要把所有二分类都认为是1就行（假设1代表real）</p><p><img src="https://pic2.zhimg.com/80/v2-4b664f463f5c390806db89e3071fa8c5_720w.jpg" alt></p><p>我们可以发现，当generator的size 为discriminator size 的 1/2 到 1/4 时，模型效果最好。</p><p>由于ELECTRA是判别式任务，不用对每个位置的整个数据分布建模，所以更parameter-efficient。</p><ul><li>模型大小的因素</li></ul><p>可以从下图的实验结果看出，ELECTRA 模型在小模型的时候，效果提升显著，随着模型大小的增加，效果降低。</p><p>同时可以看到，在相同的计算量的时候，ELECTRA的效果优于BERT。</p><p><img src="https://pic4.zhimg.com/80/v2-0187b4b8e3777682833d952f2fe9bfd3_720w.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ELECTRA详解：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/118135466&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/118135466&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://pic4.zhimg.com/80/v2-93eaff2a96a1b7d2991417ba10aab35f_720w.jpg&quot; alt&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-创新点&quot;&gt;&lt;a href=&quot;#1-创新点&quot; class=&quot;headerlink&quot; title=&quot;1. 创新点&quot;&gt;&lt;/a&gt;1. 创新点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;提出了新的模型预训练的框架，采用generator和discriminator的结合方式，但又不同于GAN&lt;/li&gt;
&lt;li&gt;将Masked Language Model的方式改为了replaced token detection&lt;/li&gt;
&lt;li&gt;因为masked language model 能有效地学习到context的信息，所以能很好地学习embedding，所以使用了weight sharing的方式将generator的embedding的信息共享给discriminator&lt;/li&gt;
&lt;li&gt;dicriminator 预测了generator输出的每个token是不是original的，从而高效地更新transformer的各个参数，使得模型的熟练速度加快&lt;/li&gt;
&lt;li&gt;该模型采用了小的generator以及discriminator的方式共同训练，并且采用了两者loss相加，使得discriminator的学习难度逐渐地提升，学习到更难的token（plausible tokens）&lt;/li&gt;
&lt;li&gt;模型在fine-tuning 的时候，丢弃generator，只使用discrinator&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="http://haokailong.top/categories/NLP/"/>
    
      <category term="Pretrained Language Model" scheme="http://haokailong.top/categories/NLP/Pretrained-Language-Model/"/>
    
    
      <category term="PLM" scheme="http://haokailong.top/tags/PLM/"/>
    
      <category term="ELECTRA" scheme="http://haokailong.top/tags/ELECTRA/"/>
    
  </entry>
  
  <entry>
    <title>RoBERTa预训练模型</title>
    <link href="http://haokailong.top/2021/03/19/RoBERTa%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>http://haokailong.top/2021/03/19/RoBERTa预训练模型/</id>
    <published>2021-03-19T12:52:24.000Z</published>
    <updated>2021-03-19T13:12:45.528Z</updated>
    
    <content type="html"><![CDATA[<p>相比于BERT的调整：</p><ol><li>训练时间更长，batch size更大，训练数据更多</li><li>移除next sentence prediction loss</li><li>训练序列更长</li><li>动态mask机制</li></ol><p>BERT原型使用的是 character-level BPE vocabulary of size 30K, RoBERTa使用了GPT2的 BPE 实现，使用的是byte而不是unicode characters作为subword的单位。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;相比于BERT的调整：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;训练时间更长，batch size更大，训练数据更多&lt;/li&gt;
&lt;li&gt;移除next sentence prediction loss&lt;/li&gt;
&lt;li&gt;训练序列更长&lt;/li&gt;
&lt;li&gt;动态mask机制&lt;/li&gt;
&lt;/ol&gt;
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://haokailong.top/categories/NLP/"/>
    
      <category term="Pretrained Language Model" scheme="http://haokailong.top/categories/NLP/Pretrained-Language-Model/"/>
    
    
      <category term="PLM" scheme="http://haokailong.top/tags/PLM/"/>
    
      <category term="RoBERTa" scheme="http://haokailong.top/tags/RoBERTa/"/>
    
      <category term="pretrained language models" scheme="http://haokailong.top/tags/pretrained-language-models/"/>
    
  </entry>
  
  <entry>
    <title>C++数组填充方法</title>
    <link href="http://haokailong.top/2021/03/19/C-%E6%95%B0%E7%BB%84%E5%A1%AB%E5%85%85%E6%96%B9%E6%B3%95/"/>
    <id>http://haokailong.top/2021/03/19/C-数组填充方法/</id>
    <published>2021-03-19T06:36:34.000Z</published>
    <updated>2021-03-19T06:52:17.018Z</updated>
    
    <content type="html"><![CDATA[<p>在对数组初始化时，往往需要赋予同样的值，例如布尔型数组全部设为false</p><p>现在要把a里面1000个全部初始化为5；str里面全部初始化为a；</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a[<span class="number">1000</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line"><span class="keyword">char</span> str[<span class="number">1000</span>] = &#123;<span class="string">'a'</span>&#125;;</span><br></pre></td></tr></table></figure><p><strong>但这样只有第一个元素被设置，其余999个自动默认是int 的0</strong>；</p><h1 id="1-memset"><a href="#1-memset" class="headerlink" title="1. memset"></a>1. memset</h1><p>首先，<strong>memset函数是逐字节进行填充</strong>，所以不适合int型数组。</p><p>对于字符型数组：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="built_in">memset</span>(str,<span class="string">'a'</span>,<span class="keyword">sizeof</span>(str))</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="2-fill"><a href="#2-fill" class="headerlink" title="2. fill"></a>2. fill</h1><p>按照单元赋值，将一个区间的元素都赋同一个值．<br>我们还是来看下实现吧：</p><blockquote><p>void fill(ForwardIt first, ForwardIt last, const T&amp; value);<br>void fill_n(OutputIt first, size n, const T&amp; value);<br>功能：<br>fill给迭代器范围[first, last)内元素均赋值为value。无返回值。<br>fill_n将first指向范围内的前n个元素赋值为value。返回first+n(c++11)</p></blockquote><p>对应的源代码为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">ForwardIterator</span>, <span class="title">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">fill</span> (<span class="title">ForwardIterator</span> <span class="title">first</span>, <span class="title">ForwardIterator</span> <span class="title">last</span>, <span class="title">const</span> <span class="title">T</span>&amp; <span class="title">val</span>)</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">while</span> (first != last) &#123;</span><br><span class="line">    *first = val;</span><br><span class="line">    ++first;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实例：</p><p>对于int a[1000]的批量填充：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line">fill(a, a + <span class="number">1000</span>, <span class="number">5</span>);</span><br></pre></td></tr></table></figure><h1 id="3-fill-n"><a href="#3-fill-n" class="headerlink" title="3. fill_n"></a>3. fill_n</h1><p>对应的源代码为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">OutputIterator</span>, <span class="title">class</span> <span class="title">Size</span>, <span class="title">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">OutputIterator</span> <span class="title">fill_n</span> (<span class="title">OutputIterator</span> <span class="title">first</span>, <span class="title">Size</span> <span class="title">n</span>, <span class="title">const</span> <span class="title">T</span>&amp; <span class="title">val</span>)</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">while</span> (n&gt;<span class="number">0</span>) &#123;</span><br><span class="line">    *first = val;</span><br><span class="line">    ++first; --n;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> first;     <span class="comment">// since C++11</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line">fill_n(a, <span class="number">1000</span>, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;　myvector (<span class="number">8</span>,<span class="number">10</span>); <span class="comment">// myvector: 10 10 10 10 10 10 10 10</span></span><br><span class="line">fill_n (myvector.begin(),<span class="number">4</span>,<span class="number">20</span>); <span class="comment">// myvector: 20 20 20 20 10 10 10 10</span></span><br><span class="line">fill_n (myvector.begin()+<span class="number">3</span>,<span class="number">3</span>,<span class="number">33</span>); <span class="comment">// myvector: 20 20 20 33 33 33 10 10</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在对数组初始化时，往往需要赋予同样的值，例如布尔型数组全部设为false&lt;/p&gt;
&lt;p&gt;现在要把a里面1000个全部初始化为5；str里面全部初始化为a；&lt;/p&gt;
&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; a[&lt;span class=&quot;number&quot;&gt;1000&lt;/span&gt;] = &amp;#123;&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&amp;#125;; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;char&lt;/span&gt; str[&lt;span class=&quot;number&quot;&gt;1000&lt;/span&gt;] = &amp;#123;&lt;span class=&quot;string&quot;&gt;&#39;a&#39;&lt;/span&gt;&amp;#125;;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;但这样只有第一个元素被设置，其余999个自动默认是int 的0&lt;/strong&gt;；&lt;/p&gt;
&lt;h1 id=&quot;1-memset&quot;&gt;&lt;a href=&quot;#1-memset&quot; class=&quot;headerlink&quot; title=&quot;1. memset&quot;&gt;&lt;/a&gt;1. memset&lt;/h1&gt;&lt;p&gt;首先，&lt;strong&gt;memset函数是逐字节进行填充&lt;/strong&gt;，所以不适合int型数组。&lt;/p&gt;
&lt;p&gt;对于字符型数组：&lt;/p&gt;
&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;include&lt;/span&gt; &lt;span class=&quot;meta-string&quot;&gt;&amp;lt;cstring&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;memset&lt;/span&gt;(str,&lt;span class=&quot;string&quot;&gt;&#39;a&#39;&lt;/span&gt;,&lt;span class=&quot;keyword&quot;&gt;sizeof&lt;/span&gt;(str))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://haokailong.top/categories/C/"/>
    
    
      <category term="C++" scheme="http://haokailong.top/tags/C/"/>
    
      <category term="array" scheme="http://haokailong.top/tags/array/"/>
    
      <category term="memset" scheme="http://haokailong.top/tags/memset/"/>
    
      <category term="fill" scheme="http://haokailong.top/tags/fill/"/>
    
      <category term="fill_n" scheme="http://haokailong.top/tags/fill-n/"/>
    
  </entry>
  
  <entry>
    <title>python执行原理</title>
    <link href="http://haokailong.top/2021/03/17/python%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86/"/>
    <id>http://haokailong.top/2021/03/17/python执行原理/</id>
    <published>2021-03-17T12:47:43.000Z</published>
    <updated>2021-03-17T12:59:31.346Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>转载自：<a href="https://blog.csdn.net/helloxiaozhe/article/details/78104975" target="_blank" rel="noopener">python编译过程和执行原理</a></p></blockquote><hr><h1 id="1-python执行原理"><a href="#1-python执行原理" class="headerlink" title="1. python执行原理"></a>1. python执行原理</h1><p>这里的解释执行是相对于编译执行而言的。我们都知道，使用C/C++之类的编译性语言编写的程序，是需要从源文件转换成计算机使用的机器语言，经过链接器链接之后形成了二进制的可执行文件。运行该程序的时候，就可以把二进制程序从硬盘载入到内存中并运行。</p><p>但是对于Python而言，python源码不需要编译成二进制代码，它可以直接从源代码运行程序。当我们运行python文件程序的时候，python解释器将源代码转换为字节码，然后再由python解释器来执行这些字节码。这样，python就不用担心程序的编译,库的链接加载等问题了。</p><p>对于python解释语言，有以下3方面的特性：</p><ol><li>每次运行都要进行转换成字节码，然后再有虚拟机把字节码转换成机器语言，最后才能在硬件上运行。与编译性语言相比，每次多出了编译和链接的过程，性能肯定会受到影响；而python并不是每次都需要转换字节码，解释器在转换之前会判断代码文件的修改时间是否与上一次转换后的字节码pyc文件的修改时间一致，若不一致才会重新转换。</li><li>由于不用关心程序的编译和库的链接等问题，开发的工作也就更加轻松啦。</li><li>python代码与机器底层更远了，python程序更加易于移植，基本上无需改动就能在多平台上运行。</li></ol><p>​      在具体计算机上实现一种语言，首先要确定的是表示该语言语义解释的虚拟计算机，一个关键的问题是程序执行时的基本表示是实际计算机上的机器语言还是虚拟机的机器语言。这个问题决定了语言的实现。根据这个问题的回答，可以将程序设计语言划分为两大类：编译型语言和解释型语言。</p><ol><li>编译实现的语言，如：C、C++、Fortran、Pascal、Ada。由编译型语言编写的源程序需要经过编译,汇编和链接才能输出目标代码，然后由机器执行目标代码。目标代码是有机器指令组成，不能独立运行，因为源程序中可能使用了一些汇编程序不能解释引用的库函数，而库函数又不在源程序中，此时还需要链接程序完成外部引用和目标模板调用的链接任务，最后才能输出可执行代码。</li><li>解释型语言，解释器不产生目标机器代码，而是产生中间代码，这种中间代码与机器代码不同，中间代码的解释是由软件支持的，不能直接使用在硬件上。该软件解释器通常会导致执行效率较低，用解释型语言编写的程序是由另一个可以理解中间代码的解释程序执行的。和编译的程序不同的是, 解释程序的任务是逐一将源代码的语句解释成可执行的机器指令，不需要将源程序翻译成目标代码再执行。对于解释型语言，需要一个专门的解释器来执行该程序，每条语句只有在执行是才能被翻译，这种解释型语言每执行一次就翻译一次，因而效率低下。</li><li>Java解释器，java很特殊，java是需要编译的，但是没有直接编译成机器语言，而是编译成字节码，然后在Java虚拟机上用解释的方式执行字节码。Python也使用了类似的方式，先将python编译成python字节码，然后由一个专门的python字节码解释器负责解释执行字节码。</li><li>python是一门解释语言，但是出于效率的考虑，提供了一种编译的方法。编译之后就得到pyc文件，存储了字节码。python这点和java很类似，但是java与python不同的是，python是一个解释型的语言，所以编译字节码不是一个强制的操作，事实上，编译是一个自动的过程，一般不会在意它的存在。编译成字节码可以节省加载模块的时间，提高效率。</li><li>除了效率之外，字节码的形式也增加了反向工程的难度，可以保护源代码。这个只是一定程度上的保护，反编译还是可以的。</li></ol><a id="more"></a><h1 id="2-python内部执行过程"><a href="#2-python内部执行过程" class="headerlink" title="2. python内部执行过程"></a>2. python内部执行过程</h1><h2 id="2-1-编译过程概述"><a href="#2-1-编译过程概述" class="headerlink" title="2.1. 编译过程概述"></a>2.1. 编译过程概述</h2><p>　　当我们执行Python代码的时候，在Python解释器用四个过程“拆解”我们的代码，最终被CPU执行返回给用户。</p><p>　　首先当用户键入代码交给Python处理的时候会先进行词法分析，例如用户键入关键字或者当输入关键字有误时，都会被词法分析所触发，不正确的代码将不会被执行。</p><p>　　下一步Python会进行语法分析，例如当”for i in test:”中，test后面的冒号如果被写为其他符号，代码依旧不会被执行。</p><p>　　下面进入最关键的过程，在执行Python前，Python会生成.pyc文件，这个文件就是字节码，如果我们不小心修改了字节码，Python下次重新编译该程序时会和其上次生成的字节码文件进行比较，如果不匹配则会将被修改过的字节码文件进行覆盖，以确保每次编译后字节码的准确性。</p><p>　　那么什么是字节码？字节码在Python虚拟机程序里对应的是PyCodeObject对象。.pyc文件是字节码在磁盘上的表现形式。简单来说就是在编译代码的过程中，首先会将代码中的函数、类等对象分类处理，然后生成字节码文件。有了字节码文件，CPU可以直接识别字节码文件进行处理，接着Python就可执行了。</p><h2 id="2-2-过程图解"><a href="#2-2-过程图解" class="headerlink" title="2.2. 过程图解"></a>2.2. 过程图解</h2><p><img src="/images/blog/2021/PVM.jpg" alt></p><h2 id="2-3-编译字节码"><a href="#2-3-编译字节码" class="headerlink" title="2.3. 编译字节码"></a>2.3. 编译字节码</h2><p>　　Python中有一个内置函数compile()，可以将源文件编译成codeobject，首先看这个函数的说明：</p><p>　　compile(…) compile(source, filename, mode[, flags[, dont_inherit]]) -&gt; code object</p><p>　　参数1：源文件的内容字符串</p><p>　　参数2：源文件名称</p><p>　　参数3：exec-编译module，single-编译一个声明，eval-编译一个表达式 一般使用前三个参数就够了</p><p>　　使用示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#src_file.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#some function</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(d=<span class="number">0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    c=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"hello"</span></span><br><span class="line"></span><br><span class="line">a=<span class="number">9</span></span><br><span class="line"></span><br><span class="line">b=<span class="number">8</span></span><br><span class="line"></span><br><span class="line">f()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=open(<span class="string">'src_file.py'</span>,<span class="string">'r'</span>).read()    <span class="comment">#命令行模式中打开源文件进行编译</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>co=compile(a,<span class="string">'src_file'</span>,<span class="string">'exec'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(co)</span><br><span class="line"></span><br><span class="line">&lt;type <span class="string">'code'</span>&gt;    <span class="comment">#编译出了codeobject对象</span></span><br></pre></td></tr></table></figure><h2 id="2-4-codeobject对象的属性"><a href="#2-4-codeobject对象的属性" class="headerlink" title="2.4. codeobject对象的属性"></a>2.4. codeobject对象的属性</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_names    <span class="comment">#所有的符号名称</span></span><br><span class="line">(<span class="string">'f'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>)</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_name    <span class="comment">#模块名、函数名、类名</span></span><br><span class="line">&lt;module&gt;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_consts    <span class="comment">#常量集合、函数f和两个int常量a,b，d</span></span><br><span class="line">(0, &lt;code object f at 0xb7273b18, file <span class="string">"src_file"</span>, line 2&gt;, 9, 8, None)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_consts[1].co_varnames    <span class="comment">#可以看到f函数也是一个codeobject,打印f中的局部变量</span></span><br><span class="line">(<span class="string">'c'</span>,)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_code    <span class="comment">#字节码指令</span></span><br><span class="line">dZdZdZedS</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_consts[1].co_firstlineno    <span class="comment">#代码块在文件中的起始行号</span></span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_stacksize    <span class="comment">#代码栈大小</span></span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_filename    <span class="comment">#文件名</span></span><br><span class="line">src_file    <span class="comment">#模块名、函数名、类名</span></span><br></pre></td></tr></table></figure><p>codeobject的co_code代表了字节码，这个字节码有什么含义？我们可以使用dis模块进行python的反编译：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dis</span><br><span class="line">dis.dis(co)</span><br></pre></td></tr></table></figure><p>　从反编译的结果来看，python字节码其实是模仿的x86的汇编，将代码编译成一条一条的指令交给一个虚拟的cpu去执行。</p><ul><li>第一列：行号</li><li>第二列：指令在代码块中的偏移量</li><li>第三列：指令</li><li>第四列：操作数</li><li>第五列：操作数说明</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dis</span><br><span class="line"></span><br><span class="line">dis.dis(co)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; output</span><br><span class="line"></span><br><span class="line"> <span class="number">2</span>        <span class="number">0</span> LOAD_CONST               <span class="number">0</span> (<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">          <span class="number">3</span> LOAD_CONST               <span class="number">1</span> (&lt;code object f at <span class="number">0xb7273b18</span>, file <span class="string">"src_file"</span>, line <span class="number">2</span>&gt;)</span><br><span class="line"></span><br><span class="line">          <span class="number">6</span> MAKE_FUNCTION            <span class="number">1</span></span><br><span class="line"></span><br><span class="line">          <span class="number">9</span> STORE_NAME               <span class="number">0</span> (f)</span><br><span class="line"></span><br><span class="line"> <span class="number">5</span>        <span class="number">12</span> LOAD_CONST              <span class="number">2</span> (<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">          <span class="number">15</span> STORE_NAME              <span class="number">1</span> (a)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="number">6</span>        <span class="number">18</span> LOAD_CONST              <span class="number">3</span> (<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">          <span class="number">21</span> STORE_NAME              <span class="number">2</span> (b)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="number">7</span>        <span class="number">24</span> LOAD_NAME               <span class="number">0</span> (f)</span><br><span class="line"></span><br><span class="line">          <span class="number">27</span> CALL_FUNCTION           <span class="number">0</span></span><br><span class="line"></span><br><span class="line">          <span class="number">30</span> POP_TOP            </span><br><span class="line"></span><br><span class="line">          <span class="number">31</span> LOAD_CONST              <span class="number">4</span> (None)</span><br><span class="line"></span><br><span class="line">          <span class="number">34</span> RETURN_VALUE</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;转载自：&lt;a href=&quot;https://blog.csdn.net/helloxiaozhe/article/details/78104975&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;python编译过程和执行原理&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;

&lt;h1 id=&quot;1-python执行原理&quot;&gt;&lt;a href=&quot;#1-python执行原理&quot; class=&quot;headerlink&quot; title=&quot;1. python执行原理&quot;&gt;&lt;/a&gt;1. python执行原理&lt;/h1&gt;&lt;p&gt;这里的解释执行是相对于编译执行而言的。我们都知道，使用C/C++之类的编译性语言编写的程序，是需要从源文件转换成计算机使用的机器语言，经过链接器链接之后形成了二进制的可执行文件。运行该程序的时候，就可以把二进制程序从硬盘载入到内存中并运行。&lt;/p&gt;
&lt;p&gt;但是对于Python而言，python源码不需要编译成二进制代码，它可以直接从源代码运行程序。当我们运行python文件程序的时候，python解释器将源代码转换为字节码，然后再由python解释器来执行这些字节码。这样，python就不用担心程序的编译,库的链接加载等问题了。&lt;/p&gt;
&lt;p&gt;对于python解释语言，有以下3方面的特性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每次运行都要进行转换成字节码，然后再有虚拟机把字节码转换成机器语言，最后才能在硬件上运行。与编译性语言相比，每次多出了编译和链接的过程，性能肯定会受到影响；而python并不是每次都需要转换字节码，解释器在转换之前会判断代码文件的修改时间是否与上一次转换后的字节码pyc文件的修改时间一致，若不一致才会重新转换。&lt;/li&gt;
&lt;li&gt;由于不用关心程序的编译和库的链接等问题，开发的工作也就更加轻松啦。&lt;/li&gt;
&lt;li&gt;python代码与机器底层更远了，python程序更加易于移植，基本上无需改动就能在多平台上运行。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;​      在具体计算机上实现一种语言，首先要确定的是表示该语言语义解释的虚拟计算机，一个关键的问题是程序执行时的基本表示是实际计算机上的机器语言还是虚拟机的机器语言。这个问题决定了语言的实现。根据这个问题的回答，可以将程序设计语言划分为两大类：编译型语言和解释型语言。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;编译实现的语言，如：C、C++、Fortran、Pascal、Ada。由编译型语言编写的源程序需要经过编译,汇编和链接才能输出目标代码，然后由机器执行目标代码。目标代码是有机器指令组成，不能独立运行，因为源程序中可能使用了一些汇编程序不能解释引用的库函数，而库函数又不在源程序中，此时还需要链接程序完成外部引用和目标模板调用的链接任务，最后才能输出可执行代码。&lt;/li&gt;
&lt;li&gt;解释型语言，解释器不产生目标机器代码，而是产生中间代码，这种中间代码与机器代码不同，中间代码的解释是由软件支持的，不能直接使用在硬件上。该软件解释器通常会导致执行效率较低，用解释型语言编写的程序是由另一个可以理解中间代码的解释程序执行的。和编译的程序不同的是, 解释程序的任务是逐一将源代码的语句解释成可执行的机器指令，不需要将源程序翻译成目标代码再执行。对于解释型语言，需要一个专门的解释器来执行该程序，每条语句只有在执行是才能被翻译，这种解释型语言每执行一次就翻译一次，因而效率低下。&lt;/li&gt;
&lt;li&gt;Java解释器，java很特殊，java是需要编译的，但是没有直接编译成机器语言，而是编译成字节码，然后在Java虚拟机上用解释的方式执行字节码。Python也使用了类似的方式，先将python编译成python字节码，然后由一个专门的python字节码解释器负责解释执行字节码。&lt;/li&gt;
&lt;li&gt;python是一门解释语言，但是出于效率的考虑，提供了一种编译的方法。编译之后就得到pyc文件，存储了字节码。python这点和java很类似，但是java与python不同的是，python是一个解释型的语言，所以编译字节码不是一个强制的操作，事实上，编译是一个自动的过程，一般不会在意它的存在。编译成字节码可以节省加载模块的时间，提高效率。&lt;/li&gt;
&lt;li&gt;除了效率之外，字节码的形式也增加了反向工程的难度，可以保护源代码。这个只是一定程度上的保护，反编译还是可以的。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="Others" scheme="http://haokailong.top/categories/Others/"/>
    
    
      <category term="python" scheme="http://haokailong.top/tags/python/"/>
    
      <category term="PVM" scheme="http://haokailong.top/tags/PVM/"/>
    
      <category term="执行原理" scheme="http://haokailong.top/tags/%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86/"/>
    
      <category term="编译" scheme="http://haokailong.top/tags/%E7%BC%96%E8%AF%91/"/>
    
      <category term="解释" scheme="http://haokailong.top/tags/%E8%A7%A3%E9%87%8A/"/>
    
  </entry>
  
  <entry>
    <title>conda添加源</title>
    <link href="http://haokailong.top/2021/03/15/conda%E6%B7%BB%E5%8A%A0%E6%BA%90/"/>
    <id>http://haokailong.top/2021/03/15/conda添加源/</id>
    <published>2021-03-15T07:08:52.000Z</published>
    <updated>2021-03-15T12:28:11.658Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-通过命令行"><a href="#1-通过命令行" class="headerlink" title="1. 通过命令行"></a>1. 通过命令行</h1><p>添加清华源，命令行中直接输入以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置搜索时显示通道地址</span></span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><p>如果还需要<strong>pytorch</strong>，就添加pytorch镜像：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br></pre></td></tr></table></figure><p>添加中科大源：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/</span><br><span class="line"> </span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="2-通过配置文件"><a href="#2-通过配置文件" class="headerlink" title="2. 通过配置文件"></a>2. 通过配置文件</h1><p>在Linux系统中，可以直接将以下配置写入<code>~/.condarc</code>中：</p><p><strong>使用<a href="https://mirrors.bfsu.edu.cn/help/anaconda/" target="_blank" rel="noopener">BFSU的镜像：</a></strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">channels:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">defaults</span></span><br><span class="line"><span class="attr">show_channel_urls:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">default_channels:</span></span><br><span class="line"><span class="attr">  - https:</span><span class="string">//mirrors.bfsu.edu.cn/anaconda/pkgs/main</span></span><br><span class="line"><span class="attr">  - https:</span><span class="string">//mirrors.bfsu.edu.cn/anaconda/pkgs/r</span></span><br><span class="line"><span class="attr">  - https:</span><span class="string">//mirrors.bfsu.edu.cn/anaconda/pkgs/msys2</span></span><br><span class="line"><span class="attr">custom_channels:</span></span><br><span class="line"><span class="attr">  conda-forge:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br><span class="line"><span class="attr">  msys2:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br><span class="line"><span class="attr">  bioconda:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br><span class="line"><span class="attr">  menpo:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br><span class="line"><span class="attr">  pytorch:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br><span class="line"><span class="attr">  simpleitk:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br></pre></td></tr></table></figure><h1 id="3-删源"><a href="#3-删源" class="headerlink" title="3. 删源"></a>3. 删源</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --remove-key channels</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-通过命令行&quot;&gt;&lt;a href=&quot;#1-通过命令行&quot; class=&quot;headerlink&quot; title=&quot;1. 通过命令行&quot;&gt;&lt;/a&gt;1. 通过命令行&lt;/h1&gt;&lt;p&gt;添加清华源，命令行中直接输入以下命令：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 设置搜索时显示通道地址&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda config --&lt;span class=&quot;built_in&quot;&gt;set&lt;/span&gt; show_channel_urls yes&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果还需要&lt;strong&gt;pytorch&lt;/strong&gt;，就添加pytorch镜像：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;添加中科大源：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda config --&lt;span class=&quot;built_in&quot;&gt;set&lt;/span&gt; show_channel_urls yes&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Others" scheme="http://haokailong.top/categories/Others/"/>
    
      <category term="Tool" scheme="http://haokailong.top/categories/Others/Tool/"/>
    
    
      <category term="conda" scheme="http://haokailong.top/tags/conda/"/>
    
      <category term="tsinghua" scheme="http://haokailong.top/tags/tsinghua/"/>
    
  </entry>
  
  <entry>
    <title>C++二维数组传参</title>
    <link href="http://haokailong.top/2021/03/14/C-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%BC%A0%E5%8F%82/"/>
    <id>http://haokailong.top/2021/03/14/C-二维数组传参/</id>
    <published>2021-03-14T04:59:37.000Z</published>
    <updated>2021-03-15T06:25:39.794Z</updated>
    
    <content type="html"><![CDATA[<p>在LeetCode上，总是需要用到对不固定维度的二维数组传参，传参有多种方式，这里进行一下探究和复习。</p><h1 id="1-一级指针"><a href="#1-一级指针" class="headerlink" title="1. 一级指针"></a>1. 一级指针</h1><p>高维数组在栈中存放时内存是连续的，也可以看作一维数组，并手动计算索引对应的地址。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">(<span class="keyword">int</span> *dp, <span class="keyword">int</span> m, <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;++i) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n;++j) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; *(dp + i * n + j) &lt;&lt; <span class="string">' '</span>; </span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> m, n;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; m &gt;&gt; n;</span><br><span class="line"><span class="keyword">int</span> dp[m][n];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;++i) <span class="comment">// 初始化</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n;++j)</span><br><span class="line">dp[i][j] = i + j;</span><br><span class="line"> </span><br><span class="line">func((<span class="keyword">int</span>*)dp, m, n);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0 1 2 </span><br><span class="line">1 2 3</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="2-二级指针"><a href="#2-二级指针" class="headerlink" title="2. 二级指针"></a>2. 二级指针</h1><p>使用<strong>new</strong>在堆中申请空间，然后传参。即指针的指针。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">(<span class="keyword">int</span> **dp, <span class="keyword">int</span> m, <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;++i) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n;++j) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; dp[i][j] &lt;&lt; <span class="string">' '</span>; </span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> m, n;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; m &gt;&gt; n;</span><br><span class="line"><span class="keyword">int</span> **dp = <span class="keyword">new</span> <span class="keyword">int</span>*[m];  <span class="comment">// 在堆中申请空间 </span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;++i)</span><br><span class="line">dp[i] = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;++i) <span class="comment">// 初始化</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n;++j)</span><br><span class="line">dp[i][j] = i + j;</span><br><span class="line"> </span><br><span class="line">func(dp, m, n);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0 1 2 </span><br><span class="line">1 2 3 </span><br><span class="line">2 3 4</span><br></pre></td></tr></table></figure><p>这种方式申请的内存不在栈中，而是在堆中。而且，<strong><em>由于多次申请了内存，不同的行间内存空间是不连续的</em></strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在LeetCode上，总是需要用到对不固定维度的二维数组传参，传参有多种方式，这里进行一下探究和复习。&lt;/p&gt;
&lt;h1 id=&quot;1-一级指针&quot;&gt;&lt;a href=&quot;#1-一级指针&quot; class=&quot;headerlink&quot; title=&quot;1. 一级指针&quot;&gt;&lt;/a&gt;1. 一级指针&lt;/h1&gt;&lt;p&gt;高维数组在栈中存放时内存是连续的，也可以看作一维数组，并手动计算索引对应的地址。&lt;/p&gt;
&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;include&lt;/span&gt;&lt;span class=&quot;meta-string&quot;&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;namespace&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;std&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; *dp, &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; m, &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; n)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;i&amp;lt;m;++i) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; j=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;j&amp;lt;n;++j) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;			&lt;span class=&quot;built_in&quot;&gt;cout&lt;/span&gt; &amp;lt;&amp;lt; *(dp + i * n + j) &amp;lt;&amp;lt; &lt;span class=&quot;string&quot;&gt;&#39; &#39;&lt;/span&gt;; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;built_in&quot;&gt;cout&lt;/span&gt; &amp;lt;&amp;lt; &lt;span class=&quot;built_in&quot;&gt;endl&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; m, n;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;built_in&quot;&gt;cin&lt;/span&gt; &amp;gt;&amp;gt; m &amp;gt;&amp;gt; n;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; dp[m][n];&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;i&amp;lt;m;++i) &lt;span class=&quot;comment&quot;&gt;// 初始化&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; j=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;j&amp;lt;n;++j)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;			dp[i][j] = i + j;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;			 &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	func((&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;*)dp, m, n);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;输出结果为：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;0 1 2 &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;1 2 3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Algorithms" scheme="http://haokailong.top/categories/Algorithms/"/>
    
      <category term="LeetCode" scheme="http://haokailong.top/categories/Algorithms/LeetCode/"/>
    
    
      <category term="C++" scheme="http://haokailong.top/tags/C/"/>
    
      <category term="LeetCode" scheme="http://haokailong.top/tags/LeetCode/"/>
    
      <category term="Array" scheme="http://haokailong.top/tags/Array/"/>
    
  </entry>
  
  <entry>
    <title>pandas.merge()函数</title>
    <link href="http://haokailong.top/2021/03/13/pandas-merge-%E5%87%BD%E6%95%B0/"/>
    <id>http://haokailong.top/2021/03/13/pandas-merge-函数/</id>
    <published>2021-03-13T12:29:36.000Z</published>
    <updated>2021-03-13T13:04:18.760Z</updated>
    
    <content type="html"><![CDATA[<p>在使用pandas时，合并两个DataFrame，可以采用不同的连接方法，类似于数据库中表格的join操作，对不同的操作进行实验尝试。</p><h1 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import pandas</span><br><span class="line">&gt;&gt;&gt; sheet1 = pandas.DataFrame(&#123;<span class="string">"ID"</span>:[<span class="string">"A01"</span>, <span class="string">"B01"</span>, <span class="string">"A02"</span>], <span class="string">"name"</span>:[<span class="string">"Bob"</span>, <span class="string">"Mary"</span>, <span class="string">"Bob"</span>], <span class="string">"age"</span>: [20, 22, 35]&#125;)</span><br><span class="line">&gt;&gt;&gt; sheet1</span><br><span class="line">    ID  name  age</span><br><span class="line">0  A01   Bob   20</span><br><span class="line">1  B01  Mary   22</span><br><span class="line">2  A02   Bob   35</span><br><span class="line">&gt;&gt;&gt; sheet2 = pandas.DataFrame(&#123;<span class="string">"name"</span>: [<span class="string">"Bob"</span>, <span class="string">"Mary"</span>, <span class="string">"Jack"</span>], <span class="string">"country"</span>: [<span class="string">"US"</span>, <span class="string">"Singapore"</span>, <span class="string">"China"</span>]&#125;)</span><br><span class="line">&gt;&gt;&gt; sheet2</span><br><span class="line">   name    country</span><br><span class="line">0   Bob         US</span><br><span class="line">1  Mary  Singapore</span><br><span class="line">2  Jack      China</span><br></pre></td></tr></table></figure><h1 id="1-left"><a href="#1-left" class="headerlink" title="1. left"></a>1. left</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sheet1.merge(sheet2, how=<span class="string">"left"</span>, on=<span class="string">"name"</span>)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    ID  name  age    country</span><br><span class="line">0  A01   Bob   20         US</span><br><span class="line">1  B01  Mary   22  Singapore</span><br><span class="line">2  A02   Bob   35         US</span><br></pre></td></tr></table></figure><p>按照左表的”name”将两个sheet的属性合并。</p><a id="more"></a><h1 id="2-right"><a href="#2-right" class="headerlink" title="2. right"></a>2. right</h1><p>同样是按照”name”合并，但是如果指定合并方式为”right”的话，则会保留右侧的所有“name”，如果一个”name”可以匹配到多个左表中记录，则合并后对应同样数量的多个记录。对于没有匹配的”name”值，补”NaN”</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sheet1.merge(sheet2, how=<span class="string">"right"</span>, on=<span class="string">"name"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    ID  name   age    country</span><br><span class="line">0  A01   Bob  20.0         US</span><br><span class="line">1  A02   Bob  35.0         US</span><br><span class="line">2  B01  Mary  22.0  Singapore</span><br><span class="line">3  NaN  Jack   NaN      China</span><br></pre></td></tr></table></figure><h1 id="3-inner"><a href="#3-inner" class="headerlink" title="3. inner"></a>3. inner</h1><p>使用inner合并，是取两张表的公共部分，即取交集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sheet1.merge(sheet2, how=<span class="string">"inner"</span>, on=<span class="string">"name"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    ID  name  age    country</span><br><span class="line">0  A01   Bob   20         US</span><br><span class="line">1  A02   Bob   35         US</span><br><span class="line">2  B01  Mary   22  Singapore</span><br></pre></td></tr></table></figure><p>由于在给出的例子中，左表中的name有”Bob”，“Mary”，均在右表中，所以合并结果与left合并完全相同。</p><h1 id="4-outer"><a href="#4-outer" class="headerlink" title="4. outer"></a>4. outer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sheet1.merge(sheet2, how=<span class="string">"outer"</span>, on=<span class="string">"name"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    ID  name   age    country</span><br><span class="line">0  A01   Bob  20.0         US</span><br><span class="line">1  A02   Bob  35.0         US</span><br><span class="line">2  B01  Mary  22.0  Singapore</span><br><span class="line">3  NaN  Jack   NaN      China</span><br></pre></td></tr></table></figure><p>使用outer合并，是取两个表的key的并集，在举的例子里，结果与right合并完全相同。</p><h1 id="5-cross"><a href="#5-cross" class="headerlink" title="5. cross"></a>5. cross</h1><p>笛卡尔积，不常使用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在使用pandas时，合并两个DataFrame，可以采用不同的连接方法，类似于数据库中表格的join操作，对不同的操作进行实验尝试。&lt;/p&gt;
&lt;h1 id=&quot;建表&quot;&gt;&lt;a href=&quot;#建表&quot; class=&quot;headerlink&quot; title=&quot;建表&quot;&gt;&lt;/a&gt;建表&lt;/h1&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt; import pandas&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt; sheet1 = pandas.DataFrame(&amp;#123;&lt;span class=&quot;string&quot;&gt;&quot;ID&quot;&lt;/span&gt;:[&lt;span class=&quot;string&quot;&gt;&quot;A01&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;B01&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;A02&quot;&lt;/span&gt;], &lt;span class=&quot;string&quot;&gt;&quot;name&quot;&lt;/span&gt;:[&lt;span class=&quot;string&quot;&gt;&quot;Bob&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;Mary&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;Bob&quot;&lt;/span&gt;], &lt;span class=&quot;string&quot;&gt;&quot;age&quot;&lt;/span&gt;: [20, 22, 35]&amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt; sheet1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ID  name  age&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;0  A01   Bob   20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;1  B01  Mary   22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2  A02   Bob   35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt; sheet2 = pandas.DataFrame(&amp;#123;&lt;span class=&quot;string&quot;&gt;&quot;name&quot;&lt;/span&gt;: [&lt;span class=&quot;string&quot;&gt;&quot;Bob&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;Mary&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;Jack&quot;&lt;/span&gt;], &lt;span class=&quot;string&quot;&gt;&quot;country&quot;&lt;/span&gt;: [&lt;span class=&quot;string&quot;&gt;&quot;US&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;Singapore&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;China&quot;&lt;/span&gt;]&amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt; sheet2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;   name    country&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;0   Bob         US&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;1  Mary  Singapore&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2  Jack      China&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h1 id=&quot;1-left&quot;&gt;&lt;a href=&quot;#1-left&quot; class=&quot;headerlink&quot; title=&quot;1. left&quot;&gt;&lt;/a&gt;1. left&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sheet1.merge(sheet2, how=&lt;span class=&quot;string&quot;&gt;&quot;left&quot;&lt;/span&gt;, on=&lt;span class=&quot;string&quot;&gt;&quot;name&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;结果为：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;    ID  name  age    country&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;0  A01   Bob   20         US&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;1  B01  Mary   22  Singapore&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2  A02   Bob   35         US&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;按照左表的”name”将两个sheet的属性合并。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Others" scheme="http://haokailong.top/categories/Others/"/>
    
      <category term="Tool" scheme="http://haokailong.top/categories/Others/Tool/"/>
    
    
      <category term="pandas" scheme="http://haokailong.top/tags/pandas/"/>
    
      <category term="merge" scheme="http://haokailong.top/tags/merge/"/>
    
      <category term="DataFrame" scheme="http://haokailong.top/tags/DataFrame/"/>
    
  </entry>
  
  <entry>
    <title>二分类中softmax对比sigmoid</title>
    <link href="http://haokailong.top/2021/03/12/%E4%BA%8C%E5%88%86%E7%B1%BB%E4%B8%ADsoftmax%E5%AF%B9%E6%AF%94sigmoid/"/>
    <id>http://haokailong.top/2021/03/12/二分类中softmax对比sigmoid/</id>
    <published>2021-03-12T13:29:51.000Z</published>
    <updated>2021-03-12T15:04:21.908Z</updated>
    
    <content type="html"><![CDATA[<p>题外话：在Pycharm中<code>Ctrl+左键</code>就可以跳转到源码！用了这么多年竟然都不知道！😓</p><blockquote><p>本文引用自：<a href="https://www.aiuai.cn/aifarm679.html" target="_blank" rel="noopener">https://www.aiuai.cn/aifarm679.html</a></p></blockquote><h1 id="1-理论分析"><a href="#1-理论分析" class="headerlink" title="1. 理论分析"></a>1. 理论分析</h1><p>[1] Sigmoid</p><script type="math/tex; mode=display">\begin{equation} \begin{cases} p(y=1|x) = \frac{1}{1 + e ^{-\theta ^ T x}} \\ p(y=0|x) = 1 - p(y=1|x) = \frac{e ^{-\theta ^ T x}}{1 + e ^{-\theta ^ T x}} \end{cases} \end{equation}</script><p>[2] Softmax</p><script type="math/tex; mode=display">\begin{equation} \begin{cases} p(y=0|x) = \frac{e ^{\theta _0^T x} }{e ^{\theta _0^T x} + e ^{\theta _1^T x} } = \frac{e ^{(\theta _0^T - \theta _1^T)x} }{1 + e ^{(\theta _0^T - \theta _1^T) x} } \\ p(y=1|x) = 1 - p(y=0|x) \end{cases} \end{equation}</script><p>令 $\beta = -(\theta_0^T - \theta _1^T)$，则有：</p><script type="math/tex; mode=display">\begin{equation} \begin{cases} p(y=1|x) = \frac{1}{1 + e ^{-\beta ^ T x}} \\ p(y=0|x) = \frac{e ^{-\beta ^ T x}}{1 + e ^{-\beta ^ T x}} \end{cases} \end{equation}</script><p>可见，此时，Softmax 与 Sigmoid 二者理论公式的等价性.</p><h1 id="2-基于Keras的实验对比"><a href="#2-基于Keras的实验对比" class="headerlink" title="2. 基于Keras的实验对比"></a>2. 基于Keras的实验对比</h1><a id="more"></a><p>以猫狗分类的数据集为例，采用在 ImageNet 上预训练的 Xception 模型导出的特征.( <strong>复制于</strong> <a href="https://gist.github.com/ypwhs/6905ebbda99d04621f9fc00417657ae2" target="_blank" rel="noopener">ypwhs/sigmoid_and_softmax.ipynb</a> 中的代码. )</p><blockquote><p><a href="https://github.com/ypwhs/dogs_vs_cats/releases/download/gap/gap_Xception.h5" target="_blank" rel="noopener">gap_Xception.h5</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">20180520</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment">#      加载猫狗分类的特征数据</span></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="keyword">with</span> h5py.File(<span class="string">"gap_Xception.h5"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> h:</span><br><span class="line">    X = np.array(h[<span class="string">'train'</span>])</span><br><span class="line">    y = np.array(h[<span class="string">'label'</span>])</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">y_train_softmax = to_categorical(y_train) <span class="comment"># ont-hot</span></span><br><span class="line">y_test_softmax = to_categorical(y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment">#           Softmax</span></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment"># loss: Softmax Cross Entropy Loss</span></span><br><span class="line">input_tensor = Input(X.shape[<span class="number">1</span>:])</span><br><span class="line">x = input_tensor</span><br><span class="line">x = Dropout(<span class="number">0.5</span>)(x)</span><br><span class="line">softmax = Dense(<span class="number">2</span>, activation=<span class="string">'softmax'</span>)</span><br><span class="line">x = softmax(x)</span><br><span class="line">model = Model(input_tensor, x)</span><br><span class="line">model.compile(optimizer=SGD(<span class="number">1e-3</span>),  <span class="comment"># lr = 1e-3</span></span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">softmax_weights, softmax_bias = softmax.get_weights()</span><br><span class="line"></span><br><span class="line">history_softmax = model.fit(X_train, y_train_softmax, batch_size=<span class="number">128</span>, epochs=<span class="number">5</span>,</span><br><span class="line">                            validation_data=(X_test, y_test_softmax))</span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment">#           Sigmoid</span></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment"># loss: Binary(Sigmoid) Cross Entropy Loss</span></span><br><span class="line">np.random.seed(<span class="number">20180520</span>)</span><br><span class="line">input_tensor = Input(X.shape[<span class="number">1</span>:])</span><br><span class="line">x = input_tensor</span><br><span class="line">x = Dropout(<span class="number">0.5</span>)(x)</span><br><span class="line">sigmoid = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">x = sigmoid(x)</span><br><span class="line">model = Model(input_tensor, x)</span><br><span class="line">model.compile(optimizer=SGD(<span class="number">2e-3</span>),  <span class="comment"># lr = 2e-3</span></span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sigmoid 权重初始化为 Softmax 的权重计算差值</span></span><br><span class="line"><span class="comment"># beta = sigmoid weights</span></span><br><span class="line">beta = -(softmax_weights[:,<span class="number">0</span>] - softmax_weights[:,<span class="number">1</span>]).reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">sigmoid.set_weights([beta, np.zeros(<span class="number">1</span>)]) <span class="comment"># set beta to sigmoid weights</span></span><br><span class="line"></span><br><span class="line">history_sigmoid = model.fit(X_train, y_train, batch_size=<span class="number">128</span>, epochs=<span class="number">5</span>, validation_data=(X_test, y_test))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment">#           Sigmoid</span></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line">input_tensor = Input(X.shape[<span class="number">1</span>:])</span><br><span class="line">x = input_tensor</span><br><span class="line">x = Dropout(<span class="number">0.5</span>)(x)</span><br><span class="line">sigmoid = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">x = sigmoid(x)</span><br><span class="line">model = Model(input_tensor, x)</span><br><span class="line">model.compile(optimizer=SGD(<span class="number">2e-3</span>),  <span class="comment"># lr = 2e-3</span></span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># random init Sigmoid weights</span></span><br><span class="line"><span class="comment"># 随机初始化 Sigmoid 权重</span></span><br><span class="line">history_sigmoid_2 = model.fit(X_train, y_train, batch_size=<span class="number">128</span>, epochs=<span class="number">5</span>, validation_data=(X_test, y_test))</span><br></pre></td></tr></table></figure><p>训练过程输出：</p><p><strong>Softmax</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Train on 20000 samples, validate on 5000 samples</span><br><span class="line">Epoch 1/5</span><br><span class="line">20000/20000 [==============================] - 1s 68us/step - loss: 0.4711 - acc: 0.8535 - val_loss: 0.3765 - val_acc: 0.9642</span><br><span class="line">Epoch 2/5</span><br><span class="line">20000/20000 [==============================] - 0s 13us/step - loss: 0.3338 - acc: 0.9486 - val_loss: 0.2825 - val_acc: 0.9816</span><br><span class="line">Epoch 3/5</span><br><span class="line">20000/20000 [==============================] - 0s 11us/step - loss: 0.2594 - acc: 0.9726 - val_loss: 0.2279 - val_acc: 0.9858</span><br><span class="line">Epoch 4/5</span><br><span class="line">20000/20000 [==============================] - 0s 11us/step - loss: 0.2160 - acc: 0.9799 - val_loss: 0.1923 - val_acc: 0.9866</span><br><span class="line">Epoch 5/5</span><br><span class="line">20000/20000 [==============================] - 0s 13us/step - loss: 0.1860 - acc: 0.9825 - val_loss: 0.1677 - val_acc: 0.9868</span><br></pre></td></tr></table></figure><p><strong>Sigmoid With Softmax Weights</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Train on 20000 samples, validate on 5000 samples</span><br><span class="line">Epoch 1/5</span><br><span class="line">20000/20000 [==============================] - 0s 18us/step - loss: 0.4706 - acc: 0.8544 - val_loss: 0.3766 - val_acc: 0.9644</span><br><span class="line">Epoch 2/5</span><br><span class="line">20000/20000 [==============================] - 0s 12us/step - loss: 0.3346 - acc: 0.9476 - val_loss: 0.2824 - val_acc: 0.9816</span><br><span class="line">Epoch 3/5</span><br><span class="line">20000/20000 [==============================] - 0s 14us/step - loss: 0.2613 - acc: 0.9709 - val_loss: 0.2275 - val_acc: 0.9860</span><br><span class="line">Epoch 4/5</span><br><span class="line">20000/20000 [==============================] - 0s 14us/step - loss: 0.2151 - acc: 0.9789 - val_loss: 0.1923 - val_acc: 0.9868</span><br><span class="line">Epoch 5/5</span><br><span class="line">20000/20000 [==============================] - 0s 11us/step - loss: 0.1857 - acc: 0.9825 - val_loss: 0.1676 - val_acc: 0.9872</span><br></pre></td></tr></table></figure><p><strong>Sigmoid With Random Init Weight</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Train on 20000 samples, validate on 5000 samples</span><br><span class="line">Epoch 1/5</span><br><span class="line">20000/20000 [==============================] - 0s 18us/step - loss: 0.5690 - acc: 0.7607 - val_loss: 0.4415 - val_acc: 0.9718</span><br><span class="line">Epoch 2/5</span><br><span class="line">20000/20000 [==============================] - 0s 11us/step - loss: 0.3753 - acc: 0.9576 - val_loss: 0.3151 - val_acc: 0.9852</span><br><span class="line">Epoch 3/5</span><br><span class="line">20000/20000 [==============================] - 0s 12us/step - loss: 0.2819 - acc: 0.9814 - val_loss: 0.2464 - val_acc: 0.9878</span><br><span class="line">Epoch 4/5</span><br><span class="line">20000/20000 [==============================] - 0s 12us/step - loss: 0.2267 - acc: 0.9858 - val_loss: 0.2042 - val_acc: 0.9882</span><br><span class="line">Epoch 5/5</span><br><span class="line">20000/20000 [==============================] - 0s 11us/step - loss: 0.1921 - acc: 0.9872 - val_loss: 0.1759 - val_acc: 0.9882</span><br></pre></td></tr></table></figure><h3 id="2-1-训练-loss-曲线变化情况对比"><a href="#2-1-训练-loss-曲线变化情况对比" class="headerlink" title="2.1 训练 loss 曲线变化情况对比"></a>2.1 训练 loss 曲线变化情况对比</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history_sigmoid.history[<span class="string">'loss'</span>])</span><br><span class="line">plt.plot(history_softmax.history[<span class="string">'loss'</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(history_sigmoid.history[<span class="string">'val_loss'</span>])</span><br><span class="line">plt.plot(history_softmax.history[<span class="string">'val_loss'</span>])</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line"></span><br><span class="line">plt.legend([<span class="string">'sigmoid_loss'</span>, <span class="string">'softmax_loss'</span>, </span><br><span class="line">            <span class="string">'sigmoid_val_loss'</span>, <span class="string">'softmax_val_loss'</span>], loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2021/sigmoid_softmax_loss.jpg" alt></p><p>从图中可知，Sigmoid 和 Softmax 的训练曲线几乎完全重合.</p><h3 id="2-2-Loss-差值可视化对比"><a href="#2-2-Loss-差值可视化对比" class="headerlink" title="2.2 Loss 差值可视化对比"></a>2.2 Loss 差值可视化对比</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(np.array(history_sigmoid.history[<span class="string">'val_loss'</span>]) - np.array(history_softmax.history[<span class="string">'val_loss'</span>]))</span><br><span class="line">plt.plot(np.array(history_sigmoid.history[<span class="string">'val_loss'</span>]) - np.array(history_sigmoid_2.history[<span class="string">'val_loss'</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line"></span><br><span class="line">plt.legend([<span class="string">'sigmoid_softmax_beta_gap'</span>, <span class="string">'sigmoid_random_weight_gap'</span>], loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2021/loss_difference.jpg" alt></p><p>图中<strong>蓝色曲线</strong>几乎一直是 0，其表示 Sigmoid 和 Softmax 训练的模型的 loss 差异性很小. 但<strong>黄色曲线</strong> 的差值相对就较大，其采用的随机初始化 Sigmoid 权重值，影响了训练过程中的 loss 曲线的变化.</p><p>也就是说，如果设置了正确的 beta 值，Sigmoid 与 Softmax 的效果可认为是等价的.</p><h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h1><p>对于二分类问题，</p><p>[1] - Sigmoid 与 Softmax 完全等价.</p><p>[2] - Sigmoid 与 Softmax 分类器的权值可以相互转换.</p><p>[3] - Softmax 的学习率是 Sigmoid 学习率的2倍. (如：1e-3与2e-3)</p><p>[4] - Softmax 会比 Sigmoid 浪费 2 倍的权值空间(权重参数是两倍).</p><hr color="blue"><p><strong>PS: 根据任务需要，手动修改损失函数，是一项必要的技能。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;题外话：在Pycharm中&lt;code&gt;Ctrl+左键&lt;/code&gt;就可以跳转到源码！用了这么多年竟然都不知道！😓&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文引用自：&lt;a href=&quot;https://www.aiuai.cn/aifarm679.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.aiuai.cn/aifarm679.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;1-理论分析&quot;&gt;&lt;a href=&quot;#1-理论分析&quot; class=&quot;headerlink&quot; title=&quot;1. 理论分析&quot;&gt;&lt;/a&gt;1. 理论分析&lt;/h1&gt;&lt;p&gt;[1] Sigmoid&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{equation} \begin{cases} p(y=1|x) = \frac{1}{1 + e ^{-\theta ^ T x}} \\ p(y=0|x) = 1 - p(y=1|x) = \frac{e ^{-\theta ^ T x}}{1 + e ^{-\theta ^ T x}} \end{cases} \end{equation}&lt;/script&gt;&lt;p&gt;[2] Softmax&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{equation} \begin{cases} p(y=0|x) = \frac{e ^{\theta _0^T x} }{e ^{\theta _0^T x} + e ^{\theta _1^T x} } = \frac{e ^{(\theta _0^T - \theta _1^T)x} }{1 + e ^{(\theta _0^T - \theta _1^T) x} } \\ p(y=1|x) = 1 - p(y=0|x) \end{cases} \end{equation}&lt;/script&gt;&lt;p&gt;令 $\beta = -(\theta_0^T - \theta _1^T)$，则有：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \begin{cases} p(y=1|x) = \frac{1}{1 + e ^{-\beta ^ T x}} \\ p(y=0|x) = \frac{e ^{-\beta ^ T x}}{1 + e ^{-\beta ^ T x}} \end{cases} \end{equation}&lt;/script&gt;&lt;p&gt;可见，此时，Softmax 与 Sigmoid 二者理论公式的等价性.&lt;/p&gt;
&lt;h1 id=&quot;2-基于Keras的实验对比&quot;&gt;&lt;a href=&quot;#2-基于Keras的实验对比&quot; class=&quot;headerlink&quot; title=&quot;2. 基于Keras的实验对比&quot;&gt;&lt;/a&gt;2. 基于Keras的实验对比&lt;/h1&gt;
    
    </summary>
    
    
      <category term="Neural Networks" scheme="http://haokailong.top/categories/Neural-Networks/"/>
    
    
      <category term="pytorch" scheme="http://haokailong.top/tags/pytorch/"/>
    
      <category term="softmax" scheme="http://haokailong.top/tags/softmax/"/>
    
      <category term="sigmoid" scheme="http://haokailong.top/tags/sigmoid/"/>
    
      <category term="binary classification" scheme="http://haokailong.top/tags/binary-classification/"/>
    
  </entry>
  
  <entry>
    <title>The log-sum-exp trick</title>
    <link href="http://haokailong.top/2021/03/12/The-log-sum-exp-trick/"/>
    <id>http://haokailong.top/2021/03/12/The-log-sum-exp-trick/</id>
    <published>2021-03-12T12:18:42.000Z</published>
    <updated>2021-03-12T12:31:19.021Z</updated>
    
    <content type="html"><![CDATA[<p>在机器学习和神经网络中，$\log()$是一个常用的技巧。</p><p>例如，对于函数求导时，使用$\log()$可以简化计算，避免乘法法则。</p><script type="math/tex; mode=display">\frac{\partial}{\partial x} \log[f(x)g(x)] = \frac{\partial}{\partial x}f(x) + \frac{\partial}{\partial x} g(x).</script><h2 id="在softmax-函数中的运用"><a href="#在softmax-函数中的运用" class="headerlink" title="在softmax()函数中的运用"></a>在softmax()函数中的运用</h2><p>在softmax()函数中，按照以下公式，将得分转化为对应的概率：</p><script type="math/tex; mode=display">\frac{\exp(x_m)}{\sum_{i=1}^N \exp(x_n)}</script><p>如果分母过大，则很容易溢出，按照以下方式进行转化：</p><script type="math/tex; mode=display">y = \log \sum_{i=1}^n \exp(x_n) \\e^y = \sum_{i=1}^n\exp(x_n) \\e^y = e^c\sum_{i=1}^n\exp(x_n-c) \\y = c + \log \sum_{i=1}^n \exp(x_n-c)</script><p>如果设置$c = \max{x_1,…,x_n}$，则最大不超过1</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在机器学习和神经网络中，$\log()$是一个常用的技巧。&lt;/p&gt;
&lt;p&gt;例如，对于函数求导时，使用$\log()$可以简化计算，避免乘法法则。&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\frac{\partial}{\part
      
    
    </summary>
    
    
      <category term="Algorithms" scheme="http://haokailong.top/categories/Algorithms/"/>
    
    
      <category term="pytorch" scheme="http://haokailong.top/tags/pytorch/"/>
    
      <category term="log-sum-exp" scheme="http://haokailong.top/tags/log-sum-exp/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode 84 递增栈</title>
    <link href="http://haokailong.top/2021/03/12/Leetcode-84-%E9%80%92%E5%A2%9E%E6%A0%88/"/>
    <id>http://haokailong.top/2021/03/12/Leetcode-84-递增栈/</id>
    <published>2021-03-12T04:30:19.000Z</published>
    <updated>2021-03-12T06:22:53.858Z</updated>
    
    <content type="html"><![CDATA[<h1 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h1><ol><li>最终矩形必然截断到某一高度</li><li>针对每一个高度，只需找到左右两侧的更高高度</li><li>递增栈的数据结构</li></ol><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">largestRectangleArea</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; heights)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; slope &#123;<span class="number">-1</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=heights.size(); ++i) &#123;</span><br><span class="line">      <span class="keyword">while</span> (slope.back() &gt;= <span class="number">0</span> &amp;&amp; (i == heights.size() || heights[slope.back()] &gt;= heights[i])) &#123;</span><br><span class="line">        <span class="keyword">int</span> height = heights[slope.back()];</span><br><span class="line">        slope.pop_back();</span><br><span class="line">        ret = <span class="built_in">std</span>::max(ret, (i-slope.back()<span class="number">-1</span>)*height);</span><br><span class="line">      &#125;</span><br><span class="line">      slope.push_back(i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>异常巧妙😲</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;要点&quot;&gt;&lt;a href=&quot;#要点&quot; class=&quot;headerlink&quot; title=&quot;要点&quot;&gt;&lt;/a&gt;要点&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;最终矩形必然截断到某一高度&lt;/li&gt;
&lt;li&gt;针对每一个高度，只需找到左右两侧的更高高度&lt;/li&gt;
&lt;li&gt;递增栈的数据结构&lt;/
      
    
    </summary>
    
    
      <category term="Algorithms" scheme="http://haokailong.top/categories/Algorithms/"/>
    
      <category term="LeetCode" scheme="http://haokailong.top/categories/Algorithms/LeetCode/"/>
    
    
      <category term="leetcode" scheme="http://haokailong.top/tags/leetcode/"/>
    
      <category term="stack" scheme="http://haokailong.top/tags/stack/"/>
    
      <category term="递增栈" scheme="http://haokailong.top/tags/%E9%80%92%E5%A2%9E%E6%A0%88/"/>
    
  </entry>
  
  <entry>
    <title>阿里面试要点</title>
    <link href="http://haokailong.top/2021/03/04/%E9%98%BF%E9%87%8C%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/"/>
    <id>http://haokailong.top/2021/03/04/阿里面试要点/</id>
    <published>2021-03-04T07:14:02.000Z</published>
    <updated>2021-03-04T07:25:41.266Z</updated>
    
    <content type="html"><![CDATA[<ol><li>需要有自己的职业规划，简单一些也可以？</li></ol><p>答：首先，熟悉业务，将算法能力用到真正的业务中；其次，主要专注于迁移学习和多模态NLP两个方向，产生更多的研究成果；最后，理解产业，能够主动发掘方向和着力点。</p><ol><li>面试一般经过：初面、终面、交叉面、HR面。</li><li>为了对自己求职的岗位有更深的理解，可以去查看同样岗位的社招要求。面经可适度参考。</li><li>笔试采用牛客网，可以自动补全，需手动处理IO。</li><li>面试练习可以采用模拟面试的方法，争取做到游刃有余、落落大方。STAR法则 (situation, target, action, result) 可以让叙述更清晰。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;需要有自己的职业规划，简单一些也可以？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;答：首先，熟悉业务，将算法能力用到真正的业务中；其次，主要专注于迁移学习和多模态NLP两个方向，产生更多的研究成果；最后，理解产业，能够主动发掘方向和着力点。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;面试一般
      
    
    </summary>
    
    
      <category term="Others" scheme="http://haokailong.top/categories/Others/"/>
    
      <category term="Interview" scheme="http://haokailong.top/categories/Others/Interview/"/>
    
    
      <category term="Interview" scheme="http://haokailong.top/tags/Interview/"/>
    
      <category term="阿里巴巴" scheme="http://haokailong.top/tags/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4/"/>
    
      <category term="Alibaba" scheme="http://haokailong.top/tags/Alibaba/"/>
    
      <category term="春招" scheme="http://haokailong.top/tags/%E6%98%A5%E6%8B%9B/"/>
    
  </entry>
  
  <entry>
    <title>再生核希尔伯特空间 (RKHS)</title>
    <link href="http://haokailong.top/2021/01/16/%E5%86%8D%E7%94%9F%E6%A0%B8%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4-RKHS/"/>
    <id>http://haokailong.top/2021/01/16/再生核希尔伯特空间-RKHS/</id>
    <published>2021-01-16T10:51:04.000Z</published>
    <updated>2021-01-16T12:30:21.401Z</updated>
    
    <content type="html"><![CDATA[<h1 id="希尔伯特空间"><a href="#希尔伯特空间" class="headerlink" title="希尔伯特空间"></a>希尔伯特空间</h1><p>先来说一下什么是<strong>希尔伯特空间</strong>。<br>这个概念听起来高大上，其实是个非常简单的概念。<br>先说什么是<strong>线性空间</strong></p><h2 id="线性空间"><a href="#线性空间" class="headerlink" title="线性空间"></a>线性空间</h2><p>线性空间即定义了数乘和加法的空间。这个就是具有线性结构的空间。<br>有了线性空间的概念之后，因为有数乘和加法，所以空间中可以找到一组基底（Basis）能够通过线性组合得到空间中所有的点。</p><h2 id="度量空间和赋范空间"><a href="#度量空间和赋范空间" class="headerlink" title="度量空间和赋范空间"></a>度量空间和赋范空间</h2><p>距离的定义必须满足如下三个条件：</p><ol><li><p>d(x,y)≥0;d(x,y)=0 的充要条件是x=y即非负性</p></li><li><p>d(x,y)=d(y,x);对称性</p></li><li><p>d(x,z)+d(z,y)≥d(x,y)满足三角不等式。</p></li></ol><p>定义了距离的空间叫<strong>度量空间</strong>。<br>定义了距离的线性空间叫<strong>线性度量空间</strong></p><p>接下来再定义范数||x||，范数的定义必须满足：</p><ol><li>||x||≥0即非负性</li><li>||αx||=|α|||x||</li><li>||x||+||y||≥||x+y||满足三角不等式</li></ol><p><font color="orange">所以范数这个概念，可以看成从零点到x的距离</font>，同时比价第二条（2），即数乘可以提取出来。<br>所以：<strong>由范数可以定义距离，即d(x,y)=||x−y||，但是距离不可以定义范数因为距离的定义，不满足范数的第二条条件</strong></p><p><strong>因为</strong>，$||x|| = d(0,x)$</p><p><strong>但</strong> $||\alpha x|| = d(0,\alpha x) \neq |\alpha||x||$</p><p><strong>举个栗子</strong>，$d(x,y) = \frac{\sqrt{\sum(x_i-y_i)^2}}{1 + \sqrt{\sum(x_i-y_i)^2}}$</p><p><strong>这个满足距离定义，但是不满足范数定义。所以范数是比距离更具体的一个东西</strong></p><p><strong>而定义了范数的空间，叫赋范空间和度量空间。另外完备的赋范空间叫巴拿赫空间。而定义了范数的线性空间，叫赋范线性空间</strong></p><h2 id="希尔伯特空间-1"><a href="#希尔伯特空间-1" class="headerlink" title="希尔伯特空间"></a><strong>希尔伯特空间</strong></h2><p><strong>定义了范数之后，还没有定义角度。那就再来定义角度，所以可以定义内积如下：</strong></p><p><strong>1. 对称性</strong></p><p><strong>2. 对第一变元的线性性质，即$&lt;\alpha x,y&gt; = \alpha&lt; x,y&gt;$</strong></p><p><strong>3. 正定性</strong></p><p><strong>另外还要再说一下函数空间的内积。一个函数可以看成一个无穷维的向量。</strong></p><p><strong>将一个函数按照x进行采样，可以得到一个函数的表示为一个向量的形式</strong></p><p>$(f(x_0),f(x_1),f(x_2),…,f(x_n))$</p><p><strong>如果采样的间隔变得无穷的小，则这个函数就可以表示为一个无穷维的向量。所以一个函数空间的内积可以定义为：</strong></p><p>$\int f(x)g(x)dx$，由内积可以导出范数，但是范数不可以导出内积。因为可以定义 $||x||^2=<x,x>$</x,x></p><p>另外函数空间的概念，还可以从另外一个角度来思考，例如：</p><ol><li>泰勒级数展开，可以看成将一个函数用${x^i}_0^∞$作为基底表示的一个空间</li><li>傅立叶级数展开，即将一个函数用三角函数的形式进行无穷维的展开</li></ol><p>一个n维的的空间，且定义了内积，就叫欧几里德空间（即有线性结构和夹角，垂直，投影这些）。</p><p><strong>引入无穷维的空间（一般指函数空间），具有线性结构同时定义了内积，同时还具有完备性的空间就叫希尔伯特空间</strong></p><p>比如上面讲的傅立叶变换就是一个希尔伯特空间。</p><h1 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;希尔伯特空间&quot;&gt;&lt;a href=&quot;#希尔伯特空间&quot; class=&quot;headerlink&quot; title=&quot;希尔伯特空间&quot;&gt;&lt;/a&gt;希尔伯特空间&lt;/h1&gt;&lt;p&gt;先来说一下什么是&lt;strong&gt;希尔伯特空间&lt;/strong&gt;。&lt;br&gt;这个概念听起来高大上，其实是个非常简
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://haokailong.top/categories/Machine-Learning/"/>
    
      <category term="Transfer Learning" scheme="http://haokailong.top/categories/Machine-Learning/Transfer-Learning/"/>
    
    
      <category term="RKHS" scheme="http://haokailong.top/tags/RKHS/"/>
    
      <category term="Kernel" scheme="http://haokailong.top/tags/Kernel/"/>
    
      <category term="核方法" scheme="http://haokailong.top/tags/%E6%A0%B8%E6%96%B9%E6%B3%95/"/>
    
      <category term="Hilbert Spaces" scheme="http://haokailong.top/tags/Hilbert-Spaces/"/>
    
  </entry>
  
  <entry>
    <title>最大平均差异MMD</title>
    <link href="http://haokailong.top/2021/01/16/%E6%9C%80%E5%A4%A7%E5%B9%B3%E5%9D%87%E5%B7%AE%E5%BC%82MMD/"/>
    <id>http://haokailong.top/2021/01/16/最大平均差异MMD/</id>
    <published>2021-01-16T05:05:07.000Z</published>
    <updated>2021-01-16T10:52:30.637Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MMD理解"><a href="#MMD理解" class="headerlink" title="MMD理解"></a>MMD理解</h1><h2 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h2><p>MMD：maximum mean discrepancy。最大平均差异。最先提出的时候用于双样本的检测（two-sample test）问题<a href="https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf" target="_blank" rel="noopener">A kernel two sample test</a>，用于判断两个分布p和q是否相同。它的基本假设是：如果对于所有以分布生成的样本空间为输入的函数f，如果两个分布生成的足够多的样本在f上的对应的像的均值都相等，那么那么可以认为这两个分布是同一个分布。如果这个值足够小，就认为两个分布相同，否则就认为它们不相同。同时这个值也用来判断两个分布之间的相似程度。</p><h2 id="2-数学步骤"><a href="#2-数学步骤" class="headerlink" title="2.数学步骤"></a>2.数学步骤</h2><p>如果用F表示一个在样本空间上的连续函数集，那么MMD可以用下面的式子表示：</p><script type="math/tex; mode=display">\mathrm{MMD}[\mathcal{F},p,q] := \sup_{f\in\mathcal{F}}(\mathbf{E}_{x\sim p}[f(x)] -\mathbf{E}_{y\sim q}[f(y)])</script><p>$\sup$表示上界。</p><p>假设X和Y分别是从分布p和q通过独立同分布(i.i.d.)采样得到的两个数据集，数据集的大小分别为m和n。基于X和Y可以得到MMD的经验估计(empirical estimate)为：</p><p>$\mathrm{MMD}[\mathcal{F},X,Y] := \sup<em>{f\in\mathcal{F}}(\frac{1}{m}\sum</em>{i=1}^mf(x<em>i) -\frac{1}{n}\sum</em>{i=1}^nf(y_i))$</p><p>在给定两个分布的观测集X,Y的情况下，这个结果会严重依赖于给定的函数集F。为了能表示MMD的性质：当且仅当p和q是相同分布的时候MMD为0，那么要求F足够rich；另一方面为了使检验具有足够的连续性（be consistent in power），从而使得MMD的经验估计可以随着观测集规模增大迅速收敛到它的期望，F必须足够restrictive。文中证明了当F是universal RKHS上的（unit ball）单位球时，可以满足上面两个性质。</p><a id="more"></a><h3 id="2-1再生核希尔伯特空间"><a href="#2-1再生核希尔伯特空间" class="headerlink" title="2.1再生核希尔伯特空间"></a>2.1再生核希尔伯特空间</h3><p>这部分讲述了在RHKS (Reproducing Kernel Hilbert Spaces) 上单位球（unit ball）作为F时，通过有限的观测来对MMD进行估计，并且设立一些MMD可以用来区分概率度量的条件。<br>在RKHS上，每个f对应一个feature map。在feature map的基础上，首先对于某个分布p定义一个mean embedding of p，它满足如下的性质：</p><p>$\mu_p \in \mathcal{H}$ such that $\mathbf{E}_xf = \langle f, \mu_p \rangle$ for all $f \in \mathcal{H}$</p><p>mean embedding存在是有约束条件的[1]。在p和q的mean embedding存在的条件下，MMD的平方可以表示如下：</p><script type="math/tex; mode=display">\begin{align}\mathrm{MMD}^2[\mathcal{F},p,q] & = [\sup_{||f||_\mathcal{H} \leq1}(\mathbf{E}_x[f(x)]-\mathbf{E}_y[f(y)])]^2\\& = [\sup_{||f||_\mathcal{H} \leq 1}\langle \mu_p-\mu_q,f\rangle_\mathcal{H}]^2\\& = ||\mu_p - \mu_q||_{\mathcal{H}}^2\end{align}</script><font color="orange">这里有一些地方没有看懂，$\langle \rangle$是什么意思？$\mathrm{MMD}$最后一步又是如何推导的？</font><p>下面是关于MMD作为一个Borel probability measures时，对F的一个约束及其证明，要求F：be a unit ball in a universal RKHS。比如Gaussian和Laplace RKHSs。进一步在给定了RKHS对应核函数，这个MMD的平方可以表示：</p><p>$\mathrm{MMD}^2[\mathcal{F},p,q] = \mathbf{E}<em>{x,x’} [k(x,x’)] - 2\mathbf{E}</em>{x,y} [k(x,y)] + \mathbf{E}_{y,y’}[k(y,y’)]$</p><p>x和x’分别表示两个服从于p的随机变量，y和y‘分别表示服从q的随机变量。对于上面的一个统计估计可以表示为：</p><p>$\mathrm{MMD}[\mathcal{F},X,Y] = [\frac{1}{m^2}\sum<em>{i,j=1}^m k(x_i,x_j) - \frac{2}{mn} \sum </em>{i,j=1}^{m,n} k(x<em>i,y_j) + \frac{1}{n^2} \sum</em>{i,j=1}^{n}k(y_i,y_j)]^{\frac{1}{2}}$</p><p>对于一个two-sample test, 给定的null hypothesis: p和q是相同，以及the alternative hypothesis: p和q不等。这个通过将test statistic和一个给定的阈值相比较得到，如果MMD大于阈值，那么就reject null hypothesis，也就是两个分布不同。如果MMD小于某个阈值，就接受null hypothesis。由于MMD的计算时使用的是有限的样本数，这里会出现两种类型的错误：第一种错误出现在null hypothesis被错误的拒绝了；也就是本来两个分布相同，但是却被判定为不同。反之，第二种错误出现在null hypothesis被错误的接受了。文章[1]中提供了许多关于hypothesis test的方法，这里不讨论。<br>在domain adaptation中，经常用到MMD来在特征学习的时候构造正则项来约束学到的表示，使得两个域上的特征尽可能相同。从上面的定义看，我们在判断两个分布p和q的时候，需要将观测样本首先映射到RKHS空间上，然后再判断。<em>但实际上很多文章直接将观测样本用于计算，省了映射的那个步骤。</em></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;MMD理解&quot;&gt;&lt;a href=&quot;#MMD理解&quot; class=&quot;headerlink&quot; title=&quot;MMD理解&quot;&gt;&lt;/a&gt;MMD理解&lt;/h1&gt;&lt;h2 id=&quot;1-定义&quot;&gt;&lt;a href=&quot;#1-定义&quot; class=&quot;headerlink&quot; title=&quot;1.定义&quot;&gt;&lt;/a&gt;1.定义&lt;/h2&gt;&lt;p&gt;MMD：maximum mean discrepancy。最大平均差异。最先提出的时候用于双样本的检测（two-sample test）问题&lt;a href=&quot;https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;A kernel two sample test&lt;/a&gt;，用于判断两个分布p和q是否相同。它的基本假设是：如果对于所有以分布生成的样本空间为输入的函数f，如果两个分布生成的足够多的样本在f上的对应的像的均值都相等，那么那么可以认为这两个分布是同一个分布。如果这个值足够小，就认为两个分布相同，否则就认为它们不相同。同时这个值也用来判断两个分布之间的相似程度。&lt;/p&gt;
&lt;h2 id=&quot;2-数学步骤&quot;&gt;&lt;a href=&quot;#2-数学步骤&quot; class=&quot;headerlink&quot; title=&quot;2.数学步骤&quot;&gt;&lt;/a&gt;2.数学步骤&lt;/h2&gt;&lt;p&gt;如果用F表示一个在样本空间上的连续函数集，那么MMD可以用下面的式子表示：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{MMD}[\mathcal{F},p,q] := \sup_{f\in\mathcal{F}}(\mathbf{E}_{x\sim p}[f(x)] -\mathbf{E}_{y\sim q}[f(y)])&lt;/script&gt;&lt;p&gt;$\sup$表示上界。&lt;/p&gt;
&lt;p&gt;假设X和Y分别是从分布p和q通过独立同分布(i.i.d.)采样得到的两个数据集，数据集的大小分别为m和n。基于X和Y可以得到MMD的经验估计(empirical estimate)为：&lt;/p&gt;
&lt;p&gt;$\mathrm{MMD}[\mathcal{F},X,Y] := \sup&lt;em&gt;{f\in\mathcal{F}}(\frac{1}{m}\sum&lt;/em&gt;{i=1}^mf(x&lt;em&gt;i) -\frac{1}{n}\sum&lt;/em&gt;{i=1}^nf(y_i))$&lt;/p&gt;
&lt;p&gt;在给定两个分布的观测集X,Y的情况下，这个结果会严重依赖于给定的函数集F。为了能表示MMD的性质：当且仅当p和q是相同分布的时候MMD为0，那么要求F足够rich；另一方面为了使检验具有足够的连续性（be consistent in power），从而使得MMD的经验估计可以随着观测集规模增大迅速收敛到它的期望，F必须足够restrictive。文中证明了当F是universal RKHS上的（unit ball）单位球时，可以满足上面两个性质。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://haokailong.top/categories/Machine-Learning/"/>
    
      <category term="Transfer Learning" scheme="http://haokailong.top/categories/Machine-Learning/Transfer-Learning/"/>
    
    
      <category term="Domain Adaptation" scheme="http://haokailong.top/tags/Domain-Adaptation/"/>
    
      <category term="MMD" scheme="http://haokailong.top/tags/MMD/"/>
    
      <category term="Maximum Mean Discrepancy" scheme="http://haokailong.top/tags/Maximum-Mean-Discrepancy/"/>
    
      <category term="Transfer Learning" scheme="http://haokailong.top/tags/Transfer-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度子域适应网络</title>
    <link href="http://haokailong.top/2021/01/15/%E6%B7%B1%E5%BA%A6%E5%AD%90%E5%9F%9F%E9%80%82%E5%BA%94%E7%BD%91%E7%BB%9C/"/>
    <id>http://haokailong.top/2021/01/15/深度子域适应网络/</id>
    <published>2021-01-15T12:17:38.000Z</published>
    <updated>2021-01-15T13:31:18.793Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h1><p>深度学习需要大量的标签样本，为解决标签问题提出了迁移学习，即从相关的source domain 去学习标签好的数据。但由于不同域间的数据分布也不同，所以学习得到的模型泛化能力不高。<br>在训练、测试数据的分布有变动的情况下去学习一个判别模型叫做domain adaptation 或transfer distributions。<br>在深度特征学习中嵌入domain adaptation模块去提取固定特征 已经证明能带来新的优势。之前的domain adaptation都是在全局域上做迁移，导致一个域内不同类别的数据会被混淆，因此不能学习到好的特征结构。</p><h1 id="2、方法"><a href="#2、方法" class="headerlink" title="2、方法"></a>2、方法</h1><h2 id="2-1新方法的概念"><a href="#2-1新方法的概念" class="headerlink" title="2.1新方法的概念"></a>2.1新方法的概念</h2><p>提出DSAN网络：在DANs网络的基础上对其子域来增强特征的表现能力。<br>为了实现正确的对齐，早期使用的方法是MMD：把source和target用一个相同的映射映射在一个再生核希尔伯特空间（RKHS）中，然后求映射后两部分数据的均值差异。现设计了一个局部最大平均差（LMMD）（它在考虑不同样本权重的情况下，测量源域和目标域中相关子域的经验分布的核平均嵌入之间的Hilbert-Schmidt范数）LMMD方法可以在大多数前馈网络模型中实现，并且可以使用标准反向传播进行有效的训练。</p><h2 id="2-2方法的实现"><a href="#2-2方法的实现" class="headerlink" title="2.2方法的实现"></a>2.2方法的实现</h2><h3 id="2-2-1网络的结构"><a href="#2-2-1网络的结构" class="headerlink" title="2.2.1网络的结构"></a>2.2.1网络的结构</h3><p><img src="/images/blog/2021/DSAN.png" alt></p><p>在ResNet的基础上添加LMMD模块来使得相关子域更相近。LMMD计算公式如下:</p><script type="math/tex; mode=display">\hat{d}_\mathcal{H}(p,q) = \frac{1}{C} \sum_{c=1}^C||\sum_{\mathbf{x}_i^s \in \mathcal{D}_s} w_i^{sc}\phi(\mathbf{x}_i^s) - \sum_{\mathbf{x}_j^t \in \mathcal{D}_t} w_j^{tc}\phi(\mathbf{x}_j^t)||_\mathcal{H}^2</script><script type="math/tex; mode=display">w_i^c = \frac{y_{ic}}{\sum_{(\mathbf{x}_j,\mathbf{y}_j)\in\mathcal{D}}y_{jc}}</script><a id="more"></a><p>最终DASN网络的损失函数如下：</p><script type="math/tex; mode=display">\min_f \frac{1}{n_s} \sum_{i=1}^{n_s}J(f(\mathbf{x}_i^s),\mathbf{y}_i^s) + \lambda\sum_{l\in L}\hat{d}_l(p,q)</script><h2 id="2-3背景"><a href="#2-3背景" class="headerlink" title="2.3背景"></a>2.3背景</h2><p>DAN是在DDC（deep domain Confusion）的基础上发展来的：<br>DAN解决了DDC的两个问题：<br>DDC只适配了一层网络，可能还是不够，因为Jason的工作中已经明确指出不同层都是可以迁移的。所以DAN就多适配几层；<br>DDC是用了单一核的MMD，单一固定的核可能不是最优的核。DAN用了多核的MMD（MK-MMD），效果比DDC更好。<br>总结：DANs是多层适配和多核MMD。</p><p>迁移学习目前的潮流有两种：<br>第一种是基于统计矩匹配的方法，即最大均值偏差（MMD）、中心矩差异（CMD）<br>第二种常用的方法是基于对抗性损失，它鼓励来自不同领域的样本对于领域标签是非歧视性的，即借用了GAN的思想<br>一般来说，采取adversarial loss的效果比statistic moment matching-based 效果好。<br>但这篇论文用的DSAN证明能取得更好的效果。</p><h1 id="3、实验结果分析："><a href="#3、实验结果分析：" class="headerlink" title="3、实验结果分析："></a>3、实验结果分析：</h1><p>在OFFICE31、CLEF-D等数据集上测试得出：<br>DASN与MMD的模型：能提高10-20%个百分点的精确率。<br>DASN与主流的（带对抗损失）模型比较 ：能提高5个百分点的精确率。</p><h1 id="4、结论"><a href="#4、结论" class="headerlink" title="4、结论"></a>4、结论</h1><p>DSAN预测能力不仅高于主流的对抗损失模型、速度也更快、而且易于实现。</p><p><img src="/images/blog/2021/DSAN-result.png" alt></p><p><img src="/images/blog/2021/DSAN-result1.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1、简介&quot;&gt;&lt;a href=&quot;#1、简介&quot; class=&quot;headerlink&quot; title=&quot;1、简介&quot;&gt;&lt;/a&gt;1、简介&lt;/h1&gt;&lt;p&gt;深度学习需要大量的标签样本，为解决标签问题提出了迁移学习，即从相关的source domain 去学习标签好的数据。但由于不同域间的数据分布也不同，所以学习得到的模型泛化能力不高。&lt;br&gt;在训练、测试数据的分布有变动的情况下去学习一个判别模型叫做domain adaptation 或transfer distributions。&lt;br&gt;在深度特征学习中嵌入domain adaptation模块去提取固定特征 已经证明能带来新的优势。之前的domain adaptation都是在全局域上做迁移，导致一个域内不同类别的数据会被混淆，因此不能学习到好的特征结构。&lt;/p&gt;
&lt;h1 id=&quot;2、方法&quot;&gt;&lt;a href=&quot;#2、方法&quot; class=&quot;headerlink&quot; title=&quot;2、方法&quot;&gt;&lt;/a&gt;2、方法&lt;/h1&gt;&lt;h2 id=&quot;2-1新方法的概念&quot;&gt;&lt;a href=&quot;#2-1新方法的概念&quot; class=&quot;headerlink&quot; title=&quot;2.1新方法的概念&quot;&gt;&lt;/a&gt;2.1新方法的概念&lt;/h2&gt;&lt;p&gt;提出DSAN网络：在DANs网络的基础上对其子域来增强特征的表现能力。&lt;br&gt;为了实现正确的对齐，早期使用的方法是MMD：把source和target用一个相同的映射映射在一个再生核希尔伯特空间（RKHS）中，然后求映射后两部分数据的均值差异。现设计了一个局部最大平均差（LMMD）（它在考虑不同样本权重的情况下，测量源域和目标域中相关子域的经验分布的核平均嵌入之间的Hilbert-Schmidt范数）LMMD方法可以在大多数前馈网络模型中实现，并且可以使用标准反向传播进行有效的训练。&lt;/p&gt;
&lt;h2 id=&quot;2-2方法的实现&quot;&gt;&lt;a href=&quot;#2-2方法的实现&quot; class=&quot;headerlink&quot; title=&quot;2.2方法的实现&quot;&gt;&lt;/a&gt;2.2方法的实现&lt;/h2&gt;&lt;h3 id=&quot;2-2-1网络的结构&quot;&gt;&lt;a href=&quot;#2-2-1网络的结构&quot; class=&quot;headerlink&quot; title=&quot;2.2.1网络的结构&quot;&gt;&lt;/a&gt;2.2.1网络的结构&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/images/blog/2021/DSAN.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;在ResNet的基础上添加LMMD模块来使得相关子域更相近。LMMD计算公式如下:&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\hat{d}_\mathcal{H}(p,q) = \frac{1}{C} \sum_{c=1}^C||\sum_{\mathbf{x}_i^s \in \mathcal{D}_s} w_i^{sc}\phi(\mathbf{x}_i^s) - \sum_{\mathbf{x}_j^t \in \mathcal{D}_t} w_j^{tc}\phi(\mathbf{x}_j^t)||_\mathcal{H}^2&lt;/script&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
w_i^c = \frac{y_{ic}}{\sum_{(\mathbf{x}_j,\mathbf{y}_j)\in\mathcal{D}}y_{jc}}&lt;/script&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://haokailong.top/categories/Machine-Learning/"/>
    
      <category term="Transfer Learning" scheme="http://haokailong.top/categories/Machine-Learning/Transfer-Learning/"/>
    
    
      <category term="Domain Adaptation" scheme="http://haokailong.top/tags/Domain-Adaptation/"/>
    
      <category term="transfer learning" scheme="http://haokailong.top/tags/transfer-learning/"/>
    
      <category term="领域适应" scheme="http://haokailong.top/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/"/>
    
      <category term="迁移学习" scheme="http://haokailong.top/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>什么是KL散度？</title>
    <link href="http://haokailong.top/2021/01/15/%E4%BB%80%E4%B9%88%E6%98%AFKL%E6%95%A3%E5%BA%A6%EF%BC%9F/"/>
    <id>http://haokailong.top/2021/01/15/什么是KL散度？/</id>
    <published>2021-01-15T08:42:04.000Z</published>
    <updated>2021-01-15T11:55:59.336Z</updated>
    
    <content type="html"><![CDATA[<p><code>Kullback-Leibler Divergence</code>，即<code>K-L散度</code>，是一种<strong>量化两种概率分布P和Q之间差异</strong>的方式，又叫<code>相对熵</code>。在概率学和统计学上，我们经常会使用一种<code>更简单的、近似的分布</code>来替代<code>观察数据</code>或<code>太复杂的分布</code>。K-L散度能帮助我们度量使用一个分布来近似另一个分布时所损失的信息量。</p><blockquote><p>假设我们是一群太空科学家，经过遥远的旅行，来到了一颗新发现的星球。在这个星球上，生存着一种长有牙齿的蠕虫，引起了我们的研究兴趣。我们发现这种蠕虫生有10颗牙齿，但是因为不注意口腔卫生，又喜欢嚼东西，许多蠕虫会掉牙。收集大量样本之后，我们得到关于蠕虫牙齿数量的经验分布，如下图所示</p></blockquote><p><img src="/images/blog/2021/teeth-distribution.webp" alt></p><p>显然我们的原始数据并非均分布的，但也不是我们已知的分布，至少不是常见的分布。作为备选，我们想到的另一种简单模型是<code>二项式分布binomial distribution</code>。蠕虫嘴里面共有<code>n=10</code>个牙槽，每个牙槽出现牙齿与否为独立事件，且概率均为<code>p</code>。则蠕虫牙齿数量即为期望值<code>E[x]=n*p</code>，真实期望值即为观察数据的平均值，比如说<code>5.7</code>，则<code>p=0.57</code>，得到如下图所示的二项式分布：<br><a id="more"></a></p><p><img src="/images/blog/2021/binomial.webp" alt></p><p>对比一下原始数据，可以看出均分布和二项分布都不能完全描述原始分布。</p><p><img src="/images/blog/2021/comparison.webp" alt></p><p>可是，我们不禁要问，哪一种分布更加接近原始分布呢？<br>已经有许多度量误差的方式存在，但是我们所要考虑的是减小发送的信息量。上面讨论的均分布和二项式分布都把问题规约到只需要两个参数，牙齿数量和概率值（均分布只需要牙齿数量即可）。那么哪个分布保留了更多的原始数据分布的信息呢？这个时候就需要K-L散度登场了。</p><h2 id="数据的熵"><a href="#数据的熵" class="headerlink" title="数据的熵"></a>数据的熵</h2><p>K-L散度源于信息论。信息论主要研究如何量化数据中的信息。最重要的信息度量单位是<code>熵</code>Entropy，一般用<code>H</code>表示。分布的熵的公式如下：</p><p>$H=-\sum_{i=1}^N p(x_i) \cdot \log p(x_i)$</p><p>上面对数没有确定底数，可以是<code>2</code>、<code>e</code>或<code>10</code>，等等。如果我们使用以<code>2</code>为底的对数计算H值的话，可以把这个值看作是编码信息所需要的最少二进制位个数bits。上面空间蠕虫的例子中，信息指的是根据观察所得的经验分布给出的蠕虫牙齿数量。计算可以得到原始数据概率分布的熵值为<code>3.12 bits</code>。这个值只是告诉我们编码蠕虫牙齿数量概率的信息需要的二进制位<code>bit</code>的位数。<br>可是熵值并没有给出压缩数据到最小熵值的方法，即如何编码数据才能达到最优（存储空间最优）。优化信息编码是一个非常有意思的主题，但并不是理解K-L散度所必须的。熵的主要作用是告诉我们最优编码信息方案的理论下界（存储空间），以及度量数据的信息量的一种方式。理解了熵，我们就知道有多少信息蕴含在数据之中，现在我们就可以计算当我们用一个带参数的概率分布来近似替代原始数据分布的时候，到底损失了多少信息。请继续看下节内容。</p><h2 id="K-L散度度量信息损失"><a href="#K-L散度度量信息损失" class="headerlink" title="K-L散度度量信息损失"></a>K-L散度度量信息损失</h2><p>只需要稍加修改<code>熵H</code>的计算公式就能得到<code>K-L散度</code>的计算公式。设<code>p</code>为观察得到的概率分布，<code>q</code>为另一分布来近似<code>p</code>，则<code>p</code>、<code>q</code>的<code>K-L散度</code>为：</p><p>$D<em>{KL}(p||q) = \sum</em>{i=1}^N p(x_i) \cdot(\log p(x_i) - \log q(x_i))$</p><p>显然，根据上面的公式，K-L散度其实是数据的原始分布p和近似分布q之间的对数差值的期望。如果继续用<code>2</code>为底的对数计算，则<strong>K-L散度值表示信息损失的二进制位数</strong>。下面公式以期望表达K-L散度：</p><font color="orange">可以理解为在真实分布下，预测分布与真实分布的能量（熵）差异。</font><p>一般，K-L散度以下面的书写方式更常见：</p><p>$D<em>{KL}(p||q) = \sum</em>{i=1}^N p(x_i) \cdot \log \frac{p(x_i)}{q(x_i)}$</p><p>OK，现在我们知道当用一个分布来近似另一个分布时如何计算信息损失量了。接下来，让我们重新回到最开始的蠕虫牙齿数量概率分布的问题。首先是用均分布来近似原始分布的K-L散度：</p><p>$D_{kl}(Observed||Uniform) = 0.338$</p><p>接下来计算用二项式分布近似原始分布的K-L散度：$0.477$</p><p>通过上面的计算可以看出，使用均分布近似原始分布的信息损失要比用二项式分布近似小。所以，如果要从均分布和二项式分布中选择一个的话，均分布更好些。</p><h2 id="散度并非距离"><a href="#散度并非距离" class="headerlink" title="散度并非距离"></a>散度并非距离</h2><p>很自然地，一些同学把K-L散度看作是不同分布之间距离的度量。这是不对的，因为从K-L散度的计算公式就可以看出它不符合对称性（距离度量应该满足对称性）。<code>Dkl (Observed || Binomial) != Dkl (Binomial || Observed)</code>。也就是说，用<code>p</code>近似<code>q</code>和用<code>q</code>近似<code>p</code>，二者所损失的信息并不是一样的。</p><h2 id="使用K-L散度优化模型"><a href="#使用K-L散度优化模型" class="headerlink" title="使用K-L散度优化模型"></a>使用K-L散度优化模型</h2><p>前面使用的二项式分布的参数是概率 <code>p=0.57</code>，是原始数据的均值。<code>p</code>的值域在 [0, 1] 之间，我们要选择一个<code>p</code>值，建立二项式分布，目的是最小化近似误差，即K-L散度。那么<code>0.57</code>是最优的吗？<br> 下图是原始数据分布和二项式分布的K-L散度变化随二项式分布参数<code>p</code>变化情况：</p><p><img src="/images/blog/2021/divergence.webp" alt></p><p>通过上面的曲线图可以看出，K-L散度值在圆点处最小，即<code>p=0.57</code>。所以我们之前的二项式分布模型已经是最优的二项式模型了。注意，我已经说了，是而像是模型，这里只限定在二项式模型范围内。</p><p>前面只考虑了均分布模型和二项式分布模型，接下来我们考虑另外一种模型来近似原始数据。首先把原始数据分成两部分，1）0-5颗牙齿的概率和 2）6-10颗牙齿的概率。一只蠕虫的牙齿数量<code>x=i</code>的概率为<code>p/5</code>; <code>x=j</code>的概率为<code>(1-p) / 6</code>，<code>i=0,1,2,3,4,5</code>; <code>j=6,7,8,9,10</code>。<br>Aha，我们自己建立了一个新的（奇怪的）模型来近似原始的分布，模型只有一个参数<code>p</code>，像前面那样优化二项式分布的时候所做的一样，让我们画出K-L散度值随<code>p</code>变化的情况：<br><img src="/images/blog/2021/divergence1.webp" alt></p><p>当<code>p=0.47</code>时，K-L值取最小值<code>0.338</code>。似曾相识吗？对，这个值和使用均分布的K-L散度值是一样的（这并不能说明什么）！我们自己都说了，这是个奇怪的模型，在K-L值相同的情况下，更倾向于使用更常见的、更简单的均分布模型。回头看，我们在这一小节中使用K-L散度作为目标方程，分别找到了二项式分布模型的参数<code>p=0.57</code>和上面这个随手建立的模型的参数<code>p=0.47</code>。是的，这就是本节的重点：<strong>使用K-L散度作为目标方程来优化模型</strong>。当然，本节中的模型都只有一个参数，也可以拓展到有更多参数的高维模型中。</p><h2 id="变分自编码器VAEs和变分贝叶斯法"><a href="#变分自编码器VAEs和变分贝叶斯法" class="headerlink" title="变分自编码器VAEs和变分贝叶斯法"></a>变分自编码器VAEs和变分贝叶斯法</h2><p>如果你熟悉神经网络，你肯能已经猜到我们接下来要学习的内容。除去神经网络结构的细节信息不谈，整个神经网络模型其实是在构造一个参数数量巨大的函数（百万级，甚至更多），不妨记为<code>f(x)</code>，通过设定目标函数，可以训练神经网络逼近非常复杂的真实函数<code>g(x)</code>。训练的关键是要设定目标函数，反馈给神经网络当前的表现如何。训练过程就是不断减小目标函数值的过程。</p><p>我们已经知道K-L散度用来度量在逼近一个分布时的信息损失量。K-L散度能够赋予神经网络近似表达非常复杂数据分布的能力。变分自编码器（Variational Autoencoders，VAEs）是一种能够学习最佳近似数据集中信息的常用方法，<a href="https://links.jianshu.com/go?to=https%3A%2F%2Farxiv.org%2Fabs%2F1606.05908" target="_blank" rel="noopener">Tutorial on Variational Autoencoders 2016</a>是一篇关于VAEs的非常不错的教程，里面讲述了如何构建VAE的细节。 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fmedium.com%2F%40dmonn%2Fwhat-are-variational-autoencoders-a-simple-explanation-ea7dccafb0e3" target="_blank" rel="noopener">What are Variational Autoencoders? A simple explanation</a>简单介绍了VAEs，<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fblog.keras.io%2Fbuilding-autoencoders-in-keras.html" target="_blank" rel="noopener">Building Autoencoders in Keras</a>介绍了如何利用Keras库实现几种自编码器。</p><p>变分贝叶斯方法（Variational Bayesian Methods）是一种更常见的方法。<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.countbayesie.com%2Fblog%2F2015%2F3%2F3%2F6-amazing-trick-with-monte-carlo-simulations" target="_blank" rel="noopener">这篇文章</a>介绍了强大的蒙特卡洛模拟方法能够解决很多概率问题。蒙特卡洛模拟能够帮助解决许多贝叶斯推理问题中的棘手积分问题，尽管计算开销很大。包括VAE在内的变分贝叶斯方法，都能用K-L散度生成优化的近似分布，这种方法对棘手积分问题能进行更高效的推理。更多变分推理（Variational Inference）的知识可以访问<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fedwardlib.org%2F" target="_blank" rel="noopener">Edward library for python</a>。</p><h2 id="计算KL散度的注意事项"><a href="#计算KL散度的注意事项" class="headerlink" title="计算KL散度的注意事项"></a>计算KL散度的注意事项</h2><p><img src="/images/blog/2021/KL-notice.webp" alt></p><p><img src="/images/blog/2021/KL-notice1.webp" alt></p><ol><li>信息熵、交叉熵、相对熵</li></ol><ul><li><p>信息熵，即熵，香浓熵。编码方案完美时，最短平均编码长度。</p></li><li><p>交叉熵，cross-entropy。编码方案不一定完美时（由于对概率分布的估计不一定正确），平均编码长度。是神经网络常用的损失函数。</p></li><li><p>相对熵，即K-L散度，relative entropy。编码方案不一定完美时，平均编码长度相对于最小值的增加值。<br> 更详细对比，见知乎<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F41252833" target="_blank" rel="noopener">如何通俗的解释交叉熵与相对熵?</a></p></li></ul><ol><li>为什么在神经网络中使用交叉熵损失函数，而不是K-L散度？<br>K-L散度=交叉熵-熵，即 <code>DKL( p||q )=H(p,q)−H(p)</code>。<br>在神经网络所涉及到的范围内，<code>H(p)</code>不变，则<code>DKL( p||q )</code>等价<code>H(p,q)</code>。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Kullback-Leibler Divergence&lt;/code&gt;，即&lt;code&gt;K-L散度&lt;/code&gt;，是一种&lt;strong&gt;量化两种概率分布P和Q之间差异&lt;/strong&gt;的方式，又叫&lt;code&gt;相对熵&lt;/code&gt;。在概率学和统计学上，我们经常会使用一种&lt;code&gt;更简单的、近似的分布&lt;/code&gt;来替代&lt;code&gt;观察数据&lt;/code&gt;或&lt;code&gt;太复杂的分布&lt;/code&gt;。K-L散度能帮助我们度量使用一个分布来近似另一个分布时所损失的信息量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假设我们是一群太空科学家，经过遥远的旅行，来到了一颗新发现的星球。在这个星球上，生存着一种长有牙齿的蠕虫，引起了我们的研究兴趣。我们发现这种蠕虫生有10颗牙齿，但是因为不注意口腔卫生，又喜欢嚼东西，许多蠕虫会掉牙。收集大量样本之后，我们得到关于蠕虫牙齿数量的经验分布，如下图所示&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/images/blog/2021/teeth-distribution.webp&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;显然我们的原始数据并非均分布的，但也不是我们已知的分布，至少不是常见的分布。作为备选，我们想到的另一种简单模型是&lt;code&gt;二项式分布binomial distribution&lt;/code&gt;。蠕虫嘴里面共有&lt;code&gt;n=10&lt;/code&gt;个牙槽，每个牙槽出现牙齿与否为独立事件，且概率均为&lt;code&gt;p&lt;/code&gt;。则蠕虫牙齿数量即为期望值&lt;code&gt;E[x]=n*p&lt;/code&gt;，真实期望值即为观察数据的平均值，比如说&lt;code&gt;5.7&lt;/code&gt;，则&lt;code&gt;p=0.57&lt;/code&gt;，得到如下图所示的二项式分布：&lt;br&gt;
    
    </summary>
    
    
      <category term="Algorithms" scheme="http://haokailong.top/categories/Algorithms/"/>
    
    
      <category term="K-L divergence" scheme="http://haokailong.top/tags/K-L-divergence/"/>
    
      <category term="KL散度" scheme="http://haokailong.top/tags/KL%E6%95%A3%E5%BA%A6/"/>
    
      <category term="相对熵" scheme="http://haokailong.top/tags/%E7%9B%B8%E5%AF%B9%E7%86%B5/"/>
    
  </entry>
  
  <entry>
    <title>sklearn.metrics.precision_recall_curve()源码阅读</title>
    <link href="http://haokailong.top/2021/01/12/sklearn-metrics-precision-recall-curve-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>http://haokailong.top/2021/01/12/sklearn-metrics-precision-recall-curve-源码阅读/</id>
    <published>2021-01-12T03:53:24.000Z</published>
    <updated>2021-01-12T05:54:36.180Z</updated>
    
    <content type="html"><![CDATA[<h1 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@_deprecate_positional_args</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">precision_recall_curve</span><span class="params">(y_true, probas_pred, *, pos_label=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                           sample_weight=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y_true : ndarray of shape (n_samples,)</span></span><br><span class="line"><span class="string">        True binary labels. If labels are not either &#123;-1, 1&#125; or &#123;0, 1&#125;, then</span></span><br><span class="line"><span class="string">        pos_label should be explicitly given.</span></span><br><span class="line"><span class="string">    probas_pred : ndarray of shape (n_samples,)</span></span><br><span class="line"><span class="string">        Estimated probabilities or output of a decision function.</span></span><br><span class="line"><span class="string">    pos_label : int or str, default=None</span></span><br><span class="line"><span class="string">        The label of the positive class.</span></span><br><span class="line"><span class="string">        When ``pos_label=None``, if y_true is in &#123;-1, 1&#125; or &#123;0, 1&#125;,</span></span><br><span class="line"><span class="string">        ``pos_label`` is set to 1, otherwise an error will be raised.</span></span><br><span class="line"><span class="string">    sample_weight : array-like of shape (n_samples,), default=None</span></span><br><span class="line"><span class="string">        Sample weights.</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    precision : ndarray of shape (n_thresholds + 1,)</span></span><br><span class="line"><span class="string">        Precision values such that element i is the precision of</span></span><br><span class="line"><span class="string">        predictions with score &gt;= thresholds[i] and the last element is 1.</span></span><br><span class="line"><span class="string">    recall : ndarray of shape (n_thresholds + 1,)</span></span><br><span class="line"><span class="string">        Decreasing recall values such that element i is the recall of</span></span><br><span class="line"><span class="string">        predictions with score &gt;= thresholds[i] and the last element is 0.</span></span><br><span class="line"><span class="string">    thresholds : ndarray of shape (n_thresholds,)</span></span><br><span class="line"><span class="string">        Increasing thresholds on the decision function used to compute</span></span><br><span class="line"><span class="string">        precision and recall. n_thresholds &lt;= len(np.unique(probas_pred)).</span></span><br><span class="line"><span class="string">    See Also</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    plot_precision_recall_curve : Plot Precision Recall Curve for binary</span></span><br><span class="line"><span class="string">        classifiers.</span></span><br><span class="line"><span class="string">    PrecisionRecallDisplay : Precision Recall visualization.</span></span><br><span class="line"><span class="string">    average_precision_score : Compute average precision from prediction scores.</span></span><br><span class="line"><span class="string">    det_curve: Compute error rates for different probability thresholds.</span></span><br><span class="line"><span class="string">    roc_curve : Compute Receiver operating characteristic (ROC) curve.</span></span><br><span class="line"><span class="string">    Examples</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; import numpy as np</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; from sklearn.metrics import precision_recall_curve</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y_true = np.array([0, 0, 1, 1])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y_scores = np.array([0.1, 0.4, 0.35, 0.8])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; precision, recall, thresholds = precision_recall_curve(</span></span><br><span class="line"><span class="string">    ...     y_true, y_scores)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; precision</span></span><br><span class="line"><span class="string">    array([0.66666667, 0.5       , 1.        , 1.        ])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; recall</span></span><br><span class="line"><span class="string">    array([1. , 0.5, 0.5, 0. ])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; thresholds</span></span><br><span class="line"><span class="string">    array([0.35, 0.4 , 0.8 ])</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred,</span><br><span class="line">                                             pos_label=pos_label,</span><br><span class="line">                                             sample_weight=sample_weight)</span><br><span class="line"></span><br><span class="line">    precision = tps / (tps + fps)</span><br><span class="line">    precision[np.isnan(precision)] = <span class="number">0</span></span><br><span class="line">    recall = tps / tps[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># stop when full recall attained</span></span><br><span class="line">    <span class="comment"># and reverse the outputs so recall is decreasing</span></span><br><span class="line">    last_ind = tps.searchsorted(tps[<span class="number">-1</span>])</span><br><span class="line">    sl = slice(last_ind, <span class="literal">None</span>, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> np.r_[precision[sl], <span class="number">1</span>], np.r_[recall[sl], <span class="number">0</span>], thresholds[sl]</span><br></pre></td></tr></table></figure><a id="more"></a><p>然后用到<code>_binary_clf_curve</code>，去看一眼：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_binary_clf_curve</span><span class="params">(y_true, y_score, pos_label=None, sample_weight=None)</span>:</span></span><br><span class="line">    <span class="string">"""Calculate true and false positives per binary classification threshold.</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y_true : ndarray of shape (n_samples,)</span></span><br><span class="line"><span class="string">        True targets of binary classification.</span></span><br><span class="line"><span class="string">    y_score : ndarray of shape (n_samples,)</span></span><br><span class="line"><span class="string">        Estimated probabilities or output of a decision function.</span></span><br><span class="line"><span class="string">    pos_label : int or str, default=None</span></span><br><span class="line"><span class="string">        The label of the positive class.</span></span><br><span class="line"><span class="string">    sample_weight : array-like of shape (n_samples,), default=None</span></span><br><span class="line"><span class="string">        Sample weights.</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    fps : ndarray of shape (n_thresholds,)</span></span><br><span class="line"><span class="string">        A count of false positives, at index i being the number of negative</span></span><br><span class="line"><span class="string">        samples assigned a score &gt;= thresholds[i]. The total number of</span></span><br><span class="line"><span class="string">        negative samples is equal to fps[-1] (thus true negatives are given by</span></span><br><span class="line"><span class="string">        fps[-1] - fps).</span></span><br><span class="line"><span class="string">    tps : ndarray of shape (n_thresholds,)</span></span><br><span class="line"><span class="string">        An increasing count of true positives, at index i being the number</span></span><br><span class="line"><span class="string">        of positive samples assigned a score &gt;= thresholds[i]. The total</span></span><br><span class="line"><span class="string">        number of positive samples is equal to tps[-1] (thus false negatives</span></span><br><span class="line"><span class="string">        are given by tps[-1] - tps).</span></span><br><span class="line"><span class="string">    thresholds : ndarray of shape (n_thresholds,)</span></span><br><span class="line"><span class="string">        Decreasing score values.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Check to make sure y_true is valid</span></span><br><span class="line">    y_type = type_of_target(y_true)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (y_type == <span class="string">"binary"</span> <span class="keyword">or</span></span><br><span class="line">            (y_type == <span class="string">"multiclass"</span> <span class="keyword">and</span> pos_label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"&#123;0&#125; format is not supported"</span>.format(y_type))</span><br><span class="line"></span><br><span class="line">    check_consistent_length(y_true, y_score, sample_weight)</span><br><span class="line">    y_true = column_or_1d(y_true)</span><br><span class="line">    y_score = column_or_1d(y_score)</span><br><span class="line">    assert_all_finite(y_true)</span><br><span class="line">    assert_all_finite(y_score)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        sample_weight = column_or_1d(sample_weight)</span><br><span class="line"></span><br><span class="line">    pos_label = _check_pos_label_consistency(pos_label, y_true)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># make y_true a boolean vector</span></span><br><span class="line">    y_true = (y_true == pos_label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort scores and corresponding truth values</span></span><br><span class="line">    desc_score_indices = np.argsort(y_score, kind=<span class="string">"mergesort"</span>)[::<span class="number">-1</span>]</span><br><span class="line">    y_score = y_score[desc_score_indices]</span><br><span class="line">    y_true = y_true[desc_score_indices]</span><br><span class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        weight = sample_weight[desc_score_indices]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        weight = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># y_score typically has many tied values. Here we extract</span></span><br><span class="line">    <span class="comment"># the indices associated with the distinct values. We also</span></span><br><span class="line">    <span class="comment"># concatenate a value for the end of the curve.</span></span><br><span class="line">    distinct_value_indices = np.where(np.diff(y_score))[<span class="number">0</span>]</span><br><span class="line">    threshold_idxs = np.r_[distinct_value_indices, y_true.size - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># accumulate the true positives with decreasing threshold</span></span><br><span class="line">    tps = stable_cumsum(y_true * weight)[threshold_idxs]</span><br><span class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># express fps as a cumsum to ensure fps is increasing even in</span></span><br><span class="line">        <span class="comment"># the presence of floating point errors</span></span><br><span class="line">        fps = stable_cumsum((<span class="number">1</span> - y_true) * weight)[threshold_idxs]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        fps = <span class="number">1</span> + threshold_idxs - tps</span><br><span class="line">    <span class="keyword">return</span> fps, tps, y_score[threshold_idxs]</span><br></pre></td></tr></table></figure><p>使用到numpy进行归并排序，还用到了<code>::</code>写法，即seq[start​ : end : s​tep]，</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;主函数&quot;&gt;&lt;a href=&quot;#主函数&quot; class=&quot;headerlink&quot; title=&quot;主函数&quot;&gt;&lt;/a&gt;主函数&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@_deprecate_positional_args&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;precision_recall_curve&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(y_true, probas_pred, *, pos_label=None,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                           sample_weight=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    ----------&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    y_true : ndarray of shape (n_samples,)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        True binary labels. If labels are not either &amp;#123;-1, 1&amp;#125; or &amp;#123;0, 1&amp;#125;, then&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        pos_label should be explicitly given.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    probas_pred : ndarray of shape (n_samples,)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        Estimated probabilities or output of a decision function.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    pos_label : int or str, default=None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        The label of the positive class.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        When ``pos_label=None``, if y_true is in &amp;#123;-1, 1&amp;#125; or &amp;#123;0, 1&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        ``pos_label`` is set to 1, otherwise an error will be raised.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    sample_weight : array-like of shape (n_samples,), default=None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        Sample weights.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    Returns&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    -------&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    precision : ndarray of shape (n_thresholds + 1,)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        Precision values such that element i is the precision of&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        predictions with score &amp;gt;= thresholds[i] and the last element is 1.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    recall : ndarray of shape (n_thresholds + 1,)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        Decreasing recall values such that element i is the recall of&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        predictions with score &amp;gt;= thresholds[i] and the last element is 0.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    thresholds : ndarray of shape (n_thresholds,)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        Increasing thresholds on the decision function used to compute&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        precision and recall. n_thresholds &amp;lt;= len(np.unique(probas_pred)).&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    See Also&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    --------&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    plot_precision_recall_curve : Plot Precision Recall Curve for binary&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;        classifiers.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    PrecisionRecallDisplay : Precision Recall visualization.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    average_precision_score : Compute average precision from prediction scores.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    det_curve: Compute error rates for different probability thresholds.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    roc_curve : Compute Receiver operating characteristic (ROC) curve.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    Examples&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    --------&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; import numpy as np&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import precision_recall_curve&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; y_true = np.array([0, 0, 1, 1])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; y_scores = np.array([0.1, 0.4, 0.35, 0.8])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; precision, recall, thresholds = precision_recall_curve(&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    ...     y_true, y_scores)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; precision&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    array([0.66666667, 0.5       , 1.        , 1.        ])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; recall&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    array([1. , 0.5, 0.5, 0. ])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; thresholds&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    array([0.35, 0.4 , 0.8 ])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                                             pos_label=pos_label,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                                             sample_weight=sample_weight)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    precision = tps / (tps + fps)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    precision[np.isnan(precision)] = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    recall = tps / tps[&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# stop when full recall attained&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# and reverse the outputs so recall is decreasing&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    last_ind = tps.searchsorted(tps[&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    sl = slice(last_ind, &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; np.r_[precision[sl], &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;], np.r_[recall[sl], &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], thresholds[sl]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Algorithms" scheme="http://haokailong.top/categories/Algorithms/"/>
    
    
      <category term="source code" scheme="http://haokailong.top/tags/source-code/"/>
    
      <category term="PR-Curve" scheme="http://haokailong.top/tags/PR-Curve/"/>
    
      <category term="sklearn" scheme="http://haokailong.top/tags/sklearn/"/>
    
      <category term="scikit-learn" scheme="http://haokailong.top/tags/scikit-learn/"/>
    
      <category term="precision_recall_curve" scheme="http://haokailong.top/tags/precision-recall-curve/"/>
    
  </entry>
  
  <entry>
    <title>箱线图</title>
    <link href="http://haokailong.top/2021/01/11/%E7%AE%B1%E7%BA%BF%E5%9B%BE/"/>
    <id>http://haokailong.top/2021/01/11/箱线图/</id>
    <published>2021-01-11T13:41:29.000Z</published>
    <updated>2021-01-11T13:48:35.996Z</updated>
    
    <content type="html"><![CDATA[<p>总是可以看到箱线图，但是却不知道怎么读？今天又看到了，就做一个了解。</p><h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>箱形图（Box-plot）又称为盒须图、盒式图或箱线图，是一种用作显示一组数据分散情况资料的统计图。因形状如箱子而得名。在各种领域也经常被使用，常见于<a href="https://baike.baidu.com/item/品质管理/9207881" target="_blank" rel="noopener">品质管理</a>。它主要用于反映原始数据分布的特征，还可以进行多组数据分布特征的比 较。箱线图的绘制方法是：先找出一组数据的上边缘、下边缘、中位数和两个四分位数；然后， 连接两个四分位数画出箱体；再将上边缘和下边缘与箱体相连接，中位数在箱体中间。</p><p><img src="/images/blog/2021/box-plot.jpg" alt></p><p>“盒式图”或叫”<a href="https://baike.baidu.com/item/盒须图" target="_blank" rel="noopener">盒须图</a>“”箱形图”boxplot（也称箱须图(Box-whiskerPlot）须图又称为箱形图，其绘制须使用常用的<a href="https://baike.baidu.com/item/统计量" target="_blank" rel="noopener">统计量</a>，能提供有关数据位置和分散情况的关键信息，尤其在比较不同的母体数据时更可表现其差异。</p><p>如上图所示，标示了图中每条线表示的含义，其中应用到了分位值（数）的概念。</p><p>主要包含六个数据节点，将一组数据从大到小排列，分别计算出他的上边缘，上<a href="https://baike.baidu.com/item/四分位数" target="_blank" rel="noopener">四分位数</a>Q3，<a href="https://baike.baidu.com/item/中位数" target="_blank" rel="noopener">中位数</a>，下四分位数Q1，下边缘，还有一个<a href="https://baike.baidu.com/item/异常值" target="_blank" rel="noopener">异常值</a>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;总是可以看到箱线图，但是却不知道怎么读？今天又看到了，就做一个了解。&lt;/p&gt;
&lt;h1 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h1&gt;&lt;p&gt;箱形图（Box-plot）又称为盒须图、盒式图或箱线图，
      
    
    </summary>
    
    
      <category term="Others" scheme="http://haokailong.top/categories/Others/"/>
    
      <category term="Tool" scheme="http://haokailong.top/categories/Others/Tool/"/>
    
    
      <category term="箱线图" scheme="http://haokailong.top/tags/%E7%AE%B1%E7%BA%BF%E5%9B%BE/"/>
    
      <category term="box-plot" scheme="http://haokailong.top/tags/box-plot/"/>
    
  </entry>
  
  <entry>
    <title>异常检测</title>
    <link href="http://haokailong.top/2021/01/11/%E6%B7%B1%E5%BA%A6%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    <id>http://haokailong.top/2021/01/11/深度异常检测/</id>
    <published>2021-01-11T02:48:56.000Z</published>
    <updated>2021-01-11T14:35:11.033Z</updated>
    
    <content type="html"><![CDATA[<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>根据维基百科：</p><blockquote><p>在<a href="https://zh.wikipedia.org/wiki/数据挖掘" target="_blank" rel="noopener">数据挖掘</a>中，<strong>异常检测</strong>（英语：<strong>anomaly detection</strong>）对不符合预期模式或<a href="https://zh.wikipedia.org/w/index.php?title=数据集&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">数据集</a>中其他项目的项目、事件或观测值的识别。<a href="https://zh.wikipedia.org/wiki/异常检测#cite_note-ChandolaSurvey-1" target="_blank" rel="noopener">[1]</a> 通常异常项目会转变成<a href="https://zh.wikipedia.org/w/index.php?title=银行欺诈&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">银行欺诈</a>、结构缺陷、医疗问题、文本错误等类型的问题。异常也被称为离群值、新奇、噪声、偏差和例外。</p></blockquote><p>有三大类异常检测方法。<a href="https://zh.wikipedia.org/wiki/异常检测#cite_note-ChandolaSurvey-1" target="_blank" rel="noopener">[1]</a> 在假设数据集中大多数实例都是正常的前提下，<strong>无监督异常检测</strong>方法能通过寻找与其他数据最不匹配的实例来检测出未标记测试数据的异常。<strong>监督式异常检测</strong>方法需要一个已经被标记“正常”与“异常”的数据集，并涉及到训练分类器（与许多其他的<a href="https://zh.wikipedia.org/wiki/分类问题" target="_blank" rel="noopener">统计分类问题</a>的关键区别是异常检测的内在不均衡性）。<strong>半监督式异常检测</strong>方法根据一个给定的<em>正常</em>训练数据集创建一个表示正常行为的模型，然后检测由学习模型生成的测试实例的可能性。</p><a id="more"></a><h1 id="检测方法"><a href="#检测方法" class="headerlink" title="检测方法"></a>检测方法</h1><p>【1】基于统计模型的方法：首先建立一个数据模型，<font color="orange">异常是那些同模型不能完美拟合的对象</font>；如果模型是<strong>簇的集合</strong>，则异常是不显著属于任何簇的对象；在使用<strong>回归模型</strong>时，异常是相对远离预测值的对象。</p><p>【2】基于邻近度的方法：通常可以在对象之间定义邻近性度量，异常对象是那些远离其他对象的对象。</p><p>【3】基于密度的方法：仅当一个点的局部密度显著低于它的大部分近邻时才将其分类为离群点。</p><p>【4】基于聚类的方法：聚类分析用于发现局部强相关的对象组，而异常检测用来发现不与其他对象强相关的对象。因此，聚类分析非常自然的可以用于离群点检测。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h1&gt;&lt;p&gt;根据维基百科：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在&lt;a href=&quot;https://zh.wikipedia.org/wiki/数据挖掘&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;数据挖掘&lt;/a&gt;中，&lt;strong&gt;异常检测&lt;/strong&gt;（英语：&lt;strong&gt;anomaly detection&lt;/strong&gt;）对不符合预期模式或&lt;a href=&quot;https://zh.wikipedia.org/w/index.php?title=数据集&amp;amp;action=edit&amp;amp;redlink=1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;数据集&lt;/a&gt;中其他项目的项目、事件或观测值的识别。&lt;a href=&quot;https://zh.wikipedia.org/wiki/异常检测#cite_note-ChandolaSurvey-1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; 通常异常项目会转变成&lt;a href=&quot;https://zh.wikipedia.org/w/index.php?title=银行欺诈&amp;amp;action=edit&amp;amp;redlink=1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;银行欺诈&lt;/a&gt;、结构缺陷、医疗问题、文本错误等类型的问题。异常也被称为离群值、新奇、噪声、偏差和例外。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有三大类异常检测方法。&lt;a href=&quot;https://zh.wikipedia.org/wiki/异常检测#cite_note-ChandolaSurvey-1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; 在假设数据集中大多数实例都是正常的前提下，&lt;strong&gt;无监督异常检测&lt;/strong&gt;方法能通过寻找与其他数据最不匹配的实例来检测出未标记测试数据的异常。&lt;strong&gt;监督式异常检测&lt;/strong&gt;方法需要一个已经被标记“正常”与“异常”的数据集，并涉及到训练分类器（与许多其他的&lt;a href=&quot;https://zh.wikipedia.org/wiki/分类问题&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;统计分类问题&lt;/a&gt;的关键区别是异常检测的内在不均衡性）。&lt;strong&gt;半监督式异常检测&lt;/strong&gt;方法根据一个给定的&lt;em&gt;正常&lt;/em&gt;训练数据集创建一个表示正常行为的模型，然后检测由学习模型生成的测试实例的可能性。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Algorithms" scheme="http://haokailong.top/categories/Algorithms/"/>
    
    
      <category term="异常检测" scheme="http://haokailong.top/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
      <category term="anomaly detection" scheme="http://haokailong.top/tags/anomaly-detection/"/>
    
      <category term="deep" scheme="http://haokailong.top/tags/deep/"/>
    
  </entry>
  
  <entry>
    <title>DSRE测试时的batch_size</title>
    <link href="http://haokailong.top/2021/01/10/DSRE%E6%B5%8B%E8%AF%95%E6%97%B6%E7%9A%84batch-size/"/>
    <id>http://haokailong.top/2021/01/10/DSRE测试时的batch-size/</id>
    <published>2021-01-10T03:49:37.000Z</published>
    <updated>2021-01-10T04:47:51.981Z</updated>
    
    <content type="html"><![CDATA[<p>在进行远程监督关系抽取测试阶段，由于每个包中含有的句子数不同，少的只含有1个句子的包有1149个，但是最多的1个包含有138个句子。担心使用BERT测试时，会出现爆显存的问题。</p><h1 id="显存占用"><a href="#显存占用" class="headerlink" title="显存占用"></a>显存占用</h1><p>在评估时batch_size设为8也完全不会超显存，才占用9875MB左右。<br>将batch_size设为64占用16547MB左右显存，<strong>峰值28539MB</strong>，再多可能就超了。</p><h1 id="是否截断？"><a href="#是否截断？" class="headerlink" title="是否截断？"></a>是否截断？</h1><p>测试集中含有1758个包，每个包53个类别，总类别数为：</p><p>$1758 * 53 = 93174$</p><p>是否需要对其截断呢？</p><p>按照DISTRE的方式，截取前50000个，得到的<code>auc</code>值为：0.0290428</p><p>若不截断，得到的<code>auc</code>值为：0.0290428</p><p>可以看到是否截断对<code>auc</code>值的计算还是有影响的，保留的越多，理所应当对应的面积就越大，所以<code>auc</code>值就越高。<strong><em>但是，由于后面数值过小，所以可以忽略不计，也可能对结果没有影响。</em></strong></p><a id="more"></a><h1 id="不同batch-size对auc影响"><a href="#不同batch-size对auc影响" class="headerlink" title="不同batch_size对auc影响"></a>不同batch_size对auc影响</h1><p>当batch_size设为64，<code>auc</code>值为：0.0290429</p><p>当batch_size设为8，  <code>auc</code>值为：0.0292178</p><p>当batch_size设为1，  <code>auc</code>值为：0.0290395</p><font color="orange">为什么不同的batch_size设置会导致计算的`auc`结果不同呢？从理论上讲这个是不合理的。</font>打印logits和labels查看，发现**确实不同**：当batch_size=1:<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.00892705</span> <span class="number">0.00151212</span> <span class="number">0.00814364</span> <span class="number">0.00080609</span> <span class="number">0.00226246</span> <span class="number">0.00350421</span></span><br><span class="line">  <span class="number">0.0011088</span>  <span class="number">0.01335552</span> <span class="number">0.00449597</span> <span class="number">0.00577065</span>]</span><br><span class="line"> [<span class="number">0.00675167</span> <span class="number">0.00236401</span> <span class="number">0.00788459</span> <span class="number">0.0009495</span>  <span class="number">0.00175571</span> <span class="number">0.00618613</span></span><br><span class="line">  <span class="number">0.00157026</span> <span class="number">0.03045319</span> <span class="number">0.00293169</span> <span class="number">0.01477546</span>]</span><br><span class="line"> [<span class="number">0.00760224</span> <span class="number">0.00332207</span> <span class="number">0.00867613</span> <span class="number">0.00068079</span> <span class="number">0.00131133</span> <span class="number">0.00436077</span></span><br><span class="line">  <span class="number">0.00407064</span> <span class="number">0.03283269</span> <span class="number">0.00417252</span> <span class="number">0.00687293</span>]]</span><br></pre></td></tr></table></figure>当batch_size=8:<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.00892705</span> <span class="number">0.00151212</span> <span class="number">0.00814363</span> <span class="number">0.00080609</span> <span class="number">0.00226246</span> <span class="number">0.00350421</span></span><br><span class="line">  <span class="number">0.0011088</span>  <span class="number">0.01335552</span> <span class="number">0.00449597</span> <span class="number">0.00577066</span>]</span><br><span class="line"> [<span class="number">0.00675167</span> <span class="number">0.00236401</span> <span class="number">0.00788459</span> <span class="number">0.0009495</span>  <span class="number">0.00175571</span> <span class="number">0.00618613</span></span><br><span class="line">  <span class="number">0.00157026</span> <span class="number">0.03045319</span> <span class="number">0.00293169</span> <span class="number">0.01477545</span>]</span><br><span class="line"> [<span class="number">0.00675167</span> <span class="number">0.00236401</span> <span class="number">0.00788459</span> <span class="number">0.0009495</span>  <span class="number">0.00175571</span> <span class="number">0.00618613</span></span><br><span class="line">  <span class="number">0.00157026</span> <span class="number">0.03045319</span> <span class="number">0.00293169</span> <span class="number">0.01477545</span>]]</span><br></pre></td></tr></table></figure>但是输入数据是没有shuffle的，打印查看，确实输入顺序保持一致。<font color="red">***估计是BERT中的normalization层对于不同的batch_size会有不同的小的偏置，导致结果的轻微波动！***</font>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在进行远程监督关系抽取测试阶段，由于每个包中含有的句子数不同，少的只含有1个句子的包有1149个，但是最多的1个包含有138个句子。担心使用BERT测试时，会出现爆显存的问题。&lt;/p&gt;
&lt;h1 id=&quot;显存占用&quot;&gt;&lt;a href=&quot;#显存占用&quot; class=&quot;headerlink&quot; title=&quot;显存占用&quot;&gt;&lt;/a&gt;显存占用&lt;/h1&gt;&lt;p&gt;在评估时batch_size设为8也完全不会超显存，才占用9875MB左右。&lt;br&gt;将batch_size设为64占用16547MB左右显存，&lt;strong&gt;峰值28539MB&lt;/strong&gt;，再多可能就超了。&lt;/p&gt;
&lt;h1 id=&quot;是否截断？&quot;&gt;&lt;a href=&quot;#是否截断？&quot; class=&quot;headerlink&quot; title=&quot;是否截断？&quot;&gt;&lt;/a&gt;是否截断？&lt;/h1&gt;&lt;p&gt;测试集中含有1758个包，每个包53个类别，总类别数为：&lt;/p&gt;
&lt;p&gt;$1758 * 53 = 93174$&lt;/p&gt;
&lt;p&gt;是否需要对其截断呢？&lt;/p&gt;
&lt;p&gt;按照DISTRE的方式，截取前50000个，得到的&lt;code&gt;auc&lt;/code&gt;值为：0.0290428&lt;/p&gt;
&lt;p&gt;若不截断，得到的&lt;code&gt;auc&lt;/code&gt;值为：0.0290428&lt;/p&gt;
&lt;p&gt;可以看到是否截断对&lt;code&gt;auc&lt;/code&gt;值的计算还是有影响的，保留的越多，理所应当对应的面积就越大，所以&lt;code&gt;auc&lt;/code&gt;值就越高。&lt;strong&gt;&lt;em&gt;但是，由于后面数值过小，所以可以忽略不计，也可能对结果没有影响。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Information Extraction" scheme="http://haokailong.top/categories/Information-Extraction/"/>
    
      <category term="Relation Extraction" scheme="http://haokailong.top/categories/Information-Extraction/Relation-Extraction/"/>
    
      <category term="Distant Supervision" scheme="http://haokailong.top/categories/Information-Extraction/Relation-Extraction/Distant-Supervision/"/>
    
    
      <category term="relation extraction" scheme="http://haokailong.top/tags/relation-extraction/"/>
    
      <category term="distant supervision" scheme="http://haokailong.top/tags/distant-supervision/"/>
    
      <category term="batch_size" scheme="http://haokailong.top/tags/batch-size/"/>
    
      <category term="batch" scheme="http://haokailong.top/tags/batch/"/>
    
      <category term="auc" scheme="http://haokailong.top/tags/auc/"/>
    
  </entry>
  
  <entry>
    <title>论文题目怎么起？</title>
    <link href="http://haokailong.top/2021/01/09/%E8%AE%BA%E6%96%87%E9%A2%98%E7%9B%AE%E6%80%8E%E4%B9%88%E8%B5%B7%EF%BC%9F/"/>
    <id>http://haokailong.top/2021/01/09/论文题目怎么起？/</id>
    <published>2021-01-09T09:34:32.000Z</published>
    <updated>2021-01-09T10:37:00.764Z</updated>
    
    <content type="html"><![CDATA[<p>最近在写论文准备投稿，但是论文题目还没有想好，该怎么给论文起一个好名字呢？去看了一些相关工作的论文题目，总结一些经验。</p><h1 id="论文题目"><a href="#论文题目" class="headerlink" title="论文题目"></a>论文题目</h1><h2 id="问句式"><a href="#问句式" class="headerlink" title="问句式"></a>问句式</h2><p>Are Noisy Sentences Useless for Distant Supervised Relation Extraction?</p><p>这种可以给人留下较深的印象，在众多的论文中脱颖而出。而且很清晰地表达了论文的核心部分。类似的还有大名鼎鼎的：</p><p>Attention is All You Need</p><h2 id="冒号式"><a href="#冒号式" class="headerlink" title="冒号式"></a>冒号式</h2><p>以冒号将题目分为两部分，前面一部分是方法或模型名；后一部分是针对的任务。比如：</p><p>Uncover the Ground Truth Relations in Distant Supervision: A Neural Expectation-Maximization Framework</p><p>From Bag of Sentences to Document: Distantly Supervised Relation Extraction via Machine Reading Comprehension</p><p>Towards Accurate and Consistent Evaluation: A Dataset for Distantly-Supervised Relation Extraction</p><h2 id="动名词"><a href="#动名词" class="headerlink" title="动名词"></a>动名词</h2><p>使用动词的动名词形式表达使用的主要技术：</p><p>Reducing Wrong Labels in Distant Supervision for Relation Extraction</p><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol><li>用Method, Approach还是Framework?</li><li>要不要在标题中加上False Negative Problems?</li><li>要不要强调Semi-Supervised?</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在写论文准备投稿，但是论文题目还没有想好，该怎么给论文起一个好名字呢？去看了一些相关工作的论文题目，总结一些经验。&lt;/p&gt;
&lt;h1 id=&quot;论文题目&quot;&gt;&lt;a href=&quot;#论文题目&quot; class=&quot;headerlink&quot; title=&quot;论文题目&quot;&gt;&lt;/a&gt;论文题目&lt;/h
      
    
    </summary>
    
    
      <category term="Others" scheme="http://haokailong.top/categories/Others/"/>
    
    
      <category term="paper" scheme="http://haokailong.top/tags/paper/"/>
    
      <category term="title" scheme="http://haokailong.top/tags/title/"/>
    
  </entry>
  
</feed>
