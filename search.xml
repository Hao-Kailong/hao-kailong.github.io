<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>搞懂七和弦</title>
      <link href="/2021/05/30/%E6%90%9E%E6%87%82%E4%B8%83%E5%92%8C%E5%BC%A6/"/>
      <url>/2021/05/30/%E6%90%9E%E6%87%82%E4%B8%83%E5%92%8C%E5%BC%A6/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/blog/2021/seven.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Music </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chord </tag>
            
            <tag> guitar </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BibTeX</title>
      <link href="/2021/05/15/BibTeX/"/>
      <url>/2021/05/15/BibTeX/</url>
      
        <content type="html"><![CDATA[<ul><li>‘’’@article’’’<br>:期刊杂志的论文<br>:<em>必要域: author, title, journal, year.<br>:</em>可选域: volume, number, pages, month, note.<br><em>‘’’@book’’’<br>:公开出版的图书<br>:</em>必要域: author/editor, title, publisher, year.<br>:*可选域: volume/number, series, address, edition, month, note.</li></ul><ul><li>‘’’@booklet’’’<br>:无出版商或作者的图书<br>:<em>必要域: title.<br>:</em>可选域: author, howpublished, address, month, year, note.</li><li>‘’’@conference’’’<br>:等价于 inproceedings<br>:<em> 必要域: author, title, booktitle, year.<br>:</em> 可选域: editor, volume/number, series, pages, address, month, organization, publisher, note.</li><li>‘’’@inbook’’’<br>:书籍的一部分章节<br>:<em> 必要域: author/editor, title, chapter and/or pages, publisher, year.<br>:</em> 可选域: volume/number, series, type, address, edition, month, note.</li><li>‘’’@incollection’’’<br>:书籍中带独立标题的章节<br>:<em> 必要域: author, title, booktitle, publisher, year.<br>:</em> 可选域: editor, volume/number, series, type, chapter, pages, address, edition, month, note.</li><li>‘’’@inproceedings’’’<br>:会议论文集中的一篇<br>:<em>必要域: author, title, booktitle, year.<br>:</em>可选域: editor, volume/number, series, pages, address, month, organization, publisher, note.</li><li>‘’’@manual’’’<br>:技术文档<br>:<em>必要域: title.<br>:</em>可选域: author, organization, address, edition, month, year, note.</li><li>‘’’@mastersthesis’’’<br>:硕士论文<br>:<em>必要域: author, title, school, year.<br>:</em>可选域: type, address, month, note.</li><li>‘’’@misc’’’<br>:其他<br>:<em>必要域: none<br>:</em>可选域: author, title, howpublished, month, year, note.</li><li>‘’’@phdthesis’’’<br>:博士论文<br>:<em>必要域: author, title, year, school.<br>:</em>可选域: address, month, keywords, note.</li><li>‘’’@proceedings’’’<br>:会议论文集<br>:<em> 必要域: title, year.<br>:</em> 可选域: editor, volume/number, series, address, month, organization, publisher, note.</li><li>‘’’@techreport’’’<br>:教育，商业机构的技术报告<br>:<em>必要域: author, title, institution, year.<br>:</em>可选域: type, number, address, month, note.</li><li>‘’’@unpublished’’’<br>:未出版的论文，图书<br>:<em>必要域: author, title, note.<br>:</em>可选域: month, year.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> Bibtex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex算法排版</title>
      <link href="/2021/05/10/Latex%E7%AE%97%E6%B3%95%E6%8E%92%E7%89%88/"/>
      <url>/2021/05/10/Latex%E7%AE%97%E6%B3%95%E6%8E%92%E7%89%88/</url>
      
        <content type="html"><![CDATA[<h2 id="方式一："><a href="#方式一：" class="headerlink" title="方式一："></a>方式一：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">\usepackage[noend]&#123;algpseudocode&#125;</span><br><span class="line">\usepackage&#123;algorithmicx,algorithm&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;algorithm&#125;[t]</span><br><span class="line">\caption&#123;algorithm caption&#125; %算法的名字</span><br><span class="line">\hspace*&#123;0.02in&#125; &#123;\bf Input:&#125; %算法的输入， \hspace*&#123;0.02in&#125;用来控制位置，同时利用 \\ 进行换行</span><br><span class="line">input parameters A, B, C\\</span><br><span class="line">\hspace*&#123;0.02in&#125; &#123;\bf Output:&#125; %算法的结果输出</span><br><span class="line">output result</span><br><span class="line">\begin&#123;algorithmic&#125;[1]</span><br><span class="line">\State some description % \State 后写一般语句</span><br><span class="line">\For&#123;condition&#125; % For 语句，需要和EndFor对应</span><br><span class="line">　　\State ...</span><br><span class="line">　　\If&#123;condition&#125; % If 语句，需要和EndIf对应</span><br><span class="line">　　　　\State ...</span><br><span class="line">　　\Else</span><br><span class="line">　　　　\State ...</span><br><span class="line">　　\EndIf</span><br><span class="line">\EndFor</span><br><span class="line">\While&#123;condition&#125; % While语句，需要和EndWhile对应</span><br><span class="line">　　\State ...</span><br><span class="line">\EndWhile</span><br><span class="line">\State \Return result</span><br><span class="line">\end&#123;algorithmic&#125;</span><br><span class="line">\end&#123;algorithm&#125;</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="/images/blog/2021/algorithm1.png" alt></p><p> <strong>注意</strong></p><p><strong>1. 关键字的大小写问题，否则会出现 Undefined control sequence.</strong></p><p><strong>2. 控制流要前后对应。如果有 While，但没有 EndWhile，否则会出现 Some blocks are not closed。</strong></p><a id="more"></a><h2 id="方式二："><a href="#方式二：" class="headerlink" title="方式二："></a>方式二：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">\usepackage[ruled]&#123;algorithm2e&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;algorithm&#125;[H]</span><br><span class="line">\caption&#123;algorithm caption&#125;%算法名字</span><br><span class="line">\LinesNumbered %要求显示行号</span><br><span class="line">\KwIn&#123;input parameters A, B, C&#125;%输入参数</span><br><span class="line">\KwOut&#123;output result&#125;%输出</span><br><span class="line">some description\; %\;用于换行</span><br><span class="line">\For&#123;condition&#125;&#123;</span><br><span class="line">　　only if\;</span><br><span class="line">　　\If&#123;condition&#125;&#123;</span><br><span class="line">　　　　1\;</span><br><span class="line">　　&#125;</span><br><span class="line">&#125;</span><br><span class="line">\While&#123;not at end of this document&#125;&#123;</span><br><span class="line">　　if and else\;</span><br><span class="line">　　\eIf&#123;condition&#125;&#123;</span><br><span class="line">　　　　1\;</span><br><span class="line">　　&#125;&#123;</span><br><span class="line">　　　　2\;</span><br><span class="line">　　&#125;</span><br><span class="line">&#125;</span><br><span class="line">\ForEach&#123;condition&#125;&#123;</span><br><span class="line">　　\If&#123;condition&#125;&#123;</span><br><span class="line">　　　　1\;</span><br><span class="line">　　&#125;</span><br><span class="line">&#125;</span><br><span class="line">\end&#123;algorithm&#125;</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="/images/blog/2021/algorithm2.png" alt></p><h2 id="方式三："><a href="#方式三：" class="headerlink" title="方式三："></a>方式三：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\usepackage[ruled,vlined]&#123;algorithm2e&#125;</span><br><span class="line"></span><br><span class="line">%...同方式二</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="/images/blog/2021/algorithm3.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex中公式过长</title>
      <link href="/2021/04/27/Latex%E4%B8%AD%E5%85%AC%E5%BC%8F%E8%BF%87%E9%95%BF/"/>
      <url>/2021/04/27/Latex%E4%B8%AD%E5%85%AC%E5%BC%8F%E8%BF%87%E9%95%BF/</url>
      
        <content type="html"><![CDATA[<p>Latex中公式过长，解决方法主要有以下几种：</p><h2 id="解决方案1"><a href="#解决方案1" class="headerlink" title="解决方案1"></a>解决方案1</h2><p>使用amsmath package的split环境</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">F = \&#123;F_&#123;x&#125; \in  F_&#123;c&#125; &amp;: (|S| &gt; |C|) \\</span><br><span class="line">&amp;\quad \cap (\text&#123;minPixels&#125; &lt; |S| &lt; \text&#123;maxPixels&#125;) \\</span><br><span class="line">&amp;\quad \cap (|S_&#123;\text&#123;conected&#125;&#125;| &gt; |S| - \epsilon) \&#125;</span><br><span class="line">\end&#123;split&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure><h2 id="解决方案2"><a href="#解决方案2" class="headerlink" title="解决方案2"></a>解决方案2</h2><p>使用 \! 命令，在等号、加号等两侧进行微调</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;eqnarray&#125;</span><br><span class="line">\dot&#123;x&#125;(t)\！=\！\bar&#123;A&#125;_&#123;i&#125;x(t)+\bar&#123;B&#125;_&#123;i_&#123;1&#125;&#125;x(t)+\bar&#123;B&#125;_&#123;i_&#123;2&#125;&#125;x(t)+\bar&#123;B&#125;_&#123;i_&#123;3&#125;&#125;[a_&#123;i&#125;(t)\！+\！b_&#123;i&#125;(t)].</span><br><span class="line">\end&#123;eqnarray&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="解决方案3"><a href="#解决方案3" class="headerlink" title="解决方案3"></a>解决方案3</h2><p>缩小公式：</p><ul><li>编号不缩小</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\resizebox&#123;.9\hsize&#125;&#123;!&#125;&#123;$A+B+C+D+E+F+G+H+I+J+K+L+M+N+O+P+Q+R+S+T+U+V+W+X+Y+Z$&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure><ul><li>编号缩小</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;small&#125;</span><br><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\ldots</span><br><span class="line">\end&#123;equation&#125;</span><br><span class="line">\end&#123;small&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> LaTeX </tag>
            
            <tag> equation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LaTeX在双栏模式下插入跨栏图表</title>
      <link href="/2021/04/27/LaTeX%E5%9C%A8%E5%8F%8C%E6%A0%8F%E6%A8%A1%E5%BC%8F%E4%B8%8B%E6%8F%92%E5%85%A5%E8%B7%A8%E6%A0%8F%E5%9B%BE%E8%A1%A8/"/>
      <url>/2021/04/27/LaTeX%E5%9C%A8%E5%8F%8C%E6%A0%8F%E6%A8%A1%E5%BC%8F%E4%B8%8B%E6%8F%92%E5%85%A5%E8%B7%A8%E6%A0%8F%E5%9B%BE%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>Latex中插入图片的命令是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure&#125;</span><br><span class="line">\centering</span><br><span class="line">\includegraphics[width=8cm]&#123;picture.png&#125;</span><br><span class="line">\caption&#123;this is a picture&#125;</span><br><span class="line">\label&#123;fig:picture01&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><p>在双栏编辑模式下，图片只能在一栏中显示，而且如果图片的宽度超过单栏文本宽度，则只能显示其中一部分，剩下的部分会”溢出“。</p><p><strong>在双栏模式下插入跨栏图，方法其实很简单，将环境改为带*即可</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure*&#125;</span><br><span class="line">...</span><br><span class="line">\end&#123;figure*&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> LaTeX </tag>
            
            <tag> figure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Failed to initialize NVML</title>
      <link href="/2021/04/24/Failed-to-initialize-NVML/"/>
      <url>/2021/04/24/Failed-to-initialize-NVML/</url>
      
        <content type="html"><![CDATA[<p>在使用<code>nvidia-smi</code>命令时报错，</p><blockquote><p>Failed to initialize NVML: Driver/library version mismatch</p></blockquote><p>原因：</p><p>Nvidia 驱动器内核版本错误</p><p>解决方案：</p><ol><li><p><code>lsmod | grep nvidia</code></p></li><li><p>执行以下命令：</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo rmmod nvidia_drm</span><br><span class="line">sudo rmmod nvidia_modeset</span><br><span class="line">sudo rmmod nvidia_uvm</span><br><span class="line">sudo rmmod nvidia</span><br></pre></td></tr></table></figure><p>问题解决！</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nvidia-smi </tag>
            
            <tag> NVIDIA </tag>
            
            <tag> NVML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解Adam</title>
      <link href="/2021/04/23/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Adam/"/>
      <url>/2021/04/23/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Adam/</url>
      
        <content type="html"><![CDATA[<p>基于随机梯度下降（SGD）的优化算法在科研和工程的很多领域里都是极其核心的。很多理论或工程问题都可以转化为对目标函数进行最小化的数学问题。</p><p>按吴恩达老师所说的，梯度下降（Gradient Descent）就好比一个人想从高山上奔跑到山谷最低点，用最快的方式（steepest）奔向最低的位置（minimum）。</p><p><strong>SGD基本公式</strong></p><p>$\theta = \theta - \eta \cdot \nabla_{\theta} J (\theta; x;y) $</p><p><strong>动量(Momentum)</strong></p><p>$v<em>t = \gamma v</em>{t-1} + \eta \nabla_{\theta} J(\theta)$</p><p>$\theta = \theta - v_t$</p><p>基本的mini-batch SGD优化算法在深度学习取得很多不错的成绩。然而也存在一些问题需解决：</p><ol><li><p>选择恰当的初始学习率很困难。</p></li><li><p>学习率调整策略受限于预先指定的调整规则。</p></li><li><p>相同的学习率被应用于各个参数。</p></li><li><p>高度非凸的误差函数的优化过程，如何避免陷入大量的局部次优解或鞍点。</p></li></ol><a id="more"></a><h2 id="自适应优化"><a href="#自适应优化" class="headerlink" title="自适应优化"></a>自适应优化</h2><p>针对简单的SGD及Momentum存在的问题，2011年<strong>John Duchi</strong>等发布了AdaGrad优化算法(Adaptive Gradient，自适应梯度)，它能够对每个不同的参数调整不同的学习率，对频繁变化的参数以更小的步长进行更新，而稀疏的参数以更大的步长进行更新。</p><p>公式：</p><script type="math/tex; mode=display">g_t = \nabla_\theta J(\theta_{t-1}) \\\theta_{t+1} = \theta_{t} - \alpha \cdot g_t / \sqrt{\sum_{i=1}^tg_{t,i}^2}</script><p>gt表示第t时间步的梯度（向量，包含各个参数对应的偏导数，gt,i表示第i个参数t时刻偏导数）</p><p>gt2表示第t时间步的梯度平方（向量，由gt各元素自己进行平方运算所得，即Element-wise）</p><p>与SGD的核心区别在于计算更新步长时，增加了分母：<strong>梯度平方累积和的平方根</strong>。此项能够累积各个参数gt,i的历史梯度平方，频繁更新的梯度，则累积的分母项逐渐偏大，那么更新的步长(stepsize)相对就会变小，而稀疏的梯度，则导致累积的分母项中对应值比较小，那么更新的步长则相对比较大。</p><p>AdaGrad能够自动为不同参数适应不同的学习率（平方根的分母项相当于对学习率α进进行了自动调整，然后再乘以本次梯度），大多数的框架实现采用默认学习率α=0.01即可完成比较好的收敛。</p><p>优势：在数据分布稀疏的场景，能更好利用稀疏梯度的信息，比标准的SGD算法更有效地收敛。</p><p>缺点：主要缺陷来自分母项的对梯度平方不断累积，随之时间步地增加，分母项越来越大，最终导致学习率收缩到太小无法进行有效更新。</p><h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>RMSProp是Geoffrey Hinton教授在教案中提到的算法，结合梯度平方的指数移动平均数来调节学习率的变化。能够在不稳定（Non-Stationary）的目标函数情况下进行很好地收敛。</p><p>Hinton教授讲述RMSProp算法的材料：</p><p><a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a></p><p>计算t时间步的梯度：</p><script type="math/tex; mode=display">g_t = \nabla_\theta J(\theta_{t-1})</script><p>计算梯度平方的指数移动平均数（Exponential Moving Average），γ是遗忘因子（或称为指数衰减率），依据经验，默认设置为0.9。</p><script type="math/tex; mode=display">v_t = \gamma(v_{t-1}) + (1-\gamma)g_t^2</script><p>梯度更新时候，与AdaGrad类似，只是更新的梯度平方的期望（指数移动均值），其中ε=10^-8，避免除数为0。默认学习率α=0.001。</p><script type="math/tex; mode=display">\theta_t = \theta_{t-1} - \alpha \cdot g_t / (\sqrt{v_t} + \epsilon)</script><p>优势：能够克服AdaGrad梯度急剧减小的问题，在很多应用中都展示出优秀的学习率自适应能力。尤其在不稳定(Non-Stationary)的目标函数下，比基本的SGD、Momentum、AdaGrad表现更良好。</p><h2 id="Adam优化器"><a href="#Adam优化器" class="headerlink" title="Adam优化器"></a>Adam优化器</h2><p>2014年12月，Kingma和Lei Ba两位学者提出了Adam优化器，结合AdaGrad和RMSProp两种优化算法的优点。对梯度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second</p><p>Moment Estimation，即梯度的未中心化的方差）进行综合考虑，计算出更新步长。</p><p>主要包含以下几个显著的优点：</p><ol><li><p>实现简单，计算高效，对内存需求少</p></li><li><p>参数的更新不受梯度的伸缩变换影响</p></li><li><p>超参数具有很好的解释性，且通常无需调整或仅需很少的微调</p></li><li><p>更新的步长能够被限制在大致的范围内（初始学习率）</p></li><li><p>能自然地实现步长退火过程（自动调整学习率）</p></li><li><p>很适合应用于大规模的数据及参数的场景</p></li><li><p>适用于不稳定目标函数</p></li><li><p>适用于梯度稀疏或梯度存在很大噪声的问题</p></li></ol><p>综合Adam在很多情况下算作默认工作性能比较优秀的优化器。</p><h4 id="Adam实现原理"><a href="#Adam实现原理" class="headerlink" title="Adam实现原理"></a>Adam实现原理</h4><p>算法伪代码：</p><p><img src="https://upload-images.jianshu.io/upload_images/10046814-c2db68e06531e759.png?imageMogr2/auto-orient/strip|imageView2/2/w/897/format/webp" alt></p><h4 id="Adam更新规则"><a href="#Adam更新规则" class="headerlink" title="Adam更新规则"></a>Adam更新规则</h4><p>计算t时间步的梯度：</p><script type="math/tex; mode=display">g_t = \nabla_\theta J(\theta_{t-1})</script><p>首先，计算梯度的指数移动平均数，m0 初始化为0。</p><p>类似于Momentum算法，综合考虑之前时间步的梯度动量。</p><p>β1 系数为指数衰减率，控制权重分配（动量与当前梯度），通常取接近于1的值。</p><p>默认为0.9</p><script type="math/tex; mode=display">m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t</script><p>下图简单展示出时间步1~20时，各个时间步的梯度随着时间的累积占比情况。</p><p><img src="https://upload-images.jianshu.io/upload_images/10046814-ee46996a8c36bdc7.png?imageMogr2/auto-orient/strip|imageView2/2/w/1048/format/webp" alt></p><p>其次，计算梯度平方的指数移动平均数，v0初始化为0。</p><p>β2 系数为指数衰减率，控制之前的梯度平方的影响情况。</p><p>类似于RMSProp算法，对梯度平方进行加权均值。</p><p>默认为0.999</p><script type="math/tex; mode=display">v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2</script><p>第三，由于m0初始化为0，会导致mt偏向于0，尤其在训练初期阶段。</p><p>所以，此处需要对梯度均值mt进行偏差纠正，降低偏差对训练初期的影响。</p><script type="math/tex; mode=display">\hat{m_t} = m_t / (1-\beta_1^t)</script><p>第四，与m0 类似，因为v0初始化为0导致训练初始阶段vt偏向0，对其进行纠正。</p><script type="math/tex; mode=display">\hat{v_t} = v_t / (1 - \beta_2^t)</script><p>第五，更新参数，初始的学习率α乘以梯度均值与梯度方差的平方根之比。</p><p>其中默认学习率α=0.001</p><p>ε=10^-8，避免除数变为0。</p><p>由表达式可以看出，对更新的步长计算，能够从梯度均值及梯度平方两个角度进行自适应地调节，而不是直接由当前梯度决定。</p><script type="math/tex; mode=display">\theta_t = \theta_{t-1} - \alpha \cdot \hat{m_t} / (\sqrt{\hat{v_t}} + \epsilon)</script><p>代码地址：<a href="https://link.jianshu.com/?t=https%3A%2F%2Fgithub.com%2Fdream-catcher%2Flearning_blogs%2Fblob%2Fmaster%2FAdam_Optimizer%2Fadam_optimizer.py" target="_blank" rel="noopener">https://github.com/dream-catcher/learning_blogs/blob/master/Adam_Optimizer/adam_optimizer.py</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/aebcaf8af76e" target="_blank" rel="noopener">简单认识Adam优化器 </a></p><p><a href="http://ruder.io/optimizing-gradient-descent/" target="_blank" rel="noopener">http://ruder.io/optimizing-gradient-descent/</a></p><p><a href="http://ruder.io/deep-learning-optimization-2017/index.html" target="_blank" rel="noopener">http://ruder.io/deep-learning-optimization-2017/index.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SGD </tag>
            
            <tag> Adam </tag>
            
            <tag> Momentum </tag>
            
            <tag> AdaGrad </tag>
            
            <tag> RMSProp </tag>
            
            <tag> optimizer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LSGAN-最小二乘GAN</title>
      <link href="/2021/04/21/LSGAN-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98GAN/"/>
      <url>/2021/04/21/LSGAN-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98GAN/</url>
      
        <content type="html"><![CDATA[<h1 id="LSGAN原理"><a href="#LSGAN原理" class="headerlink" title="LSGAN原理"></a>LSGAN原理</h1><p>GAN是以对抗的方式逼近概率分布。但是直接使用该方法，<strong>会随着判别器越来越好而生成器无法与其对抗，进而形成梯度消失的问题。所以不论是WGAN，还是本节中的LSGAN，都是试图使用不同的距离度量（loss值）</strong>，从而构建一个不仅稳定，同时还收敛迅速的生成对抗网络。</p><p>WGAN使用的是Wasserstein理论来构建度量距离。而<strong>LSGAN使用了另一个方法，即使用了更加平滑和非饱和梯度的损失函数——最小二乘来代替原来的Sigmoid交叉熵</strong>。这是由于L2正则独有的特性，在数据偏离目标时会有一个与其偏离距离成比例的惩罚，再将其拉回来，从而使数据的偏离不会越来越远。</p><p>相对于WGAN而言，LSGAN的loss简单很多。直接将传统的GAN中的loss变为平方差即可。</p><blockquote><p> <mark>即LSGan的核心就是损失函数变为D的loss是真实样本和1作差的平方+模拟样本和0作差的平方；G的loss是模拟样本和1作差的平方（L2正则化）</mark></p><p><mark> 而WGan的核心是损失函数变为真实值和虚拟值的差（L1正则化）</mark></p><p><mark> 原始GAN的损失函数是D的loss都是真实样本和1作交叉熵，模拟样本和0作交叉熵；G的loss是模拟样本和1作交叉熵。（交叉熵）</mark></p></blockquote><p>判别器的损失：</p><p>$\mathrm{D<em>{loss}} = \mathrm{tf.reduce_sum(\mathrm{tf.square(\mathrm{D(X</em>{real})-1)}}} + \mathrm{tf.square(\mathrm{D}(X_{fake}))})/2$</p><p>生成器的损失：</p><p>$\mathrm{G<em>{loss}} = \mathrm{tf.reduce_sum} ( \mathrm{tf.square(\mathrm{D}(X</em>{fake}})))/2$</p><p> 为什么要除以2？和以前的原理一样，在对平方求导时会得到一个系数2，与事先的1/2运算正好等于1。</p><a id="more"></a><h1 id="知识点-皮尔逊卡方检验"><a href="#知识点-皮尔逊卡方检验" class="headerlink" title="知识点 - 皮尔逊卡方检验"></a>知识点 - 皮尔逊卡方检验</h1><p>$\mathcal{X}^2$拟合优度检验的主要思想是这样的，针对每个指定的值，理论计算出来的频率与实际收集到的数据统计出来的频率之间总是存在一些偏差，把每一个指定值的偏差以平方的形式加起来，如果这个值比较小，则说明分布拟合得较好，如果这个值很大，则说明实际收集到的数据与目标分布并不相同，需要去寻找其它恰当的分布。</p><p>为此，卡尔·皮尔逊引入了一个重要的统计量——$\mathcal{X} ^2$统计量。</p><script type="math/tex; mode=display">\mathcal{X}^2 = \sum_{i=1}^n \frac{(f_i - F_i) ^2}{F_i}</script><p>其中，$f_i$是实际观察到的量，$F_i$是运用目标分布计算出的量。卡尔-皮尔逊证明了当样本大小趋于无穷时，统计量收敛于$\mathcal{X}^2$分布。</p><p>更多细节可以参考：</p><p><a href="https://zhuanlan.zhihu.com/p/25165318" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25165318</a></p>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
          <category> GAN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> Adversarial </tag>
            
            <tag> Generative </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>面试总结-腾讯、百度、字节</title>
      <link href="/2021/04/13/%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93-%E8%85%BE%E8%AE%AF%E3%80%81%E7%99%BE%E5%BA%A6%E3%80%81%E5%AD%97%E8%8A%82/"/>
      <url>/2021/04/13/%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93-%E8%85%BE%E8%AE%AF%E3%80%81%E7%99%BE%E5%BA%A6%E3%80%81%E5%AD%97%E8%8A%82/</url>
      
        <content type="html"><![CDATA[<h1 id="腾讯"><a href="#腾讯" class="headerlink" title="腾讯"></a>腾讯</h1><p>知识蒸馏：</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/90049906" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/90049906</a></p></blockquote><p>置信度学习：</p><blockquote><p>如何判断样本标注的靠谱程度？置信度学习 (CL) 简述</p><p><a href="https://www.pianshen.com/article/19761107982/" target="_blank" rel="noopener">https://www.pianshen.com/article/19761107982/</a></p></blockquote><p>主动学习：</p><blockquote><p>什么是主动学习？(Active Learning)</p><p><a href="https://www.zhihu.com/question/352299820" target="_blank" rel="noopener">https://www.zhihu.com/question/352299820</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 百度 </tag>
            
            <tag> Baidu </tag>
            
            <tag> 腾讯 </tag>
            
            <tag> Tencent </tag>
            
            <tag> 字节跳动 </tag>
            
            <tag> ByteDance </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于K-Means的异常值检测</title>
      <link href="/2021/04/13/%E5%9F%BA%E4%BA%8EK-Means%E7%9A%84%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B/"/>
      <url>/2021/04/13/%E5%9F%BA%E4%BA%8EK-Means%E7%9A%84%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用kmeans算法聚类消费行为特征数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#参数可视化</span></span><br><span class="line">inputfile = <span class="string">''</span> <span class="comment">#销量及其他属性数据</span></span><br><span class="line">k = <span class="number">3</span>  <span class="comment">#聚类类别</span></span><br><span class="line">threshold = <span class="number">2</span> <span class="comment">#离群点阈值</span></span><br><span class="line">iteration = <span class="number">500</span> <span class="comment">#聚类最大循环次数</span></span><br><span class="line">data = pd.read_excel(inputfile,index_col=<span class="string">"Id"</span>) <span class="comment">#读取数据</span></span><br><span class="line">data_zs = <span class="number">1.0</span>*(data-data.mean())/data.std() <span class="comment">#数据标准化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">model = KMeans(n_clusters=k,n_jobs=<span class="number">4</span>,max_iter=iteration) <span class="comment">#分为k类，并发数4</span></span><br><span class="line">model.fit(data_zs) <span class="comment">#开始聚类</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#标准化数据及其类别</span></span><br><span class="line">r = pd.concat([data_zs,pd.Series(model.labels_,index=data.index)],axis=<span class="number">1</span>)</span><br><span class="line">r.columns = list(data.columns)+[<span class="string">"聚类类别"</span>] <span class="comment">#重命名表头</span></span><br><span class="line"></span><br><span class="line">norm = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):<span class="comment">#逐一处理</span></span><br><span class="line">    norm_tmp =r[[<span class="string">'R'</span>,<span class="string">'F'</span>,<span class="string">'M'</span>]][r[<span class="string">'聚类类别'</span>]==i]-model.cluster_centers_[i]</span><br><span class="line">    norm_tmp = norm_tmp.apply(np.linalg.norm,axis=<span class="number">1</span>) <span class="comment">#求出绝对距离</span></span><br><span class="line">    norm.append(norm_tmp/norm_tmp.median()) <span class="comment">#求相对距离并添加</span></span><br><span class="line"></span><br><span class="line">norm = pd.concat(norm) <span class="comment">#合并</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>] <span class="comment">#用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span> <span class="comment">#用来正常显示负号</span></span><br><span class="line">norm[norm &lt;= threshold].plot(style = <span class="string">'go'</span>) <span class="comment">#正常点</span></span><br><span class="line"></span><br><span class="line">discrete_points = norm[norm&gt;threshold] <span class="comment">#离群点</span></span><br><span class="line">discrete_points.plot(style=<span class="string">'ro'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(discrete_points)): <span class="comment">#离群点做标记</span></span><br><span class="line">  id = discrete_points.index[i]</span><br><span class="line">  n = discrete_points.iloc[i]</span><br><span class="line">  plt.annotate(<span class="string">'(%s, %0.2f)'</span>%(id, n), xy = (id, n), xytext = (id, n))</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">u'编号'</span>)</span><br><span class="line">plt.ylabel(<span class="string">u'相对距离'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> K-Means </tag>
            
            <tag> 异常值检测 </tag>
            
            <tag> outliers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>剑指offer-丑数</title>
      <link href="/2021/04/11/%E5%89%91%E6%8C%87offer-%E4%B8%91%E6%95%B0/"/>
      <url>/2021/04/11/%E5%89%91%E6%8C%87offer-%E4%B8%91%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">GetUglyNumber_Solution</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index &lt; <span class="number">7</span>)<span class="keyword">return</span> index;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(index);</span><br><span class="line">        res[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> t2 = <span class="number">0</span>, t3 = <span class="number">0</span>, t5 = <span class="number">0</span>, i;</span><br><span class="line">        <span class="keyword">for</span> (i = <span class="number">1</span>; i &lt; index; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            res[i] = min(res[t2] * <span class="number">2</span>, min(res[t3] * <span class="number">3</span>, res[t5] * <span class="number">5</span>));</span><br><span class="line">            <span class="keyword">if</span> (res[i] == res[t2] * <span class="number">2</span>)t2++;</span><br><span class="line">            <span class="keyword">if</span> (res[i] == res[t3] * <span class="number">3</span>)t3++;</span><br><span class="line">            <span class="keyword">if</span> (res[i] == res[t5] * <span class="number">5</span>)t5++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res[index - <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Key Point: <strong>每一个丑数都是由之前的丑数乘二、乘三、乘五得到的</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> LeetCode </tag>
            
            <tag> 剑指offer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>形象理解进程与线程的区别</title>
      <link href="/2021/04/10/%E5%BD%A2%E8%B1%A1%E7%90%86%E8%A7%A3%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2021/04/10/%E5%BD%A2%E8%B1%A1%E7%90%86%E8%A7%A3%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在知乎上看到了一个大佬的答案，感觉很形象：</p><p><a href="https://www.zhihu.com/question/25532384/answer/81152571" target="_blank" rel="noopener">https://www.zhihu.com/question/25532384/answer/81152571</a></p><p><a href="http://biaodianfu.com" target="_blank" rel="noopener">biaodianfu</a>的回答：</p></blockquote><p>看了一遍排在前面的答案，类似”<strong>进程是资源分配的最小单位，线程是CPU调度的最小单位“</strong>这样的回答感觉太抽象，都不太容易让人理解。</p><p>做个简单的比喻：进程=火车，线程=车厢</p><ul><li>线程在进程下行进（单纯的车厢无法运行）</li><li>一个进程可以包含多个线程（一辆火车可以有多个车厢）</li><li>不同进程间数据很难共享（一辆火车上的乘客很难换到另外一辆火车，比如站点换乘）</li><li>同一进程下不同线程间数据很易共享（A车厢换到B车厢很容易）</li><li>进程要比线程消耗更多的计算机资源（采用多列火车相比多个车厢更耗资源）</li><li>进程间不会相互影响，一个线程挂掉将导致整个进程挂掉（一列火车不会影响到另外一列火车，但是如果一列火车上中间的一节车厢着火了，将影响到所有车厢）</li><li>进程可以拓展到多机，线程最多扩展到多核CPU，而不能扩展到多机（不同火车可以开在多个轨道上，同一火车的车厢不能在行进的不同的轨道上）</li><li>进程使用的内存地址可以上锁，即一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。（比如火车上的洗手间）－”互斥锁”</li><li>进程使用的内存地址可以限定使用量（比如火车上的餐厅，最多只允许多少人进入，如果满了需要在门口等，等有人出来了才能进去）－“信号量”</li></ul>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 进程 </tag>
            
            <tag> 线程 </tag>
            
            <tag> 操作系统 </tag>
            
            <tag> OS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度2022春招面经</title>
      <link href="/2021/04/10/%E7%99%BE%E5%BA%A62022%E6%98%A5%E6%8B%9B%E9%9D%A2%E7%BB%8F/"/>
      <url>/2021/04/10/%E7%99%BE%E5%BA%A62022%E6%98%A5%E6%8B%9B%E9%9D%A2%E7%BB%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><ul><li>介绍简历项目</li><li>在项目中遇到了什么困难？</li><li>算法题：有8升、3升、5升的水杯，8升满，其余空，如何得到4升水？<ul><li>8升    3升    5升</li><li>8    0    0</li><li>3    0    5</li><li>3    3    2</li><li>6    0    2</li><li>1    2    5</li><li>1    3    4</li></ul></li></ul><h1 id="二面"><a href="#二面" class="headerlink" title="二面"></a>二面</h1><a id="more"></a><ul><li>给定下述代码，提出存在的问题以及优化方法</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TONUM(x) x - <span class="meta-string">'0'</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">matoi</span><span class="params">(<span class="keyword">char</span> *p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> res;</span><br><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"><span class="keyword">for</span> (i=<span class="number">0</span>;i&lt;<span class="built_in">strlen</span>(p);i++)</span><br><span class="line">&#123;</span><br><span class="line">res = res * <span class="number">10</span> + TONUM(p[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>存在的问题：</p><ol><li><code>res</code>没有初始化</li><li>没有<code>#include&lt;string.h&gt;</code></li><li>可能存在<code>int</code>型溢出</li></ol><p>优化：</p><ol><li><code>res</code>初始化为零</li><li><code>#include&lt;string.h&gt;</code></li><li>在<code>res</code>更新前进行溢出判断，是否超出<code>2147483647</code></li><li>前置零可直接跳过，提高效率</li><li><code>n=strlen(p)</code>，避免重复调用</li></ol><ul><li>共享单车如何找到最近的车辆？对问题建模。</li></ul><p>建模为二维坐标，在数据库中按照横纵轴从小到大的顺序排列，然后二分查找。可以分区存储，将各个城市存到不同的表格中，减小搜索空间。</p><ul><li>都知道哪些排序算法？快排简单介绍。</li><li>进程与线程的区别？</li><li>进程间的通信方法？</li></ul><h1 id="三面"><a href="#三面" class="headerlink" title="三面"></a>三面</h1><ul><li>自己的兴趣爱好？</li><li>简历上的论文详细介绍</li><li>实习想要做什么方向？</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>面试官都挺好的，友善轻松的氛围。面试体验不错~希望自己顺利拿到offer~</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 百度 </tag>
            
            <tag> Baidu </tag>
            
            <tag> interview </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DivideMix: Learning with Noisy Labels as Semi-supervised Learning</title>
      <link href="/2021/04/09/DivideMix-Learning-with-Noisy-Labels-as-Semi-supervised-Learning/"/>
      <url>/2021/04/09/DivideMix-Learning-with-Noisy-Labels-as-Semi-supervised-Learning/</url>
      
        <content type="html"><![CDATA[<h1 id="1-任务"><a href="#1-任务" class="headerlink" title="1. 任务"></a>1. 任务</h1><p>提升模型在远程监督数据（含噪音标签的数据）建模中的性能</p><p>远程监督：Distant Supervision，将已有的知识库对应到丰富的非结构化数据中，从而生成大量的训练数据</p><h1 id="2-背景"><a href="#2-背景" class="headerlink" title="2. 背景"></a>2. 背景</h1><p>模型发展：深度神经网络（ DNNs ）结构复杂性，需要更多的数据来拟合</p><p>高质量的标注数据耗时、代价昂贵</p><p>远程监督方法获取数据标签不纯净</p><p>有研究表明，DNNs 可轻易地过拟合噪音标签，导致模型泛化性能差</p><h1 id="3-噪音标签学习研究现状"><a href="#3-噪音标签学习研究现状" class="headerlink" title="3. 噪音标签学习研究现状"></a>3. 噪音标签学习研究现状</h1><p>Learning with Noisy Labels (LNL) 主流方法：损失校正方法</p><ul><li>噪音转移矩阵：利用 DNNs 的预测结果修正。<em>高噪音比</em>时效果差，DNNs 的预测结果受噪音影响大</li></ul><ul><li>Reweight 噪音数据，使其损失占比小。一般认为损失小的为干净样本</li></ul><p>半监督学习（SSL）</p><ul><li>数据：labeled and unlabeled data</li><li>旨在通过利用未标注数据提升模型的性能</li><li>MixMatch：一个框架中统一几种先进SSL方法</li></ul><h1 id="4-DivideMix"><a href="#4-DivideMix" class="headerlink" title="4. DivideMix"></a>4. DivideMix</h1><p><img src="https://pic4.zhimg.com/80/v2-12cf60095f2bbad803c2b60e7437e2bf_720w.jpg" alt></p><a id="more"></a><p><img src="https://pic1.zhimg.com/80/v2-86ae8211b4b26dd847f34c04b034aa1c_720w.jpg" alt></p><p>为了避免在self-traning过程中的<strong>确认偏差</strong>导致的误差累积，同时训练两个网络去过滤彼此的错误：Epoch-level隐式的teaching &amp; Batch-level显式的teaching</p><p><strong>Co-Divide</strong></p><p>DNNs学习干净的数据比噪音数据快，干净数据低损失</p><p>使用交叉熵损失函数来计算每个样本的损失：</p><p>$\mathcal{l} (\theta) = {-\sum<em>{c=1}^{C} y_i^c\log(\mathrm{p}</em>{model}^c(x<em>i;\theta))}</em>{i=1}^N$</p><h2 id="4-1-高斯混合模型"><a href="#4-1-高斯混合模型" class="headerlink" title="4.1. 高斯混合模型"></a>4.1. 高斯混合模型</h2><p><strong>高斯混合模型</strong>（Gaussian Mixture Model ，GMM），由于其在分布锐度上的灵活性较好，能够更好地区分干净和噪音样本，所以本文使用 GMM，<strong>在每个样本的损失分布上动态拟合高斯混合模型</strong>：$wi =p(g|l_i)$ . 通过在$w_i$上设置阈值t，将数据分为干净数据（标注数据）和噪音数据（未标注数据），用于后续 SSL</p><p>A、B网络由于不同的(随机)参数初始化、不同的训练数据划分、不同的(随机)小批序列以及不同的训练目标，使得两个网络保持了一定的离散性，使得两种网络具有不同的过滤不同类型错误的能力，使得模型对噪声的鲁棒性更强</p><h2 id="4-2-非对称噪音置信度惩罚"><a href="#4-2-非对称噪音置信度惩罚" class="headerlink" title="4.2. 非对称噪音置信度惩罚"></a>4.2. 非对称噪音置信度惩罚</h2><p>为了模型的收敛，起始几个epoch会用全部数据训练，即warm-up,对在均匀随机的对称噪音数据是有效的，但是本文讨论的数据噪音是不对（类条件）称的，因此在warm-up的过程中会快速的拟合噪音，产生低熵预测，导致GMM不能有效的区分干净和噪音数据，本文添加一个负熵项惩罚置信预测，使其产生的预测能后产生低熵损失，有利于产生高置信度的标签</p><p>$\mathcal{H} = -\sum<em>c \mathrm{p}</em>{model}^c(x;\theta) \log (\mathrm{p}_{model}^c(x;\theta))$</p><p><img src="https://pic1.zhimg.com/80/v2-674851389a1a8776cda043f2198a95bc_720w.jpg" alt></p><h1 id="5-评价"><a href="#5-评价" class="headerlink" title="5. 评价"></a>5. 评价</h1><blockquote><p>Co-teaching的思路+MixMatch缝合出来的东西。唯一觉得有意思的地方就是问题设定的强制转换。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Noisy Label </category>
          
      </categories>
      
      
        <tags>
            
            <tag> distant supervision </tag>
            
            <tag> noisy label </tag>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>递减栈</title>
      <link href="/2021/04/08/%E9%80%92%E5%87%8F%E6%A0%88/"/>
      <url>/2021/04/08/%E9%80%92%E5%87%8F%E6%A0%88/</url>
      
        <content type="html"><![CDATA[<h1 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h1><p>定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; Sdata;</span><br><span class="line"><span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; Smin;  <span class="comment">// 递减栈，保留当前最小值 </span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> value)</span> </span>&#123;</span><br><span class="line">        Sdata.push(value);</span><br><span class="line">        <span class="keyword">if</span> (Smin.empty() || value &lt;= Smin.top())  <span class="comment">// 注意小于等于符号，重复元素 </span></span><br><span class="line">        Smin.push(value);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!Smin.empty() &amp;&amp; Sdata.top() &lt;= Smin.top())</span><br><span class="line">        Smin.pop();</span><br><span class="line">        Sdata.pop();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">top</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Sdata.top();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">min</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Smin.top();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>递增栈，递减栈，递减队列在刷题时多次遇到，需要格外注意。</p><p>其实有种DP的感觉在里面，需要领会👓</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> LeetCode </tag>
            
            <tag> 剑指offer </tag>
            
            <tag> Stack </tag>
            
            <tag> 栈 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++运算符优先级</title>
      <link href="/2021/04/08/C-%E8%BF%90%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E7%BA%A7/"/>
      <url>/2021/04/08/C-%E8%BF%90%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E7%BA%A7/</url>
      
        <content type="html"><![CDATA[<p>在使用逻辑和算数运算符时，需要注意运算符的优先级，不然会出现意想不到的错误。</p><div class="table-container"><table><thead><tr><th style="text-align:center">优先级</th><th style="text-align:center">运算符</th><th style="text-align:center">说明</th><th style="text-align:center">结合性</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">::</td><td style="text-align:center">范围解析</td><td style="text-align:center">自左向右</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">++  —</td><td style="text-align:center">后缀自增/后缀自减</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">()</td><td style="text-align:center">括号</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">[]</td><td style="text-align:center">数组下标</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">.</td><td style="text-align:center">成员选择（对象）</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">−&gt;</td><td style="text-align:center">成员选择（指针）</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">++  —</td><td style="text-align:center">前缀自增/前缀自减</td><td style="text-align:center">自右向左</td></tr><tr><td style="text-align:center">+  −</td><td style="text-align:center">加/减</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">!  ~</td><td style="text-align:center">逻辑非/按位取反</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">(type)</td><td style="text-align:center">强制类型转换</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">*</td><td style="text-align:center">取指针指向的值</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">&amp;</td><td style="text-align:center">某某的地址</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">sizeof</td><td style="text-align:center">某某的大小</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">new,new[]</td><td style="text-align:center">动态内存分配/动态数组内存分配</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">delete,delete[]</td><td style="text-align:center">动态内存释放/动态数组内存释放</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">.<em>  -&gt;</em></td><td style="text-align:center">成员对象选择/成员指针选择</td><td style="text-align:center">自左向右</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">*  /   %</td><td style="text-align:center">乘法/除法/取余</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">+  −</td><td style="text-align:center">加号/减号</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">&lt;&lt;  &gt;&gt;</td><td style="text-align:center">位左移/位右移</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">&lt;  &lt;=</td><td style="text-align:center">小于/小于等于</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">&gt;  &gt;=</td><td style="text-align:center">大于/大于等于</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">==  !=</td><td style="text-align:center">等于/不等于</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">10</td><td style="text-align:center">&amp;</td><td style="text-align:center">按位与</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">11</td><td style="text-align:center">^</td><td style="text-align:center">按位异或</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">12</td><td style="text-align:center">\</td><td style="text-align:center"></td><td style="text-align:center">按位或</td><td></td></tr><tr><td style="text-align:center">13</td><td style="text-align:center">&amp;&amp;</td><td style="text-align:center">与运算</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">14</td><td style="text-align:center">\</td><td style="text-align:center">\</td><td style="text-align:center"></td><td>或运算</td><td></td></tr><tr><td style="text-align:center">15</td><td style="text-align:center">?:</td><td style="text-align:center">三目运算符</td><td style="text-align:center">自右向左</td></tr><tr><td style="text-align:center">16</td><td style="text-align:center">=</td><td style="text-align:center">赋值</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">+=  −=</td><td style="text-align:center">相加后赋值/相减后赋值</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">*=  /=   %=</td><td style="text-align:center">相乘后赋值/相除后赋值/取余后赋值</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">&lt;&lt;=  &gt;&gt;=</td><td style="text-align:center">位左移赋值/位右移赋值</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">&amp;=  ^=  \</td><td style="text-align:center">=</td><td style="text-align:center">位与运算后赋值/位异或运算后赋值/位或运算后赋值</td><td style="text-align:center"></td><td></td></tr><tr><td style="text-align:center">17</td><td style="text-align:center">throw</td><td style="text-align:center">抛出异常</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">18</td><td style="text-align:center">,</td><td style="text-align:center">逗号</td><td style="text-align:center">自左向右</td></tr></tbody></table></div><p>如果记不清楚的话，加一个括号总是没错的！(●’◡’●)</p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 优先级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++ STL中的sort()排序稳定吗？</title>
      <link href="/2021/04/08/C-STL%E4%B8%AD%E7%9A%84sort-%E6%8E%92%E5%BA%8F%E7%A8%B3%E5%AE%9A%E5%90%97%EF%BC%9F/"/>
      <url>/2021/04/08/C-STL%E4%B8%AD%E7%9A%84sort-%E6%8E%92%E5%BA%8F%E7%A8%B3%E5%AE%9A%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="解答"><a href="#解答" class="headerlink" title="解答"></a>解答</h1><p><code>sort()</code>排序<strong>当然不是稳定排序</strong>，sort是主要用到了快速排序（平均时间复杂度为O(nlogn)），还结合了插入排序（时间复杂度为O(n2)）和堆排序（时间复杂度为O(nlogn)）。</p><p><img src="https://pic1.zhimg.com/80/v2-8fa032e195365f77fb6b980a4ed71958_720w.jpg" alt></p><a id="more"></a><h1 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h1><p><img src="https://img-blog.csdnimg.cn/20190826224720541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzMTUyMDUy,size_16,color_FFFFFF,t_70" alt></p><p>Effective STL对如何选择排序函数总结的很好：<br>1）若需对vector, string, deque, 或array容器进行全排序，你可选择sort或stable_sort；<br>2）若只需对vector, string, deque, 或array容器中取得top n的元素，部分排序partial_sort是首选.<br>3）若对于vector, string, deque, 或array容器，你需要找到第n个位置的元素或者你需要得到top n且不关系top n中的内部 顺序，nth_element是最 理想的；<br>4）若你需要从标准序列容器或者array中把满足某个条件 或者不满足某个条件的元素分开，你最好使用partition或stable_partition；<br>5）若使用的list容器，你可以直接使用partition和stable_partition算法，你可以使用list::sort代替sort和stable_sort排序。</p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> sort </tag>
            
            <tag> STL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex基本表格绘制</title>
      <link href="/2021/04/08/Latex%E5%9F%BA%E6%9C%AC%E8%A1%A8%E6%A0%BC%E7%BB%98%E5%88%B6/"/>
      <url>/2021/04/08/Latex%E5%9F%BA%E6%9C%AC%E8%A1%A8%E6%A0%BC%E7%BB%98%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>————————————————<br>版权声明：本文为CSDN博主「派大星爱摸鱼」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/JueChenYi/article/details/77116011" target="_blank" rel="noopener">https://blog.csdn.net/JueChenYi/article/details/77116011</a></p></blockquote><h1 id="1-基本格式"><a href="#1-基本格式" class="headerlink" title="1. 基本格式"></a>1. 基本格式</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;article&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;tabular&#125;&#123;cc&#125;%一个c表示有一列，格式为居中显示(center)</span><br><span class="line">(1,1)&amp;(1,2)\\%第一行第一列和第二列  中间用&amp;连接</span><br><span class="line">(2,1)&amp;(2,2)\\%第二行第一列和第二列  中间用&amp;连接</span><br><span class="line">\end&#123;tabular&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20170812124223652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSnVlQ2hlbllp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h1 id="2-添加竖线和横线"><a href="#2-添加竖线和横线" class="headerlink" title="2. 添加竖线和横线"></a>2. 添加竖线和横线</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;article&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;tabular&#125;&#123;|c|c|&#125;% 通过添加 | 来表示是否需要绘制竖线</span><br><span class="line">\hline  % 在表格最上方绘制横线</span><br><span class="line">(1,1)&amp;(1,2)\\</span><br><span class="line">\hline  %在第一行和第二行之间绘制横线</span><br><span class="line">(2,1)&amp;(2,2)\\</span><br><span class="line">\hline % 在表格最下方绘制横线</span><br><span class="line">\end&#123;tabular&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20170812124527902?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSnVlQ2hlbllp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h1 id="3-设置每一列的单元格格式"><a href="#3-设置每一列的单元格格式" class="headerlink" title="3. 设置每一列的单元格格式"></a>3. 设置每一列的单元格格式</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;article&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;tabular&#125;&#123;|l|c|r|&#125; %l(left)居左显示 r(right)居右显示 c居中显示</span><br><span class="line">\hline </span><br><span class="line">Name&amp;Steve&amp;Bill\\</span><br><span class="line">\hline  </span><br><span class="line">Matlab&amp;Mathmatica&amp;Maple\\</span><br><span class="line">\hline </span><br><span class="line">\end&#123;tabular&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20170812125307589?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSnVlQ2hlbllp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><a id="more"></a><h1 id="4-常见的三线表"><a href="#4-常见的三线表" class="headerlink" title="4. 常见的三线表"></a>4. 常见的三线表</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[UTF8]&#123;ctexart&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;tabular&#125;&#123;ccc&#125;</span><br><span class="line">\hline</span><br><span class="line">姓名&amp; 学号&amp; 性别\\</span><br><span class="line">\hline</span><br><span class="line">Steve Jobs&amp; 001&amp; Male\\</span><br><span class="line">Bill Gates&amp; 002&amp; Female\\</span><br><span class="line">\hline</span><br><span class="line">\end&#123;tabular&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20170812125712341?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSnVlQ2hlbllp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p>通常来说，我们会希望表格的第一根线和最后一根线比表格中的横线更粗一些。</p><p>booktabs 宏包为我们提供了这个功能，加载 booktabs 宏包之后可以使用 \toprule 和 \bottomrule 命令分别画出表格头和表格底的粗横线，而用 \midrule 画出表格中的横线。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[UTF8]&#123;ctexart&#125;</span><br><span class="line">\usepackage&#123;booktabs&#125; %需要加载宏包&#123;booktabs&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;tabular&#125;&#123;ccc&#125;</span><br><span class="line">\toprule  %添加表格头部粗线</span><br><span class="line">姓名&amp; 学号&amp; 性别\\</span><br><span class="line">\midrule  %添加表格中横线</span><br><span class="line">Steve Jobs&amp; 001&amp; Male\\</span><br><span class="line">Bill Gates&amp; 002&amp; Female\\</span><br><span class="line">\bottomrule %添加表格底部粗线</span><br><span class="line">\end&#123;tabular&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20170812130235218?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSnVlQ2hlbllp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h1 id="5-table环境"><a href="#5-table环境" class="headerlink" title="5. table环境"></a>5. table环境</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[UTF8]&#123;ctexart&#125;</span><br><span class="line">\usepackage&#123;booktabs&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;table&#125;[!htbp]</span><br><span class="line">\centering</span><br><span class="line">\caption&#123;这是一张三线表&#125;\label&#123;tab:aStrangeTable&#125;%添加标题 设置标签</span><br><span class="line">\begin&#123;tabular&#125;&#123;ccc&#125;</span><br><span class="line">\toprule</span><br><span class="line">姓名&amp; 学号&amp; 性别\\</span><br><span class="line">\midrule</span><br><span class="line">Steve Jobs&amp; 001&amp; Male\\</span><br><span class="line">Bill Gates&amp; 002&amp; Female\\</span><br><span class="line">\bottomrule</span><br><span class="line">\end&#123;tabular&#125;</span><br><span class="line">%\caption&#123;这是一张三线表&#125;\label&#123;tab:aStrangeTable&#125;  标题放在这里也是可以的</span><br><span class="line">\end&#123;table&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p>{table}有若干可选参数 [!htbp]<br><code>h</code>代表here,将表格排在当前文字位置<br><code>t</code> 表示将表格放在下一页的 top (页首)<br><code>b</code> 表示将表格放在当前页的 bottom (底部)<br><code>!</code>表示忽略美观因素，尽可能按照参数指定的方式来处理表格浮动位置。<br>表格将会按照所给参数，依次尝试按照每个参数进行排版，当无法排版时，将会按照下一个参数</p><h1 id="6-单元格合并"><a href="#6-单元格合并" class="headerlink" title="6. 单元格合并"></a>6. 单元格合并</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[UTF8]&#123;ctexart&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;table&#125;[!htbp]</span><br><span class="line">\centering</span><br><span class="line">\begin&#123;tabular&#125;&#123;|c|c|c|&#125;</span><br><span class="line">\hline</span><br><span class="line">\multicolumn&#123;3&#125;&#123;|c|&#125;&#123;学生信息&#125;\\ % 用\multicolumn&#123;3&#125;表示横向合并三列 </span><br><span class="line">                        % |c|表示居中并且单元格两侧添加竖线 最后是文本</span><br><span class="line">\hline</span><br><span class="line">姓名&amp;学号&amp;性别\\</span><br><span class="line">\hline</span><br><span class="line">Jack&amp; 001&amp; Male\\</span><br><span class="line">\hline</span><br><span class="line">Angela&amp; 002&amp; Female\\</span><br><span class="line">\hline</span><br><span class="line">\end&#123;tabular&#125;</span><br><span class="line">\caption&#123;这是一张三线表&#125;</span><br><span class="line">\end&#123;table&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20170812134011249?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSnVlQ2hlbllp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[UTF8]&#123;ctexart&#125;</span><br><span class="line">\usepackage&#123;multirow&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;table&#125;[!htbp]</span><br><span class="line">\centering</span><br><span class="line">\begin&#123;tabular&#125;&#123;|c|c|c|c|c|c|c|&#125; %表格7列 全部居中显示</span><br><span class="line">\hline</span><br><span class="line">\multicolumn&#123;7&#125;&#123;|c|&#125;&#123;事件&#125;\\  %横向合并7列单元格  两侧添加竖线</span><br><span class="line">\hline</span><br><span class="line">\multirow&#123;4&#125;*&#123;策略&#125;&amp;50&amp;0&amp;100&amp;200&amp;300&amp;300\\  %纵向合并4行单元格 </span><br><span class="line">\cline&#123;2-7&#125;  %为第二列到第七列添加横线</span><br><span class="line">&amp;100&amp;100&amp;0&amp;100&amp;200&amp;200\\</span><br><span class="line">\cline&#123;2-7&#125;</span><br><span class="line">&amp;150&amp;200&amp;100&amp;0&amp;100&amp;200\\</span><br><span class="line">\cline&#123;2-7&#125;</span><br><span class="line">&amp;200&amp;300&amp;200&amp;100&amp;0&amp;300\\</span><br><span class="line">\hline</span><br><span class="line">\end&#123;tabular&#125;</span><br><span class="line">\end&#123;table&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20170812134954627?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSnVlQ2hlbllp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p>横向合并和纵向合并可以嵌套，代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[UTF8]&#123;ctexart&#125;</span><br><span class="line">\usepackage&#123;multirow&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;table&#125;[!htbp]</span><br><span class="line">\centering</span><br><span class="line">\begin&#123;tabular&#125;&#123;|c|c|c|c|c|c|c|&#125;</span><br><span class="line">\hline</span><br><span class="line"></span><br><span class="line">\multicolumn&#123;2&#125;&#123;|c|&#125;&#123; \multirow&#123;2&#125;*&#123;$S_i$&#125; &#125;&amp; \multicolumn&#123;4&#125;&#123;c|&#125;&#123;事件&#125; &amp;\multirow&#123;2&#125;*&#123;max&#125;\\</span><br><span class="line">\cline&#123;3-6&#125;</span><br><span class="line">\multicolumn&#123;2&#125;&#123;|c|&#125;&#123;&#125;&amp;50&amp;100&amp;150&amp;200&amp;\\</span><br><span class="line">\hline</span><br><span class="line">\multirow&#123;4&#125;*&#123;策略&#125;&amp;50&amp;0&amp;100&amp;200&amp;300&amp;300\\</span><br><span class="line">\cline&#123;2-7&#125;</span><br><span class="line">&amp;100&amp;100&amp;0&amp;100&amp;200&amp;200\\</span><br><span class="line">\cline&#123;2-7&#125;</span><br><span class="line">&amp;150&amp;200&amp;100&amp;0&amp;100&amp;200\\</span><br><span class="line">\cline&#123;2-7&#125;</span><br><span class="line">&amp;200&amp;300&amp;200&amp;100&amp;0&amp;300\\</span><br><span class="line">\hline</span><br><span class="line">\end&#123;tabular&#125;</span><br><span class="line">\end&#123;table&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20170812140522713?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSnVlQ2hlbllp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h1 id="7-斜线表头"><a href="#7-斜线表头" class="headerlink" title="7. 斜线表头"></a>7. 斜线表头</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;UTF8&#125;&#123;ctexart&#125;</span><br><span class="line">\usepackage&#123;diagbox&#125; % 加载宏包</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;table&#125;[!htbp]</span><br><span class="line">\centering</span><br><span class="line">\begin&#123;tabular&#125;&#123;|c|c|c|c|&#125;</span><br><span class="line">\hline</span><br><span class="line">\diagbox&#123;甲&#125;&#123;$\alpha_&#123;i,j&#125;$&#125;&#123;乙&#125;&amp;$\beta_1$&amp;$\beta_2$&amp;$\beta_3$\\ %添加斜线表头</span><br><span class="line">\hline</span><br><span class="line">$\alpha_1$&amp;-4&amp;0&amp;-8\\</span><br><span class="line">\hline</span><br><span class="line">$\alpha_2$&amp;3&amp;2&amp;4\\</span><br><span class="line">\hline</span><br><span class="line">$\alpha_3$&amp;16&amp;1&amp;-9\\</span><br><span class="line">\hline</span><br><span class="line">$\alpha_4$&amp;-1&amp;1&amp;7\\</span><br><span class="line">\hline</span><br><span class="line">\end&#123;tabular&#125;</span><br><span class="line">\end&#123;table&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20170812141817940?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSnVlQ2hlbllp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h1 id="8-其他"><a href="#8-其他" class="headerlink" title="8. 其他"></a>8. 其他</h1><p>对于多行多列嵌套斜线表头的情况，不加参数会出现斜线不处于对角线位置的情况</p><p><img src="https://img-blog.csdnimg.cn/20181123122742980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0p1ZUNoZW5ZaQ==,size_16,color_FFFFFF,t_70" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;ctexart&#125;</span><br><span class="line">\usepackage&#123;multirow&#125;</span><br><span class="line">\usepackage&#123;diagbox&#125; % 加载宏包</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;table&#125;[!htbp]</span><br><span class="line">  \centering</span><br><span class="line">  \begin&#123;tabular&#125;&#123;|c|c|c|c|c|c|c|&#125;</span><br><span class="line">   \hline</span><br><span class="line">   \multicolumn&#123;2&#125;&#123;|c|&#125;&#123;\multirow&#123;2&#125;*&#123;\diagbox[innerwidth=2cm]&#123;$S_i$&#125;&#123;$\lambda_i$&#125;&#125;&#125;&amp; \multicolumn&#123;4&#125;&#123;c|&#125;&#123;事件&#125; &amp;\multirow&#123;2&#125;*&#123;max&#125;\\</span><br><span class="line">   \cline&#123;3-6&#125;</span><br><span class="line">   \multicolumn&#123;2&#125;&#123;|c|&#125;&#123;&#125;&amp;50&amp;100&amp;150&amp;200&amp;\\</span><br><span class="line">   \hline</span><br><span class="line">   \multirow&#123;4&#125;*&#123;策略&#125;&amp;50&amp;0&amp;100&amp;200&amp;300&amp;300\\</span><br><span class="line">   \cline&#123;2-7&#125;</span><br><span class="line">   &amp;100&amp;100&amp;0&amp;100&amp;200&amp;200\\</span><br><span class="line">   \cline&#123;2-7&#125;</span><br><span class="line">   &amp;150&amp;200&amp;100&amp;0&amp;100&amp;200\\</span><br><span class="line">   \cline&#123;2-7&#125;</span><br><span class="line">   &amp;200&amp;300&amp;200&amp;100&amp;0&amp;300\\</span><br><span class="line">   \hline</span><br><span class="line">  \end&#123;tabular&#125;</span><br><span class="line"> \end&#123;table&#125;</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181123123351647.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0p1ZUNoZW5ZaQ==,size_16,color_FFFFFF,t_70" alt></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> table </tag>
            
            <tag> overleaf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>笔试经验总结</title>
      <link href="/2021/04/04/%E7%AC%94%E8%AF%95%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
      <url>/2021/04/04/%E7%AC%94%E8%AF%95%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>已经有过几次笔试的经验了，也经历了不少的失败和挫折，做一个总结：</p><ol><li>ACM格式比LeetCode要难，因为要自己处理输入输出，需要保证处理的正确而且速度快；另外，需要自己考虑到所有的corner case。<strong>在LeetCode上会提示测试通不过的样例，不要看！</strong>这是一个很不好的习惯，会形成依赖。要在脑海中自己把问题考虑周全。</li><li>要先把题目看一遍，<strong>如果某个题短时间内不能有一个</strong><font color="orange"><strong>清晰直接</strong></font><strong>的思路，多半是自己搞错了</strong>。不要浪费时间！</li><li>要熟练掌握C++中的STL和常用函数。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACM </tag>
            
            <tag> OJ </tag>
            
            <tag> LeetCode </tag>
            
            <tag> Interview </tag>
            
            <tag> 春招 </tag>
            
            <tag> 笔试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>int long 和 long long 的区别</title>
      <link href="/2021/04/03/int-long-%E5%92%8C-long-long-%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2021/04/03/int-long-%E5%92%8C-long-long-%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><h3 id="int"><a href="#int" class="headerlink" title="int"></a>int</h3><p>(4个字节，32位)</p><p>unsigned int: 0 ~ 4294967295</p><p>int: -2147483648 ~ 2147483647</p><h3 id="int32"><a href="#int32" class="headerlink" title="_int32"></a>_int32</h3><p>(4个字节，32位)<br>unsigned _int32: 0～4294967295<br>_int32: -2147483648 ~ 2147483647</p><h3 id="long"><a href="#long" class="headerlink" title="long"></a>long</h3><p>(4个字节，32位)<br>unsigned long: 0～4294967295<br>long: -2147483648～2147483647</p><h3 id="long-long"><a href="#long-long" class="headerlink" title="long long"></a>long long</h3><p>(8个字节，64位)<br>unsigned long long：0~1844674407370955161<br>long long：-9223372036854775808~9223372036854775807</p><a id="more"></a><h3 id="int64"><a href="#int64" class="headerlink" title="_int64"></a>_int64</h3><p>(8个字节，64位)<br>unsigned _int64：0~18446744073709551615<br>_int64：-9223372036854775808~9223372036854775807</p><h2 id="为什么有long和long-long"><a href="#为什么有long和long-long" class="headerlink" title="为什么有long和long long?"></a>为什么有long和long long?</h2><ul><li>早期的操作系统是16位系统</li></ul><p>int用两个字节表示，范围是-32768 ~ 32767</p><p>long用四个字节表示，范围是-2147483648 ~ 2147483647</p><ul><li>后来发展到32位操作系统</li></ul><p>int用四个字节表示，与long相同</p><ul><li>目前操作系统已经发展到64位操作系统，<strong>但因程序编译工艺的不同，两者表现出不同的差别</strong>:</li></ul><p>32位编译系统：int占四字节，与long相同</p><p>64位编译系统：int占四字节，long占八字节，long数据范围变为：$-2^{63} \sim 2^{63}-1$</p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> int </tag>
            
            <tag> long </tag>
            
            <tag> long long </tag>
            
            <tag> 整型 </tag>
            
            <tag> integer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速排序</title>
      <link href="/2021/04/01/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/"/>
      <url>/2021/04/01/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="一、算法思想"><a href="#一、算法思想" class="headerlink" title="一、算法思想"></a>一、算法思想</h1><p>快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。</p><h1 id="二、实现原理"><a href="#二、实现原理" class="headerlink" title="二、实现原理"></a>二、实现原理</h1><p>2.1、设置两个变量 low、high，<strong>排序开始时：low=0，high=size-1。</strong><br>2.2、整个数组找基准正确位置，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面</p><ul><li>默认数组的第一个数为基准数据，赋值给key，即key=array[low]。</li><li>因为默认数组的第一个数为基准，<strong>所以从后面开始向前搜索（high—）</strong>，找到第一个<strong>小于</strong>key的array[high]，就将 array[high] 赋给 array[low]，即 array[low] = array[high]。（循环条件是 array[high] &gt;= key；结束时 array[high] &lt; key）</li><li>此时从前面开始向后搜索（low++），找到第一个<strong>大于</strong>key的array[low]，就将 array[low] 赋给 array[high]，即 array[high] = array[low]。（循环条件是 array[low] &lt;= key；结束时 array[low] &gt; key）</li><li>循环 2-3 步骤，直到 low=high，该位置就是基准位置。</li><li>把基准数据赋给当前位置。</li></ul><p>2.3、第一趟找到的基准位置，作为下一趟的分界点。<br>2.4、递归调用（recursive）分界点前和分界点后的子数组排序，重复2.2、2.3、2.4的步骤。<br>2.5、最终就会得到排序好的数组。</p><a id="more"></a><h1 id="三、稳定性"><a href="#三、稳定性" class="headerlink" title="三、稳定性"></a>三、稳定性</h1><p>快速排序算法是不稳定的。也就是说，相同大小的元素可能在排序前后位置有所调换。</p><p>可以将排序算法实现为稳定算法，但是可能会带来性能的下降。</p><blockquote><p>参考CSDN博客：</p><p><a href="https://blog.csdn.net/gaoxueyi551/article/details/89413936" target="_blank" rel="noopener">https://blog.csdn.net/gaoxueyi551/article/details/89413936</a></p></blockquote><h1 id="四、C-实现"><a href="#四、C-实现" class="headerlink" title="四、C++实现"></a>四、C++实现</h1><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quicksort</span><span class="params">(<span class="keyword">int</span>* a, <span class="keyword">int</span> s, <span class="keyword">int</span> e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (s &gt;= e)  <span class="comment">// 终止条件 </span></span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line"><span class="keyword">int</span> pivot = a[s];</span><br><span class="line"><span class="keyword">int</span> high = e;</span><br><span class="line"><span class="keyword">int</span> low = s;</span><br><span class="line"><span class="keyword">while</span> (low &lt; high) &#123;</span><br><span class="line"><span class="keyword">while</span> (low &lt; high &amp;&amp; a[high] &gt;= pivot) --high;</span><br><span class="line">a[low] = a[high];</span><br><span class="line"><span class="keyword">while</span> (low &lt; high &amp;&amp; a[low] &lt;= pivot) ++low;</span><br><span class="line">a[high] = a[low];</span><br><span class="line">&#125;</span><br><span class="line">a[low] = pivot;  <span class="comment">// low == high</span></span><br><span class="line">quicksort(a, s, low - <span class="number">1</span>);</span><br><span class="line">quicksort(a, low + <span class="number">1</span>, e);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> a[<span class="number">10</span>] = &#123;<span class="number">0</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">9</span>,<span class="number">4</span>,<span class="number">8</span>&#125;;</span><br><span class="line">quicksort(a, <span class="number">0</span>, <span class="number">10</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;++i)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; a[i] &lt;&lt; <span class="string">' '</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> sort </tag>
            
            <tag> quick sort </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文件路径分割</title>
      <link href="/2021/03/30/%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84%E5%88%86%E5%89%B2/"/>
      <url>/2021/03/30/%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84%E5%88%86%E5%89%B2/</url>
      
        <content type="html"><![CDATA[<p>直接垒代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">file_path = <span class="string">"E:/tt/abc.py"</span></span><br><span class="line">filepath,fullflname = os.path.split(file_path)</span><br><span class="line">fname,ext = os.path.splitext(fullflname)</span><br></pre></td></tr></table></figure><p>运行结果:<br>filepath为文件的目录,即E:/tt<br>fullflname为文件名的全名，即abc.py<br>fname为文件的名字,即abc<br>ext为文件的扩展名,即.py</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> os </tag>
            
            <tag> split </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python遍历文件夹获取文件</title>
      <link href="/2021/03/29/python%E9%81%8D%E5%8E%86%E6%96%87%E4%BB%B6%E5%A4%B9%E8%8E%B7%E5%8F%96%E6%96%87%E4%BB%B6/"/>
      <url>/2021/03/29/python%E9%81%8D%E5%8E%86%E6%96%87%E4%BB%B6%E5%A4%B9%E8%8E%B7%E5%8F%96%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="1-手写递归"><a href="#1-手写递归" class="headerlink" title="1. 手写递归"></a>1. 手写递归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getallfile</span><span class="params">(path)</span>:</span></span><br><span class="line">    allfilelist=os.listdir(path)</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> allfilelist:</span><br><span class="line">        filepath=os.path.join(path,file)</span><br><span class="line">        <span class="comment">#判断是不是文件夹</span></span><br><span class="line">        <span class="keyword">if</span> os.path.isdir(filepath):</span><br><span class="line">            getallfile(filepath)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        allfile.append(filepath)</span><br><span class="line">    <span class="keyword">return</span> allfile</span><br></pre></td></tr></table></figure><h2 id="2-os-walk"><a href="#2-os-walk" class="headerlink" title="2. os.walk()"></a>2. os.walk()</h2><p><code>os.walk(top, topdown=True, οnerrοr=None, followlinks=False)</code><br>返回一个3个元素的元祖，(dirpath, dirnames, filenames),<br>dirpath：要列出指定目录的路径<br>dirnames：目录下的所有文件夹<br>filenames：目录下的所有文件</p><a id="more"></a><p>参数一：top – 根目录下的每一个文件夹(包含它自己), 产生3-元组 (dirpath, dirnames, filenames)【文件夹路径, 文件夹名字, 文件名】。</p><p>参数二：topdown –可选，为True或者没有指定, 一个目录的的3-元组将比它的任何子文件夹的3-元组先产生 (目录自上而下)。如果topdown为 False, 一个目录的3-元组将比它的任何子文件夹的3-元组后产生 (目录自下而上)。</p><p>参数三：onerror – 可选，是一个函数; 它调用时有一个参数, 一个OSError实例。报告这错误后，继续walk,或者抛出exception终止walk。</p><p>参数四：followlinks – 设置为 true，则通过软链接访问目录。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getallfiles</span><span class="params">(path)</span>:</span></span><br><span class="line">    allfile=[]</span><br><span class="line">    <span class="keyword">for</span> dirpath,dirnames,filenames <span class="keyword">in</span> os.walk(path):</span><br><span class="line">        <span class="keyword">for</span> dir <span class="keyword">in</span> dirnames:</span><br><span class="line">            allfile.append(os.path.join(dirpath,dir))</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> filenames:</span><br><span class="line">            allfile.append(os.path.join(dirpath, name))</span><br><span class="line">    <span class="keyword">return</span> allfile</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> walk </tag>
            
            <tag> os </tag>
            
            <tag> recursive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>直观理解线性变换</title>
      <link href="/2021/03/29/%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2/"/>
      <url>/2021/03/29/%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2/</url>
      
        <content type="html"><![CDATA[<p>直观一点理解，线性变换是一种运动，即让每一个输入向量都<strong>移动</strong>到对应输出向量的位置。</p><p>那么这样，线性变换这个关于向量的移动，就可以直观地变成了坐标系上点的运动。</p><p>简单来说，线性变换可以看作是坐标系“<strong>整体、均匀</strong>”的变换。</p><p>更详细地参见以下这个视频：</p><p><a href="https://www.bilibili.com/video/av6043439" target="_blank" rel="noopener">https://www.bilibili.com/video/av6043439</a></p><p>而<strong>仿射变换 (Affine Transformation)</strong>会改变原点的位置，而<strong>线性变换 (Linear Transformation)</strong>不改变原点的位置。</p>]]></content>
      
      
      <categories>
          
          <category> Mathmatic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linear Transformation </tag>
            
            <tag> Affine Transformation </tag>
            
            <tag> 线性变换 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SMOTE上采样方法</title>
      <link href="/2021/03/29/SMOTE%E4%B8%8A%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95/"/>
      <url>/2021/03/29/SMOTE%E4%B8%8A%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>SMOTE or <strong>Synthetic Minority Oversampling Technique</strong></p><p>经典的上采样方法是直接将少数类别的样本重复若干次。但是这种方法没有带来新的信息。</p><blockquote><p>SMOTE works by utilizing a <strong>k-nearest neighbor</strong> algorithm to create synthetic data. SMOTE first start by choosing random data from the minority class, then k-nearest neighbors from the data are set. Synthetic data would then made between the random data and the randomly selected k-nearest neighbor. Let me show you the example below.</p></blockquote><h2 id="1-SMOTE使用"><a href="#1-SMOTE使用" class="headerlink" title="1. SMOTE使用"></a>1. SMOTE使用</h2><p>SMOTE适用于特征值为连续值而不是离散值。这样才可以进行插值，产生新的训练数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborns <span class="keyword">as</span> sns</span><br><span class="line"><span class="comment">#I read the csv churn data into variable called df. Here I would only use two continuous features CreditScore and Age with the target Exited</span></span><br><span class="line">df_example = df[[<span class="string">'CreditScore'</span>, <span class="string">'Age'</span>, <span class="string">'Exited'</span>]]</span><br><span class="line">sns.scatterplot(data = df, x =<span class="string">'CreditScore'</span>, y = <span class="string">'Age'</span>, hue = <span class="string">'Exited'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://miro.medium.com/max/487/1*53v1-PbF5uCiSZnLKDJZ5w.png" alt></p><p>然后使用SMOTE上采样，查看采样之后的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U imbalanced-learn</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Importing SMOTE</span></span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</span><br><span class="line"><span class="comment">#Oversampling the data</span></span><br><span class="line">smote = SMOTE(random_state = <span class="number">101</span>)</span><br><span class="line">X, y = smote.fit_resample(df[[<span class="string">'CreditScore'</span>, <span class="string">'Age'</span>]], df[<span class="string">'Exited'</span>])</span><br><span class="line"><span class="comment">#Creating a new Oversampling Data Frame</span></span><br><span class="line">df_oversampler = pd.DataFrame(X, columns = [<span class="string">'CreditScore'</span>, <span class="string">'Age'</span>])</span><br><span class="line">df_oversampler[<span class="string">'Exited'</span>]</span><br><span class="line">sns.countplot(df_oversampler[<span class="string">'Exited'</span>])</span><br><span class="line">sns.scatterplot(data = df_oversampler, x =<span class="string">'CreditScore'</span>, y = <span class="string">'Age'</span>, hue = <span class="string">'Exited'</span>)</span><br></pre></td></tr></table></figure><a id="more"></a><p><img src="https://miro.medium.com/max/498/1*BVlx5Mo-1L3pbnKCT_N9Nw.png" alt></p><h2 id="2-SMOTE原理"><a href="#2-SMOTE原理" class="headerlink" title="2. SMOTE原理"></a>2. SMOTE原理</h2><p>算法流程：</p><ol><li>对于少数类中每一个样本$x$，以欧氏距离为标准计算它到少数类样本集$S_{min}$所有样本的距离，得到其k近邻。</li><li>根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本$x$，从其k近邻中随机选择若干个样本，假设选择的近邻为$x_n$</li><li>对于每一个随机选出的近邻$x_n$，分别与原样本按照如下的公式构建新的样本</li></ol><script type="math/tex; mode=display">x_{new} = x + \mathrm{rand}（0,1）*|x - x_n|</script><p>很直白很容易理解，就是用某个样本与其近邻做插值。</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SMOTE </tag>
            
            <tag> oversampling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通俗理解困惑度(Perplexity)</title>
      <link href="/2021/03/29/%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3%E5%9B%B0%E6%83%91%E5%BA%A6-Perplexity/"/>
      <url>/2021/03/29/%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3%E5%9B%B0%E6%83%91%E5%BA%A6-Perplexity/</url>
      
        <content type="html"><![CDATA[<h2 id="一、语言模型"><a href="#一、语言模型" class="headerlink" title="一、语言模型"></a><strong>一、语言模型</strong></h2><p>本文尽量通俗解释一下困惑度的概念。既然题目中写了是用来评价语言模型的好坏，那么首先来看一下语言模型：</p><p><strong>简单地说，语言模型就是用来计算一个句子的概率的模型，也就是判断一句话是否是人话的概率？</strong></p><p>那么如何计算一个句子的概率呢？给定句子（词语序列）</p><p>$S= W_1, …, W_k$</p><p>它的概率可以表示为：</p><p>$\mathrm{P}(x) = \mathrm{P}(W<em>1, …, W_k) = \mathrm{P}(W_1)\mathrm{P}(W_2|W_1)…\mathrm{P}(W_k|W_1,…,W</em>{k-1})$</p><p>也就是说在给定一句话的前k个词，我们希望<strong>语言模型</strong>可以预测第k+1个词是什么，即给出一个第k+1个词可能出现的概率的分布。</p><p>那么如何学习到一个语言模型呢，这里不详细叙述。</p><h2 id="二、如何评价一个语言模型好坏"><a href="#二、如何评价一个语言模型好坏" class="headerlink" title="二、如何评价一个语言模型好坏"></a><strong>二、如何评价一个语言模型好坏</strong></h2><p>在得到不同的语言模型（一元语言模型、二元语言模型….）的时候，我们如何判断一个语言模型是否好还是坏，一般有两种方法：</p><p>1、一种方法将其应用到具体的问题当中，比如机器翻译、speech recognition、spelling corrector等。然后看这个语言模型在这些任务中的表现（extrinsic evaluation，or in-vivo evaluation）。但是，这种方法一方面难以操作，另一方面可能非常耗时，可能跑一个evaluation需要大量时间，费时难操作。</p><p>2、针对第一种方法的缺点，大家想是否可以根据与语言模型自身的一些特性，来设计一种简单易行，而又行之有效的评测指标。于是，人们就发明了perplexity这个指标。</p><p>困惑度（perplexity）的基本思想是：<strong>给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，</strong>公式如下：</p><p>$\mathrm{PP}(W) = \mathrm{P}(w_1, …,w_N) ^ {-\frac{1}{N}} = \sqrt[N]{\frac{1}{\mathrm{P}(w_1,…,w_N)}} $</p><p>由公式可知，<strong>句子概率越大，语言模型越好，迷惑度越小。</strong></p><a id="more"></a><p>下面是一些 ngra­m 模型经 训练文本后在测试集上的困惑度值：</p><p><img src="https://pic1.zhimg.com/80/v2-da6384d62d15cd61e36d5749ff127670_720w.jpg" alt></p><p>可以看到， trigram 模型经训练后，相比于 unigram 模型，困惑度由955跌减至74，这是十分可观的结果。</p><p>这节的核心就是<strong>句子概率越大，语言模型越好，迷惑度越小。</strong></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Pretrained Language Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PLM </tag>
            
            <tag> Perplexity </tag>
            
            <tag> 困惑度 </tag>
            
            <tag> Pretrained Language Model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>点互信息</title>
      <link href="/2021/03/29/%E7%82%B9%E4%BA%92%E4%BF%A1%E6%81%AF/"/>
      <url>/2021/03/29/%E7%82%B9%E4%BA%92%E4%BF%A1%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<p>在数据挖掘或者信息检索的相关资料里，经常会用到PMI（Pointwise Mutual Information）这个指标来衡量两个事物之间的相关性。PMI的定义如下：</p><script type="math/tex; mode=display">\mathrm{PMI}(x, y) = \log \frac{\mathrm{p}(x,y)}{\mathrm{p}(x)\mathrm{p}(y)}</script><p>这个定义所体现的原理其实是相当直白的。<strong>在概率论中，我们知道，如果x跟y不相关，则 P(x,y) = P(x)P(y)。二者相关性越大，则 P(x,y) 就相比于 P(x)P(y) 越大。</strong>根据条件概率公式，你还可以写成</p><script type="math/tex; mode=display">\mathrm{PMI}(x,y) \equiv \log \frac{\mathrm{p}(x,y)}{\mathrm{p}(x)\mathrm{p}(y)} \equiv \log \frac{\mathrm{p}(x|y)}{\mathrm{p}(x)} \equiv \log \frac{\mathrm{p}(y|x)}{\mathrm{p}(y)}</script><p>这里的log来自于信息论的理论，而且 log 1 = 0 ，也恰恰表明P(x,y) = P(x)P(y)，相关性为0，而且log是单调递增函数，所以 “P(x,y) 就相比于 P(x)P(y) 越大，x 和 y 相关性越大” 这一性质也得到保留。</p><p>但是若$\mathrm{p}(x,y) = 0$，那就是你有可能会去计算 log 0 = -inf，即得到一个负无穷。为此人们通常会计算一个PPMI（Positive PMI）来避免出现 -inf，即</p><script type="math/tex; mode=display">\mathrm{PPMI}(x, y) =\max( \log \frac{\mathrm{p}(x,y)}{\mathrm{p}(x)\mathrm{p}(y)}, 0)</script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 点互信息 </tag>
            
            <tag> Pointwise Mutual Information </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生成模型vs判别模型</title>
      <link href="/2021/03/29/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8Bvs%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2021/03/29/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8Bvs%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>版权声明：本文为CSDN博主「CAM-TAY」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/u010358304/article/details/79748153" target="_blank" rel="noopener">https://blog.csdn.net/u010358304/article/details/79748153</a></p></blockquote><p>从概率分布的角度考虑，对于一堆样本数据，每个均有特征Xi对应分类标记yi。</p><p>生成模型：学习得到联合概率分布P(x,y)，即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。</p><p>判别模型：学习得到条件概率分布P(y|x)，即在特征x出现的情况下标记y出现的概率。</p><p>数据要求：生成模型需要的数据量比较大，能够较好地估计概率密度；而判别模型对数据样本量的要求没有那么多。</p><p><img src="https://img-blog.csdn.net/20180329224547767?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAzNTgzMDQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生成模型 </tag>
            
            <tag> 判别模型 </tag>
            
            <tag> generative </tag>
            
            <tag> discriminative </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nvidia-smi报错NVIDIA-SMI has failed because it couldn&#39;t...</title>
      <link href="/2021/03/29/nvidia-smi%E6%8A%A5%E9%94%99NVIDIA-SMI-has-failed-because-it-couldn-t/"/>
      <url>/2021/03/29/nvidia-smi%E6%8A%A5%E9%94%99NVIDIA-SMI-has-failed-because-it-couldn-t/</url>
      
        <content type="html"><![CDATA[<font color="red">NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</font><p>原因是版本不匹配导致的检测显卡驱动程序异常。</p><h1 id="1-检测版本"><a href="#1-检测版本" class="headerlink" title="1. 检测版本"></a>1. 检测版本</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install dkms</span><br></pre></td></tr></table></figure><p>然后查看对应的驱动版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /usr/src</span><br></pre></td></tr></table></figure><p>对应结果为目录：<code>nvidia-418.87.01</code></p><h1 id="2-重新生成对应的驱动模块"><a href="#2-重新生成对应的驱动模块" class="headerlink" title="2. 重新生成对应的驱动模块"></a>2. 重新生成对应的驱动模块</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dkms install -m nvidia -v 418.87.01</span><br></pre></td></tr></table></figure><p>祝大家科研顺利🤭</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nvidia </tag>
            
            <tag> NVIDIA-SMI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>unordered_map使用方法</title>
      <link href="/2021/03/28/unordered-map%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
      <url>/2021/03/28/unordered-map%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<blockquote><p>版权声明：本文为CSDN博主「Cypress1010」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/Cypress1010/article/details/53669409" target="_blank" rel="noopener">https://blog.csdn.net/Cypress1010/article/details/53669409</a></p></blockquote><h2 id="unordered-map-无序映射"><a href="#unordered-map-无序映射" class="headerlink" title="unordered_map(无序映射)"></a>unordered_map(无序映射)</h2><p>对于map，前面已经提到过，其内部数据结构为红黑树，因此所有元素插入到map里面都会排好序，而且搜索过程为平衡二叉树搜索，因此时间复杂度为O(logN)。我们知道还有一种快速的搜索方法，那边是哈希(又名散列)，利用哈希函数，通过哈希值能快速的查找到所需元素。unordered_map便是采用这种数据结构实现，unordered _map与map的使用基本一样，都是key/value之间的映射，只是他们内部采用的数据结构不一样。</p><blockquote><p>由于unordered_map内部是用散列表来实现快速查找，因此其内部元素完全是一种无序状态。哈希表利用哈希函数，将关键字的哈希值放都一个桶(bucket)里面，具有相同哈希值的放入到同一个桶。</p></blockquote><h2 id="头文件：-include"><a href="#头文件：-include" class="headerlink" title="头文件：#include "></a>头文件：#include <unordered_map></unordered_map></h2><p>构造函数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="built_in">string</span>&gt; stringmap;</span><br><span class="line"></span><br><span class="line">stringmap first;  <span class="comment">// empty</span></span><br><span class="line"><span class="function">stringmap <span class="title">second</span><span class="params">(&#123;&#123;<span class="string">"apple"</span>, <span class="string">"red"</span>&#125;,&#123;<span class="string">"lemon"</span>, <span class="string">"yellow"</span>&#125;&#125;)</span></span>;  <span class="comment">// init list</span></span><br><span class="line"><span class="function">stringmap <span class="title">third</span><span class="params">(&#123;&#123;<span class="string">"orange"</span>, <span class="string">"orange"</span>&#125;,&#123;<span class="string">"strawberry"</span>, <span class="string">"red"</span>&#125;&#125;)</span></span>;</span><br><span class="line"><span class="function">stringmap <span class="title">fourth</span><span class="params">(third)</span></span>;  <span class="comment">// copy</span></span><br><span class="line"><span class="function">stringmap <span class="title">fifth</span><span class="params">(fourth.begin(), fourth.end())</span></span>;  <span class="comment">// range</span></span><br></pre></td></tr></table></figure><p>容器大小</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span> &lt;&lt; second.empty() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; second.size() &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><p>获取元素</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span> &lt;&lt; second[<span class="number">0</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; second.at(<span class="number">0</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><p>元素查找</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt;::iterator it;</span><br><span class="line">it = second.find(<span class="string">"banana"</span>);  <span class="comment">//返回查找到元素的iterator，如未查找到，返回end()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (it != second.end())</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; (*it).first &lt;&lt; <span class="string">" "</span> &lt;&lt; (*it).second &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><p>元素修改</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; second;</span><br><span class="line">second[<span class="string">'a'</span>] = <span class="number">1</span>;</span><br><span class="line">second[<span class="string">'b'</span>] = <span class="number">2</span>;</span><br><span class="line">second[<span class="string">'c'</span>] = <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除元素</span></span><br><span class="line">second.erase ( second.begin() );      <span class="comment">// erasing by iterator</span></span><br><span class="line">second.erase (<span class="string">'a'</span>);             <span class="comment">// erasing by key</span></span><br><span class="line">second.erase ( second.find(<span class="string">'c'</span>), second.end() ); <span class="comment">// erasing by range</span></span><br><span class="line"></span><br><span class="line">second.clear();  <span class="comment">//清空</span></span><br><span class="line">second.swap(first);   <span class="comment">//互换</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; second.bucket_count() &lt;&lt; <span class="built_in">endl</span>;  <span class="comment">//返回桶的数量</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; second.bucket_size() &lt;&lt; <span class="built_in">endl</span>;  <span class="comment">//返回每个桶的大小</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; second.bucket(<span class="string">'a'</span>) &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//返回当前元素在哪个桶</span></span><br><span class="line">second.rehash(<span class="number">10</span>);   <span class="comment">//设置桶的数量</span></span><br></pre></td></tr></table></figure><h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><p>unordered_map内部使用哈希表进行存储与搜索。由于需要使用hash来进行映射，因此需要判断两个关键字是否相等，对于内部类型，可以直接进行判断，如果是用户自定义类型，则需要重载”==”运算符，指定如何判断两个关键字是否相等。以下是在网上摘录的一段代码，个人觉得比较详细的unordered_map的使用方法，这里只是其中一种使用方法：利用函数对象。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="built_in">string</span> name;</span><br><span class="line"><span class="keyword">int</span> age;</span><br><span class="line">Person(<span class="built_in">string</span> name, <span class="keyword">int</span> age)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">this</span>-&gt;name =  name;</span><br><span class="line"><span class="keyword">this</span>-&gt;age = age;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* the object of hash function */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">PersonHash</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">size_t</span> <span class="keyword">operator</span>()(<span class="keyword">const</span> Person&amp; per) <span class="keyword">const</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> hash&lt;<span class="built_in">string</span>&gt;()(per.name) ^ hash&lt;<span class="keyword">int</span>&gt;()(per.age);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* the object of compare */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">PersonCmp</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> Person&amp; personA, <span class="keyword">const</span> Person&amp; personB)</span> <span class="keyword">const</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> personA.name == personB.name &amp;&amp; personA.age == personB.age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* define the unordered_map type */</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="built_in">unordered_map</span>&lt;Person, <span class="keyword">int</span>, PersonHash, PersonCmp&gt; umap;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">umap m;</span><br><span class="line"><span class="function">Person <span class="title">p1</span><span class="params">(<span class="string">"Tom1"</span>,<span class="number">20</span>)</span></span>;</span><br><span class="line"><span class="function">Person <span class="title">p2</span><span class="params">(<span class="string">"Tom2"</span>,<span class="number">22</span>)</span></span>;</span><br><span class="line"><span class="function">Person <span class="title">p3</span><span class="params">(<span class="string">"Tom3"</span>,<span class="number">22</span>)</span></span>;</span><br><span class="line"><span class="function">Person <span class="title">p4</span><span class="params">(<span class="string">"Tom4"</span>,<span class="number">23</span>)</span></span>;</span><br><span class="line"><span class="function">Person <span class="title">p5</span><span class="params">(<span class="string">"Tom5"</span>,<span class="number">24</span>)</span></span>;</span><br><span class="line">m.insert(umap::value_type(p3, <span class="number">100</span>));</span><br><span class="line">m.insert(umap::value_type(p4, <span class="number">100</span>));</span><br><span class="line">m.insert(umap::value_type(p5, <span class="number">100</span>));</span><br><span class="line">m.insert(umap::value_type(p1, <span class="number">100</span>));</span><br><span class="line">m.insert(umap::value_type(p2, <span class="number">100</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 这里打印出来的顺序于插入顺序并不相同，确切的说是完全无序的 */</span></span><br><span class="line"><span class="keyword">for</span>(umap::iterator iter = m.begin(); iter != m.end(); iter++)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; iter-&gt;first.name &lt;&lt; <span class="string">"\t"</span> &lt;&lt; iter-&gt;first.age &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> STL </tag>
            
            <tag> unordered_map </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cin, cin.getline(), getline()的用法</title>
      <link href="/2021/03/28/cin-cin-getline-getline-%E7%9A%84%E7%94%A8%E6%B3%95/"/>
      <url>/2021/03/28/cin-cin-getline-getline-%E7%9A%84%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="1-cin"><a href="#1-cin" class="headerlink" title="1. cin"></a>1. cin</h1><p>用法1：输入一个数字或字符</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     <span class="keyword">int</span> a,b;</span><br><span class="line">     <span class="built_in">cin</span>&gt;&gt;a&gt;&gt;b;</span><br><span class="line">     <span class="built_in">cout</span>&lt;&lt;a+b&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用法2：接收一个字符串，遇“空格”、“TAB”、“回车”就结束</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     <span class="keyword">char</span> a[<span class="number">20</span>];</span><br><span class="line">     <span class="built_in">cin</span>&gt;&gt;a;</span><br><span class="line">     <span class="built_in">cout</span>&lt;&lt;a&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>输入：hello<br>输出：hello</p><p>输入：hello world<br>输出：hello</p></blockquote><h1 id="2-cin-getline"><a href="#2-cin-getline" class="headerlink" title="2. cin.getline()"></a>2. cin.getline()</h1><p>用法:接收一个字符串，可以接收空格并输出</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     <span class="keyword">char</span> m[<span class="number">20</span>];</span><br><span class="line">     <span class="built_in">cin</span>.getline(m,<span class="number">5</span>);</span><br><span class="line">     <span class="built_in">cout</span>&lt;&lt;m&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><blockquote><p>输入：hello world<br>输出：hell</p></blockquote><p><strong>接收5个字符到m中，其中最后一个为’\0’，所以只看到4个字符输出；</strong></p><p>如果把5改成20：</p><blockquote><p>输入：hello world<br>输出：hello world</p></blockquote><p>延伸：<br>1、cin.getline()实际上有三个参数，<strong>cin.getline(接收字符串的变量,接收字符个数,结束字符)</strong><br>2、当第三个参数省略时，系统默认为’\0’<br>3、如果将例子中<strong>cin.getline()改为cin.getline(m,5,’a’)</strong>;当输入jlkjkljkl时输出jklj，输入jkaljkljkl时，输出jk</p><h1 id="3-getline"><a href="#3-getline" class="headerlink" title="3. getline()"></a>3. getline()</h1><p>用法：接收一个字符串，可以接收空格并输出，需包含<code>#include&lt;string&gt;</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">string</span> str;</span><br><span class="line">getline(<span class="built_in">cin</span>,str);</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;str&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>输入：hello world<br>输出：hello world</p></blockquote><h1 id="4-注意的问题"><a href="#4-注意的问题" class="headerlink" title="4. 注意的问题"></a>4. 注意的问题</h1><p>1、<strong>cin.getline()属于istream流，而getline()属于string流，是不一样的两个函数</strong></p><p>2、当同时使用 <strong>cin ,  getline()</strong> 时，需要注意的是，在cin输入流完成之后，getline()之前，需要通过</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">string</span> str=<span class="string">"\n"</span>;</span><br><span class="line">getline(<span class="built_in">cin</span>,str);</span><br></pre></td></tr></table></figure><p>的方式将<strong>回车符</strong>作为输入流cin以清除缓存，如果不这样做的话，在控制台上就不会出现getline()的输入提示，而直接跳过，因为程序默认地将之前的变量作为输入流。</p><p>看下面一段程序：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> age;</span><br><span class="line">    <span class="comment">//standard input(cin)</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"Please enter an integer value as your age: "</span>;</span><br><span class="line">    <span class="built_in">cin</span>&gt;&gt;age;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"Your ager is: "</span>&lt;&lt;age&lt;&lt;<span class="string">".\n"</span>;</span><br><span class="line">    <span class="comment">//cin and string</span></span><br><span class="line">    <span class="built_in">string</span> mystr;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"What's your name? "</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">///////////////////////////</span></span><br><span class="line">    mystr=<span class="string">"\n"</span>;</span><br><span class="line">    getline(<span class="built_in">cin</span>,mystr);</span><br><span class="line">    <span class="comment">///////////////////////////</span></span><br><span class="line">    </span><br><span class="line">    getline(<span class="built_in">cin</span>,mystr);</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"Hello,"</span>&lt;&lt;mystr&lt;&lt;<span class="string">".\n"</span>;</span><br><span class="line">    <span class="keyword">char</span> sex;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"Please enter a F or M as your sex: "</span>;</span><br><span class="line">    <span class="built_in">cin</span>&gt;&gt;sex;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"Your sex is: "</span>&lt;&lt;sex&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"What's your favorite team? "</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">///////////////////////////</span></span><br><span class="line">    mystr=<span class="string">"\n"</span>;</span><br><span class="line">    getline(<span class="built_in">cin</span>,mystr);</span><br><span class="line">    <span class="comment">///////////////////////////</span></span><br><span class="line">    </span><br><span class="line">    getline(<span class="built_in">cin</span>,mystr);</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"I like "</span>&lt;&lt;mystr&lt;&lt;<span class="string">".\n"</span>;</span><br><span class="line"></span><br><span class="line">    system(<span class="string">"pause"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果为：</p><blockquote><p>Please enter an integer value as your age: 17<br>Your ager is: 17.<br>What’s your name?<br>Kevin<br>Hello,Kevin.<br>Please enter a F or M as your sex: M<br>Your sex is: M<br>What’s your favorite team? Rocket<br>I like Rocket.<br>请按任意键继续. . .</p></blockquote><p>如果不添加<code>///////////////////////////</code>之间的代码，运行结果为：</p><blockquote><p>Please enter an integer value as your age: 17<br>Your ager is: 17.<br>What’s your name?<br>Hello,.<br>Please enter a F or M as your sex: M<br>Your sex is: M<br>What’s your favorite team? I like .<br>请按任意键继续. . .</p></blockquote><p>其实<code>mystr = &#39;\n&#39;</code>可以去掉。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACM </tag>
            
            <tag> C++ </tag>
            
            <tag> OJ </tag>
            
            <tag> cin </tag>
            
            <tag> getline </tag>
            
            <tag> C </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scanf输入字符型BUG</title>
      <link href="/2021/03/28/scanf%E8%BE%93%E5%85%A5%E5%AD%97%E7%AC%A6%E5%9E%8BBUG/"/>
      <url>/2021/03/28/scanf%E8%BE%93%E5%85%A5%E5%AD%97%E7%AC%A6%E5%9E%8BBUG/</url>
      
        <content type="html"><![CDATA[<p>问题原因是：<strong>键盘缓冲区残余信息</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt; </span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="keyword">int</span> a; </span><br><span class="line">    <span class="keyword">char</span> c;</span><br><span class="line">    <span class="keyword">do</span> </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;a); </span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%c"</span>,&amp;c); </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"a=%d     c=%c/n"</span>,a,c); </span><br><span class="line">    &#125;<span class="keyword">while</span>(c!=<span class="string">'N'</span>); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>scanf(&quot;%c&quot;, &amp;c);</code>这句不能正常接收字符,什么原因呢？</p><p>我们用<code>printf(&quot;c=%d\n&quot;,c);</code>将c用int表示出来，看看scanf()函数赋给C到底是什么，结果是 c=10, ASCII值为10</p><p>是什么？换行即<code>\n</code>.</p><p>对了，我们每击打一下”Enter”键，向键盘缓冲区发去一个“回车”(\r),一个“换行”(\n),在这里\r被scanf()函数处理掉了（姑且这么认为吧^_^），而\n被scanf()函数“错误”地赋给了c.</p><p>解决办法：</p><ul><li>可以在两个scanf()函数之后加个fflush(stdin);</li><li>还有加getch(); </li><li>getchar();也可以</li></ul><p>但是要视具体scanf()语句加那个，这里就不分析了。但是加fflush(stdin);不管什么情况都可行。</p><h4 id="scanf-quot-c-quot-是会读入空格以及换行的，所以使用时一定要慎重。"><a href="#scanf-quot-c-quot-是会读入空格以及换行的，所以使用时一定要慎重。" class="headerlink" title="scanf(&quot;%c&quot;)是会读入空格以及换行的，所以使用时一定要慎重。"></a><code>scanf(&quot;%c&quot;)</code>是会读入空格以及换行的，所以使用时一定要慎重。</h4>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> C </tag>
            
            <tag> scanf </tag>
            
            <tag> getchar </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACM输入输出</title>
      <link href="/2021/03/28/ACM%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/"/>
      <url>/2021/03/28/ACM%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/</url>
      
        <content type="html"><![CDATA[<p>在之前的笔试中，由于不熟悉ACM模式的输入输出，吃了大亏！所以在此总结和学习，吃一堑长一智。</p><blockquote><p>版权声明：本文为CSDN博主「yang1young」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/qiao1245/article/details/53020326" target="_blank" rel="noopener">https://blog.csdn.net/qiao1245/article/details/53020326</a></p></blockquote><h1 id="1-C-输入"><a href="#1-C-输入" class="headerlink" title="1. C++输入"></a>1. C++输入</h1><h2 id="1-1-只有一组测试数据"><a href="#1-1-只有一组测试数据" class="headerlink" title="1.1. 只有一组测试数据"></a>1.1. 只有一组测试数据</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt; iostream &gt;   </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     <span class="keyword">int</span> a,b; </span><br><span class="line">     <span class="built_in">cin</span> &gt;&gt; a &gt;&gt; b;</span><br><span class="line">     <span class="built_in">cout</span> &lt;&lt; a+b &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-2-有多组测试数据，直到读至输入文件结尾为止"><a href="#1-2-有多组测试数据，直到读至输入文件结尾为止" class="headerlink" title="1.2. 有多组测试数据，直到读至输入文件结尾为止"></a>1.2. 有多组测试数据，直到读至输入文件结尾为止</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt; iostream &gt;    </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">       <span class="keyword">int</span> a,b;</span><br><span class="line">       <span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; a &gt;&gt; b)</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; a+b &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">       <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-3-在开始的时候输入一个N，接下来是N组数据"><a href="#1-3-在开始的时候输入一个N，接下来是N组数据" class="headerlink" title="1.3. 在开始的时候输入一个N，接下来是N组数据"></a>1.3. 在开始的时候输入一个N，接下来是N组数据</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a, b, n;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; n;</span><br><span class="line">    <span class="keyword">while</span> (n--) &#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; a &gt;&gt; b;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; a + b &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-4-输入不说明有多少组数据，但以某个特殊输入为结束标志"><a href="#1-4-输入不说明有多少组数据，但以某个特殊输入为结束标志" class="headerlink" title="1.4. 输入不说明有多少组数据，但以某个特殊输入为结束标志"></a>1.4. 输入不说明有多少组数据，但以某个特殊输入为结束标志</h2><p>例如A+B，题目中说明以<code>0, 0</code>结束输入，可以按照以下写法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> a, b;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; a &gt;&gt; b &amp;&amp; (a || b)) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; a + b &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-5-重定向输入，保存历史"><a href="#1-5-重定向输入，保存历史" class="headerlink" title="1.5. 重定向输入，保存历史"></a>1.5. 重定向输入，保存历史</h2><a id="more"></a><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    freopen(<span class="string">"input.txt"</span>, <span class="string">"r"</span>, <span class="built_in">stdin</span>);  <span class="comment">// 输入将被重定向到文件</span></span><br><span class="line">    <span class="keyword">int</span> a, b;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; a &gt;&gt; b;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; a + b &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码从文件<font color="orange">“input.txt”</font>中读取输入，然后计算后输入到标准输出。</p><p>也可以用<code>freopen</code>函数将标准输出重定向到文件，示例代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  freopen (<span class="string">"output.txt"</span>,<span class="string">"w"</span>,<span class="built_in">stdout</span>);</span><br><span class="line">  <span class="built_in">printf</span> (<span class="string">"This sentence is redirected to a file.\n"</span>);</span><br><span class="line">  <span class="built_in">cout</span> &lt;&lt; <span class="string">"hello freopen function"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">  fclose (<span class="built_in">stdout</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在文件<font color="orange">“output.txt”</font>中显示为：</p><blockquote><p>This sentence is redirected to a file.</p><p>hello freopen function</p></blockquote><h2 id="1-6-输入字符串"><a href="#1-6-输入字符串" class="headerlink" title="1.6. 输入字符串"></a>1.6. 输入字符串</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> buf[<span class="number">255</span>];</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">cin</span>.getline(buf, <span class="number">255</span>)) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; buf &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在C++中读入字符串通常使用cin.getline函数，可以接受用户的输入的字符，直到已达指定个数，或者用户输入了特定的字符。它的函数声明形式（函数原型）如下：</p><blockquote><p>istream&amp; getline(char line[], int size, char endchar= ‘\n’);<br>char line[]： 就是一个字符数组，用户输入的内容将存入在该数组内。<br>int size : 最多接受几个字符，用户超过size的输入都将不被接受。<br>char endchar :当用户输入endchar指定的字符时，自动结束，默认是回车符。</p></blockquote><p><strong>使用<code>getline()</code>函数读取字符串，字符串中可以包含空格</strong></p><p>另一种读取字符串的方式是使用string，但是这种方式是无法读入空格的。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">string</span> buf;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; buf) &#123;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; buf &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输入：</p><blockquote><p>hello world</p></blockquote><p>输出：</p><blockquote><p>hello</p><p>world</p></blockquote><p>默认按照空格进行了切分，读入了两个字符串。</p><h1 id="2-C-输出"><a href="#2-C-输出" class="headerlink" title="2. C++输出"></a>2. C++输出</h1><p>输出有不同的格式要求，不注意的话经常会出现 <code>Presentation Error</code>，而且 PC2 很多时候还判断不出来输出格式错误，就简单的判为 <code>Wrong Answer</code>，所以输出格式一定要注意。</p><h3 id="1、一组输出接着一组输出，中间没有空行"><a href="#1、一组输出接着一组输出，中间没有空行" class="headerlink" title="1、一组输出接着一组输出，中间没有空行"></a>1、一组输出接着一组输出，中间没有空行</h3><p>这也是最简单的，请看题目 <a href="http://acm.sdut.edu.cn/onlinejudge2/index.php/Home/Index/problemdetail/pid/1010" target="_blank" rel="noopener">SDUT 1010</a>。</p><p>C 代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a, b;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;a, &amp;b) != EOF) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, a + b);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每输出一组结果换行就可以了。</p><h3 id="2、一组接着一组，每一组后面有一空行"><a href="#2、一组接着一组，每一组后面有一空行" class="headerlink" title="2、一组接着一组，每一组后面有一空行"></a>2、一组接着一组，每一组后面有一空行</h3><p>请看题目 <a href="http://acm.sdut.edu.cn/onlinejudge2/index.php/Home/Index/problemdetail/pid/1016" target="_blank" rel="noopener">SDUT 1016</a>。</p><p>C 代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a, b;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;a, &amp;b) != EOF) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n\n"</span>, a + b);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每输出一组结果后输出两个换行就可以了。</p><h3 id="3、一组接着一组，每两组之间有一个空行"><a href="#3、一组接着一组，每两组之间有一个空行" class="headerlink" title="3、一组接着一组，每两组之间有一个空行"></a>3、一组接着一组，每两组之间有一个空行</h3><p>注意与前一种区分开，请看题目 <a href="http://acm.sdut.edu.cn/onlinejudge2/index.php/Home/Index/problemdetail/pid/1017" target="_blank" rel="noopener">SDUT 1017</a>。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n, m, sum, a;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;m);</span><br><span class="line">        sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(m--) &#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;a);</span><br><span class="line">            sum += a;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, sum);</span><br><span class="line">        <span class="keyword">if</span> (i != n - <span class="number">1</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>判断是否到达最后一组测试数据了，如果不是最后一组测试数据就多输出一个换行。</p><p><strong><em>很多题目都要求在输出数据的恰当位置加空行。一个空行就是一个单独的”\n”。这里，有的题目说：“After each test case, you should output one blank line”，而有的题目说：“Between each test case, you should ouput one blank line”。要注意After和Between的区别，因为如果多了一或少了空行，将导致Presentation Error甚至Wrong Answer。</em></strong></p><h1 id="3-C语言输入输出"><a href="#3-C语言输入输出" class="headerlink" title="3. C语言输入输出"></a>3. C语言输入输出</h1><p>有时候C++的输入输出可能太慢，不妨使用C语言的输入输出</p><p>主要方法有：</p><blockquote><p>printf ();//把键盘中的各类数据,加以格式控制输出到显示器屏幕上;<br>scanf ();//从键盘上输入各类数据,并存放到程序变量中;<br>puts ()://把数组变量中的一个字符串常量输出到显示器屏幕上;<br>gets ()://从键盘上输入一个字符串常量并放到程序的数组中;<br>putchar ()://把变量中的一个字符常量输出到显示器屏幕上;<br>getchar ()://从键盘上输入一个字符常量,此常量就是该函数的值;<br>sscanf()://从一个字符串中提取各类数据。</p></blockquote><h2 id="3-1-printf"><a href="#3-1-printf" class="headerlink" title="3.1. printf"></a>3.1. printf</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"C Programming\n"</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"Number = %d\n"</span>, <span class="number">5</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"number1 = %f\n"</span>, <span class="number">13.5</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"number2 = %f\n"</span>, <span class="number">12.4</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"character = %c\n"</span>, <span class="string">'c'</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果：</p><blockquote><p>C Programming<br>Number = 5<br>number1 = 13.500000<br>number2 = 12.400000<br>character = c</p></blockquote><h2 id="3-2-scanf"><a href="#3-2-scanf" class="headerlink" title="3.2. scanf"></a>3.2. scanf</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> num;</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;num);</span><br><span class="line"></span><br><span class="line"><span class="keyword">float</span> num1;</span><br><span class="line"><span class="keyword">double</span> num2;</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%f"</span>, &amp;num1);</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%lf"</span>, &amp;num2);</span><br><span class="line"></span><br><span class="line">fflush(<span class="built_in">stdin</span>);  <span class="comment">// 清除缓冲区 </span></span><br><span class="line"><span class="keyword">char</span> c;</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%c"</span>, &amp;c);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> a;</span><br><span class="line"><span class="keyword">float</span> b;</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%d%f"</span>, &amp;a, &amp;b);</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"num = %d\n"</span>, num);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"num1 = %f\n"</span>, num1);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"num2 = %lf\n"</span>, num2);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"character = %c\n"</span>, c);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"multiple values = %d, %f"</span>, a, b);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果为：</p><blockquote><p>num = 15<br>num1 = 1.200000<br>num2 = 3.560000<br>character = z<br>multiple values = 7, 7.770000</p></blockquote><h2 id="3-3-puts-gets"><a href="#3-3-puts-gets" class="headerlink" title="3.3. puts, gets"></a>3.3. puts, gets</h2><p><code>gets()</code>与<code>scanf(&quot;%s&quot;)</code>均可用于读取字符串。</p><p>1.不同点：</p><p>scanf不能接受空格、制表符Tab、回车等；</p><p><strong>而gets能够接受空格、制表符Tab和回车等；</strong></p><p>scanf ：当遇到回车，空格和tab键会自动在字符串后面添加’\0’，但是回车，空格和tab键仍会留在输入的缓冲区中。</p><p><strong>gets：可接受回车键之前输入的所有字符，并用’\0’替代 ‘\n’.回车键不会留在输入缓冲区中</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">char</span> buf[<span class="number">100</span>];</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%s"</span>, buf);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%s\n"</span>, buf);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>输入: hello world</p><p>输出: hello</p></blockquote><p>从键盘输入字符串<code>hello world</code>时，遇到空格，scanf()就认为输入结束了，所以buf中存放的字符串是 ‘hello\0’。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> a[<span class="number">10</span>];</span><br><span class="line">    <span class="keyword">char</span> c;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%s"</span>,a);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"a中保存的字符串为：%s\n"</span>,a);</span><br><span class="line">    c=getchar();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"c中保存的字符为：%c "</span>,c);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输入asdf回车，<strong>因为scanf会将回车保留在缓冲区中，所以回车会紧接着被c取得而不需要再额外输入，所以a中存储的是’asdf\0’，字符c=’\n’</strong>。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> b[<span class="number">10</span>];</span><br><span class="line">    <span class="keyword">char</span> d;</span><br><span class="line">    gets(b);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"b中保存的字符串为：%s\n"</span>,b);</span><br><span class="line">    d=getchar();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"d中保存的字符为：%c"</span>,d);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接着输入<code>as df</code>回车，因为gets会将’\n’替换成’\0’，所以b字符串中保留的是<code>’as df\0’</code>，并且还是要继续输入d的值。</p><p><strong><code>printf()</code>与<code>puts()</code>不同点：</strong><br>puts（）在输出字符串时会将’\0’自动转换成’\n’进行输出，也就是说，puts方法输出完字符串后会自动换行。</p><h2 id="3-4-putchar-getchar"><a href="#3-4-putchar-getchar" class="headerlink" title="3.4. putchar, getchar"></a>3.4. putchar, getchar</h2><p><code>putchar()</code>是put character的缩写。作用是输出<strong>一个</strong>字符的值。</p><p>向计算机输入一个字符可以调用<code>getchar()</code>函数。</p><font color="orange">**强烈建议输入字符时均使用`getchar()`代替`scanf("%c")`，避免出现意想不到的BUG**</font><h2 id="3-5-sscanf"><a href="#3-5-sscanf" class="headerlink" title="3.5. sscanf"></a>3.5. sscanf</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sscanf</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *str, <span class="keyword">const</span> <span class="keyword">char</span> *format, ...)</span></span></span><br></pre></td></tr></table></figure><p>从字符串读取格式化输入。</p><p>sscanf用法跟scanf差不多，只不过scanf是从标准输入stdin中读入，而sscanf函数是从指定的字符串中读取。相比scanf函数，sscanf函数多了第一个参数，传入一个字符串。</p><p><strong>取指定长度的字符串</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> str[<span class="number">100</span>];</span><br><span class="line">    <span class="built_in">sscanf</span>(<span class="string">"123456"</span>,<span class="string">"%3s"</span>,str);</span><br><span class="line">    <span class="built_in">puts</span>(str);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>123</p></blockquote><p><strong>将表示数字的字符串转换成整型变量</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> res;</span><br><span class="line">    <span class="built_in">sscanf</span>(<span class="string">"123456"</span>, <span class="string">"%d"</span>, &amp;res);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,res);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>123456</p></blockquote><p><strong>取需要的字符串</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> year,month,day;</span><br><span class="line">    <span class="built_in">sscanf</span>(<span class="string">"2017.12.5"</span>,<span class="string">"%d.%d.%d"</span>,&amp;year,&amp;month,&amp;day);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"year:%d,month:%d,day:%d\n"</span>,year,month,day);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>year:2017,month:12,day:5</p></blockquote><h2 id="3-6-带格式化的字符串输出"><a href="#3-6-带格式化的字符串输出" class="headerlink" title="3.6. 带格式化的字符串输出"></a>3.6. 带格式化的字符串输出</h2><p>有些题目要求输出这样的字符串：<code>abc*****de****f</code>，其中“*”代表空格。<br>要求是这样的：str1在前5个字符中左对齐，str2在第6到第10个字符中右对齐，str3在第11到第15个字符中右对齐。<br>可行的做法是，先初始化一个数组，用’ ‘（空格）填充，再在相应的位置填相应的内容。用程序来表述：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> str[<span class="number">1000</span>];</span><br><span class="line"><span class="keyword">char</span> str1[] = <span class="string">"abc"</span>, str2[] = <span class="string">"de"</span>, str3[] = <span class="string">"f"</span>;</span><br><span class="line"><span class="built_in">memset</span>(str, <span class="string">' '</span>, <span class="number">1000</span> * <span class="keyword">sizeof</span>(<span class="keyword">char</span>));  <span class="comment">// 用空格填充</span></span><br><span class="line"><span class="built_in">sprintf</span>(str, <span class="string">"%s"</span>, str1);</span><br><span class="line">str[<span class="built_in">strlen</span>(str1)] = <span class="string">' '</span>;</span><br><span class="line"><span class="built_in">sprintf</span>(str + <span class="number">5</span>, <span class="string">"%5s"</span>, str2);</span><br><span class="line">str[<span class="number">10</span>] = <span class="string">' '</span>;</span><br><span class="line"><span class="built_in">sprintf</span>(str + <span class="number">10</span>, <span class="string">"%5s"</span>, str3);</span><br><span class="line">str[<span class="number">15</span>] = <span class="string">'\0'</span>;</span><br><span class="line"><span class="built_in">puts</span>(str);</span><br></pre></td></tr></table></figure><p>关键的部分：<br>（1）在调用sprintf后，要清除不恰当字符串结束符（第5,7行）；<br>（2）在恰当的位置添加字符串结束符（第9行）。</p><h2 id="3-7-二维数组的输出"><a href="#3-7-二维数组的输出" class="headerlink" title="3.7. 二维数组的输出"></a>3.7. 二维数组的输出</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i, j;</span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; nRow; i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; nCol; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (j &gt; <span class="number">0</span>)</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">" "</span>);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%d"</span>, a[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="built_in">puts</span>(<span class="string">""</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><font color="blue">**注意每行最后不要多输出1个空格**</font><h1 id="4-格式化"><a href="#4-格式化" class="headerlink" title="4. 格式化"></a>4. 格式化</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%d  以十进制形式输出带符号整数(正数不输出符号)   </span><br><span class="line">%o  以八进制形式输出无符号整数(不输出前缀O)   </span><br><span class="line">%x  以十六进制形式输出无符号整数(不输出前缀OX)   </span><br><span class="line">%u  以十进制形式输出无符号整数   </span><br><span class="line">%f  以小数形式输出单精度实数   </span><br><span class="line">%lf 以小数形式输出双精度实数 </span><br><span class="line">%e  以指数形式输出单、双精度实数   </span><br><span class="line">%g  以%f%e中较短的输出宽度输出单、双精度实数   </span><br><span class="line">%c  输出单个字符   </span><br><span class="line">%s  输出字符串</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">float</span> a = <span class="number">125.78</span>;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%f\n"</span>, a);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%e\n"</span>, a);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果为：</p><blockquote><p>125.779999<br>1.257800e+002</p></blockquote><h2 id="4-1-C语言精度控制"><a href="#4-1-C语言精度控制" class="headerlink" title="4.1. C语言精度控制"></a>4.1. C语言精度控制</h2><p>精度格式符以“.”开头，后跟十进制整数。意义是：如果输出数字，则表示小数的位数；如果输出的是字符， 则表示输出字符的个数；若实际位数大于所定义的精度数，则截去超过的部分。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">double</span> a = <span class="number">1.1</span>;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%.2f\n"</span>, a);  <span class="comment">// 保留两位小数，补齐</span></span><br><span class="line"><span class="keyword">double</span> b = <span class="number">123456789.456</span>;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%.2f\n"</span>, b);  <span class="comment">// 保留两位小数，截断</span></span><br><span class="line"><span class="keyword">double</span> c=<span class="number">24212345.24232</span>; </span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%020.4f\n"</span>, c);  <span class="comment">// 保留四位小数，输出占20位，若有空余的位补0</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><blockquote><p>1.10<br>123456789.46<br>000000024212345.2423</p></blockquote><h2 id="4-2-C-精度控制"><a href="#4-2-C-精度控制" class="headerlink" title="4.2. C++精度控制"></a>4.2. C++精度控制</h2><p>(只作了解，在笔试具体使用时，还是要用<code>printf()</code>)</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">float</span> num = <span class="number">1.25</span>;</span><br></pre></td></tr></table></figure><p>1 设置对齐方式</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span>.setf(ios::right); <span class="comment">// 设置对齐方式</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; num &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><blockquote><p>1.25</p></blockquote><p>2 设置输出宽度</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span>.setf(ios::right); <span class="comment">// 设置对齐方式</span></span><br><span class="line"><span class="built_in">cout</span>.width(<span class="number">8</span>); <span class="comment">//设置输出宽度</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; num &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><blockquote><p>​    1.25</p></blockquote><p>3 将宽度多余的部分用某个字符（如：’0’）填充</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span>.setf(ios::right); <span class="comment">// 设置对齐方式</span></span><br><span class="line"><span class="built_in">cout</span>.width(<span class="number">8</span>); <span class="comment">//设置输出宽度</span></span><br><span class="line"><span class="built_in">cout</span>.fill(<span class="string">'0'</span>); <span class="comment">//将多余的空格用0填充</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; num &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><blockquote><p>00001.25</p></blockquote><p>4 设置精度：保留**位有效数字，如果小数点最后面有0，则自动去掉</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span>.setf(ios::right); <span class="comment">// 设置对齐方式</span></span><br><span class="line"><span class="built_in">cout</span>.width(<span class="number">8</span>); <span class="comment">//设置输出宽度</span></span><br><span class="line"><span class="built_in">cout</span>.fill(<span class="string">'0'</span>); <span class="comment">//将多余的空格用0填充</span></span><br><span class="line"><span class="built_in">cout</span>.precision(<span class="number">2</span>); <span class="comment">//设置输出精度，保留有效数字</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; num &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><blockquote><p>000001.2</p></blockquote><p>5 保留小数点后**位数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span>.setf(ios::right); <span class="comment">// 设置对齐方式</span></span><br><span class="line"><span class="built_in">cout</span>.width(<span class="number">8</span>); <span class="comment">//设置输出宽度</span></span><br><span class="line"><span class="built_in">cout</span>.fill(<span class="string">'0'</span>); <span class="comment">//将多余的空格用0填充</span></span><br><span class="line"><span class="built_in">cout</span>.flags(ios::fixed);</span><br><span class="line"><span class="built_in">cout</span>.precision(<span class="number">4</span>); <span class="comment">//设置输出精度，</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; num &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><blockquote><p>001.2500</p></blockquote><p>6 保留小数点后有效的位数。如：1.25 保留4位有效数字后的1.250中0的显示，要依靠cout.setf(ios::showpoint)函数（因为保留有效数字的函数不会保留没用的0有效位）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span>.setf(ios::right); <span class="comment">// 设置对齐方式</span></span><br><span class="line"><span class="built_in">cout</span>.width(<span class="number">8</span>); <span class="comment">//设置输出宽度</span></span><br><span class="line"><span class="built_in">cout</span>.fill(<span class="string">'0'</span>); <span class="comment">//将多余的空格用0填充</span></span><br><span class="line"><span class="built_in">cout</span>.setf(ios::showpoint); <span class="comment">//将小数精度后面的0显示出来</span></span><br><span class="line"><span class="built_in">cout</span>.precision(<span class="number">4</span>); <span class="comment">//设置输出精度，保留有效数字</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; num &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><blockquote><p>0001.250</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACM </tag>
            
            <tag> C++ </tag>
            
            <tag> OJ </tag>
            
            <tag> iostream </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>整除3的最大和</title>
      <link href="/2021/03/28/%E6%95%B4%E9%99%A43%E7%9A%84%E6%9C%80%E5%A4%A7%E5%92%8C/"/>
      <url>/2021/03/28/%E6%95%B4%E9%99%A43%E7%9A%84%E6%9C%80%E5%A4%A7%E5%92%8C/</url>
      
        <content type="html"><![CDATA[<p>对应于LeetCode. 1262. Greatest Sum Divisible by Three.</p><p>关键在于想到<code>模三取余</code>的操作。这道题及其变种是经常遇到的题目。</p><p>解决方案：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxSumDivThree</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = nums.size();</span><br><span class="line">        <span class="keyword">int</span> **matrix = <span class="keyword">new</span> <span class="keyword">int</span>* [n];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;++i)</span><br><span class="line">        matrix[i] = <span class="keyword">new</span> <span class="keyword">int</span> [<span class="number">3</span>];</span><br><span class="line">        </span><br><span class="line">       <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">3</span>;++j)  <span class="comment">// 初始化 </span></span><br><span class="line">   matrix[<span class="number">0</span>][j] = <span class="number">0</span>; </span><br><span class="line">matrix[<span class="number">0</span>][nums[<span class="number">0</span>]%<span class="number">3</span>] = nums[<span class="number">0</span>];</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;n;++i) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">3</span>;++j) </span><br><span class="line">        matrix[i][j] = matrix[i<span class="number">-1</span>][j];</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">3</span>;++j) &#123;</span><br><span class="line">        <span class="keyword">int</span> sum = nums[i] + matrix[i<span class="number">-1</span>][j];</span><br><span class="line">        matrix[i][sum%<span class="number">3</span>] = max(sum, matrix[i][sum%<span class="number">3</span>]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> matrix[n<span class="number">-1</span>][<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> DP </tag>
            
            <tag> 动态规划 </tag>
            
            <tag> Dynamic Programming </tag>
            
            <tag> dp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch分布式训练-DistributedDataParallel</title>
      <link href="/2021/03/24/Pytorch%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-DistributedDataParallel/"/>
      <url>/2021/03/24/Pytorch%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-DistributedDataParallel/</url>
      
        <content type="html"><![CDATA[<p>与 DataParallel 的单进程控制多 GPU 不同，在 distributed 的帮助下，我们只需要编写一份代码，torch 就会自动将其分配给n个进程，分别在n个 GPU 上运行。</p><p>和单进程训练不同的是，多进程训练需要注意以下事项：</p><ul><li>在喂数据的时候，一个batch被分到了好几个进程，每个进程在取数据的时候要确保拿到的是不同的数据（<code>DistributedSampler</code>）；</li><li>要告诉每个进程自己是谁，使用哪块GPU（<code>args.local_rank</code>）；</li><li>在做BatchNormalization的时候要注意同步数据。</li></ul><h2 id="1-使用方式"><a href="#1-使用方式" class="headerlink" title="1. 使用方式"></a>1. 使用方式</h2><h3 id="1-1-启动方式的改变"><a href="#1-1-启动方式的改变" class="headerlink" title="1.1. 启动方式的改变"></a>1.1. 启动方式的改变</h3><p>在多进程的启动方面，我们不用自己手写 multiprocess 进行一系列复杂的CPU、GPU分配任务，PyTorch为我们提供了一个很方便的启动器 <code>torch.distributed.launch</code> 用于启动文件，所以我们运行训练代码的方式就变成了这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 main.py</span><br></pre></td></tr></table></figure><p>其中的 <code>--nproc_per_node</code> 参数用于指定为当前主机创建的进程数，由于我们是单机多卡，所以这里node数量为1，所以我们这里设置为所使用的GPU数量即可。</p><h3 id="1-2-初始化"><a href="#1-2-初始化" class="headerlink" title="1.2. 初始化"></a>1.2. 初始化</h3><p>在启动器为我们启动python脚本后，在执行过程中，启动器会将当前进程的（其实就是 GPU的）index 通过参数传递给 python，我们可以这样获得当前进程的 index：即通过参数 <code>local_rank</code> 来告诉我们<strong>当前进程使用的是哪个GPU</strong>，用于我们在每个进程中指定不同的device：</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">()</span>:</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--local_rank'</span>, type=int, default=<span class="number">0</span>，help=<span class="string">'node rank for distributed training'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="keyword">return</span> args</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    args = parse()</span><br><span class="line">    torch.cuda.set_device(args.local_rank)</span><br><span class="line">    torch.distributed.init_process_group(</span><br><span class="line">        <span class="string">'nccl'</span>,</span><br><span class="line">        init_method=<span class="string">'env://'</span></span><br><span class="line">    )</span><br><span class="line">    device = torch.device(<span class="string">f'cuda:<span class="subst">&#123;args.local_rank&#125;</span>'</span>)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>其中 torch.distributed.<strong>init_process_group</strong> 用于<strong>初始化GPU通信方式（NCCL）和参数的获取方式（env代表通过环境变量）</strong>。使用 init_process_group 设置GPU之间通信使用的后端和端口，通过 NCCL 实现 GPU 通信。</p><h3 id="1-3-DataLoader"><a href="#1-3-DataLoader" class="headerlink" title="1.3. DataLoader"></a>1.3. DataLoader</h3><p>在读取数据的时候，我们要保证一个batch里的数据<strong>被均摊到每个进程上</strong>，每个进程都能获取到不同的数据，但如果我们手动去告诉每个进程拿哪些数据的话太麻烦了，PyTorch也为我们封装好了这一方法。之后，使用 <strong>DistributedSampler 对数据集进行划分</strong>。如此前我们介绍的那样，它能帮助我们将每个 batch 划分成几个 partition，在当前进程中只需要获取和 rank 对应的那个 partition 进行训练。</p><p>所以我们在初始化 <code>data loader</code> 的时候需要使用到 <code>torch.utils.data.distributed.DistributedSampler</code> 这个特性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)</span><br></pre></td></tr></table></figure><p>这样就能给<strong>每个进程一个不同的 sampler</strong>，告诉每个<strong>进程自己分别取哪些数据</strong>。</p><h3 id="1-4-模型的初始化"><a href="#1-4-模型的初始化" class="headerlink" title="1.4. 模型的初始化"></a>1.4. 模型的初始化</h3><p>和 <code>nn.DataParallel</code> 的方式一样，我们对于模型的初始化也是简单的一句话就行了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])</span><br></pre></td></tr></table></figure><p>使用 DistributedDataParallel 包装模型，它能帮助我们为不同 GPU 上求得的梯度进行 all reduce（即汇总不同 GPU 计算所得的梯度，并同步计算结果）。all reduce 后不同 GPU 中模型的梯度均为 all reduce 之前各 GPU 梯度的均值。</p><h3 id="1-5-同步Batch-Normalization"><a href="#1-5-同步Batch-Normalization" class="headerlink" title="1.5. 同步Batch Normalization"></a>1.5. 同步Batch Normalization</h3><p><a href="https://zhuanlan.zhihu.com/p/40496177" target="_blank" rel="noopener">为什么要同步BN？</a></p><p>现有的标准 Batch Normalization 因为使用数据并行（Data Parallel），是单卡的实现模式，只对单个卡上对样本进行归一化，相当于减小了批量大小（batch-size）（详见BN工作原理部分）。 对于比较消耗显存的训练任务时，往往单卡上的相对批量过小，影响模型的收敛效果。<strong>之前在我们在图像语义分割的实验中，Jerry和我就发现使用大模型的效果反而变差，实际上就是BN在作怪</strong>。 跨卡同步 Batch Normalization 可以使用全局的样本进行归一化，这样相当于‘增大‘了批量大小，这样训练效果不再受到使用 GPU 数量的影响。 最近在图像分割、物体检测的论文中，使用跨卡BN也会显著地提高实验效果，所以跨卡 BN 已然成为竞赛刷分、发论文的必备神器。</p><p><font color="red">可惜 PyTorch 并没有为我们实现这一功能</font>，在接下来的介绍中我们会在 <code>apex</code> 中看到这一功能。</p><h2 id="2-汇总"><a href="#2-汇总" class="headerlink" title="2. 汇总"></a>2. 汇总</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> DataParallel</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> distributed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(<span class="number">30000</span>, <span class="number">50</span>, padding_idx=<span class="number">0</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">50</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, ids, bag_labels, E1s)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param ids: (sents, seq_len) &lt;- (8, 120)</span></span><br><span class="line"><span class="string">        :param bag_labels: (bags, class_num) &lt;- (3, 2)</span></span><br><span class="line"><span class="string">        :param E1s: (bags, variable) &lt;- (3, variable)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        print(<span class="string">'ids '</span>, ids.shape)</span><br><span class="line">        print(<span class="string">'bag_labels '</span>, bag_labels.shape)</span><br><span class="line">        print(<span class="string">'E1s '</span>, E1s)</span><br><span class="line">        print(<span class="string">'============================='</span>)</span><br><span class="line">        x = self.embedding(ids)</span><br><span class="line">        x = x.mean(dim=<span class="number">1</span>)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Data</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Data, self).__init__()</span><br><span class="line">        self.x = torch.randint(<span class="number">0</span>, <span class="number">9999</span>, size=(<span class="number">1000</span>, <span class="number">120</span>))  <span class="comment"># 1000 samples</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.x[item]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        ids = torch.stack(data, dim=<span class="number">0</span>)</span><br><span class="line">        bag_labels = torch.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">        E1s = [[<span class="number">15</span>, <span class="number">39</span>], [<span class="number">9537</span>], [<span class="number">1</span>, <span class="number">70</span>, <span class="number">5132</span>]]</span><br><span class="line">        <span class="keyword">return</span> ids, bag_labels, E1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--local_rank'</span>, default=<span class="number">-1</span>, type=int,  <span class="comment"># 用于启动器告知当前进程GPU号</span></span><br><span class="line">                        help=<span class="string">'node rank for distributed training'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    distributed.init_process_group(<span class="string">"nccl"</span>)</span><br><span class="line">    torch.cuda.set_device(args.local_rank)</span><br><span class="line"></span><br><span class="line">    train_set = Data()</span><br><span class="line">    train_sampler = torch.utils.data.distributed.DistributedSampler(train_set)</span><br><span class="line">    train_loader = DataLoader(train_set, batch_size=<span class="number">8</span>, sampler=train_sampler, drop_last=<span class="literal">True</span>,</span><br><span class="line">                              collate_fn=train_set.generate_batch)</span><br><span class="line"></span><br><span class="line">    device = torch.device(args.local_rank)</span><br><span class="line">    model = Model().to(device)</span><br><span class="line">    model = DistributedDataParallel(model, device_ids=[args.local_rank])</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">    loss_func = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        print(<span class="string">"epoch &#123;&#125;"</span>.format(epoch))</span><br><span class="line">        <span class="keyword">for</span> step, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">            ids = data[<span class="number">0</span>].to(device)</span><br><span class="line">            bag_labels = data[<span class="number">1</span>].to(device)</span><br><span class="line">            E1s = data[<span class="number">2</span>]</span><br><span class="line">            logits = model(ids, bag_labels, E1s)</span><br><span class="line">            labels = torch.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">8</span>, )).cuda()</span><br><span class="line">            loss = loss_func(logits, labels)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br></pre></td></tr></table></figure><p>在使用时，调用 torch.distributed.launch 启动器启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 run.py</span><br></pre></td></tr></table></figure><h3 id="2-1-数据切分"><a href="#2-1-数据切分" class="headerlink" title="2.1. 数据切分"></a>2.1. 数据切分</h3><p>在实验时发现，使用<code>DistributedDataParallel</code>不会对数据进行切分，应该是与使用了多个进程有关。因此可以用于对数据要求比较苛刻的实验环境。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> DistributedDataParallel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DataParallel数据切分的方式</title>
      <link href="/2021/03/24/DataParallel%E6%95%B0%E6%8D%AE%E5%88%87%E5%88%86%E7%9A%84%E6%96%B9%E5%BC%8F/"/>
      <url>/2021/03/24/DataParallel%E6%95%B0%E6%8D%AE%E5%88%87%E5%88%86%E7%9A%84%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>最近在使用DataParallel时，需要仔细探讨Pytorch默认的数据划分方式，于是做个实验进行尝试。</p><p>先说结论：</p><h2 id="1-结论"><a href="#1-结论" class="headerlink" title="1. 结论"></a>1. 结论</h2><ol><li>对于tensor型数据，平均划分；如果不能整除，从前到后补充</li><li>对于非tensor型数据，复制到每台设备</li></ol><a id="more"></a><h2 id="2-实验"><a href="#2-实验" class="headerlink" title="2. 实验"></a>2. 实验</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> DataParallel</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(<span class="number">30000</span>, <span class="number">50</span>, padding_idx=<span class="number">0</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">50</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, ids, bag_labels, E1s)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param ids: (sents, seq_len) &lt;- (8, 120)</span></span><br><span class="line"><span class="string">        :param bag_labels: (bags, class_num) &lt;- (3, 2)</span></span><br><span class="line"><span class="string">        :param E1s: (bags, variable) &lt;- (3, variable)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        print(<span class="string">"ids "</span>, ids.shape)</span><br><span class="line">        print(<span class="string">"bag_labels "</span>, bag_labels.shape)</span><br><span class="line">        print(<span class="string">"E1s "</span>, E1s)</span><br><span class="line">        print(<span class="string">'============================='</span>)</span><br><span class="line">        x = self.embedding(ids)</span><br><span class="line">        x = x.mean(dim=<span class="number">1</span>)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Data</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Data, self).__init__()</span><br><span class="line">        self.x = torch.randint(<span class="number">0</span>, <span class="number">9999</span>, size=(<span class="number">1000</span>, <span class="number">120</span>))  <span class="comment"># 1000 samples</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.x[item]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 设置可见显卡 (物理层)</span></span><br><span class="line">    os.environ[<span class="string">"CUDA_DEVICE_ORDER"</span>] = <span class="string">"PCI_BUS_ID"</span></span><br><span class="line">    os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"1, 2"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置并行显卡 (逻辑层)</span></span><br><span class="line">    device = torch.device(<span class="string">"cuda:0"</span>)</span><br><span class="line">    model = Model().to(device)  <span class="comment"># 发送到设备</span></span><br><span class="line">    model = DataParallel(model, device_ids=[<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># 多卡训练</span></span><br><span class="line">    loader = DataLoader(Data(), batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> step, data <span class="keyword">in</span> enumerate(loader):</span><br><span class="line">            data = data.to(device)</span><br><span class="line">            bag_labels = torch.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">3</span>, <span class="number">2</span>)).to(device)</span><br><span class="line">            E1s = [[<span class="number">15</span>, <span class="number">39</span>], [<span class="number">9537</span>], [<span class="number">1</span>, <span class="number">70</span>, <span class="number">5132</span>]]</span><br><span class="line">            logits = model(data, bag_labels, E1s)</span><br><span class="line">            print(<span class="string">"step &#123;&#125;"</span>.format(step))</span><br></pre></td></tr></table></figure><p>打印结果为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">step 87</span><br><span class="line">ids  torch.Size([4, 120])</span><br><span class="line">bag_labels  torch.Size([2, 2])</span><br><span class="line">E1s  [[15, 39], [9537], [1, 70, 5132]]</span><br><span class="line">=============================</span><br><span class="line">ids  torch.Size([4, 120])</span><br><span class="line">bag_labels  torch.Size([1, 2])</span><br><span class="line">E1s  [[15, 39], [9537], [1, 70, 5132]]</span><br><span class="line">=============================</span><br><span class="line">step 88</span><br><span class="line">ids  torch.Size([4, 120])</span><br><span class="line">bag_labels  torch.Size([2, 2])</span><br><span class="line">E1s  [[15, 39], [9537], [1, 70, 5132]]</span><br><span class="line">=============================</span><br><span class="line">ids  torch.Size([4, 120])</span><br><span class="line">bag_labels  torch.Size([1, 2])</span><br><span class="line">E1s  [[15, 39], [9537], [1, 70, 5132]]</span><br><span class="line">=============================</span><br><span class="line">......</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> DataParallel </tag>
            
            <tag> tensor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA_DEVICE_ORDER</title>
      <link href="/2021/03/24/CUDA-DEVICE-ORDER/"/>
      <url>/2021/03/24/CUDA-DEVICE-ORDER/</url>
      
        <content type="html"><![CDATA[<p>NVML工具nvidia-smi按设备的PCI Bus ID 为设备分配索引序号，由于PCI Bus ID 是硬件相关的，我们把设备的 PCI Bus ID 或者按该ID分配的索引号为物理ID。</p><p>CUDA应用运行时进行设备查询（比如deviceQuery）返回的设备ID可能与物理ID不一致，譬如下面这位网友遇到的情况：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## NVML id 是 物理ID，CUDA index是CUDA应用查询返回的设备ID</span></span><br><span class="line">NVML id 0 maps to Cuda index 4</span><br><span class="line">NVML id 1 maps to Cuda index 5</span><br><span class="line">NVML id 2 maps to Cuda index 6</span><br><span class="line">NVML id 3 maps to Cuda index 7</span><br><span class="line">NVML id 4 maps to Cuda index 0</span><br><span class="line">NVML id 5 maps to Cuda index 1</span><br><span class="line">NVML id 6 maps to Cuda index 2</span><br><span class="line">NVML id 7 maps to Cuda index 3</span><br><span class="line">NVML id 8 maps to Cuda index 8</span><br><span class="line">NVML id 9 maps to Cuda index 9</span><br><span class="line">NVML id 10 maps to Cuda index 10</span><br><span class="line">NVML id 11 maps to Cuda index 11</span><br><span class="line">NVML id 12 maps to Cuda index 12</span><br><span class="line">NVML id 13 maps to Cuda index 13</span><br><span class="line">NVML id 14 maps to Cuda index 14</span><br><span class="line">NVML id 15 maps to Cuda index 15</span><br></pre></td></tr></table></figure><p>我们可以通过设置 CUDA_DEVICE_ORDER = PCI_BUS_ID 来要求运行时设备查询按照 PCI_BUS_ID 的顺序索引，从而使得  设备ID=物理ID  保证CUDA应用按期望使用指定设备。目前最佳的设置方法是同时设置 CUDA_DEVICE_ORDER = PCI_BUS_ID 和 CUDA_VISIBLE_DEVICES 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">"CUDA_DEVICE_ORDER"</span>] = <span class="string">"PCI_BUS_ID"</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"2, 3"</span></span><br></pre></td></tr></table></figure><p>当你添加这两行代码后，那么device_ids[0]默认的就是第2号卡，你的模型也会初始化在第2号卡上了，而不会占用第0号卡了。<strong>这里简单说一下设置上面两行代码后，那么对这个程序而言可见的只有2和3号卡，和其他的卡没有关系，这是物理上的号卡，逻辑上来说其实是对应0和1号卡，即device_ids[0]对应的就是第2号卡，device_ids[1]对应的就是第3号卡</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> CUDA_DEVICE_ORDER </tag>
            
            <tag> CUDA_VISIBLE_DEVICES </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++变长数组</title>
      <link href="/2021/03/23/C-%E5%8F%98%E9%95%BF%E6%95%B0%E7%BB%84/"/>
      <url>/2021/03/23/C-%E5%8F%98%E9%95%BF%E6%95%B0%E7%BB%84/</url>
      
        <content type="html"><![CDATA[<p>之前在DEV C++中，对于变长数组，习惯采用以下写法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> <span class="built_in">array</span>[m][n];</span><br></pre></td></tr></table></figure><p>但是，在按照这种方式书写三维数组时，出现了内存错误。</p><p>查阅网上的博客，有这种说法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;  </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;    </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span>  </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> len;  </span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; len;  </span><br><span class="line">    <span class="comment">//用指针p指向new动态分配的长度为len*sizeof(int)的内存空间  </span></span><br><span class="line">    <span class="keyword">int</span> *p = <span class="keyword">new</span> <span class="keyword">int</span>[len];  </span><br><span class="line">    <span class="comment">/*注意int *p = new int[len];这一句，你不能这样做：  </span></span><br><span class="line"><span class="comment">    int p[len];  </span></span><br><span class="line"><span class="comment">    C++编译器会报错说len的大小不能确定，因为用这种形式声明数组，数组的大小需要在编译时确定。</span></span><br><span class="line"><span class="comment">    而且这样也不行：</span></span><br><span class="line"><span class="comment">    int p[] = new int[len];  </span></span><br><span class="line"><span class="comment">    编译器会说不能把int*型转化为int[]型，因为用new开辟了一段内存空间后会返回这段内存的首地址，所以要把这个地址赋给一个指针，所以要用： </span></span><br><span class="line"><span class="comment">    int *p = new int[len]*/</span></span><br><span class="line">    <span class="keyword">delete</span>[] p; <span class="comment">//注意要注销指针p，使程序释放用new开辟的内存空间</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>可能是因为编译器的版本不同。但是，按照<code>new</code>申请内存应该是一种最为规范的做法，在任何版本都不会出错。</p><h2 id="使用vector"><a href="#使用vector" class="headerlink" title="使用vector"></a>使用vector</h2><p>使用C++标准模版库（STL）中的vector（向量）也可以实现变长数组：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; len;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="built_in">array</span>(len); <span class="comment">// 声明变长数组</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;++i) &#123;</span><br><span class="line">        <span class="built_in">array</span>[i] = i;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">array</span>[i] &lt;&lt; <span class="string">' '</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> array </tag>
            
            <tag> dynamic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo error spawn failed解决办法</title>
      <link href="/2021/03/20/hexo-error-spawn-failed%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
      <url>/2021/03/20/hexo-error-spawn-failed%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>出现错误：</p><p><code>error: spawn failed...</code></p><p>总结：</p><p>问题大多是因为<code>git</code>进行<code>push</code>或者<code>hexo d</code>的时候改变了一些<code>.deploy_git</code>文件下的内容。</p><p>解决办法：</p><ol><li>删除<code>.deploy_git</code>文件夹;</li><li>输入<code>git config --global core.autocrlf false</code></li><li>然后，依次执行：<br><code>hexo clean</code><br><code>hexo g</code><br><code>hexo d</code></li></ol><p>问题解决。暴力直接，有效。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ELECTRA预训练模型</title>
      <link href="/2021/03/19/ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
      <url>/2021/03/19/ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>ELECTRA详解：</p><p><a href="https://zhuanlan.zhihu.com/p/118135466" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/118135466</a></p></blockquote><p><img src="https://pic4.zhimg.com/80/v2-93eaff2a96a1b7d2991417ba10aab35f_720w.jpg" alt></p><h2 id="1-创新点"><a href="#1-创新点" class="headerlink" title="1. 创新点"></a>1. 创新点</h2><ul><li>提出了新的模型预训练的框架，采用generator和discriminator的结合方式，但又不同于GAN</li><li>将Masked Language Model的方式改为了replaced token detection</li><li>因为masked language model 能有效地学习到context的信息，所以能很好地学习embedding，所以使用了weight sharing的方式将generator的embedding的信息共享给discriminator</li><li>dicriminator 预测了generator输出的每个token是不是original的，从而高效地更新transformer的各个参数，使得模型的熟练速度加快</li><li>该模型采用了小的generator以及discriminator的方式共同训练，并且采用了两者loss相加，使得discriminator的学习难度逐渐地提升，学习到更难的token（plausible tokens）</li><li>模型在fine-tuning 的时候，丢弃generator，只使用discrinator</li></ul><a id="more"></a><h2 id="2-模型架构"><a href="#2-模型架构" class="headerlink" title="2. 模型架构"></a>2. 模型架构</h2><p>BERT存在预训练和fine-tuning的mismatch，因为在fine-tuning阶段，并不会有[MASK]的token。</p><p>ELECTRA由两部分组成，分别是generator以及discriminator，两个都是transformer的encoder结构，只是两者的size不同：</p><ul><li><p>generator：就是一个小的 masked language model（通常是 1/4 的discriminator的size），该模块的具体作用是他采用了经典的bert的MLM方式：</p></li><li><ul><li>首先随机选取15%的tokens，替代为[MASK]token，（取消了bert的80%[MASK],10%unchange, 10% random replaced 的操作，具体原因也是因为没必要，因为我们finetuning使用的discriminator)</li><li>使用generator去训练模型，使得模型预测masked token，得到corrupted tokens</li><li>generator的目标函数和bert一样，都是希望被masked的能够被还原成原本的original tokens<br>如上图， token，<code>the</code> 和 <code>cooked</code> 被随机选为被masked，然后generator预测得到corrupted tokens，变成了<code>the</code>和<code>ate</code></li></ul></li></ul><ul><li><p>discriminator：discriminator的接收被generator corrupt之后的输入，discriminator的作用是分辨输入的每一个token是original的还是replaced，注意：如果generator生成的token和原始token一致，那么这个token仍然是original的</p></li><li><ul><li>所以，对于每个token，discriminator都会进行一个二分类，最后获得loss</li></ul></li></ul><p>以上的方式被称为replaced token detection。</p><ul><li>learn from all tokens (instead of 15%)</li><li>compute efficient</li><li>paramter efficient</li><li>improves downstream task performance</li></ul><h3 id="2-1-个人理解"><a href="#2-1-个人理解" class="headerlink" title="2.1. 个人理解"></a>2.1. 个人理解</h3><p>可以把generator看作之前任意一种预训练语言模型，如BERT, RoBERTa等，借鉴了GAN的思想。但是否同样存在训练困难的问题？例如generator性能足够好，导致discriminator输出均为<code>original</code>?</p><h2 id="3-如何训练"><a href="#3-如何训练" class="headerlink" title="3. 如何训练"></a>3. 如何训练</h2><p>该模型采用了minimize the combined loss的方式进行训练：</p><script type="math/tex; mode=display">\min_{\theta_G, \theta_D} \sum_{\vec{x} \in X} L_{MLM}(\vec{x},\theta_{G}) + \lambda L_{Dis}(\vec{x}, \theta_{D})</script><p>其中</p><p>$L<em>{MLM}(\vec{x}, \theta</em>{G})$对于输入$\vec{x} = [x_1, …, x_n]$，经过generator之后得到编码了上下文信息的vector representation，$h(\vec{x}) = [h_1,…,h_n]$，对于位置t，其被替换为[MASK]，那么它的output probability（经过softmax）为：</p><script type="math/tex; mode=display">p_G(x_t|\vec{x}) = \frac{\exp(\mathrm{e}(x_t)^T \cdot \mathrm{h_g}(\vec{x})_t)}{\sum_{x^{'}} \exp(\mathrm{e}(x^{'})^T \cdot \mathrm{h_g} (\vec{x})_t)}</script><p>其中$\mathrm{e}(x_t)$为单词的embedding表示，而$\mathrm{h_g}(\vec{x})_t$为经过generator之后的隐藏层表示。</p><p>然后计算交叉熵损失。</p><p><br></p><p>对于discriminator：</p><script type="math/tex; mode=display">D(\vec{x},t) = \mathrm{sigmoid} (w^Th_D(\vec{x})_t)</script><p>然后计算BCE (binary cross entropy) 损失。</p><h3 id="3-1-反向传播"><a href="#3-1-反向传播" class="headerlink" title="3.1. 反向传播"></a>3.1. 反向传播</h3><p><strong><em>在训练过程中，discriminator的loss不会反向传播到generator</em></strong>（因为generator的sampling的步骤导致）</p><font color="red">但是如果采用这种做法，是否generator存在的必要性就没有了？完全可以采用手动替换的方式，首先进行较为明显的替换，例如用动词替换名词。随着训练的进行，逐渐改为用BERT等预训练模型的预测结果作为替换，增大检测难度。emmm</font><h3 id="3-2-其他训练方式"><a href="#3-2-其他训练方式" class="headerlink" title="3.2. 其他训练方式"></a>3.2. 其他训练方式</h3><p>作者显然也考虑到这一点，在论文中提出了其他训练方式：</p><ul><li>一种是GAN：ELECTRA以一种对抗学习的思想来训练。作者将生成器的目标函数由最小化MLM loss换成了最大化判别器在被替换token上的loss。用强化学习Policy Gradient的思想，将被替换token的交叉熵作为生成器的reward，然后进行梯度下降。强化方法优化下来生成器在MLM任务上可以达到54%的准确率，而之前MLE优化下可以达到65%。</li><li>一种是two-stage 训练，先训练 $L_{MLM}$ n steps，然后froze住 generator，再训练 discriminator n steps<br>但是效果都没有共同训练效果好</li></ul><h2 id="4-超参数"><a href="#4-超参数" class="headerlink" title="4. 超参数"></a>4. 超参数</h2><p>文章中提到了，如果generator过强，那么discriminator就无法成功训练，这其实也很好理解。因为generator非常强，那么预测出来的token都非常好，即都是original tokens，那么discrinator 并不需要如何学习就收敛，因为它只需要把所有二分类都认为是1就行（假设1代表real）</p><p><img src="https://pic2.zhimg.com/80/v2-4b664f463f5c390806db89e3071fa8c5_720w.jpg" alt></p><p>我们可以发现，当generator的size 为discriminator size 的 1/2 到 1/4 时，模型效果最好。</p><p>由于ELECTRA是判别式任务，不用对每个位置的整个数据分布建模，所以更parameter-efficient。</p><ul><li>模型大小的因素</li></ul><p>可以从下图的实验结果看出，ELECTRA 模型在小模型的时候，效果提升显著，随着模型大小的增加，效果降低。</p><p>同时可以看到，在相同的计算量的时候，ELECTRA的效果优于BERT。</p><p><img src="https://pic4.zhimg.com/80/v2-0187b4b8e3777682833d952f2fe9bfd3_720w.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Pretrained Language Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PLM </tag>
            
            <tag> ELECTRA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RoBERTa预训练模型</title>
      <link href="/2021/03/19/RoBERTa%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
      <url>/2021/03/19/RoBERTa%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>相比于BERT的调整：</p><ol><li>训练时间更长，batch size更大，训练数据更多</li><li>移除next sentence prediction loss</li><li>训练序列更长</li><li>动态mask机制</li></ol><p>BERT原型使用的是 character-level BPE vocabulary of size 30K, RoBERTa使用了GPT2的 BPE 实现，使用的是byte而不是unicode characters作为subword的单位。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Pretrained Language Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PLM </tag>
            
            <tag> RoBERTa </tag>
            
            <tag> pretrained language models </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++数组填充方法</title>
      <link href="/2021/03/19/C-%E6%95%B0%E7%BB%84%E5%A1%AB%E5%85%85%E6%96%B9%E6%B3%95/"/>
      <url>/2021/03/19/C-%E6%95%B0%E7%BB%84%E5%A1%AB%E5%85%85%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>在对数组初始化时，往往需要赋予同样的值，例如布尔型数组全部设为false</p><p>现在要把a里面1000个全部初始化为5；str里面全部初始化为a；</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a[<span class="number">1000</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line"><span class="keyword">char</span> str[<span class="number">1000</span>] = &#123;<span class="string">'a'</span>&#125;;</span><br></pre></td></tr></table></figure><p><strong>但这样只有第一个元素被设置，其余999个自动默认是int 的0</strong>；</p><h1 id="1-memset"><a href="#1-memset" class="headerlink" title="1. memset"></a>1. memset</h1><p>首先，<strong>memset函数是逐字节进行填充</strong>，所以不适合int型数组。</p><p>对于字符型数组：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="built_in">memset</span>(str,<span class="string">'a'</span>,<span class="keyword">sizeof</span>(str))</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="2-fill"><a href="#2-fill" class="headerlink" title="2. fill"></a>2. fill</h1><p>按照单元赋值，将一个区间的元素都赋同一个值．<br>我们还是来看下实现吧：</p><blockquote><p>void fill(ForwardIt first, ForwardIt last, const T&amp; value);<br>void fill_n(OutputIt first, size n, const T&amp; value);<br>功能：<br>fill给迭代器范围[first, last)内元素均赋值为value。无返回值。<br>fill_n将first指向范围内的前n个元素赋值为value。返回first+n(c++11)</p></blockquote><p>对应的源代码为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">ForwardIterator</span>, <span class="title">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">fill</span> (<span class="title">ForwardIterator</span> <span class="title">first</span>, <span class="title">ForwardIterator</span> <span class="title">last</span>, <span class="title">const</span> <span class="title">T</span>&amp; <span class="title">val</span>)</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">while</span> (first != last) &#123;</span><br><span class="line">    *first = val;</span><br><span class="line">    ++first;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实例：</p><p>对于int a[1000]的批量填充：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line">fill(a, a + <span class="number">1000</span>, <span class="number">5</span>);</span><br></pre></td></tr></table></figure><h1 id="3-fill-n"><a href="#3-fill-n" class="headerlink" title="3. fill_n"></a>3. fill_n</h1><p>对应的源代码为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">OutputIterator</span>, <span class="title">class</span> <span class="title">Size</span>, <span class="title">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">OutputIterator</span> <span class="title">fill_n</span> (<span class="title">OutputIterator</span> <span class="title">first</span>, <span class="title">Size</span> <span class="title">n</span>, <span class="title">const</span> <span class="title">T</span>&amp; <span class="title">val</span>)</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">while</span> (n&gt;<span class="number">0</span>) &#123;</span><br><span class="line">    *first = val;</span><br><span class="line">    ++first; --n;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> first;     <span class="comment">// since C++11</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line">fill_n(a, <span class="number">1000</span>, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;　myvector (<span class="number">8</span>,<span class="number">10</span>); <span class="comment">// myvector: 10 10 10 10 10 10 10 10</span></span><br><span class="line">fill_n (myvector.begin(),<span class="number">4</span>,<span class="number">20</span>); <span class="comment">// myvector: 20 20 20 20 10 10 10 10</span></span><br><span class="line">fill_n (myvector.begin()+<span class="number">3</span>,<span class="number">3</span>,<span class="number">33</span>); <span class="comment">// myvector: 20 20 20 33 33 33 10 10</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> array </tag>
            
            <tag> memset </tag>
            
            <tag> fill </tag>
            
            <tag> fill_n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python执行原理</title>
      <link href="/2021/03/17/python%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86/"/>
      <url>/2021/03/17/python%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>转载自：<a href="https://blog.csdn.net/helloxiaozhe/article/details/78104975" target="_blank" rel="noopener">python编译过程和执行原理</a></p></blockquote><hr><h1 id="1-python执行原理"><a href="#1-python执行原理" class="headerlink" title="1. python执行原理"></a>1. python执行原理</h1><p>这里的解释执行是相对于编译执行而言的。我们都知道，使用C/C++之类的编译性语言编写的程序，是需要从源文件转换成计算机使用的机器语言，经过链接器链接之后形成了二进制的可执行文件。运行该程序的时候，就可以把二进制程序从硬盘载入到内存中并运行。</p><p>但是对于Python而言，python源码不需要编译成二进制代码，它可以直接从源代码运行程序。当我们运行python文件程序的时候，python解释器将源代码转换为字节码，然后再由python解释器来执行这些字节码。这样，python就不用担心程序的编译,库的链接加载等问题了。</p><p>对于python解释语言，有以下3方面的特性：</p><ol><li>每次运行都要进行转换成字节码，然后再有虚拟机把字节码转换成机器语言，最后才能在硬件上运行。与编译性语言相比，每次多出了编译和链接的过程，性能肯定会受到影响；而python并不是每次都需要转换字节码，解释器在转换之前会判断代码文件的修改时间是否与上一次转换后的字节码pyc文件的修改时间一致，若不一致才会重新转换。</li><li>由于不用关心程序的编译和库的链接等问题，开发的工作也就更加轻松啦。</li><li>python代码与机器底层更远了，python程序更加易于移植，基本上无需改动就能在多平台上运行。</li></ol><p>​      在具体计算机上实现一种语言，首先要确定的是表示该语言语义解释的虚拟计算机，一个关键的问题是程序执行时的基本表示是实际计算机上的机器语言还是虚拟机的机器语言。这个问题决定了语言的实现。根据这个问题的回答，可以将程序设计语言划分为两大类：编译型语言和解释型语言。</p><ol><li>编译实现的语言，如：C、C++、Fortran、Pascal、Ada。由编译型语言编写的源程序需要经过编译,汇编和链接才能输出目标代码，然后由机器执行目标代码。目标代码是有机器指令组成，不能独立运行，因为源程序中可能使用了一些汇编程序不能解释引用的库函数，而库函数又不在源程序中，此时还需要链接程序完成外部引用和目标模板调用的链接任务，最后才能输出可执行代码。</li><li>解释型语言，解释器不产生目标机器代码，而是产生中间代码，这种中间代码与机器代码不同，中间代码的解释是由软件支持的，不能直接使用在硬件上。该软件解释器通常会导致执行效率较低，用解释型语言编写的程序是由另一个可以理解中间代码的解释程序执行的。和编译的程序不同的是, 解释程序的任务是逐一将源代码的语句解释成可执行的机器指令，不需要将源程序翻译成目标代码再执行。对于解释型语言，需要一个专门的解释器来执行该程序，每条语句只有在执行是才能被翻译，这种解释型语言每执行一次就翻译一次，因而效率低下。</li><li>Java解释器，java很特殊，java是需要编译的，但是没有直接编译成机器语言，而是编译成字节码，然后在Java虚拟机上用解释的方式执行字节码。Python也使用了类似的方式，先将python编译成python字节码，然后由一个专门的python字节码解释器负责解释执行字节码。</li><li>python是一门解释语言，但是出于效率的考虑，提供了一种编译的方法。编译之后就得到pyc文件，存储了字节码。python这点和java很类似，但是java与python不同的是，python是一个解释型的语言，所以编译字节码不是一个强制的操作，事实上，编译是一个自动的过程，一般不会在意它的存在。编译成字节码可以节省加载模块的时间，提高效率。</li><li>除了效率之外，字节码的形式也增加了反向工程的难度，可以保护源代码。这个只是一定程度上的保护，反编译还是可以的。</li></ol><a id="more"></a><h1 id="2-python内部执行过程"><a href="#2-python内部执行过程" class="headerlink" title="2. python内部执行过程"></a>2. python内部执行过程</h1><h2 id="2-1-编译过程概述"><a href="#2-1-编译过程概述" class="headerlink" title="2.1. 编译过程概述"></a>2.1. 编译过程概述</h2><p>　　当我们执行Python代码的时候，在Python解释器用四个过程“拆解”我们的代码，最终被CPU执行返回给用户。</p><p>　　首先当用户键入代码交给Python处理的时候会先进行词法分析，例如用户键入关键字或者当输入关键字有误时，都会被词法分析所触发，不正确的代码将不会被执行。</p><p>　　下一步Python会进行语法分析，例如当”for i in test:”中，test后面的冒号如果被写为其他符号，代码依旧不会被执行。</p><p>　　下面进入最关键的过程，在执行Python前，Python会生成.pyc文件，这个文件就是字节码，如果我们不小心修改了字节码，Python下次重新编译该程序时会和其上次生成的字节码文件进行比较，如果不匹配则会将被修改过的字节码文件进行覆盖，以确保每次编译后字节码的准确性。</p><p>　　那么什么是字节码？字节码在Python虚拟机程序里对应的是PyCodeObject对象。.pyc文件是字节码在磁盘上的表现形式。简单来说就是在编译代码的过程中，首先会将代码中的函数、类等对象分类处理，然后生成字节码文件。有了字节码文件，CPU可以直接识别字节码文件进行处理，接着Python就可执行了。</p><h2 id="2-2-过程图解"><a href="#2-2-过程图解" class="headerlink" title="2.2. 过程图解"></a>2.2. 过程图解</h2><p><img src="/images/blog/2021/PVM.jpg" alt></p><h2 id="2-3-编译字节码"><a href="#2-3-编译字节码" class="headerlink" title="2.3. 编译字节码"></a>2.3. 编译字节码</h2><p>　　Python中有一个内置函数compile()，可以将源文件编译成codeobject，首先看这个函数的说明：</p><p>　　compile(…) compile(source, filename, mode[, flags[, dont_inherit]]) -&gt; code object</p><p>　　参数1：源文件的内容字符串</p><p>　　参数2：源文件名称</p><p>　　参数3：exec-编译module，single-编译一个声明，eval-编译一个表达式 一般使用前三个参数就够了</p><p>　　使用示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#src_file.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#some function</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(d=<span class="number">0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    c=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"hello"</span></span><br><span class="line"></span><br><span class="line">a=<span class="number">9</span></span><br><span class="line"></span><br><span class="line">b=<span class="number">8</span></span><br><span class="line"></span><br><span class="line">f()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=open(<span class="string">'src_file.py'</span>,<span class="string">'r'</span>).read()    <span class="comment">#命令行模式中打开源文件进行编译</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>co=compile(a,<span class="string">'src_file'</span>,<span class="string">'exec'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(co)</span><br><span class="line"></span><br><span class="line">&lt;type <span class="string">'code'</span>&gt;    <span class="comment">#编译出了codeobject对象</span></span><br></pre></td></tr></table></figure><h2 id="2-4-codeobject对象的属性"><a href="#2-4-codeobject对象的属性" class="headerlink" title="2.4. codeobject对象的属性"></a>2.4. codeobject对象的属性</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_names    <span class="comment">#所有的符号名称</span></span><br><span class="line">(<span class="string">'f'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>)</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_name    <span class="comment">#模块名、函数名、类名</span></span><br><span class="line">&lt;module&gt;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_consts    <span class="comment">#常量集合、函数f和两个int常量a,b，d</span></span><br><span class="line">(0, &lt;code object f at 0xb7273b18, file <span class="string">"src_file"</span>, line 2&gt;, 9, 8, None)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_consts[1].co_varnames    <span class="comment">#可以看到f函数也是一个codeobject,打印f中的局部变量</span></span><br><span class="line">(<span class="string">'c'</span>,)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_code    <span class="comment">#字节码指令</span></span><br><span class="line">dZdZdZedS</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_consts[1].co_firstlineno    <span class="comment">#代码块在文件中的起始行号</span></span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_stacksize    <span class="comment">#代码栈大小</span></span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> co.co_filename    <span class="comment">#文件名</span></span><br><span class="line">src_file    <span class="comment">#模块名、函数名、类名</span></span><br></pre></td></tr></table></figure><p>codeobject的co_code代表了字节码，这个字节码有什么含义？我们可以使用dis模块进行python的反编译：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dis</span><br><span class="line">dis.dis(co)</span><br></pre></td></tr></table></figure><p>　从反编译的结果来看，python字节码其实是模仿的x86的汇编，将代码编译成一条一条的指令交给一个虚拟的cpu去执行。</p><ul><li>第一列：行号</li><li>第二列：指令在代码块中的偏移量</li><li>第三列：指令</li><li>第四列：操作数</li><li>第五列：操作数说明</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dis</span><br><span class="line"></span><br><span class="line">dis.dis(co)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; output</span><br><span class="line"></span><br><span class="line"> <span class="number">2</span>        <span class="number">0</span> LOAD_CONST               <span class="number">0</span> (<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">          <span class="number">3</span> LOAD_CONST               <span class="number">1</span> (&lt;code object f at <span class="number">0xb7273b18</span>, file <span class="string">"src_file"</span>, line <span class="number">2</span>&gt;)</span><br><span class="line"></span><br><span class="line">          <span class="number">6</span> MAKE_FUNCTION            <span class="number">1</span></span><br><span class="line"></span><br><span class="line">          <span class="number">9</span> STORE_NAME               <span class="number">0</span> (f)</span><br><span class="line"></span><br><span class="line"> <span class="number">5</span>        <span class="number">12</span> LOAD_CONST              <span class="number">2</span> (<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">          <span class="number">15</span> STORE_NAME              <span class="number">1</span> (a)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="number">6</span>        <span class="number">18</span> LOAD_CONST              <span class="number">3</span> (<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">          <span class="number">21</span> STORE_NAME              <span class="number">2</span> (b)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="number">7</span>        <span class="number">24</span> LOAD_NAME               <span class="number">0</span> (f)</span><br><span class="line"></span><br><span class="line">          <span class="number">27</span> CALL_FUNCTION           <span class="number">0</span></span><br><span class="line"></span><br><span class="line">          <span class="number">30</span> POP_TOP            </span><br><span class="line"></span><br><span class="line">          <span class="number">31</span> LOAD_CONST              <span class="number">4</span> (None)</span><br><span class="line"></span><br><span class="line">          <span class="number">34</span> RETURN_VALUE</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> PVM </tag>
            
            <tag> 执行原理 </tag>
            
            <tag> 编译 </tag>
            
            <tag> 解释 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>conda添加源</title>
      <link href="/2021/03/15/conda%E6%B7%BB%E5%8A%A0%E6%BA%90/"/>
      <url>/2021/03/15/conda%E6%B7%BB%E5%8A%A0%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<h1 id="1-通过命令行"><a href="#1-通过命令行" class="headerlink" title="1. 通过命令行"></a>1. 通过命令行</h1><p>添加清华源，命令行中直接输入以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置搜索时显示通道地址</span></span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><p>如果还需要<strong>pytorch</strong>，就添加pytorch镜像：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br></pre></td></tr></table></figure><p>添加中科大源：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/</span><br><span class="line"> </span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="2-通过配置文件"><a href="#2-通过配置文件" class="headerlink" title="2. 通过配置文件"></a>2. 通过配置文件</h1><p>在Linux系统中，可以直接将以下配置写入<code>~/.condarc</code>中：</p><p><strong>使用<a href="https://mirrors.bfsu.edu.cn/help/anaconda/" target="_blank" rel="noopener">BFSU的镜像：</a></strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">channels:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">defaults</span></span><br><span class="line"><span class="attr">show_channel_urls:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">default_channels:</span></span><br><span class="line"><span class="attr">  - https:</span><span class="string">//mirrors.bfsu.edu.cn/anaconda/pkgs/main</span></span><br><span class="line"><span class="attr">  - https:</span><span class="string">//mirrors.bfsu.edu.cn/anaconda/pkgs/r</span></span><br><span class="line"><span class="attr">  - https:</span><span class="string">//mirrors.bfsu.edu.cn/anaconda/pkgs/msys2</span></span><br><span class="line"><span class="attr">custom_channels:</span></span><br><span class="line"><span class="attr">  conda-forge:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br><span class="line"><span class="attr">  msys2:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br><span class="line"><span class="attr">  bioconda:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br><span class="line"><span class="attr">  menpo:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br><span class="line"><span class="attr">  pytorch:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br><span class="line"><span class="attr">  simpleitk:</span> <span class="attr">https://mirrors.bfsu.edu.cn/anaconda/cloud</span></span><br></pre></td></tr></table></figure><h1 id="3-删源"><a href="#3-删源" class="headerlink" title="3. 删源"></a>3. 删源</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --remove-key channels</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> conda </tag>
            
            <tag> tsinghua </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++二维数组传参</title>
      <link href="/2021/03/14/C-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%BC%A0%E5%8F%82/"/>
      <url>/2021/03/14/C-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%BC%A0%E5%8F%82/</url>
      
        <content type="html"><![CDATA[<p>在LeetCode上，总是需要用到对不固定维度的二维数组传参，传参有多种方式，这里进行一下探究和复习。</p><h1 id="1-一级指针"><a href="#1-一级指针" class="headerlink" title="1. 一级指针"></a>1. 一级指针</h1><p>高维数组在栈中存放时内存是连续的，也可以看作一维数组，并手动计算索引对应的地址。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">(<span class="keyword">int</span> *dp, <span class="keyword">int</span> m, <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;++i) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n;++j) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; *(dp + i * n + j) &lt;&lt; <span class="string">' '</span>; </span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> m, n;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; m &gt;&gt; n;</span><br><span class="line"><span class="keyword">int</span> dp[m][n];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;++i) <span class="comment">// 初始化</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n;++j)</span><br><span class="line">dp[i][j] = i + j;</span><br><span class="line"> </span><br><span class="line">func((<span class="keyword">int</span>*)dp, m, n);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0 1 2 </span><br><span class="line">1 2 3</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="2-二级指针"><a href="#2-二级指针" class="headerlink" title="2. 二级指针"></a>2. 二级指针</h1><p>使用<strong>new</strong>在堆中申请空间，然后传参。即指针的指针。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">(<span class="keyword">int</span> **dp, <span class="keyword">int</span> m, <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;++i) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n;++j) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; dp[i][j] &lt;&lt; <span class="string">' '</span>; </span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> m, n;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; m &gt;&gt; n;</span><br><span class="line"><span class="keyword">int</span> **dp = <span class="keyword">new</span> <span class="keyword">int</span>*[m];  <span class="comment">// 在堆中申请空间 </span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;++i)</span><br><span class="line">dp[i] = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;++i) <span class="comment">// 初始化</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n;++j)</span><br><span class="line">dp[i][j] = i + j;</span><br><span class="line"> </span><br><span class="line">func(dp, m, n);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0 1 2 </span><br><span class="line">1 2 3 </span><br><span class="line">2 3 4</span><br></pre></td></tr></table></figure><p>这种方式申请的内存不在栈中，而是在堆中。而且，<strong><em>由于多次申请了内存，不同的行间内存空间是不连续的</em></strong>。</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> LeetCode </tag>
            
            <tag> Array </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pandas.merge()函数</title>
      <link href="/2021/03/13/pandas-merge-%E5%87%BD%E6%95%B0/"/>
      <url>/2021/03/13/pandas-merge-%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>在使用pandas时，合并两个DataFrame，可以采用不同的连接方法，类似于数据库中表格的join操作，对不同的操作进行实验尝试。</p><h1 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import pandas</span><br><span class="line">&gt;&gt;&gt; sheet1 = pandas.DataFrame(&#123;<span class="string">"ID"</span>:[<span class="string">"A01"</span>, <span class="string">"B01"</span>, <span class="string">"A02"</span>], <span class="string">"name"</span>:[<span class="string">"Bob"</span>, <span class="string">"Mary"</span>, <span class="string">"Bob"</span>], <span class="string">"age"</span>: [20, 22, 35]&#125;)</span><br><span class="line">&gt;&gt;&gt; sheet1</span><br><span class="line">    ID  name  age</span><br><span class="line">0  A01   Bob   20</span><br><span class="line">1  B01  Mary   22</span><br><span class="line">2  A02   Bob   35</span><br><span class="line">&gt;&gt;&gt; sheet2 = pandas.DataFrame(&#123;<span class="string">"name"</span>: [<span class="string">"Bob"</span>, <span class="string">"Mary"</span>, <span class="string">"Jack"</span>], <span class="string">"country"</span>: [<span class="string">"US"</span>, <span class="string">"Singapore"</span>, <span class="string">"China"</span>]&#125;)</span><br><span class="line">&gt;&gt;&gt; sheet2</span><br><span class="line">   name    country</span><br><span class="line">0   Bob         US</span><br><span class="line">1  Mary  Singapore</span><br><span class="line">2  Jack      China</span><br></pre></td></tr></table></figure><h1 id="1-left"><a href="#1-left" class="headerlink" title="1. left"></a>1. left</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sheet1.merge(sheet2, how=<span class="string">"left"</span>, on=<span class="string">"name"</span>)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    ID  name  age    country</span><br><span class="line">0  A01   Bob   20         US</span><br><span class="line">1  B01  Mary   22  Singapore</span><br><span class="line">2  A02   Bob   35         US</span><br></pre></td></tr></table></figure><p>按照左表的”name”将两个sheet的属性合并。</p><a id="more"></a><h1 id="2-right"><a href="#2-right" class="headerlink" title="2. right"></a>2. right</h1><p>同样是按照”name”合并，但是如果指定合并方式为”right”的话，则会保留右侧的所有“name”，如果一个”name”可以匹配到多个左表中记录，则合并后对应同样数量的多个记录。对于没有匹配的”name”值，补”NaN”</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sheet1.merge(sheet2, how=<span class="string">"right"</span>, on=<span class="string">"name"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    ID  name   age    country</span><br><span class="line">0  A01   Bob  20.0         US</span><br><span class="line">1  A02   Bob  35.0         US</span><br><span class="line">2  B01  Mary  22.0  Singapore</span><br><span class="line">3  NaN  Jack   NaN      China</span><br></pre></td></tr></table></figure><h1 id="3-inner"><a href="#3-inner" class="headerlink" title="3. inner"></a>3. inner</h1><p>使用inner合并，是取两张表的公共部分，即取交集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sheet1.merge(sheet2, how=<span class="string">"inner"</span>, on=<span class="string">"name"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    ID  name  age    country</span><br><span class="line">0  A01   Bob   20         US</span><br><span class="line">1  A02   Bob   35         US</span><br><span class="line">2  B01  Mary   22  Singapore</span><br></pre></td></tr></table></figure><p>由于在给出的例子中，左表中的name有”Bob”，“Mary”，均在右表中，所以合并结果与left合并完全相同。</p><h1 id="4-outer"><a href="#4-outer" class="headerlink" title="4. outer"></a>4. outer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sheet1.merge(sheet2, how=<span class="string">"outer"</span>, on=<span class="string">"name"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    ID  name   age    country</span><br><span class="line">0  A01   Bob  20.0         US</span><br><span class="line">1  A02   Bob  35.0         US</span><br><span class="line">2  B01  Mary  22.0  Singapore</span><br><span class="line">3  NaN  Jack   NaN      China</span><br></pre></td></tr></table></figure><p>使用outer合并，是取两个表的key的并集，在举的例子里，结果与right合并完全相同。</p><h1 id="5-cross"><a href="#5-cross" class="headerlink" title="5. cross"></a>5. cross</h1><p>笛卡尔积，不常使用。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
            <tag> merge </tag>
            
            <tag> DataFrame </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二分类中softmax对比sigmoid</title>
      <link href="/2021/03/12/%E4%BA%8C%E5%88%86%E7%B1%BB%E4%B8%ADsoftmax%E5%AF%B9%E6%AF%94sigmoid/"/>
      <url>/2021/03/12/%E4%BA%8C%E5%88%86%E7%B1%BB%E4%B8%ADsoftmax%E5%AF%B9%E6%AF%94sigmoid/</url>
      
        <content type="html"><![CDATA[<p>题外话：在Pycharm中<code>Ctrl+左键</code>就可以跳转到源码！用了这么多年竟然都不知道！😓</p><blockquote><p>本文引用自：<a href="https://www.aiuai.cn/aifarm679.html" target="_blank" rel="noopener">https://www.aiuai.cn/aifarm679.html</a></p></blockquote><h1 id="1-理论分析"><a href="#1-理论分析" class="headerlink" title="1. 理论分析"></a>1. 理论分析</h1><p>[1] Sigmoid</p><script type="math/tex; mode=display">\begin{equation} \begin{cases} p(y=1|x) = \frac{1}{1 + e ^{-\theta ^ T x}} \\ p(y=0|x) = 1 - p(y=1|x) = \frac{e ^{-\theta ^ T x}}{1 + e ^{-\theta ^ T x}} \end{cases} \end{equation}</script><p>[2] Softmax</p><script type="math/tex; mode=display">\begin{equation} \begin{cases} p(y=0|x) = \frac{e ^{\theta _0^T x} }{e ^{\theta _0^T x} + e ^{\theta _1^T x} } = \frac{e ^{(\theta _0^T - \theta _1^T)x} }{1 + e ^{(\theta _0^T - \theta _1^T) x} } \\ p(y=1|x) = 1 - p(y=0|x) \end{cases} \end{equation}</script><p>令 $\beta = -(\theta_0^T - \theta _1^T)$，则有：</p><script type="math/tex; mode=display">\begin{equation} \begin{cases} p(y=1|x) = \frac{1}{1 + e ^{-\beta ^ T x}} \\ p(y=0|x) = \frac{e ^{-\beta ^ T x}}{1 + e ^{-\beta ^ T x}} \end{cases} \end{equation}</script><p>可见，此时，Softmax 与 Sigmoid 二者理论公式的等价性.</p><h1 id="2-基于Keras的实验对比"><a href="#2-基于Keras的实验对比" class="headerlink" title="2. 基于Keras的实验对比"></a>2. 基于Keras的实验对比</h1><a id="more"></a><p>以猫狗分类的数据集为例，采用在 ImageNet 上预训练的 Xception 模型导出的特征.( <strong>复制于</strong> <a href="https://gist.github.com/ypwhs/6905ebbda99d04621f9fc00417657ae2" target="_blank" rel="noopener">ypwhs/sigmoid_and_softmax.ipynb</a> 中的代码. )</p><blockquote><p><a href="https://github.com/ypwhs/dogs_vs_cats/releases/download/gap/gap_Xception.h5" target="_blank" rel="noopener">gap_Xception.h5</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">20180520</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment">#      加载猫狗分类的特征数据</span></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="keyword">with</span> h5py.File(<span class="string">"gap_Xception.h5"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> h:</span><br><span class="line">    X = np.array(h[<span class="string">'train'</span>])</span><br><span class="line">    y = np.array(h[<span class="string">'label'</span>])</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">y_train_softmax = to_categorical(y_train) <span class="comment"># ont-hot</span></span><br><span class="line">y_test_softmax = to_categorical(y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment">#           Softmax</span></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment"># loss: Softmax Cross Entropy Loss</span></span><br><span class="line">input_tensor = Input(X.shape[<span class="number">1</span>:])</span><br><span class="line">x = input_tensor</span><br><span class="line">x = Dropout(<span class="number">0.5</span>)(x)</span><br><span class="line">softmax = Dense(<span class="number">2</span>, activation=<span class="string">'softmax'</span>)</span><br><span class="line">x = softmax(x)</span><br><span class="line">model = Model(input_tensor, x)</span><br><span class="line">model.compile(optimizer=SGD(<span class="number">1e-3</span>),  <span class="comment"># lr = 1e-3</span></span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">softmax_weights, softmax_bias = softmax.get_weights()</span><br><span class="line"></span><br><span class="line">history_softmax = model.fit(X_train, y_train_softmax, batch_size=<span class="number">128</span>, epochs=<span class="number">5</span>,</span><br><span class="line">                            validation_data=(X_test, y_test_softmax))</span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment">#           Sigmoid</span></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment"># loss: Binary(Sigmoid) Cross Entropy Loss</span></span><br><span class="line">np.random.seed(<span class="number">20180520</span>)</span><br><span class="line">input_tensor = Input(X.shape[<span class="number">1</span>:])</span><br><span class="line">x = input_tensor</span><br><span class="line">x = Dropout(<span class="number">0.5</span>)(x)</span><br><span class="line">sigmoid = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">x = sigmoid(x)</span><br><span class="line">model = Model(input_tensor, x)</span><br><span class="line">model.compile(optimizer=SGD(<span class="number">2e-3</span>),  <span class="comment"># lr = 2e-3</span></span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sigmoid 权重初始化为 Softmax 的权重计算差值</span></span><br><span class="line"><span class="comment"># beta = sigmoid weights</span></span><br><span class="line">beta = -(softmax_weights[:,<span class="number">0</span>] - softmax_weights[:,<span class="number">1</span>]).reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">sigmoid.set_weights([beta, np.zeros(<span class="number">1</span>)]) <span class="comment"># set beta to sigmoid weights</span></span><br><span class="line"></span><br><span class="line">history_sigmoid = model.fit(X_train, y_train, batch_size=<span class="number">128</span>, epochs=<span class="number">5</span>, validation_data=(X_test, y_test))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line"><span class="comment">#           Sigmoid</span></span><br><span class="line"><span class="comment">#---------------------------------</span></span><br><span class="line">input_tensor = Input(X.shape[<span class="number">1</span>:])</span><br><span class="line">x = input_tensor</span><br><span class="line">x = Dropout(<span class="number">0.5</span>)(x)</span><br><span class="line">sigmoid = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">x = sigmoid(x)</span><br><span class="line">model = Model(input_tensor, x)</span><br><span class="line">model.compile(optimizer=SGD(<span class="number">2e-3</span>),  <span class="comment"># lr = 2e-3</span></span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># random init Sigmoid weights</span></span><br><span class="line"><span class="comment"># 随机初始化 Sigmoid 权重</span></span><br><span class="line">history_sigmoid_2 = model.fit(X_train, y_train, batch_size=<span class="number">128</span>, epochs=<span class="number">5</span>, validation_data=(X_test, y_test))</span><br></pre></td></tr></table></figure><p>训练过程输出：</p><p><strong>Softmax</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Train on 20000 samples, validate on 5000 samples</span><br><span class="line">Epoch 1/5</span><br><span class="line">20000/20000 [==============================] - 1s 68us/step - loss: 0.4711 - acc: 0.8535 - val_loss: 0.3765 - val_acc: 0.9642</span><br><span class="line">Epoch 2/5</span><br><span class="line">20000/20000 [==============================] - 0s 13us/step - loss: 0.3338 - acc: 0.9486 - val_loss: 0.2825 - val_acc: 0.9816</span><br><span class="line">Epoch 3/5</span><br><span class="line">20000/20000 [==============================] - 0s 11us/step - loss: 0.2594 - acc: 0.9726 - val_loss: 0.2279 - val_acc: 0.9858</span><br><span class="line">Epoch 4/5</span><br><span class="line">20000/20000 [==============================] - 0s 11us/step - loss: 0.2160 - acc: 0.9799 - val_loss: 0.1923 - val_acc: 0.9866</span><br><span class="line">Epoch 5/5</span><br><span class="line">20000/20000 [==============================] - 0s 13us/step - loss: 0.1860 - acc: 0.9825 - val_loss: 0.1677 - val_acc: 0.9868</span><br></pre></td></tr></table></figure><p><strong>Sigmoid With Softmax Weights</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Train on 20000 samples, validate on 5000 samples</span><br><span class="line">Epoch 1/5</span><br><span class="line">20000/20000 [==============================] - 0s 18us/step - loss: 0.4706 - acc: 0.8544 - val_loss: 0.3766 - val_acc: 0.9644</span><br><span class="line">Epoch 2/5</span><br><span class="line">20000/20000 [==============================] - 0s 12us/step - loss: 0.3346 - acc: 0.9476 - val_loss: 0.2824 - val_acc: 0.9816</span><br><span class="line">Epoch 3/5</span><br><span class="line">20000/20000 [==============================] - 0s 14us/step - loss: 0.2613 - acc: 0.9709 - val_loss: 0.2275 - val_acc: 0.9860</span><br><span class="line">Epoch 4/5</span><br><span class="line">20000/20000 [==============================] - 0s 14us/step - loss: 0.2151 - acc: 0.9789 - val_loss: 0.1923 - val_acc: 0.9868</span><br><span class="line">Epoch 5/5</span><br><span class="line">20000/20000 [==============================] - 0s 11us/step - loss: 0.1857 - acc: 0.9825 - val_loss: 0.1676 - val_acc: 0.9872</span><br></pre></td></tr></table></figure><p><strong>Sigmoid With Random Init Weight</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Train on 20000 samples, validate on 5000 samples</span><br><span class="line">Epoch 1/5</span><br><span class="line">20000/20000 [==============================] - 0s 18us/step - loss: 0.5690 - acc: 0.7607 - val_loss: 0.4415 - val_acc: 0.9718</span><br><span class="line">Epoch 2/5</span><br><span class="line">20000/20000 [==============================] - 0s 11us/step - loss: 0.3753 - acc: 0.9576 - val_loss: 0.3151 - val_acc: 0.9852</span><br><span class="line">Epoch 3/5</span><br><span class="line">20000/20000 [==============================] - 0s 12us/step - loss: 0.2819 - acc: 0.9814 - val_loss: 0.2464 - val_acc: 0.9878</span><br><span class="line">Epoch 4/5</span><br><span class="line">20000/20000 [==============================] - 0s 12us/step - loss: 0.2267 - acc: 0.9858 - val_loss: 0.2042 - val_acc: 0.9882</span><br><span class="line">Epoch 5/5</span><br><span class="line">20000/20000 [==============================] - 0s 11us/step - loss: 0.1921 - acc: 0.9872 - val_loss: 0.1759 - val_acc: 0.9882</span><br></pre></td></tr></table></figure><h3 id="2-1-训练-loss-曲线变化情况对比"><a href="#2-1-训练-loss-曲线变化情况对比" class="headerlink" title="2.1 训练 loss 曲线变化情况对比"></a>2.1 训练 loss 曲线变化情况对比</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history_sigmoid.history[<span class="string">'loss'</span>])</span><br><span class="line">plt.plot(history_softmax.history[<span class="string">'loss'</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(history_sigmoid.history[<span class="string">'val_loss'</span>])</span><br><span class="line">plt.plot(history_softmax.history[<span class="string">'val_loss'</span>])</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line"></span><br><span class="line">plt.legend([<span class="string">'sigmoid_loss'</span>, <span class="string">'softmax_loss'</span>, </span><br><span class="line">            <span class="string">'sigmoid_val_loss'</span>, <span class="string">'softmax_val_loss'</span>], loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2021/sigmoid_softmax_loss.jpg" alt></p><p>从图中可知，Sigmoid 和 Softmax 的训练曲线几乎完全重合.</p><h3 id="2-2-Loss-差值可视化对比"><a href="#2-2-Loss-差值可视化对比" class="headerlink" title="2.2 Loss 差值可视化对比"></a>2.2 Loss 差值可视化对比</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(np.array(history_sigmoid.history[<span class="string">'val_loss'</span>]) - np.array(history_softmax.history[<span class="string">'val_loss'</span>]))</span><br><span class="line">plt.plot(np.array(history_sigmoid.history[<span class="string">'val_loss'</span>]) - np.array(history_sigmoid_2.history[<span class="string">'val_loss'</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line"></span><br><span class="line">plt.legend([<span class="string">'sigmoid_softmax_beta_gap'</span>, <span class="string">'sigmoid_random_weight_gap'</span>], loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2021/loss_difference.jpg" alt></p><p>图中<strong>蓝色曲线</strong>几乎一直是 0，其表示 Sigmoid 和 Softmax 训练的模型的 loss 差异性很小. 但<strong>黄色曲线</strong> 的差值相对就较大，其采用的随机初始化 Sigmoid 权重值，影响了训练过程中的 loss 曲线的变化.</p><p>也就是说，如果设置了正确的 beta 值，Sigmoid 与 Softmax 的效果可认为是等价的.</p><h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h1><p>对于二分类问题，</p><p>[1] - Sigmoid 与 Softmax 完全等价.</p><p>[2] - Sigmoid 与 Softmax 分类器的权值可以相互转换.</p><p>[3] - Softmax 的学习率是 Sigmoid 学习率的2倍. (如：1e-3与2e-3)</p><p>[4] - Softmax 会比 Sigmoid 浪费 2 倍的权值空间(权重参数是两倍).</p><hr color="blue"><p><strong>PS: 根据任务需要，手动修改损失函数，是一项必要的技能。</strong></p>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> softmax </tag>
            
            <tag> sigmoid </tag>
            
            <tag> binary classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The log-sum-exp trick</title>
      <link href="/2021/03/12/The-log-sum-exp-trick/"/>
      <url>/2021/03/12/The-log-sum-exp-trick/</url>
      
        <content type="html"><![CDATA[<p>在机器学习和神经网络中，$\log()$是一个常用的技巧。</p><p>例如，对于函数求导时，使用$\log()$可以简化计算，避免乘法法则。</p><script type="math/tex; mode=display">\frac{\partial}{\partial x} \log[f(x)g(x)] = \frac{\partial}{\partial x}f(x) + \frac{\partial}{\partial x} g(x).</script><h2 id="在softmax-函数中的运用"><a href="#在softmax-函数中的运用" class="headerlink" title="在softmax()函数中的运用"></a>在softmax()函数中的运用</h2><p>在softmax()函数中，按照以下公式，将得分转化为对应的概率：</p><script type="math/tex; mode=display">\frac{\exp(x_m)}{\sum_{i=1}^N \exp(x_n)}</script><p>如果分母过大，则很容易溢出，按照以下方式进行转化：</p><script type="math/tex; mode=display">y = \log \sum_{i=1}^n \exp(x_n) \\e^y = \sum_{i=1}^n\exp(x_n) \\e^y = e^c\sum_{i=1}^n\exp(x_n-c) \\y = c + \log \sum_{i=1}^n \exp(x_n-c)</script><p>如果设置$c = \max{x_1,…,x_n}$，则最大不超过1</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> log-sum-exp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Leetcode 84 递增栈</title>
      <link href="/2021/03/12/Leetcode-84-%E9%80%92%E5%A2%9E%E6%A0%88/"/>
      <url>/2021/03/12/Leetcode-84-%E9%80%92%E5%A2%9E%E6%A0%88/</url>
      
        <content type="html"><![CDATA[<h1 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h1><ol><li>最终矩形必然截断到某一高度</li><li>针对每一个高度，只需找到左右两侧的更高高度</li><li>递增栈的数据结构</li></ol><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">largestRectangleArea</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; heights)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; slope &#123;<span class="number">-1</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=heights.size(); ++i) &#123;</span><br><span class="line">      <span class="keyword">while</span> (slope.back() &gt;= <span class="number">0</span> &amp;&amp; (i == heights.size() || heights[slope.back()] &gt;= heights[i])) &#123;</span><br><span class="line">        <span class="keyword">int</span> height = heights[slope.back()];</span><br><span class="line">        slope.pop_back();</span><br><span class="line">        ret = <span class="built_in">std</span>::max(ret, (i-slope.back()<span class="number">-1</span>)*height);</span><br><span class="line">      &#125;</span><br><span class="line">      slope.push_back(i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>异常巧妙😲</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> stack </tag>
            
            <tag> 递增栈 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阿里面试要点</title>
      <link href="/2021/03/04/%E9%98%BF%E9%87%8C%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/"/>
      <url>/2021/03/04/%E9%98%BF%E9%87%8C%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<ol><li>需要有自己的职业规划，简单一些也可以？</li></ol><p>答：首先，熟悉业务，将算法能力用到真正的业务中；其次，主要专注于迁移学习和多模态NLP两个方向，产生更多的研究成果；最后，理解产业，能够主动发掘方向和着力点。</p><ol><li>面试一般经过：初面、终面、交叉面、HR面。</li><li>为了对自己求职的岗位有更深的理解，可以去查看同样岗位的社招要求。面经可适度参考。</li><li>笔试采用牛客网，可以自动补全，需手动处理IO。</li><li>面试练习可以采用模拟面试的方法，争取做到游刃有余、落落大方。STAR法则 (situation, target, action, result) 可以让叙述更清晰。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Interview </tag>
            
            <tag> 春招 </tag>
            
            <tag> 阿里巴巴 </tag>
            
            <tag> Alibaba </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>再生核希尔伯特空间 (RKHS)</title>
      <link href="/2021/01/16/%E5%86%8D%E7%94%9F%E6%A0%B8%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4-RKHS/"/>
      <url>/2021/01/16/%E5%86%8D%E7%94%9F%E6%A0%B8%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4-RKHS/</url>
      
        <content type="html"><![CDATA[<h1 id="希尔伯特空间"><a href="#希尔伯特空间" class="headerlink" title="希尔伯特空间"></a>希尔伯特空间</h1><p>先来说一下什么是<strong>希尔伯特空间</strong>。<br>这个概念听起来高大上，其实是个非常简单的概念。<br>先说什么是<strong>线性空间</strong></p><h2 id="线性空间"><a href="#线性空间" class="headerlink" title="线性空间"></a>线性空间</h2><p>线性空间即定义了数乘和加法的空间。这个就是具有线性结构的空间。<br>有了线性空间的概念之后，因为有数乘和加法，所以空间中可以找到一组基底（Basis）能够通过线性组合得到空间中所有的点。</p><h2 id="度量空间和赋范空间"><a href="#度量空间和赋范空间" class="headerlink" title="度量空间和赋范空间"></a>度量空间和赋范空间</h2><p>距离的定义必须满足如下三个条件：</p><ol><li><p>d(x,y)≥0;d(x,y)=0 的充要条件是x=y即非负性</p></li><li><p>d(x,y)=d(y,x);对称性</p></li><li><p>d(x,z)+d(z,y)≥d(x,y)满足三角不等式。</p></li></ol><p>定义了距离的空间叫<strong>度量空间</strong>。<br>定义了距离的线性空间叫<strong>线性度量空间</strong></p><p>接下来再定义范数||x||，范数的定义必须满足：</p><ol><li>||x||≥0即非负性</li><li>||αx||=|α|||x||</li><li>||x||+||y||≥||x+y||满足三角不等式</li></ol><p><font color="orange">所以范数这个概念，可以看成从零点到x的距离</font>，同时比价第二条（2），即数乘可以提取出来。<br>所以：<strong>由范数可以定义距离，即d(x,y)=||x−y||，但是距离不可以定义范数因为距离的定义，不满足范数的第二条条件</strong></p><p><strong>因为</strong>，$||x|| = d(0,x)$</p><p><strong>但</strong> $||\alpha x|| = d(0,\alpha x) \neq |\alpha||x||$</p><p><strong>举个栗子</strong>，$d(x,y) = \frac{\sqrt{\sum(x_i-y_i)^2}}{1 + \sqrt{\sum(x_i-y_i)^2}}$</p><p><strong>这个满足距离定义，但是不满足范数定义。所以范数是比距离更具体的一个东西</strong></p><p><strong>而定义了范数的空间，叫赋范空间和度量空间。另外完备的赋范空间叫巴拿赫空间。而定义了范数的线性空间，叫赋范线性空间</strong></p><h2 id="希尔伯特空间-1"><a href="#希尔伯特空间-1" class="headerlink" title="希尔伯特空间"></a><strong>希尔伯特空间</strong></h2><p><strong>定义了范数之后，还没有定义角度。那就再来定义角度，所以可以定义内积如下：</strong></p><p><strong>1. 对称性</strong></p><p><strong>2. 对第一变元的线性性质，即$&lt;\alpha x,y&gt; = \alpha&lt; x,y&gt;$</strong></p><p><strong>3. 正定性</strong></p><p><strong>另外还要再说一下函数空间的内积。一个函数可以看成一个无穷维的向量。</strong></p><p><strong>将一个函数按照x进行采样，可以得到一个函数的表示为一个向量的形式</strong></p><p>$(f(x_0),f(x_1),f(x_2),…,f(x_n))$</p><p><strong>如果采样的间隔变得无穷的小，则这个函数就可以表示为一个无穷维的向量。所以一个函数空间的内积可以定义为：</strong></p><p>$\int f(x)g(x)dx$，由内积可以导出范数，但是范数不可以导出内积。因为可以定义 $||x||^2=<x,x>$</x,x></p><p>另外函数空间的概念，还可以从另外一个角度来思考，例如：</p><ol><li>泰勒级数展开，可以看成将一个函数用${x^i}_0^∞$作为基底表示的一个空间</li><li>傅立叶级数展开，即将一个函数用三角函数的形式进行无穷维的展开</li></ol><p>一个n维的的空间，且定义了内积，就叫欧几里德空间（即有线性结构和夹角，垂直，投影这些）。</p><p><strong>引入无穷维的空间（一般指函数空间），具有线性结构同时定义了内积，同时还具有完备性的空间就叫希尔伯特空间</strong></p><p>比如上面讲的傅立叶变换就是一个希尔伯特空间。</p><h1 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h1>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Transfer Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RKHS </tag>
            
            <tag> Kernel </tag>
            
            <tag> 核方法 </tag>
            
            <tag> Hilbert Spaces </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最大平均差异MMD</title>
      <link href="/2021/01/16/%E6%9C%80%E5%A4%A7%E5%B9%B3%E5%9D%87%E5%B7%AE%E5%BC%82MMD/"/>
      <url>/2021/01/16/%E6%9C%80%E5%A4%A7%E5%B9%B3%E5%9D%87%E5%B7%AE%E5%BC%82MMD/</url>
      
        <content type="html"><![CDATA[<h1 id="MMD理解"><a href="#MMD理解" class="headerlink" title="MMD理解"></a>MMD理解</h1><h2 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h2><p>MMD：maximum mean discrepancy。最大平均差异。最先提出的时候用于双样本的检测（two-sample test）问题<a href="https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf" target="_blank" rel="noopener">A kernel two sample test</a>，用于判断两个分布p和q是否相同。它的基本假设是：如果对于所有以分布生成的样本空间为输入的函数f，如果两个分布生成的足够多的样本在f上的对应的像的均值都相等，那么那么可以认为这两个分布是同一个分布。如果这个值足够小，就认为两个分布相同，否则就认为它们不相同。同时这个值也用来判断两个分布之间的相似程度。</p><h2 id="2-数学步骤"><a href="#2-数学步骤" class="headerlink" title="2.数学步骤"></a>2.数学步骤</h2><p>如果用F表示一个在样本空间上的连续函数集，那么MMD可以用下面的式子表示：</p><script type="math/tex; mode=display">\mathrm{MMD}[\mathcal{F},p,q] := \sup_{f\in\mathcal{F}}(\mathbf{E}_{x\sim p}[f(x)] -\mathbf{E}_{y\sim q}[f(y)])</script><p>$\sup$表示上界。</p><p>假设X和Y分别是从分布p和q通过独立同分布(i.i.d.)采样得到的两个数据集，数据集的大小分别为m和n。基于X和Y可以得到MMD的经验估计(empirical estimate)为：</p><p>$\mathrm{MMD}[\mathcal{F},X,Y] := \sup<em>{f\in\mathcal{F}}(\frac{1}{m}\sum</em>{i=1}^mf(x<em>i) -\frac{1}{n}\sum</em>{i=1}^nf(y_i))$</p><p>在给定两个分布的观测集X,Y的情况下，这个结果会严重依赖于给定的函数集F。为了能表示MMD的性质：当且仅当p和q是相同分布的时候MMD为0，那么要求F足够rich；另一方面为了使检验具有足够的连续性（be consistent in power），从而使得MMD的经验估计可以随着观测集规模增大迅速收敛到它的期望，F必须足够restrictive。文中证明了当F是universal RKHS上的（unit ball）单位球时，可以满足上面两个性质。</p><a id="more"></a><h3 id="2-1再生核希尔伯特空间"><a href="#2-1再生核希尔伯特空间" class="headerlink" title="2.1再生核希尔伯特空间"></a>2.1再生核希尔伯特空间</h3><p>这部分讲述了在RHKS (Reproducing Kernel Hilbert Spaces) 上单位球（unit ball）作为F时，通过有限的观测来对MMD进行估计，并且设立一些MMD可以用来区分概率度量的条件。<br>在RKHS上，每个f对应一个feature map。在feature map的基础上，首先对于某个分布p定义一个mean embedding of p，它满足如下的性质：</p><p>$\mu_p \in \mathcal{H}$ such that $\mathbf{E}_xf = \langle f, \mu_p \rangle$ for all $f \in \mathcal{H}$</p><p>mean embedding存在是有约束条件的[1]。在p和q的mean embedding存在的条件下，MMD的平方可以表示如下：</p><script type="math/tex; mode=display">\begin{align}\mathrm{MMD}^2[\mathcal{F},p,q] & = [\sup_{||f||_\mathcal{H} \leq1}(\mathbf{E}_x[f(x)]-\mathbf{E}_y[f(y)])]^2\\& = [\sup_{||f||_\mathcal{H} \leq 1}\langle \mu_p-\mu_q,f\rangle_\mathcal{H}]^2\\& = ||\mu_p - \mu_q||_{\mathcal{H}}^2\end{align}</script><font color="orange">这里有一些地方没有看懂，$\langle \rangle$是什么意思？$\mathrm{MMD}$最后一步又是如何推导的？</font><p>下面是关于MMD作为一个Borel probability measures时，对F的一个约束及其证明，要求F：be a unit ball in a universal RKHS。比如Gaussian和Laplace RKHSs。进一步在给定了RKHS对应核函数，这个MMD的平方可以表示：</p><p>$\mathrm{MMD}^2[\mathcal{F},p,q] = \mathbf{E}<em>{x,x’} [k(x,x’)] - 2\mathbf{E}</em>{x,y} [k(x,y)] + \mathbf{E}_{y,y’}[k(y,y’)]$</p><p>x和x’分别表示两个服从于p的随机变量，y和y‘分别表示服从q的随机变量。对于上面的一个统计估计可以表示为：</p><p>$\mathrm{MMD}[\mathcal{F},X,Y] = [\frac{1}{m^2}\sum<em>{i,j=1}^m k(x_i,x_j) - \frac{2}{mn} \sum </em>{i,j=1}^{m,n} k(x<em>i,y_j) + \frac{1}{n^2} \sum</em>{i,j=1}^{n}k(y_i,y_j)]^{\frac{1}{2}}$</p><p>对于一个two-sample test, 给定的null hypothesis: p和q是相同，以及the alternative hypothesis: p和q不等。这个通过将test statistic和一个给定的阈值相比较得到，如果MMD大于阈值，那么就reject null hypothesis，也就是两个分布不同。如果MMD小于某个阈值，就接受null hypothesis。由于MMD的计算时使用的是有限的样本数，这里会出现两种类型的错误：第一种错误出现在null hypothesis被错误的拒绝了；也就是本来两个分布相同，但是却被判定为不同。反之，第二种错误出现在null hypothesis被错误的接受了。文章[1]中提供了许多关于hypothesis test的方法，这里不讨论。<br>在domain adaptation中，经常用到MMD来在特征学习的时候构造正则项来约束学到的表示，使得两个域上的特征尽可能相同。从上面的定义看，我们在判断两个分布p和q的时候，需要将观测样本首先映射到RKHS空间上，然后再判断。<em>但实际上很多文章直接将观测样本用于计算，省了映射的那个步骤。</em></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Transfer Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Domain Adaptation </tag>
            
            <tag> MMD </tag>
            
            <tag> Maximum Mean Discrepancy </tag>
            
            <tag> Transfer Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度子域适应网络</title>
      <link href="/2021/01/15/%E6%B7%B1%E5%BA%A6%E5%AD%90%E5%9F%9F%E9%80%82%E5%BA%94%E7%BD%91%E7%BB%9C/"/>
      <url>/2021/01/15/%E6%B7%B1%E5%BA%A6%E5%AD%90%E5%9F%9F%E9%80%82%E5%BA%94%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h1><p>深度学习需要大量的标签样本，为解决标签问题提出了迁移学习，即从相关的source domain 去学习标签好的数据。但由于不同域间的数据分布也不同，所以学习得到的模型泛化能力不高。<br>在训练、测试数据的分布有变动的情况下去学习一个判别模型叫做domain adaptation 或transfer distributions。<br>在深度特征学习中嵌入domain adaptation模块去提取固定特征 已经证明能带来新的优势。之前的domain adaptation都是在全局域上做迁移，导致一个域内不同类别的数据会被混淆，因此不能学习到好的特征结构。</p><h1 id="2、方法"><a href="#2、方法" class="headerlink" title="2、方法"></a>2、方法</h1><h2 id="2-1新方法的概念"><a href="#2-1新方法的概念" class="headerlink" title="2.1新方法的概念"></a>2.1新方法的概念</h2><p>提出DSAN网络：在DANs网络的基础上对其子域来增强特征的表现能力。<br>为了实现正确的对齐，早期使用的方法是MMD：把source和target用一个相同的映射映射在一个再生核希尔伯特空间（RKHS）中，然后求映射后两部分数据的均值差异。现设计了一个局部最大平均差（LMMD）（它在考虑不同样本权重的情况下，测量源域和目标域中相关子域的经验分布的核平均嵌入之间的Hilbert-Schmidt范数）LMMD方法可以在大多数前馈网络模型中实现，并且可以使用标准反向传播进行有效的训练。</p><h2 id="2-2方法的实现"><a href="#2-2方法的实现" class="headerlink" title="2.2方法的实现"></a>2.2方法的实现</h2><h3 id="2-2-1网络的结构"><a href="#2-2-1网络的结构" class="headerlink" title="2.2.1网络的结构"></a>2.2.1网络的结构</h3><p><img src="/images/blog/2021/DSAN.png" alt></p><p>在ResNet的基础上添加LMMD模块来使得相关子域更相近。LMMD计算公式如下:</p><script type="math/tex; mode=display">\hat{d}_\mathcal{H}(p,q) = \frac{1}{C} \sum_{c=1}^C||\sum_{\mathbf{x}_i^s \in \mathcal{D}_s} w_i^{sc}\phi(\mathbf{x}_i^s) - \sum_{\mathbf{x}_j^t \in \mathcal{D}_t} w_j^{tc}\phi(\mathbf{x}_j^t)||_\mathcal{H}^2</script><script type="math/tex; mode=display">w_i^c = \frac{y_{ic}}{\sum_{(\mathbf{x}_j,\mathbf{y}_j)\in\mathcal{D}}y_{jc}}</script><a id="more"></a><p>最终DASN网络的损失函数如下：</p><script type="math/tex; mode=display">\min_f \frac{1}{n_s} \sum_{i=1}^{n_s}J(f(\mathbf{x}_i^s),\mathbf{y}_i^s) + \lambda\sum_{l\in L}\hat{d}_l(p,q)</script><h2 id="2-3背景"><a href="#2-3背景" class="headerlink" title="2.3背景"></a>2.3背景</h2><p>DAN是在DDC（deep domain Confusion）的基础上发展来的：<br>DAN解决了DDC的两个问题：<br>DDC只适配了一层网络，可能还是不够，因为Jason的工作中已经明确指出不同层都是可以迁移的。所以DAN就多适配几层；<br>DDC是用了单一核的MMD，单一固定的核可能不是最优的核。DAN用了多核的MMD（MK-MMD），效果比DDC更好。<br>总结：DANs是多层适配和多核MMD。</p><p>迁移学习目前的潮流有两种：<br>第一种是基于统计矩匹配的方法，即最大均值偏差（MMD）、中心矩差异（CMD）<br>第二种常用的方法是基于对抗性损失，它鼓励来自不同领域的样本对于领域标签是非歧视性的，即借用了GAN的思想<br>一般来说，采取adversarial loss的效果比statistic moment matching-based 效果好。<br>但这篇论文用的DSAN证明能取得更好的效果。</p><h1 id="3、实验结果分析："><a href="#3、实验结果分析：" class="headerlink" title="3、实验结果分析："></a>3、实验结果分析：</h1><p>在OFFICE31、CLEF-D等数据集上测试得出：<br>DASN与MMD的模型：能提高10-20%个百分点的精确率。<br>DASN与主流的（带对抗损失）模型比较 ：能提高5个百分点的精确率。</p><h1 id="4、结论"><a href="#4、结论" class="headerlink" title="4、结论"></a>4、结论</h1><p>DSAN预测能力不仅高于主流的对抗损失模型、速度也更快、而且易于实现。</p><p><img src="/images/blog/2021/DSAN-result.png" alt></p><p><img src="/images/blog/2021/DSAN-result1.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Transfer Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Domain Adaptation </tag>
            
            <tag> transfer learning </tag>
            
            <tag> 领域适应 </tag>
            
            <tag> 迁移学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>什么是KL散度？</title>
      <link href="/2021/01/15/%E4%BB%80%E4%B9%88%E6%98%AFKL%E6%95%A3%E5%BA%A6%EF%BC%9F/"/>
      <url>/2021/01/15/%E4%BB%80%E4%B9%88%E6%98%AFKL%E6%95%A3%E5%BA%A6%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p><code>Kullback-Leibler Divergence</code>，即<code>K-L散度</code>，是一种<strong>量化两种概率分布P和Q之间差异</strong>的方式，又叫<code>相对熵</code>。在概率学和统计学上，我们经常会使用一种<code>更简单的、近似的分布</code>来替代<code>观察数据</code>或<code>太复杂的分布</code>。K-L散度能帮助我们度量使用一个分布来近似另一个分布时所损失的信息量。</p><blockquote><p>假设我们是一群太空科学家，经过遥远的旅行，来到了一颗新发现的星球。在这个星球上，生存着一种长有牙齿的蠕虫，引起了我们的研究兴趣。我们发现这种蠕虫生有10颗牙齿，但是因为不注意口腔卫生，又喜欢嚼东西，许多蠕虫会掉牙。收集大量样本之后，我们得到关于蠕虫牙齿数量的经验分布，如下图所示</p></blockquote><p><img src="/images/blog/2021/teeth-distribution.webp" alt></p><p>显然我们的原始数据并非均分布的，但也不是我们已知的分布，至少不是常见的分布。作为备选，我们想到的另一种简单模型是<code>二项式分布binomial distribution</code>。蠕虫嘴里面共有<code>n=10</code>个牙槽，每个牙槽出现牙齿与否为独立事件，且概率均为<code>p</code>。则蠕虫牙齿数量即为期望值<code>E[x]=n*p</code>，真实期望值即为观察数据的平均值，比如说<code>5.7</code>，则<code>p=0.57</code>，得到如下图所示的二项式分布：<br><a id="more"></a></p><p><img src="/images/blog/2021/binomial.webp" alt></p><p>对比一下原始数据，可以看出均分布和二项分布都不能完全描述原始分布。</p><p><img src="/images/blog/2021/comparison.webp" alt></p><p>可是，我们不禁要问，哪一种分布更加接近原始分布呢？<br>已经有许多度量误差的方式存在，但是我们所要考虑的是减小发送的信息量。上面讨论的均分布和二项式分布都把问题规约到只需要两个参数，牙齿数量和概率值（均分布只需要牙齿数量即可）。那么哪个分布保留了更多的原始数据分布的信息呢？这个时候就需要K-L散度登场了。</p><h2 id="数据的熵"><a href="#数据的熵" class="headerlink" title="数据的熵"></a>数据的熵</h2><p>K-L散度源于信息论。信息论主要研究如何量化数据中的信息。最重要的信息度量单位是<code>熵</code>Entropy，一般用<code>H</code>表示。分布的熵的公式如下：</p><p>$H=-\sum_{i=1}^N p(x_i) \cdot \log p(x_i)$</p><p>上面对数没有确定底数，可以是<code>2</code>、<code>e</code>或<code>10</code>，等等。如果我们使用以<code>2</code>为底的对数计算H值的话，可以把这个值看作是编码信息所需要的最少二进制位个数bits。上面空间蠕虫的例子中，信息指的是根据观察所得的经验分布给出的蠕虫牙齿数量。计算可以得到原始数据概率分布的熵值为<code>3.12 bits</code>。这个值只是告诉我们编码蠕虫牙齿数量概率的信息需要的二进制位<code>bit</code>的位数。<br>可是熵值并没有给出压缩数据到最小熵值的方法，即如何编码数据才能达到最优（存储空间最优）。优化信息编码是一个非常有意思的主题，但并不是理解K-L散度所必须的。熵的主要作用是告诉我们最优编码信息方案的理论下界（存储空间），以及度量数据的信息量的一种方式。理解了熵，我们就知道有多少信息蕴含在数据之中，现在我们就可以计算当我们用一个带参数的概率分布来近似替代原始数据分布的时候，到底损失了多少信息。请继续看下节内容。</p><h2 id="K-L散度度量信息损失"><a href="#K-L散度度量信息损失" class="headerlink" title="K-L散度度量信息损失"></a>K-L散度度量信息损失</h2><p>只需要稍加修改<code>熵H</code>的计算公式就能得到<code>K-L散度</code>的计算公式。设<code>p</code>为观察得到的概率分布，<code>q</code>为另一分布来近似<code>p</code>，则<code>p</code>、<code>q</code>的<code>K-L散度</code>为：</p><p>$D<em>{KL}(p||q) = \sum</em>{i=1}^N p(x_i) \cdot(\log p(x_i) - \log q(x_i))$</p><p>显然，根据上面的公式，K-L散度其实是数据的原始分布p和近似分布q之间的对数差值的期望。如果继续用<code>2</code>为底的对数计算，则<strong>K-L散度值表示信息损失的二进制位数</strong>。下面公式以期望表达K-L散度：</p><font color="orange">可以理解为在真实分布下，预测分布与真实分布的能量（熵）差异。</font><p>一般，K-L散度以下面的书写方式更常见：</p><p>$D<em>{KL}(p||q) = \sum</em>{i=1}^N p(x_i) \cdot \log \frac{p(x_i)}{q(x_i)}$</p><p>OK，现在我们知道当用一个分布来近似另一个分布时如何计算信息损失量了。接下来，让我们重新回到最开始的蠕虫牙齿数量概率分布的问题。首先是用均分布来近似原始分布的K-L散度：</p><p>$D_{kl}(Observed||Uniform) = 0.338$</p><p>接下来计算用二项式分布近似原始分布的K-L散度：$0.477$</p><p>通过上面的计算可以看出，使用均分布近似原始分布的信息损失要比用二项式分布近似小。所以，如果要从均分布和二项式分布中选择一个的话，均分布更好些。</p><h2 id="散度并非距离"><a href="#散度并非距离" class="headerlink" title="散度并非距离"></a>散度并非距离</h2><p>很自然地，一些同学把K-L散度看作是不同分布之间距离的度量。这是不对的，因为从K-L散度的计算公式就可以看出它不符合对称性（距离度量应该满足对称性）。<code>Dkl (Observed || Binomial) != Dkl (Binomial || Observed)</code>。也就是说，用<code>p</code>近似<code>q</code>和用<code>q</code>近似<code>p</code>，二者所损失的信息并不是一样的。</p><h2 id="使用K-L散度优化模型"><a href="#使用K-L散度优化模型" class="headerlink" title="使用K-L散度优化模型"></a>使用K-L散度优化模型</h2><p>前面使用的二项式分布的参数是概率 <code>p=0.57</code>，是原始数据的均值。<code>p</code>的值域在 [0, 1] 之间，我们要选择一个<code>p</code>值，建立二项式分布，目的是最小化近似误差，即K-L散度。那么<code>0.57</code>是最优的吗？<br> 下图是原始数据分布和二项式分布的K-L散度变化随二项式分布参数<code>p</code>变化情况：</p><p><img src="/images/blog/2021/divergence.webp" alt></p><p>通过上面的曲线图可以看出，K-L散度值在圆点处最小，即<code>p=0.57</code>。所以我们之前的二项式分布模型已经是最优的二项式模型了。注意，我已经说了，是而像是模型，这里只限定在二项式模型范围内。</p><p>前面只考虑了均分布模型和二项式分布模型，接下来我们考虑另外一种模型来近似原始数据。首先把原始数据分成两部分，1）0-5颗牙齿的概率和 2）6-10颗牙齿的概率。一只蠕虫的牙齿数量<code>x=i</code>的概率为<code>p/5</code>; <code>x=j</code>的概率为<code>(1-p) / 6</code>，<code>i=0,1,2,3,4,5</code>; <code>j=6,7,8,9,10</code>。<br>Aha，我们自己建立了一个新的（奇怪的）模型来近似原始的分布，模型只有一个参数<code>p</code>，像前面那样优化二项式分布的时候所做的一样，让我们画出K-L散度值随<code>p</code>变化的情况：<br><img src="/images/blog/2021/divergence1.webp" alt></p><p>当<code>p=0.47</code>时，K-L值取最小值<code>0.338</code>。似曾相识吗？对，这个值和使用均分布的K-L散度值是一样的（这并不能说明什么）！我们自己都说了，这是个奇怪的模型，在K-L值相同的情况下，更倾向于使用更常见的、更简单的均分布模型。回头看，我们在这一小节中使用K-L散度作为目标方程，分别找到了二项式分布模型的参数<code>p=0.57</code>和上面这个随手建立的模型的参数<code>p=0.47</code>。是的，这就是本节的重点：<strong>使用K-L散度作为目标方程来优化模型</strong>。当然，本节中的模型都只有一个参数，也可以拓展到有更多参数的高维模型中。</p><h2 id="变分自编码器VAEs和变分贝叶斯法"><a href="#变分自编码器VAEs和变分贝叶斯法" class="headerlink" title="变分自编码器VAEs和变分贝叶斯法"></a>变分自编码器VAEs和变分贝叶斯法</h2><p>如果你熟悉神经网络，你肯能已经猜到我们接下来要学习的内容。除去神经网络结构的细节信息不谈，整个神经网络模型其实是在构造一个参数数量巨大的函数（百万级，甚至更多），不妨记为<code>f(x)</code>，通过设定目标函数，可以训练神经网络逼近非常复杂的真实函数<code>g(x)</code>。训练的关键是要设定目标函数，反馈给神经网络当前的表现如何。训练过程就是不断减小目标函数值的过程。</p><p>我们已经知道K-L散度用来度量在逼近一个分布时的信息损失量。K-L散度能够赋予神经网络近似表达非常复杂数据分布的能力。变分自编码器（Variational Autoencoders，VAEs）是一种能够学习最佳近似数据集中信息的常用方法，<a href="https://links.jianshu.com/go?to=https%3A%2F%2Farxiv.org%2Fabs%2F1606.05908" target="_blank" rel="noopener">Tutorial on Variational Autoencoders 2016</a>是一篇关于VAEs的非常不错的教程，里面讲述了如何构建VAE的细节。 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fmedium.com%2F%40dmonn%2Fwhat-are-variational-autoencoders-a-simple-explanation-ea7dccafb0e3" target="_blank" rel="noopener">What are Variational Autoencoders? A simple explanation</a>简单介绍了VAEs，<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fblog.keras.io%2Fbuilding-autoencoders-in-keras.html" target="_blank" rel="noopener">Building Autoencoders in Keras</a>介绍了如何利用Keras库实现几种自编码器。</p><p>变分贝叶斯方法（Variational Bayesian Methods）是一种更常见的方法。<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.countbayesie.com%2Fblog%2F2015%2F3%2F3%2F6-amazing-trick-with-monte-carlo-simulations" target="_blank" rel="noopener">这篇文章</a>介绍了强大的蒙特卡洛模拟方法能够解决很多概率问题。蒙特卡洛模拟能够帮助解决许多贝叶斯推理问题中的棘手积分问题，尽管计算开销很大。包括VAE在内的变分贝叶斯方法，都能用K-L散度生成优化的近似分布，这种方法对棘手积分问题能进行更高效的推理。更多变分推理（Variational Inference）的知识可以访问<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fedwardlib.org%2F" target="_blank" rel="noopener">Edward library for python</a>。</p><h2 id="计算KL散度的注意事项"><a href="#计算KL散度的注意事项" class="headerlink" title="计算KL散度的注意事项"></a>计算KL散度的注意事项</h2><p><img src="/images/blog/2021/KL-notice.webp" alt></p><p><img src="/images/blog/2021/KL-notice1.webp" alt></p><ol><li>信息熵、交叉熵、相对熵</li></ol><ul><li><p>信息熵，即熵，香浓熵。编码方案完美时，最短平均编码长度。</p></li><li><p>交叉熵，cross-entropy。编码方案不一定完美时（由于对概率分布的估计不一定正确），平均编码长度。是神经网络常用的损失函数。</p></li><li><p>相对熵，即K-L散度，relative entropy。编码方案不一定完美时，平均编码长度相对于最小值的增加值。<br> 更详细对比，见知乎<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F41252833" target="_blank" rel="noopener">如何通俗的解释交叉熵与相对熵?</a></p></li></ul><ol><li>为什么在神经网络中使用交叉熵损失函数，而不是K-L散度？<br>K-L散度=交叉熵-熵，即 <code>DKL( p||q )=H(p,q)−H(p)</code>。<br>在神经网络所涉及到的范围内，<code>H(p)</code>不变，则<code>DKL( p||q )</code>等价<code>H(p,q)</code>。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> K-L divergence </tag>
            
            <tag> KL散度 </tag>
            
            <tag> 相对熵 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sklearn.metrics.precision_recall_curve()源码阅读</title>
      <link href="/2021/01/12/sklearn-metrics-precision-recall-curve-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
      <url>/2021/01/12/sklearn-metrics-precision-recall-curve-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@_deprecate_positional_args</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">precision_recall_curve</span><span class="params">(y_true, probas_pred, *, pos_label=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                           sample_weight=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y_true : ndarray of shape (n_samples,)</span></span><br><span class="line"><span class="string">        True binary labels. If labels are not either &#123;-1, 1&#125; or &#123;0, 1&#125;, then</span></span><br><span class="line"><span class="string">        pos_label should be explicitly given.</span></span><br><span class="line"><span class="string">    probas_pred : ndarray of shape (n_samples,)</span></span><br><span class="line"><span class="string">        Estimated probabilities or output of a decision function.</span></span><br><span class="line"><span class="string">    pos_label : int or str, default=None</span></span><br><span class="line"><span class="string">        The label of the positive class.</span></span><br><span class="line"><span class="string">        When ``pos_label=None``, if y_true is in &#123;-1, 1&#125; or &#123;0, 1&#125;,</span></span><br><span class="line"><span class="string">        ``pos_label`` is set to 1, otherwise an error will be raised.</span></span><br><span class="line"><span class="string">    sample_weight : array-like of shape (n_samples,), default=None</span></span><br><span class="line"><span class="string">        Sample weights.</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    precision : ndarray of shape (n_thresholds + 1,)</span></span><br><span class="line"><span class="string">        Precision values such that element i is the precision of</span></span><br><span class="line"><span class="string">        predictions with score &gt;= thresholds[i] and the last element is 1.</span></span><br><span class="line"><span class="string">    recall : ndarray of shape (n_thresholds + 1,)</span></span><br><span class="line"><span class="string">        Decreasing recall values such that element i is the recall of</span></span><br><span class="line"><span class="string">        predictions with score &gt;= thresholds[i] and the last element is 0.</span></span><br><span class="line"><span class="string">    thresholds : ndarray of shape (n_thresholds,)</span></span><br><span class="line"><span class="string">        Increasing thresholds on the decision function used to compute</span></span><br><span class="line"><span class="string">        precision and recall. n_thresholds &lt;= len(np.unique(probas_pred)).</span></span><br><span class="line"><span class="string">    See Also</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    plot_precision_recall_curve : Plot Precision Recall Curve for binary</span></span><br><span class="line"><span class="string">        classifiers.</span></span><br><span class="line"><span class="string">    PrecisionRecallDisplay : Precision Recall visualization.</span></span><br><span class="line"><span class="string">    average_precision_score : Compute average precision from prediction scores.</span></span><br><span class="line"><span class="string">    det_curve: Compute error rates for different probability thresholds.</span></span><br><span class="line"><span class="string">    roc_curve : Compute Receiver operating characteristic (ROC) curve.</span></span><br><span class="line"><span class="string">    Examples</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; import numpy as np</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; from sklearn.metrics import precision_recall_curve</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y_true = np.array([0, 0, 1, 1])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y_scores = np.array([0.1, 0.4, 0.35, 0.8])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; precision, recall, thresholds = precision_recall_curve(</span></span><br><span class="line"><span class="string">    ...     y_true, y_scores)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; precision</span></span><br><span class="line"><span class="string">    array([0.66666667, 0.5       , 1.        , 1.        ])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; recall</span></span><br><span class="line"><span class="string">    array([1. , 0.5, 0.5, 0. ])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; thresholds</span></span><br><span class="line"><span class="string">    array([0.35, 0.4 , 0.8 ])</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred,</span><br><span class="line">                                             pos_label=pos_label,</span><br><span class="line">                                             sample_weight=sample_weight)</span><br><span class="line"></span><br><span class="line">    precision = tps / (tps + fps)</span><br><span class="line">    precision[np.isnan(precision)] = <span class="number">0</span></span><br><span class="line">    recall = tps / tps[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># stop when full recall attained</span></span><br><span class="line">    <span class="comment"># and reverse the outputs so recall is decreasing</span></span><br><span class="line">    last_ind = tps.searchsorted(tps[<span class="number">-1</span>])</span><br><span class="line">    sl = slice(last_ind, <span class="literal">None</span>, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> np.r_[precision[sl], <span class="number">1</span>], np.r_[recall[sl], <span class="number">0</span>], thresholds[sl]</span><br></pre></td></tr></table></figure><a id="more"></a><p>然后用到<code>_binary_clf_curve</code>，去看一眼：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_binary_clf_curve</span><span class="params">(y_true, y_score, pos_label=None, sample_weight=None)</span>:</span></span><br><span class="line">    <span class="string">"""Calculate true and false positives per binary classification threshold.</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y_true : ndarray of shape (n_samples,)</span></span><br><span class="line"><span class="string">        True targets of binary classification.</span></span><br><span class="line"><span class="string">    y_score : ndarray of shape (n_samples,)</span></span><br><span class="line"><span class="string">        Estimated probabilities or output of a decision function.</span></span><br><span class="line"><span class="string">    pos_label : int or str, default=None</span></span><br><span class="line"><span class="string">        The label of the positive class.</span></span><br><span class="line"><span class="string">    sample_weight : array-like of shape (n_samples,), default=None</span></span><br><span class="line"><span class="string">        Sample weights.</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    fps : ndarray of shape (n_thresholds,)</span></span><br><span class="line"><span class="string">        A count of false positives, at index i being the number of negative</span></span><br><span class="line"><span class="string">        samples assigned a score &gt;= thresholds[i]. The total number of</span></span><br><span class="line"><span class="string">        negative samples is equal to fps[-1] (thus true negatives are given by</span></span><br><span class="line"><span class="string">        fps[-1] - fps).</span></span><br><span class="line"><span class="string">    tps : ndarray of shape (n_thresholds,)</span></span><br><span class="line"><span class="string">        An increasing count of true positives, at index i being the number</span></span><br><span class="line"><span class="string">        of positive samples assigned a score &gt;= thresholds[i]. The total</span></span><br><span class="line"><span class="string">        number of positive samples is equal to tps[-1] (thus false negatives</span></span><br><span class="line"><span class="string">        are given by tps[-1] - tps).</span></span><br><span class="line"><span class="string">    thresholds : ndarray of shape (n_thresholds,)</span></span><br><span class="line"><span class="string">        Decreasing score values.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Check to make sure y_true is valid</span></span><br><span class="line">    y_type = type_of_target(y_true)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (y_type == <span class="string">"binary"</span> <span class="keyword">or</span></span><br><span class="line">            (y_type == <span class="string">"multiclass"</span> <span class="keyword">and</span> pos_label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"&#123;0&#125; format is not supported"</span>.format(y_type))</span><br><span class="line"></span><br><span class="line">    check_consistent_length(y_true, y_score, sample_weight)</span><br><span class="line">    y_true = column_or_1d(y_true)</span><br><span class="line">    y_score = column_or_1d(y_score)</span><br><span class="line">    assert_all_finite(y_true)</span><br><span class="line">    assert_all_finite(y_score)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        sample_weight = column_or_1d(sample_weight)</span><br><span class="line"></span><br><span class="line">    pos_label = _check_pos_label_consistency(pos_label, y_true)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># make y_true a boolean vector</span></span><br><span class="line">    y_true = (y_true == pos_label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort scores and corresponding truth values</span></span><br><span class="line">    desc_score_indices = np.argsort(y_score, kind=<span class="string">"mergesort"</span>)[::<span class="number">-1</span>]</span><br><span class="line">    y_score = y_score[desc_score_indices]</span><br><span class="line">    y_true = y_true[desc_score_indices]</span><br><span class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        weight = sample_weight[desc_score_indices]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        weight = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># y_score typically has many tied values. Here we extract</span></span><br><span class="line">    <span class="comment"># the indices associated with the distinct values. We also</span></span><br><span class="line">    <span class="comment"># concatenate a value for the end of the curve.</span></span><br><span class="line">    distinct_value_indices = np.where(np.diff(y_score))[<span class="number">0</span>]</span><br><span class="line">    threshold_idxs = np.r_[distinct_value_indices, y_true.size - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># accumulate the true positives with decreasing threshold</span></span><br><span class="line">    tps = stable_cumsum(y_true * weight)[threshold_idxs]</span><br><span class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># express fps as a cumsum to ensure fps is increasing even in</span></span><br><span class="line">        <span class="comment"># the presence of floating point errors</span></span><br><span class="line">        fps = stable_cumsum((<span class="number">1</span> - y_true) * weight)[threshold_idxs]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        fps = <span class="number">1</span> + threshold_idxs - tps</span><br><span class="line">    <span class="keyword">return</span> fps, tps, y_score[threshold_idxs]</span><br></pre></td></tr></table></figure><p>使用到numpy进行归并排序，还用到了<code>::</code>写法，即seq[start​ : end : s​tep]，</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> source code </tag>
            
            <tag> PR-Curve </tag>
            
            <tag> sklearn </tag>
            
            <tag> scikit-learn </tag>
            
            <tag> precision_recall_curve </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>箱线图</title>
      <link href="/2021/01/11/%E7%AE%B1%E7%BA%BF%E5%9B%BE/"/>
      <url>/2021/01/11/%E7%AE%B1%E7%BA%BF%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<p>总是可以看到箱线图，但是却不知道怎么读？今天又看到了，就做一个了解。</p><h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>箱形图（Box-plot）又称为盒须图、盒式图或箱线图，是一种用作显示一组数据分散情况资料的统计图。因形状如箱子而得名。在各种领域也经常被使用，常见于<a href="https://baike.baidu.com/item/品质管理/9207881" target="_blank" rel="noopener">品质管理</a>。它主要用于反映原始数据分布的特征，还可以进行多组数据分布特征的比 较。箱线图的绘制方法是：先找出一组数据的上边缘、下边缘、中位数和两个四分位数；然后， 连接两个四分位数画出箱体；再将上边缘和下边缘与箱体相连接，中位数在箱体中间。</p><p><img src="/images/blog/2021/box-plot.jpg" alt></p><p>“盒式图”或叫”<a href="https://baike.baidu.com/item/盒须图" target="_blank" rel="noopener">盒须图</a>“”箱形图”boxplot（也称箱须图(Box-whiskerPlot）须图又称为箱形图，其绘制须使用常用的<a href="https://baike.baidu.com/item/统计量" target="_blank" rel="noopener">统计量</a>，能提供有关数据位置和分散情况的关键信息，尤其在比较不同的母体数据时更可表现其差异。</p><p>如上图所示，标示了图中每条线表示的含义，其中应用到了分位值（数）的概念。</p><p>主要包含六个数据节点，将一组数据从大到小排列，分别计算出他的上边缘，上<a href="https://baike.baidu.com/item/四分位数" target="_blank" rel="noopener">四分位数</a>Q3，<a href="https://baike.baidu.com/item/中位数" target="_blank" rel="noopener">中位数</a>，下四分位数Q1，下边缘，还有一个<a href="https://baike.baidu.com/item/异常值" target="_blank" rel="noopener">异常值</a>。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 箱线图 </tag>
            
            <tag> box-plot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>异常检测</title>
      <link href="/2021/01/11/%E6%B7%B1%E5%BA%A6%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
      <url>/2021/01/11/%E6%B7%B1%E5%BA%A6%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>根据维基百科：</p><blockquote><p>在<a href="https://zh.wikipedia.org/wiki/数据挖掘" target="_blank" rel="noopener">数据挖掘</a>中，<strong>异常检测</strong>（英语：<strong>anomaly detection</strong>）对不符合预期模式或<a href="https://zh.wikipedia.org/w/index.php?title=数据集&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">数据集</a>中其他项目的项目、事件或观测值的识别。<a href="https://zh.wikipedia.org/wiki/异常检测#cite_note-ChandolaSurvey-1" target="_blank" rel="noopener">[1]</a> 通常异常项目会转变成<a href="https://zh.wikipedia.org/w/index.php?title=银行欺诈&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">银行欺诈</a>、结构缺陷、医疗问题、文本错误等类型的问题。异常也被称为离群值、新奇、噪声、偏差和例外。</p></blockquote><p>有三大类异常检测方法。<a href="https://zh.wikipedia.org/wiki/异常检测#cite_note-ChandolaSurvey-1" target="_blank" rel="noopener">[1]</a> 在假设数据集中大多数实例都是正常的前提下，<strong>无监督异常检测</strong>方法能通过寻找与其他数据最不匹配的实例来检测出未标记测试数据的异常。<strong>监督式异常检测</strong>方法需要一个已经被标记“正常”与“异常”的数据集，并涉及到训练分类器（与许多其他的<a href="https://zh.wikipedia.org/wiki/分类问题" target="_blank" rel="noopener">统计分类问题</a>的关键区别是异常检测的内在不均衡性）。<strong>半监督式异常检测</strong>方法根据一个给定的<em>正常</em>训练数据集创建一个表示正常行为的模型，然后检测由学习模型生成的测试实例的可能性。</p><a id="more"></a><h1 id="检测方法"><a href="#检测方法" class="headerlink" title="检测方法"></a>检测方法</h1><p>【1】基于统计模型的方法：首先建立一个数据模型，<font color="orange">异常是那些同模型不能完美拟合的对象</font>；如果模型是<strong>簇的集合</strong>，则异常是不显著属于任何簇的对象；在使用<strong>回归模型</strong>时，异常是相对远离预测值的对象。</p><p>【2】基于邻近度的方法：通常可以在对象之间定义邻近性度量，异常对象是那些远离其他对象的对象。</p><p>【3】基于密度的方法：仅当一个点的局部密度显著低于它的大部分近邻时才将其分类为离群点。</p><p>【4】基于聚类的方法：聚类分析用于发现局部强相关的对象组，而异常检测用来发现不与其他对象强相关的对象。因此，聚类分析非常自然的可以用于离群点检测。</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 异常检测 </tag>
            
            <tag> anomaly detection </tag>
            
            <tag> deep </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DSRE测试时的batch_size</title>
      <link href="/2021/01/10/DSRE%E6%B5%8B%E8%AF%95%E6%97%B6%E7%9A%84batch-size/"/>
      <url>/2021/01/10/DSRE%E6%B5%8B%E8%AF%95%E6%97%B6%E7%9A%84batch-size/</url>
      
        <content type="html"><![CDATA[<p>在进行远程监督关系抽取测试阶段，由于每个包中含有的句子数不同，少的只含有1个句子的包有1149个，但是最多的1个包含有138个句子。担心使用BERT测试时，会出现爆显存的问题。</p><h1 id="显存占用"><a href="#显存占用" class="headerlink" title="显存占用"></a>显存占用</h1><p>在评估时batch_size设为8也完全不会超显存，才占用9875MB左右。<br>将batch_size设为64占用16547MB左右显存，<strong>峰值28539MB</strong>，再多可能就超了。</p><h1 id="是否截断？"><a href="#是否截断？" class="headerlink" title="是否截断？"></a>是否截断？</h1><p>测试集中含有1758个包，每个包53个类别，总类别数为：</p><p>$1758 * 53 = 93174$</p><p>是否需要对其截断呢？</p><p>按照DISTRE的方式，截取前50000个，得到的<code>auc</code>值为：0.0290428</p><p>若不截断，得到的<code>auc</code>值为：0.0290428</p><p>可以看到是否截断对<code>auc</code>值的计算还是有影响的，保留的越多，理所应当对应的面积就越大，所以<code>auc</code>值就越高。<strong><em>但是，由于后面数值过小，所以可以忽略不计，也可能对结果没有影响。</em></strong></p><a id="more"></a><h1 id="不同batch-size对auc影响"><a href="#不同batch-size对auc影响" class="headerlink" title="不同batch_size对auc影响"></a>不同batch_size对auc影响</h1><p>当batch_size设为64，<code>auc</code>值为：0.0290429</p><p>当batch_size设为8，  <code>auc</code>值为：0.0292178</p><p>当batch_size设为1，  <code>auc</code>值为：0.0290395</p><font color="orange">为什么不同的batch_size设置会导致计算的`auc`结果不同呢？从理论上讲这个是不合理的。</font>打印logits和labels查看，发现**确实不同**：当batch_size=1:<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.00892705</span> <span class="number">0.00151212</span> <span class="number">0.00814364</span> <span class="number">0.00080609</span> <span class="number">0.00226246</span> <span class="number">0.00350421</span></span><br><span class="line">  <span class="number">0.0011088</span>  <span class="number">0.01335552</span> <span class="number">0.00449597</span> <span class="number">0.00577065</span>]</span><br><span class="line"> [<span class="number">0.00675167</span> <span class="number">0.00236401</span> <span class="number">0.00788459</span> <span class="number">0.0009495</span>  <span class="number">0.00175571</span> <span class="number">0.00618613</span></span><br><span class="line">  <span class="number">0.00157026</span> <span class="number">0.03045319</span> <span class="number">0.00293169</span> <span class="number">0.01477546</span>]</span><br><span class="line"> [<span class="number">0.00760224</span> <span class="number">0.00332207</span> <span class="number">0.00867613</span> <span class="number">0.00068079</span> <span class="number">0.00131133</span> <span class="number">0.00436077</span></span><br><span class="line">  <span class="number">0.00407064</span> <span class="number">0.03283269</span> <span class="number">0.00417252</span> <span class="number">0.00687293</span>]]</span><br></pre></td></tr></table></figure>当batch_size=8:<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.00892705</span> <span class="number">0.00151212</span> <span class="number">0.00814363</span> <span class="number">0.00080609</span> <span class="number">0.00226246</span> <span class="number">0.00350421</span></span><br><span class="line">  <span class="number">0.0011088</span>  <span class="number">0.01335552</span> <span class="number">0.00449597</span> <span class="number">0.00577066</span>]</span><br><span class="line"> [<span class="number">0.00675167</span> <span class="number">0.00236401</span> <span class="number">0.00788459</span> <span class="number">0.0009495</span>  <span class="number">0.00175571</span> <span class="number">0.00618613</span></span><br><span class="line">  <span class="number">0.00157026</span> <span class="number">0.03045319</span> <span class="number">0.00293169</span> <span class="number">0.01477545</span>]</span><br><span class="line"> [<span class="number">0.00675167</span> <span class="number">0.00236401</span> <span class="number">0.00788459</span> <span class="number">0.0009495</span>  <span class="number">0.00175571</span> <span class="number">0.00618613</span></span><br><span class="line">  <span class="number">0.00157026</span> <span class="number">0.03045319</span> <span class="number">0.00293169</span> <span class="number">0.01477545</span>]]</span><br></pre></td></tr></table></figure>但是输入数据是没有shuffle的，打印查看，确实输入顺序保持一致。<font color="red">***估计是BERT中的normalization层对于不同的batch_size会有不同的小的偏置，导致结果的轻微波动！***</font>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> relation extraction </tag>
            
            <tag> distant supervision </tag>
            
            <tag> batch_size </tag>
            
            <tag> batch </tag>
            
            <tag> auc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文题目怎么起？</title>
      <link href="/2021/01/09/%E8%AE%BA%E6%96%87%E9%A2%98%E7%9B%AE%E6%80%8E%E4%B9%88%E8%B5%B7%EF%BC%9F/"/>
      <url>/2021/01/09/%E8%AE%BA%E6%96%87%E9%A2%98%E7%9B%AE%E6%80%8E%E4%B9%88%E8%B5%B7%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>最近在写论文准备投稿，但是论文题目还没有想好，该怎么给论文起一个好名字呢？去看了一些相关工作的论文题目，总结一些经验。</p><h1 id="论文题目"><a href="#论文题目" class="headerlink" title="论文题目"></a>论文题目</h1><h2 id="问句式"><a href="#问句式" class="headerlink" title="问句式"></a>问句式</h2><p>Are Noisy Sentences Useless for Distant Supervised Relation Extraction?</p><p>这种可以给人留下较深的印象，在众多的论文中脱颖而出。而且很清晰地表达了论文的核心部分。类似的还有大名鼎鼎的：</p><p>Attention is All You Need</p><h2 id="冒号式"><a href="#冒号式" class="headerlink" title="冒号式"></a>冒号式</h2><p>以冒号将题目分为两部分，前面一部分是方法或模型名；后一部分是针对的任务。比如：</p><p>Uncover the Ground Truth Relations in Distant Supervision: A Neural Expectation-Maximization Framework</p><p>From Bag of Sentences to Document: Distantly Supervised Relation Extraction via Machine Reading Comprehension</p><p>Towards Accurate and Consistent Evaluation: A Dataset for Distantly-Supervised Relation Extraction</p><h2 id="动名词"><a href="#动名词" class="headerlink" title="动名词"></a>动名词</h2><p>使用动词的动名词形式表达使用的主要技术：</p><p>Reducing Wrong Labels in Distant Supervision for Relation Extraction</p><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol><li>用Method, Approach还是Framework?</li><li>要不要在标题中加上False Negative Problems?</li><li>要不要强调Semi-Supervised?</li></ol>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
            <tag> title </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-Curve AUC值的两种实现方法</title>
      <link href="/2021/01/08/PR-Curve-AUC%E5%80%BC%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95/"/>
      <url>/2021/01/08/PR-Curve-AUC%E5%80%BC%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="DISTRE"><a href="#DISTRE" class="headerlink" title="DISTRE"></a>DISTRE</h1><p>DISTRE是一种手动计算的方式，代码<strong>在不修改源代码原意的基础上进行了简化</strong>，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">logits = [[<span class="number">0.3</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.9</span>],</span><br><span class="line">          [<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.9</span>, <span class="number">0.2</span>],</span><br><span class="line">          [<span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.6</span>]]</span><br><span class="line">labels = [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">          [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">          [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_pr_curve_and_predictions</span><span class="params">()</span>:</span></span><br><span class="line">    num_relation_facts = <span class="number">0</span></span><br><span class="line">    predictions = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(logits)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, len(logits[i])):  <span class="comment"># 去除NA</span></span><br><span class="line">            predictions.append(&#123;<span class="string">'score'</span>: logits[i][j], <span class="string">'is_correct'</span>: labels[i][j]&#125;)</span><br><span class="line">            num_relation_facts += labels[i][j]</span><br><span class="line">    predictions = sorted(predictions, key=<span class="keyword">lambda</span> x: x[<span class="string">'score'</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    precision_values = []</span><br><span class="line">    recall_values = []</span><br><span class="line">    <span class="keyword">for</span> idx, prediction <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        <span class="keyword">if</span> prediction[<span class="string">'is_correct'</span>]:</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">        precision_values.append(correct / (idx + <span class="number">1</span>))</span><br><span class="line">        recall_values.append(correct / num_relation_facts)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">precision_at</span><span class="params">(n)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (sum([prediction[<span class="string">'is_correct'</span>] <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions[:n]]) / n) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line">    pr_metrics = &#123;</span><br><span class="line">        <span class="string">'P/R AUC'</span>: auc(x=recall_values, y=precision_values),</span><br><span class="line">        <span class="string">'Precision@100'</span>: precision_at(<span class="number">5</span>),</span><br><span class="line">        <span class="string">'Precision@200'</span>: precision_at(<span class="number">10</span>),</span><br><span class="line">        <span class="string">'Mean'</span>: np.mean([precision_at(i) <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">5</span>, <span class="number">10</span>]])</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> precision_values, recall_values, pr_metrics</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    precision_values, recall_values, pr_metrics = compute_pr_curve_and_predictions()</span><br><span class="line">    print(precision_values)</span><br><span class="line">    print(recall_values)</span><br><span class="line">    print(pr_metrics)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">0.75</span>, <span class="number">0.8</span>, <span class="number">0.6666666666666666</span>, <span class="number">0.7142857142857143</span>, <span class="number">0.625</span>, <span class="number">0.5555555555555556</span>]</span><br><span class="line">[<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0.8</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]</span><br><span class="line">&#123;<span class="string">'P/R AUC'</span>: <span class="number">0.6930952380952381</span>, <span class="string">'Precision@100'</span>: <span class="number">80.0</span>, <span class="string">'Precision@200'</span>: <span class="number">50.0</span>, <span class="string">'Mean'</span>: <span class="number">65.0</span>&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="RESIDE"><a href="#RESIDE" class="headerlink" title="RESIDE"></a>RESIDE</h1><p>RESIDE代码调用了<a href="https://scikit-learn.org/stable/index.html" target="_blank" rel="noopener">sklearn</a>工具，代码<strong>在不修改源代码原意的基础上进行了简化</strong>，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc, average_precision_score, precision_recall_curve</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">logits = [[<span class="number">0.3</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.9</span>],</span><br><span class="line">          [<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.9</span>, <span class="number">0.2</span>],</span><br><span class="line">          [<span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.6</span>]]</span><br><span class="line">labels = [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">          [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">          [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_pr_curve_and_predictions</span><span class="params">()</span>:</span></span><br><span class="line">    y_true = np.array([e[<span class="number">1</span>:] <span class="keyword">for</span> e <span class="keyword">in</span> labels]).reshape((<span class="number">-1</span>))</span><br><span class="line">    y_scores = np.array([e[<span class="number">1</span>:] <span class="keyword">for</span> e <span class="keyword">in</span> logits]).reshape((<span class="number">-1</span>))</span><br><span class="line">    precision, recall, threshold = precision_recall_curve(y_true, y_scores)</span><br><span class="line">    area = average_precision_score(y_true, y_scores)</span><br><span class="line">    <span class="comment"># area = auc(recall, precision)</span></span><br><span class="line">    <span class="keyword">return</span> precision, recall, area</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    precision, recall, area = compute_pr_curve_and_predictions()</span><br><span class="line">    print(precision)</span><br><span class="line">    print(recall)</span><br><span class="line">    print(area)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0.71428571</span> <span class="number">0.8</span>        <span class="number">0.75</span>       <span class="number">1.</span>         <span class="number">1.</span>         <span class="number">1.</span>        ]</span><br><span class="line">[<span class="number">1.</span>  <span class="number">0.8</span> <span class="number">0.6</span> <span class="number">0.6</span> <span class="number">0.4</span> <span class="number">0.</span> ]</span><br><span class="line"><span class="number">0.9028571428571429</span></span><br></pre></td></tr></table></figure><p>若将<code>average_precision_score</code>替换为<code>auc</code>，则area变为$0.9064285714285717$</p><p>由于<code>average_precision_score</code>采用矩形计算面积，<code>auc</code>采用梯形计算面积，所以后者计算结果偏高。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p><strong><em>DISTRE的实现方式是存在问题的</em></strong>，应该采用scikit-learn的标准实现。</p>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Relation Extraction </tag>
            
            <tag> PR-Curve </tag>
            
            <tag> Distant Supervision </tag>
            
            <tag> AUC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LaTeX论文排版初尝</title>
      <link href="/2021/01/07/LaTeX%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88%E5%88%9D%E5%B0%9D/"/>
      <url>/2021/01/07/LaTeX%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88%E5%88%9D%E5%B0%9D/</url>
      
        <content type="html"><![CDATA[<h1 id="1-typeout-msg"><a href="#1-typeout-msg" class="headerlink" title="1. \typeout{msg}"></a>1. \typeout{msg}</h1><p>Prints <code>msg</code> on the terminal and in the log file.</p><h1 id="2-usepackage-amsthm"><a href="#2-usepackage-amsthm" class="headerlink" title="2. \usepackage{amsthm}"></a>2. \usepackage{amsthm}</h1><p>The amsthm package provides an enhanced version of LATEX’s \newtheorem command for defining theorem-like environments.  一个用于定理、证明的环境。</p><p>The enhanced \newtheorem recognizes a \theoremstyle specification (as in Mittelbach’s theorem package) and has a * form for defining unnumbered environments. The amsthm package also defines a proof environment that automatically adds a QED symbol at the end. AMS document classes incorporate the amsthm package, so everything described here applies to them as well.</p><h1 id="3-newtheorem-example-Example"><a href="#3-newtheorem-example-Example" class="headerlink" title="3. \newtheorem{example}{Example}"></a>3. \newtheorem{example}{Example}</h1><p>用于指定公式、定理环境。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;article&#125;</span><br><span class="line">\usepackage[utf8]&#123;inputenc&#125;</span><br><span class="line">\usepackage[english]&#123;babel&#125;</span><br><span class="line"></span><br><span class="line">\newtheorem&#123;theorem&#125;&#123;Theorem&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">\section&#123;Introduction&#125;</span><br><span class="line">Theorems can easily be defined</span><br><span class="line"></span><br><span class="line">\begin&#123;theorem&#125;</span><br><span class="line">Let $f$ be a function whose derivative exists in every point, then $f$ </span><br><span class="line">is a continuous function.</span><br><span class="line">\end&#123;theorem&#125;</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2021/theorem.png" alt></p><p>The command <code>\newtheorem{theorem}{Theorem}</code> has two parameters, the first one is the name of the environment that is defined, the second one is the word that will be printed, in boldface font, at the beginning of the environment. Once this new environment is defined it can be used normally within the document, delimited it with the marks <code>\begin{theorem}</code> and <code>\end{theorem}</code>.</p><p>更详细的：<a href="https://www.overleaf.com/learn/latex/theorems_and_proofs" target="_blank" rel="noopener">https://www.overleaf.com/learn/latex/theorems_and_proofs</a></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LaTeX </tag>
            
            <tag> IJCAI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NYT数据集PR-Curve评估指标大调研</title>
      <link href="/2021/01/07/NYT%E6%95%B0%E6%8D%AE%E9%9B%86PR-Curve%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E5%A4%A7%E8%B0%83%E7%A0%94/"/>
      <url>/2021/01/07/NYT%E6%95%B0%E6%8D%AE%E9%9B%86PR-Curve%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E5%A4%A7%E8%B0%83%E7%A0%94/</url>
      
        <content type="html"><![CDATA[<p>对于远程监督的评估指标，有一个很重要的是PR-Curve，但是，对于PR-Curve，也有不太统一的绘制方式，所以，对两个SOTA模型进行调研记录。一个是<a href="https://www.aclweb.org/anthology/D18-1157/" target="_blank" rel="noopener">RESIDE</a>，发表于2018年的EMNLP；另一个是<a href="https://www.aclweb.org/anthology/P19-1134/" target="_blank" rel="noopener">DISTRE</a>，发表于2019年的ACL。</p><h1 id="1-RESIDE"><a href="#1-RESIDE" class="headerlink" title="1. RESIDE"></a>1. RESIDE</h1><h2 id="1-1-数据建模方式"><a href="#1-1-数据建模方式" class="headerlink" title="1.1 数据建模方式"></a>1.1 数据建模方式</h2><p>使用<code>load_data(self)</code>成员函数加载数据，编写脚本对RESIDE的<code>.pkl</code>数据查看包的组织形式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = pickle.load(open(<span class="string">'F:/Dataset/NYT/RESIDE/riedel_processed.pkl'</span>, <span class="string">'rb'</span>))</span><br><span class="line">TRAIN = data[<span class="string">'train'</span>]</span><br><span class="line"><span class="keyword">for</span> bag <span class="keyword">in</span> TRAIN:</span><br><span class="line">    X = bag[<span class="string">'X'</span>]</span><br><span class="line">    Y = bag[<span class="string">'Y'</span>]</span><br><span class="line">    <span class="keyword">if</span> len(X) &gt; <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">'multiple sentences'</span>)</span><br><span class="line">    <span class="keyword">if</span> len(Y) &gt; <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">'multiple label'</span>)</span><br><span class="line">print(<span class="string">'finish'</span>)</span><br></pre></td></tr></table></figure><p>运行上述代码，multiple sentences有很多，<strong>但是没有multiple label的包。说明RESIDE是将任务建模为single-label classification问题。</strong></p><a id="more"></a><h2 id="1-2-训练时"><a href="#1-2-训练时" class="headerlink" title="1.2 训练时"></a>1.2 训练时</h2><p>训练过程主体在<code>fit()</code>函数。调用<code>run_epoch()</code>函数完成一轮训练。调用<code>getBatches()</code>获取mini-batch。</p><p>函数源码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBatches</span><span class="params">(self, data, shuffle = True)</span>:</span></span><br><span class="line"><span class="string">""" </span></span><br><span class="line"><span class="string">以包为单位，若batch_size=16，则每个mini-batch含16个包，句子个数不定。</span></span><br><span class="line"><span class="string">        Generates batches of multiple bags</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        data:Data to be used for creating batches. Dataset as list of bags where each bag is a dictionary</span></span><br><span class="line"><span class="string">        shuffle:Decides whether to shuffle the data or not.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        Generator for creating batches. </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"><span class="keyword">if</span> shuffle: random.shuffle(data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> getChunks(data, self.p.batch_size):<span class="comment"># chunk = batch</span></span><br><span class="line">batch = defaultdict(list)</span><br><span class="line"></span><br><span class="line">num = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i, bag <span class="keyword">in</span> enumerate(chunk):</span><br><span class="line">batch[<span class="string">'X'</span>]       += bag[<span class="string">'X'</span>]</span><br><span class="line">batch[<span class="string">'Pos1'</span>]    += bag[<span class="string">'Pos1'</span>]</span><br><span class="line">batch[<span class="string">'Pos2'</span>]    += bag[<span class="string">'Pos2'</span>]</span><br><span class="line">batch[<span class="string">'DepEdges'</span>]  += bag[<span class="string">'DepEdges'</span>]</span><br><span class="line">batch[<span class="string">'ProbY'</span>]   += bag[<span class="string">'ProbY'</span>]</span><br><span class="line"></span><br><span class="line">batch[<span class="string">'SubType'</span>].append(bag[<span class="string">'SubType'</span>])</span><br><span class="line">batch[<span class="string">'ObjType'</span>].append(bag[<span class="string">'ObjType'</span>])</span><br><span class="line"></span><br><span class="line">batch[<span class="string">'Y'</span>].append(bag[<span class="string">'Y'</span>])</span><br><span class="line">old_num  = num</span><br><span class="line">num += len(bag[<span class="string">'X'</span>])</span><br><span class="line"></span><br><span class="line">batch[<span class="string">'sent_num'</span>].append([old_num, num, i])<span class="comment"># 包的起止位置</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">yield</span> batch</span><br></pre></td></tr></table></figure><p><strong><em>以包为单位，若batch_size=16，则每个mini-batch含16个包，句子个数不定。</em></strong></p><p>获取mini-batch后，通过预先构建的计算图得到<em>loss</em>和<em>accuracy</em>，计算accuracy的代码为：</p><h3 id="accuracy"><a href="#accuracy" class="headerlink" title="accuracy"></a>accuracy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Accuracy'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">prob     = tf.nn.softmax(nn_out)</span><br><span class="line">y_pred   = tf.argmax(prob,    axis=<span class="number">1</span>)</span><br><span class="line">y_actual = tf.argmax(self.input_y, axis=<span class="number">1</span>)  </span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_actual), tf.float32))</span><br></pre></td></tr></table></figure><p>而input_y是通过以下形式获得：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X, Y, pos1, pos2, sent_num, sub_type, obj_type, rel_alias = batch[<span class="string">'X'</span>], batch[<span class="string">'Y'</span>], batch[<span class="string">'Pos1'</span>], batch[<span class="string">'Pos2'</span>], batch[<span class="string">'sent_num'</span>], batch[<span class="string">'SubType'</span>], batch[<span class="string">'ObjType'</span>], batch[<span class="string">'ProbY'</span>]</span><br><span class="line"></span><br><span class="line">y_hot = self.getOneHot(Y, self.num_class)  <span class="comment"># one-hot向量</span></span><br><span class="line"><span class="keyword">if</span> wLabels: feed_dict[self.input_y] = y_hot</span><br></pre></td></tr></table></figure><p>总结一下，计算accuracy是single-label的，<strong>预测值和真实标签均只有一个</strong>。</p><h3 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h3><p>计算<em>loss</em>是通过以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_loss</span><span class="params">(self, nn_out)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Computes loss based on logits and actual labels</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">nn_out:Logits for each bag in the batch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Returns</span></span><br><span class="line"><span class="string">-------</span></span><br><span class="line"><span class="string">loss:Computes loss based on prediction and actual labels of the bags</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Loss_op'</span>):</span><br><span class="line">loss  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=nn_out, labels=self.input_y))  <span class="comment"># 采用交叉熵损失</span></span><br><span class="line"><span class="comment"># 添加正则化损失项</span></span><br><span class="line"><span class="keyword">if</span> self.regularizer != <span class="literal">None</span>: loss += tf.contrib.layers.apply_regularization(self.regularizer, tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))</span><br><span class="line"><span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>总结一下，损失是通过nn_out与input_y (one_hot) 标签计算的，同样是single-label的。</p><h2 id="1-3-测试时"><a href="#1-3-测试时" class="headerlink" title="1.3 测试时"></a>1.3 测试时</h2><p>测试时用到的函数主要是<code>predict()</code>，函数主体如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sess, data, wLabels=True, shuffle=False, label=<span class="string">'Evaluating on Test'</span>)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Evaluate model on valid/test data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">sess:Session of tensorflow</span></span><br><span class="line"><span class="string">data:Data to evaluate on</span></span><br><span class="line"><span class="string">wLabels:Does data include labels or not</span></span><br><span class="line"><span class="string">shuffle:Shuffle data while before creates batches</span></span><br><span class="line"><span class="string">label:Log label to be used while logging</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Returns</span></span><br><span class="line"><span class="string">-------</span></span><br><span class="line"><span class="string">losses:Loss over the entire data</span></span><br><span class="line"><span class="string">accuracies:Overall Accuracy</span></span><br><span class="line"><span class="string">y: Actual label</span></span><br><span class="line"><span class="string">y_pred:Predicted labels</span></span><br><span class="line"><span class="string">logit_list:Logit list for each bag in the data</span></span><br><span class="line"><span class="string">y_actual_hot:One hot represetnation of actual label for each bag in the data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">losses, accuracies, y_pred, y, logit_list, y_actual_hot = [], [], [], [], [], []</span><br><span class="line">bag_cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, batch <span class="keyword">in</span> enumerate(self.getBatches(data, shuffle)):</span><br><span class="line">loss, logits, accuracy = sess.run([self.loss, self.logits, self.accuracy], feed_dict = self.create_feed_dict(batch, split=<span class="string">'test'</span>))</span><br><span class="line">losses.    append(loss)</span><br><span class="line">accuracies.append(accuracy)</span><br><span class="line"></span><br><span class="line">pred_ind      = logits.argmax(axis=<span class="number">1</span>)</span><br><span class="line">logit_list   += logits.tolist()</span><br><span class="line">y_actual_hot += self.getOneHot(batch[<span class="string">'Y'</span>], self.num_class).tolist()</span><br><span class="line">y_pred       += pred_ind.tolist()</span><br><span class="line">y      += np.argmax(self.getOneHot(batch[<span class="string">'Y'</span>], self.num_class), <span class="number">1</span>).tolist()</span><br><span class="line">bag_cnt      += len(batch[<span class="string">'sent_num'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">self.logger.info(<span class="string">'&#123;&#125; (&#123;&#125;/&#123;&#125;):\t&#123;:.5&#125;\t&#123;:.5&#125;\t&#123;&#125;'</span>.format(label, bag_cnt, len(self.data[<span class="string">'test'</span>]), np.mean(accuracies)*<span class="number">100</span>, np.mean(losses), self.p.name))</span><br><span class="line"></span><br><span class="line">self.logger.info(<span class="string">'Test Accuracy: &#123;&#125;'</span>.format(accuracy))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> np.mean(losses), np.mean(accuracies)*<span class="number">100</span>, y, y_pred, logit_list, y_actual_hot</span><br></pre></td></tr></table></figure><p>对测试集统计数据，发现redundant=36156, multi_label_bag=167，<strong><em>这一点与训练集不同，训练集中不含多个标签的包。</em></strong>在将多标签的包的标签转化为向量时，使用<code>getOneHot()</code>函数，会将标签对应位置的值置为1，<font color="orange">所以其实是multi-hot向量啦~</font>更为细节的内容请看下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y_hot = self.getOneHot(Y, self.num_class)</span><br><span class="line">proby = self.getOneHot(rel_alias, self.num_class<span class="number">-1</span>, isprob=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOneHot</span><span class="params">(self, data, num_class, isprob=False)</span>:</span></span><br><span class="line">temp = np.zeros((len(data), num_class), np.int32)</span><br><span class="line"><span class="keyword">for</span> i, ele <span class="keyword">in</span> enumerate(data):</span><br><span class="line"><span class="keyword">for</span> rel <span class="keyword">in</span> ele:  <span class="comment"># multi-label 多分类</span></span><br><span class="line"><span class="keyword">if</span> isprob:temp[i, rel<span class="number">-1</span>] = <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:temp[i, rel]   = <span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> temp</span><br></pre></td></tr></table></figure><p>与训练时一样，运行计算图，得到<em>loss</em>, <em>logits</em>, <em>accuracy</em></p><p>需要特别留意中间几行代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">logits: (bags, class_num)</span><br><span class="line">pred_ind: (bags, )  <span class="comment"># 预测1个标签</span></span><br><span class="line">logits_list: (all, class_num)</span><br><span class="line">y_actual_hot: (bags, class_num)</span><br><span class="line">y_pred: (bags, )</span><br><span class="line">y: (bags, )  <span class="comment"># 使用np.argmax()，如果有多个标签，则取第一个，顺序来自rel2id.json文件</span></span><br></pre></td></tr></table></figure><p>然后，single-label和multi-label用于不同的指标计算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""用single-label的预测值和黄金值计算精度、召回、F1"""</span></span><br><span class="line">test_prec, test_rec, test_f1   = self.calc_prec_recall_f1(y, y_pred, <span class="number">0</span>)  <span class="comment"># 0: ID for 'NA' relation</span></span><br><span class="line"><span class="string">"""用multi-label的logits和黄金值计算PR-Curve"""</span></span><br><span class="line"><span class="comment"># 绘制 PR-curve 的数据处理 #</span></span><br><span class="line">y_true   = np.array([e[<span class="number">1</span>:] <span class="keyword">for</span> e <span class="keyword">in</span> y_hot]).   reshape((<span class="number">-1</span>))</span><br><span class="line">y_scores = np.array([e[<span class="number">1</span>:] <span class="keyword">for</span> e <span class="keyword">in</span> logit_list]).reshape((<span class="number">-1</span>))</span><br><span class="line">area_pr  = average_precision_score(y_true, y_scores)</span><br></pre></td></tr></table></figure><h2 id="1-4-PR-Curve绘制"><a href="#1-4-PR-Curve绘制" class="headerlink" title="1.4 PR-Curve绘制"></a>1.4 PR-Curve绘制</h2><p>如何将上述结果用于最终绘制PR-Curve呢？首先，保存数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pickle.dump(&#123;<span class="string">'logit_list'</span>: logit_list, <span class="string">'y_hot'</span>: y_hot&#125;, open(<span class="string">"results/&#123;&#125;/precision_recall.pkl"</span>.format(self.p.name), <span class="string">'wb'</span>))</span><br></pre></td></tr></table></figure><p>然后，利用pickle加载保存的数据，并绘制最终结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(path)</span>:</span></span><br><span class="line">preds    = pickle.load(open(path, <span class="string">'rb'</span>))</span><br><span class="line">y_hot    = np.array(preds[<span class="string">'y_hot'</span>])</span><br><span class="line">logit_list = np.array(preds[<span class="string">'logit_list'</span>])</span><br><span class="line">y_hot_new       = np.reshape(np.array([x[<span class="number">1</span>:] <span class="keyword">for</span> x <span class="keyword">in</span> y_hot]),      (<span class="number">-1</span>))  <span class="comment"># 去除NA</span></span><br><span class="line">logit_list_new  = np.reshape(np.array([x[<span class="number">1</span>:] <span class="keyword">for</span> x <span class="keyword">in</span> logit_list]), (<span class="number">-1</span>))</span><br><span class="line"><span class="keyword">return</span> y_hot_new, logit_list_new</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotPR</span><span class="params">(dataset)</span>:</span></span><br><span class="line">y_true, y_scores    = loadData(<span class="string">'./results/&#123;&#125;/precision_recall.pkl'</span>.format(args.name))</span><br><span class="line">precision,recall,threshold = precision_recall_curve(y_true,y_scores)</span><br><span class="line"><span class="comment"># 也可以直接调用auc()函数</span></span><br><span class="line">    area_under       = average_precision_score(y_true, y_scores)</span><br><span class="line">print(<span class="string">'Area under the curve: &#123;:.3&#125;'</span>.format(area_under))</span><br><span class="line"></span><br><span class="line">plt.plot(recall[:], precision[:], label=<span class="string">'RESIDE'</span>, color =<span class="string">'red'</span>, lw=<span class="number">1</span>, marker = <span class="string">'o'</span>, markevery = <span class="number">0.1</span>, ms = <span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>最终绘制得到的结果为：</p><p><img src="/images/blog/2021/RESIDE.png" alt></p><h1 id="2-DISTRE"><a href="#2-DISTRE" class="headerlink" title="2. DISTRE"></a>2. DISTRE</h1><p>DISTRE与RESIDE大同小异，不再像上面那样详细的展开。</p><p>训练时，DISTRE是将<code>head###tail###relation</code>相同的record作为包，<strong>bath_size为mini-batch含有的句子数，含有的包的个数不定</strong>（<em>这一点与RESIDE不同</em>），以交叉熵作为损失函数。</p><p>测试时，将头尾实体转化为小写，并<strong>将头尾实体对相同的构成一个包</strong>，因此<font color="orange">推测应该是multi-label的</font>。</p><p>用到了<code>predict_batch_json()</code>函数，查看源码了解其原理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_batch_json</span><span class="params">(self, inputs: List[JsonDict])</span> -&gt; List[JsonDict]:</span></span><br><span class="line">        instances = self._batch_json_to_instances(inputs)</span><br><span class="line">        <span class="keyword">return</span> self.predict_batch_instance(instances)</span><br></pre></td></tr></table></figure><p>首先将JSON转化为Instance，调用函数链为：<code>predict_batch_json</code> $\to$ <code>_batch_json_to_instances</code> $\to$ <code>_json_to_instance</code> $\to$ <code>data_reader.text_to_instance</code>，然后函数将句子字符化、索引化并组织成GPT关系分类的格式。</p><p> 而<code>predict_batch_instances</code>函数内容为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@overrides</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_batch_instance</span><span class="params">(self, instances: List[Instance])</span> -&gt; List[JsonDict]:</span></span><br><span class="line">    model = self._model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        cuda_device = model._get_prediction_device()</span><br><span class="line">        dataset = Batch(instances)</span><br><span class="line">        dataset.index_instances(model.vocab)</span><br><span class="line">        model_input = util.move_to_device(dataset.as_tensor_dict(), cuda_device)</span><br><span class="line">        outputs = model.decode(model(**model_input))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sanitize(outputs)  <span class="comment"># Sanitize turns PyTorch and Numpy types into basic Python types</span></span><br></pre></td></tr></table></figure><p>这里不明白model.decode的作用是什么，所以打印模型输出，观察输出数据的形状和值。输出的<em>logits</em>形状为(1, 56)，输出的<em>clf_h</em>形状为(1, 768)，截取后结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># before decode:</span></span><br><span class="line">&#123;<span class="string">'instances'</span>: [&#123;<span class="string">'instance_id'</span>: <span class="string">'abbott#frank sinatra'</span>&#125;], <span class="string">'logits'</span>: tensor([[<span class="number">9.9998e-01</span>, <span class="number">1.9205e-06</span>, <span class="number">1.3319e-07</span>, <span class="number">9.8069e-07</span>, <span class="number">5.1156e-07</span>, <span class="number">5.5111e-08</span>,</span><br><span class="line">]], device=<span class="string">'cuda:0'</span>), <span class="string">'clf_h'</span>: tensor([[<span class="number">-0.2411</span>,  <span class="number">0.1988</span>, <span class="number">-0.3914</span>, <span class="number">-0.2383</span>,  <span class="number">1.5797</span>,  <span class="number">0.0826</span>, <span class="number">-0.3336</span>, <span class="number">-0.5499</span>]], device=<span class="string">'cuda:0'</span>)&#125;</span><br><span class="line"><span class="comment"># after decode:</span></span><br><span class="line">&#123;<span class="string">'instances'</span>: [&#123;<span class="string">'instance_id'</span>: <span class="string">'abbott#frank sinatra'</span>&#125;], <span class="string">'logits'</span>: tensor([[<span class="number">9.9998e-01</span>, <span class="number">1.9205e-06</span>, <span class="number">1.3319e-07</span>, <span class="number">9.8069e-07</span>, <span class="number">5.1156e-07</span>, <span class="number">5.5111e-08</span></span><br><span class="line">]], device=<span class="string">'cuda:0'</span>), <span class="string">'clf_h'</span>: tensor([[<span class="number">-0.2411</span>,  <span class="number">0.1988</span>, <span class="number">-0.3914</span>, <span class="number">-0.2383</span>,  <span class="number">1.5797</span>,  <span class="number">0.0826</span>, <span class="number">-0.3336</span>, <span class="number">-0.5499</span>]], device=<span class="string">'cuda:0'</span>)&#125;</span><br></pre></td></tr></table></figure><p>观察到decode之前和之后没有任何变化。可能是因为没有继承decode函数自行实现吧。</p><h1 id="3-附录"><a href="#3-附录" class="headerlink" title="3. 附录"></a>3. 附录</h1><h2 id="3-1-RESIDE重复包检测代码"><a href="#3-1-RESIDE重复包检测代码" class="headerlink" title="3.1 RESIDE重复包检测代码"></a>3.1 RESIDE重复包检测代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data = pickle.load(open(<span class="string">'F:/Dataset/NYT/RESIDE/riedel_processed.pkl'</span>, <span class="string">'rb'</span>))</span><br><span class="line">TRAIN = data[<span class="string">'train'</span>]</span><br><span class="line">results = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> bag <span class="keyword">in</span> TRAIN:</span><br><span class="line">    X = str(bag[<span class="string">'X'</span>])  <span class="comment"># hashable</span></span><br><span class="line">    Y = bag[<span class="string">'Y'</span>]</span><br><span class="line">    <span class="keyword">if</span> X <span class="keyword">not</span> <span class="keyword">in</span> results:</span><br><span class="line">        results[X] = [Y]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        results[X].append(Y)</span><br><span class="line">redundant = <span class="number">0</span>  <span class="comment"># 重复包</span></span><br><span class="line"><span class="keyword">for</span> X <span class="keyword">in</span> results:</span><br><span class="line">    <span class="keyword">if</span> len(results[X]) &gt; <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">'^^^^^^^^^^^^^^^^'</span>)</span><br><span class="line">        print(<span class="string">'X '</span>, X)</span><br><span class="line">        print(<span class="string">'Y '</span>, results[X])</span><br><span class="line">        print(<span class="string">'================'</span>)</span><br><span class="line">        redundant += <span class="number">1</span></span><br><span class="line">print(<span class="string">'redundant '</span>, redundant)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">^^^^^^^^^^^^^^^^</span><br><span class="line">X  [[7326, 18, 94, 969, 11769, 14, 367, 18, 3323, 3614, 14, 2547, 18, 94, 614, 8965, 14, 4704, 18, 2295, 43415, 14, 4466, 18, 70582, 14, 1111, 184, 3, 1541, 18, 110, 1225, 45923, 14, 415, 1006, 428, 1, 194, 36728, 14, 131, 4, 2812, 782, 1, 9584, 9966, 14, 415, 428, 1, 1059, 10674, 14, 131, 4, 592, 1, 1649, 3580, 2748, 14, 180, 428, 1, 31765, 46302, 14, 131, 4, 2812, 592, 1, 5798, 3091, 14, 131, 4, 2812, 415, 1, 35144, 6]]</span><br><span class="line">Y  [[0], [0]]</span><br><span class="line">================</span><br><span class="line">^^^^^^^^^^^^^^^^</span><br><span class="line">X  [[58, 82, 2, 3341, 131402, 373, 6483, 85, 3, 580, 1237, 4543, 4, 2, 62, 77, 2995, 5560, 26, 54, 1443, 3, 836, 1120, 104, 5, 23, 115, 89, 4, 1172, 6]]</span><br><span class="line">Y  [[8], [36]]</span><br><span class="line">================</span><br><span class="line">^^^^^^^^^^^^^^^^</span><br><span class="line">X  [[252, 147, 4, 794, 15, 5114, 16, 1, 8999, 91687, 15, 1952, 16, 1, 713, 3, 1940, 148860, 15, 7950, 16, 6]]</span><br><span class="line">Y  [[0], [0]]</span><br><span class="line">================</span><br><span class="line">redundant  28905</span><br></pre></td></tr></table></figure><p>对测试集也进行一个深入调查，发现，<strong>在测试集中存在167个多标签的包</strong>，存在redundant=36156，与训练集略有差异。打印多标签包的标签查看，随机截取部分，结果为：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@@@[48, 12]@@@</span><br><span class="line">@@@[48, 49]@@@</span><br><span class="line">@@@[48, 49, 47]@@@</span><br><span class="line">@@@[48, 13]@@@</span><br><span class="line">@@@[48, 13]@@@</span><br><span class="line">@@@[8, 36]@@@</span><br><span class="line">@@@[8, 36]@@@</span><br><span class="line">@@@[48, 49]@@@</span><br><span class="line">@@@[8, 36]@@@</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DISTRE </tag>
            
            <tag> Relation Extraction </tag>
            
            <tag> NYT </tag>
            
            <tag> PR-Curve </tag>
            
            <tag> RESIDE </tag>
            
            <tag> Distant Supervision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Fine-Tune BERT for Text Classification</title>
      <link href="/2021/01/06/How-to-Fine-Tune-BERT-for-Text-Classification/"/>
      <url>/2021/01/06/How-to-Fine-Tune-BERT-for-Text-Classification/</url>
      
        <content type="html"><![CDATA[<h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>文本分类是NLP中的一个经典任务， 通常在大型的数据集进行一些预训练的模型在文本分类上可以取得很不错的成绩。例如word2vec， CoVe(contextualized<br>word embeddings)和ELMo都取得了不错的成绩。Bert是基于双向transformer使用masked word prediction和NSP(next sentence prediction)的任务进行预训练，然后在下游任务上进行微调。Bert的出世，横扫了各大榜单。但是他的潜能已经被完全开发了吗？本文针对文本分类基于Bert探索了几种可以优化效果的方法。</p><p>这几种方法分别是：</p><ul><li>Fine-tune策略</li><li>深度预训练</li><li>多任务Fine-tune</li></ul><h3 id="策略介绍"><a href="#策略介绍" class="headerlink" title="策略介绍"></a>策略介绍</h3><h4 id="Fine-tune策略"><a href="#Fine-tune策略" class="headerlink" title="Fine-tune策略"></a>Fine-tune策略</h4><p>神经网络的不同层可以捕获不同的语法和语义信息。使用Bert去训练下游任务需要考虑几个问题：</p><ol><li>预训练的长本文，Bert的最长文本序列是512</li><li>层数选择，正如上文所述哦，每一层都会捕获不同的信息，因此我们需要选择最适合的层数</li><li>过拟合问题，因此需要考虑合适的学习率。<br>Bert的底层会学习到更多的通用的信息，文中对Bert的不同层使用了不同的学习率。 每一层的参数迭代可以如下所示：</li></ol><a id="more"></a><p>$\theta <em>t^l = \theta </em>{t-1}^l - \eta ^l \cdot \nabla _{\theta ^l} J(\theta)$</p><ol><li>其中</li></ol><ul><li>θ_t^l表示第l层第t步迭代的参数</li><li>η^l表示第l层的学习率，计算方式如下，ξ表示衰败系数，当ξ&gt;1表示学习率逐层衰减，否则表示逐层扩大。当ξ=1时和传统的Bert相同。</li></ul><p>$\eta ^{k-1} = \xi \cdot \eta ^k$</p><p>即以最后一层学习率为1，往前逐层递乘$\xi$</p><h4 id="深度预训练"><a href="#深度预训练" class="headerlink" title="深度预训练"></a>深度预训练</h4><p>Bert是在通用的语料上进行预训练的，如果要在特定领域应用文本分类，数据分布一定是有一些差距的。这时候可以考虑进行深度预训练。</p><ul><li><strong>Within-task pre-training</strong>：Bert在训练语料上进行预训练</li><li><strong>In-domain pre-training</strong>：在同一领域上的语料进行预训练</li><li><strong>Cross-domain pre-training</strong>：在不同领域上的语料进行预训练</li></ul><h4 id="多任务Fine-tune"><a href="#多任务Fine-tune" class="headerlink" title="多任务Fine-tune"></a>多任务Fine-tune</h4><p>多任务微调就是使用Bert去训练不同的下游任务但是除了最后一层，在其他层共享参数。<br>下面我们来看一下不同策略的实验结果</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>本文使用了IMDB, Yelp评论数据集用于做情感分析，TREC(公开领域的问答数据集)，yahoo问答用于问题分类，AG新闻，DBPedia和Sougou新闻做主题分类。文中使用WordPiece embeddings以##切分句子。对Sougou新闻采用”。”, “?”, “！”来分隔句子。</p><p><img src="/images/blog/2021/fine-tune1.png" alt></p><h4 id="Fine-tune策略-1"><a href="#Fine-tune策略-1" class="headerlink" title="Fine-tune策略"></a>Fine-tune策略</h4><p><strong>1. 长文本处理</strong><br>对于长文本文中做了两种处理方式，截断和切分。</p><ul><li>截断：一般来说文本中最重要的信息是开始和结尾，因此文中对于长文本做了截断处理。<ul><li>head-only：保留前510个字符</li><li>tail-only：保留后510个字符</li><li>head+tail：保留前128个和后382个字符</li></ul></li><li>切分: 将文本分成k段，每段的输入和Bert常规输入相同，第一个字符是[CLS]表示这段的加权信息。文中使用了Max-pooling, Average pooling和self-attention结合这些片段的表示。</li></ul><p>下面是实验的结果，head+tail的表示在两个数据集上的效果都比较好。应该是长文本结合了句首和句尾的信息，获取的信息比较均衡。不过奇怪的是拼接的方式整体居然不如截断，个人猜测可能是将句子切成几段之后增加了模型的不稳定性，而错误叠加起来可能就会被放大。而max-pooling和self-attention也更加强调了文本中比较有用的信息，所以整体效果优于average.</p><p><img src="/images/blog/2021/fine-tune2.png" alt></p><p><strong>可以总结出，一般直接取mean效果都是最差的，而maxpooling, attention, concat则大同小异，不同的任务上表现不同。</strong></p><p><strong>2. 层数选择</strong><br>文中对每层的效果和前四层的结果进行拼接，后四层的结果拼接以及12层的结果拼接进行了实验，发现后四层拼接和第11层的效果相同。</p><p><img src="/images/blog/2021/fine-tune3.png" alt></p><p><strong>可以得出结论，直接取最后一层的输出向量表示即可。</strong></p><p><strong>3. Catastrophic Forgetting</strong><br>Catastrophic forgetting是指在学习新知识时预训练的知识被遗忘了。文中对Bert的Catastrophic Forgetting问题进行了探索。下图是IMDB数据集上不同学习率和error-rate的曲线，可以看到比较小的学习率获得效果比较好。</p><p><img src="/images/blog/2021/fine-tune4.png" alt></p><p><strong>可以得出结论，一般对BERT进行微调时，学习率选择为1e-5级别，而且学习率越小可能效果越好。但是，过小的学习率会导致训练时间变长，同时不知道是否会引起效果的下降？</strong></p><p><strong>4. 层间学习率</strong><br>层间学习率对模型的影响，可以看到初始学习率较高的时候，衰退率应该相对较低。因为深层的模型可以学到的内容较少，需要比较低的学习率进行拟合。这是不是也意味着在某一层有一个比较固定的学习可以使模型达到最优呢？</p><p><img src="/images/blog/2021/fine-tune5.png" alt></p><h4 id="深度预训练-1"><a href="#深度预训练-1" class="headerlink" title="深度预训练"></a>深度预训练</h4><p><strong>1. Within-Task Further Pre-Training</strong><br>使用训练数据进行预训练，下图是预训练的step和测试的错误率的曲线，可以看到预训练100K轮之后错误率有所回升。</p><p><img src="/images/blog/2021/fine-tune6.png" alt></p><font color="orange">所以withIn Task pre-training可能并不是越多越好，需要找到一个合适的值！！！</font><p><strong>2. In-Domain and Cross-Domain Further Pre-Training</strong><br>文中将语料分为情感分析，问题分类和主题分类三个领域，在这些语料上按照领域内和跨领域的预训练。下图是预训练的结果，all是使用了所有领域内的语料进行了预训练，w/o是原始的bert。可以看到不管是领域内还是跨领域经过预训练的效果都比原始的Bert有所提高。<strong><em>不过需要注意，小规模语料的TREC经过领域内训练的效果变差了。</em></strong></p><p><img src="/images/blog/2021/fine-tune7.png" alt></p><p>文中同时也是用Bert的特征输入Bilstm+self-attention中进行评测，效果如下所示，其中：</p><ul><li>BERT-Feat: BERT as features</li><li>BERT-FiT: BERT + Fine-Tuning</li><li>BERT-ITPT-FiT: BERT + withIn-Task Pre-Training + Fine-Tuning</li><li>BERT-IDPT-FiT: BERT + In-Domain Pre-Training + Fine-Tuning</li><li>BERT-CDPT-FiT: BERT + Cross-Domain Pre-Training + Fine-Tuning</li></ul><p><img src="/images/blog/2021/fine-tune8.png" alt></p><p><strong><em>可以看到，添加上withIn Task Pre-Training普遍有效果上的提升！</em></strong></p><h4 id="多任务Fine-tune-1"><a href="#多任务Fine-tune-1" class="headerlink" title="多任务Fine-tune"></a>多任务Fine-tune</h4><p>论文中使用了4个英文分类的数据集(IMDB, Yelp.P, AG， DBP)进行多任务训练，同时使用了跨领域预训练的Bert模型进行对比，效果如下。可以看到多任务学习可以提高Bert的效果，与此同时在跨领域预训练Bert模型上进行多任务Fine tune的效果是最好的。</p><p><img src="/images/blog/2021/fine-tune9.png" alt></p><p><em>这个实验主要说明cross-domain BERT比较厉害，得不出其他结论。</em></p><h4 id="训练集的规模"><a href="#训练集的规模" class="headerlink" title="训练集的规模"></a>训练集的规模</h4><p>文中对Fine-tune时的训练集的规模对模型的效果的影响进行探索。可以看到当训练数据集比较少的时候，模型的错误率是比较高的，随着训练集的增大，模型的错误率下降。但是为什么横坐标从20到了100这里有一点迷惑。<br>其中Bert-Fit表示Bert Fine-tune, BERT-ITPT-Fit表示BERT + withIn-Task Pre-Training + Fine-Tuning.</p><p><img src="/images/blog/2021/fine-tune10.png" alt></p><h4 id="Bert-Large预训练"><a href="#Bert-Large预训练" class="headerlink" title="Bert Large预训练"></a>Bert Large预训练</h4><p>文中对Bert Large也进行了With task预训练，大力出奇迹，果然Bert large的效果更好</p><p><img src="/images/blog/2021/fine-tune11.png" alt></p><font color="orange">**使用bert-large往往可以取得更好的效果，但是，取得的效果提升很多时候相比于训练时间和参数数量的提升，是微不足道的。因此，需要平衡好实验结果和实验效率。**</font><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>感觉这是一篇非常扎实，考虑比较全面的实验报告，但是对于实验结果的思考和解释不多。总之使用Bert微调的时候可以考虑在领域内重新预训练，让模型多学习点知识，以及深度学习还是一如既往地大力出奇迹。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol><li><p><a href="https://arxiv.org/pdf/1905.05583.pdf" target="_blank" rel="noopener">How to Fine-Tune BERT for Text Classification</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/148720604" target="_blank" rel="noopener">如何让BERT在fine-tune小数据集时更“稳”一点</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Pretrained Language Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> fine-tune </tag>
            
            <tag> MLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPT1和GPT2</title>
      <link href="/2021/01/06/GPT1%E5%92%8CGPT2/"/>
      <url>/2021/01/06/GPT1%E5%92%8CGPT2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本片文章转载于：总结GPT1和GPT2</p><p><a href="https://blog.csdn.net/beilizhang/article/details/109282032" target="_blank" rel="noopener">https://blog.csdn.net/beilizhang/article/details/109282032</a></p></blockquote><p>参考链接：</p><ol><li><a href="https://blog.csdn.net/hyzhyzhyz12345/article/details/104181606" target="_blank" rel="noopener">https://blog.csdn.net/hyzhyzhyz12345/article/details/104181606</a></li><li><a href="https://www.jianshu.com/p/a68288613a0f" target="_blank" rel="noopener">https://www.jianshu.com/p/a68288613a0f</a></li><li><a href="https://blog.csdn.net/u012526436/article/details/87882985" target="_blank" rel="noopener">https://blog.csdn.net/u012526436/article/details/87882985</a></li><li><a href="https://www.jianshu.com/p/6c2bfa1848f2" target="_blank" rel="noopener">https://www.jianshu.com/p/6c2bfa1848f2</a></li><li><a href="https://zhuanlan.zhihu.com/p/57251615" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/57251615</a></li><li><a href="https://www.jiqizhixin.com/articles/2019-08-26-12" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-08-26-12</a></li></ol><h2 id="GPT1"><a href="#GPT1" class="headerlink" title="GPT1"></a>GPT1</h2><p>全称：Generative Pre-Training 1.0<br>论文：<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">《Improving Language Understanding by Generative Pre-Training》</a></p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>GPT1采用了NLP中常用的“预训练+微调（generative pre-training + discriminative fine-tuning）”的模式。</p><p>由于LSTM等结构在捕获长期依赖的局限性，<strong>GPT1的模型是Transformer的Decoder部分</strong>，其采用的self-attention机制较好地弥补了LSTM的弊端。</p><a id="more"></a><h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><h4 id="1-无监督的预训练阶段（Unsupervised-pre-training）"><a href="#1-无监督的预训练阶段（Unsupervised-pre-training）" class="headerlink" title="1.无监督的预训练阶段（Unsupervised pre-training）"></a>1.无监督的预训练阶段（Unsupervised pre-training）</h4><p>给定一个无监督的（无标签）tokens语料库$U = { u 1 , ⋅ ⋅ ⋅ , u n }$ ，选择标准的语言模型目标函数（即根据前k个词预测下一个词），需最大化：</p><p>$ L<em>1(U) = \sum_i\log P (u_i|u</em>{i-k}, …, u_{i-1};\Theta)$</p><p>其中，k为token的上下文窗口大小，条件概率P使用参数为Θ的神经网络建模。GPT1的结构示意图如下：</p><p><img src="/images/blog/2021/gpt.png" alt></p><p>1) 输入为前k个词和位置的embedding</p><p>$h_0 = UW_e + W_p$</p><p>其中，$U=(u<em>{-k},…,u</em>{-1})$为token上下文的one-hot向量，则$UW_e$为上下文对应的embedding。</p><p>2) 经过n层transformer-decoder层 (12层)</p><p>$h<em>l = transformerBlock(h</em>{l-1}) \forall l \in [1, n]$</p><p>3) 乘上一个token embedding矩阵，通过softmax得到概率</p><p>$P(u) = softmax(h_nW_e^T)$</p><h4 id="2-有监督的微调阶段（Supervised-fine-tuning）"><a href="#2-有监督的微调阶段（Supervised-fine-tuning）" class="headerlink" title="2.有监督的微调阶段（Supervised fine-tuning）"></a>2.有监督的微调阶段（Supervised fine-tuning）</h4><p><img src="/images/blog/2021/gpt_finetune.png" alt></p><p>对比预训练阶段，可以看出，只是多增加了“Task Classifier”模块。</p><p>对于有监督的训练，数据集C每个样本自然包括一个输入tokens序列$x^1,…,x^m$和对应的标签y，具体任务相应的目标函数为：</p><p>$L<em>2(C) = \sum</em>{(x,y)}\log P(y|x^1,…,x^m)$</p><p>其中，$ P(y|x^1,…,x^m)$ = $softmax (h_l^mW_y)$，$h_l^m$为最后一个token$x^m$对应的最后一层transformer decoder的输出。所以需要额外调整的参数只有$W_y$</p><p>最终，微调阶段的目标函数为：</p><p>$L_3(C) = L_2(C) + \lambda * L_1(C)$</p><p>上图还展示了一个细节，就是针对不同的任务，模型的输入token序列是有区别的：</p><ol><li>对于文本分类任务，输入格式与预训练时一样，[start;text;extract]；</li><li><strong>对于文本蕴含任务，在前提（premise）和假设（hypothesis）间加上了一个分隔符（delimiter）</strong>，[start;premise;delimiter;hypothesis;extract]；<br><em>注：文本蕴含任务(text entailment)，它的任务形式是：给定一个前提文本（premise），根据这个前提去推断假设文本（hypothesis）与前提文本的关系，一般分为蕴含关系（entailment）和矛盾关系（contradiction），蕴含关系（entailment）表示从前提文本中可以推断出假设文本；矛盾关系（contradiction）即假设文本与前提文本矛盾。文本蕴含的结果就是这几个概率值。</em></li><li>对于文本相似性度量任务，由于两个文本间没有相对顺序，所以把两种情况（[start;text1;delimiter;text2;extract]和[start;text2;delimiter;text1;extract]）分别处理后得到两个h l m h_{l}^{m}<em>h<strong>l</strong>m</em>后，再按位相加经过全连接层；</li><li>对于问答和常识推理任务，将上下文文档（context document）和问题（question）与不同的答案（answer）分别拼接起来（[start;context;question;delimiter;answer1];[start;context;question;delimiter;answer2]…），经过模型后，再经过softmax层。</li></ol><h2 id="GPT2"><a href="#GPT2" class="headerlink" title="GPT2"></a>GPT2</h2><p>全称：Generative Pre-Training 2.0<br>论文：<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="noopener">《Language Models are Unsupervised Multitask Learners》</a></p><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>GPT2有4个不同的版本，分别如下图所示，区别在于embedding的维度和transformer-decoder的层数。所有的模型目前在WebText上都还存在欠拟合的情况，如果给更多的时间去训练的话效果还能进一步的提升。</p><p><img src="/images/blog/2021/gpt2.png" alt></p><h2 id="GPT2与GPT1"><a href="#GPT2与GPT1" class="headerlink" title="GPT2与GPT1"></a>GPT2与GPT1</h2><p>最主要的区别还是GPT2完全变成无监督训练，直接用于下游任务，属于零样本学习（Zero-shot learning）。下面具体介绍一些区别。</p><h3 id="输入表征（Input-Representation）"><a href="#输入表征（Input-Representation）" class="headerlink" title="输入表征（Input Representation）"></a>输入表征（Input Representation）</h3><p>为了解决word级别的embedding常存在的OOV问题（out-of-vocabulary，即词典以外的词汇），而字符级的embedding效果又不太好。</p><p>本文采用了字节对编码（BPE，Byte Pair Encoding），即将频率高的字节对一起编码。<br>（以下部分内容为本人的推测）<br>GPT2可能使用的是UTF-8的编码方式，如下图所示，UTF-8会将字符以8位为一个编码单元进行编码，所以只需要有0~255对应的embedding就可以得到每个字符embedding（与原文“a byte-level version of BPE only requires a base vocabulary of size 256.”对应），如将266、128、188对应的embedding加起来就得到了“!!”的embedding。</p><p><img src="/images/blog/2021/255.png" alt></p><p>但这种效果肯定不好，所以对高频的字符串应该进行额外学习embedding，如“dog”有很高的频率一起出现，所以单独给它一个embedding。但“dog?”、“dog.”、“dog!”等也有很高的频率一起出现，如果也单独给它们一个embedding有些浪费资源，所以设置一个规则：对于包含多类字符的字符串不额外学习embedding。</p><p><strong><em>如此，GPT2不需要任何预处理即可用于任意数据集。</em></strong></p><h3 id="模型（Model）"><a href="#模型（Model）" class="headerlink" title="模型（Model）"></a>模型（Model）</h3><ol><li>Layer normalization层被移动到了每个子块的输入；</li><li>在每个self-attention block后加normaliztion；</li><li>修改residual layers的权重（$1/\sqrt{N}$，其中N为残差层的数量），即残差层的参数初始化根据网络深度进行调节；</li><li>词汇量增加到50257；</li><li>上下文大小从512增加到1024;</li><li>batchsize增加到512</li></ol><p>GPT2的模型大概八成可能也就像下图右一：</p><p><img src="/images/blog/2021/gpt2_model.png" alt></p><h3 id="参数个数统计"><a href="#参数个数统计" class="headerlink" title="参数个数统计"></a>参数个数统计</h3><p>GPT2-small的参数统计表如下：</p><p><img src="/images/blog/2021/gpt2_size.png" alt></p><p>其中，attn/c_attn和attn/c_proj分别为Self-Attention模块中的attention矩阵和映射矩阵；attn/c_proj和attn/fc分别为Feed-Forward模块中的全连接层和映射层；ln_1和ln_2为两个Layer Normalization层。</p><h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>这里直接借用了李宏毅老师的ppt：</p><p><img src="/images/blog/2021/gpt2_li.png" alt></p><p>可以看出，在使用模型时，只需要给GPT少量的文本和提示词，GPT就可以自动完成阅读理解、文本摘要和翻译等任务。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Pretrained Language Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPT </tag>
            
            <tag> OpenAI </tag>
            
            <tag> GPT2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fine-tuning GPT2</title>
      <link href="/2021/01/05/fine-tuning-GPT2/"/>
      <url>/2021/01/05/fine-tuning-GPT2/</url>
      
        <content type="html"><![CDATA[<h1 id="GPT2"><a href="#GPT2" class="headerlink" title="GPT2"></a>GPT2</h1><h2 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h2><p><em>GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.</em></p><h1 id="fine-tuning"><a href="#fine-tuning" class="headerlink" title="fine-tuning"></a>fine-tuning</h1><p>参考<a href="https://github.com/huggingface/transformers/tree/master/examples/language-modeling" target="_blank" rel="noopener">transformers官方代码</a></p><p>使用如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 python run_clm.py --model_name_or_path ../gpt2 --train_file nyt_corpus.txt --do_train --output_dir output --num_train_epochs 5 --save_steps 10000 --save_total_limit 2 --seed 666 --block_size 512</span><br></pre></td></tr></table></figure><a id="more"></a><p>结果为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'loss'</span>: 2.385447265625, <span class="string">'learning_rate'</span>: 6.140350877192982e-06, <span class="string">'epoch'</span>: 4.385964912280702&#125;                                                                                                 </span><br><span class="line">&#123;<span class="string">'loss'</span>: 2.379493408203125, <span class="string">'learning_rate'</span>: 4.954954954954955e-06, <span class="string">'epoch'</span>: 4.504504504504505&#125;                                                                                              </span><br><span class="line">&#123;<span class="string">'loss'</span>: 2.36677001953125, <span class="string">'learning_rate'</span>: 3.7695590327169273e-06, <span class="string">'epoch'</span>: 4.623044096728307&#125;                                                                                              </span><br><span class="line">&#123;<span class="string">'loss'</span>: 2.3599794921875, <span class="string">'learning_rate'</span>: 2.5841631104789e-06, <span class="string">'epoch'</span>: 4.74158368895211&#125;                                                                                                   </span><br><span class="line"> 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 20000/21090 [2:38:31&lt;08:34,  2.12it/s][INFO|trainer.py:1256] 2021-01-05 20:18:25,155 &gt;&gt; Saving model checkpoint to output/checkpoint-20000</span><br><span class="line">[INFO|configuration_utils.py:289] 2021-01-05 20:18:25,156 &gt;&gt; Configuration saved <span class="keyword">in</span> output/checkpoint-20000/config.json</span><br><span class="line">[INFO|modeling_utils.py:817] 2021-01-05 20:18:26,300 &gt;&gt; Model weights saved <span class="keyword">in</span> output/checkpoint-20000/pytorch_model.bin</span><br><span class="line">/root/venv/gpt2-fine/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and <span class="built_in">return</span> a vector.</span><br><span class="line">  warnings.warn(<span class="string">'Was asked to gather along dimension 0, but all '</span></span><br><span class="line">&#123;<span class="string">'loss'</span>: 2.38309765625, <span class="string">'learning_rate'</span>: 1.3987671882408725e-06, <span class="string">'epoch'</span>: 4.8601232811759125&#125;                                                                                                </span><br><span class="line">&#123;<span class="string">'loss'</span>: 2.37827490234375, <span class="string">'learning_rate'</span>: 2.1337126600284497e-07, <span class="string">'epoch'</span>: 4.978662873399715&#125;                                                                                              </span><br><span class="line">100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21090/21090 [2:47:13&lt;00:00,  2.41it/s][INFO|trainer.py:886] 2021-01-05 20:27:07,410 &gt;&gt; </span><br><span class="line"></span><br><span class="line">Training completed. Do not forget to share your model on huggingface.co/models =)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;<span class="string">'train_runtime'</span>: 10033.7466, <span class="string">'train_samples_per_second'</span>: 2.102, <span class="string">'epoch'</span>: 5.0&#125;                                                                                                               </span><br><span class="line">100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21090/21090 [2:47:13&lt;00:00,  2.10it/s]</span><br><span class="line">[INFO|trainer.py:1256] 2021-01-05 20:27:07,411 &gt;&gt; Saving model checkpoint to output</span><br><span class="line">[INFO|configuration_utils.py:289] 2021-01-05 20:27:07,412 &gt;&gt; Configuration saved <span class="keyword">in</span> output/config.json</span><br><span class="line">[INFO|modeling_utils.py:817] 2021-01-05 20:27:08,907 &gt;&gt; Model weights saved <span class="keyword">in</span> output/pytorch_model.bin</span><br><span class="line">01/05/2021 20:27:08 - INFO - __main__ -   ***** Train results *****</span><br><span class="line">01/05/2021 20:27:08 - INFO - __main__ -     epoch = 5.0</span><br><span class="line">01/05/2021 20:27:08 - INFO - __main__ -     train_runtime = 10033.7466</span><br><span class="line">01/05/2021 20:27:08 - INFO - __main__ -     train_samples_per_second = 2.102</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Pretrained Language Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPT </tag>
            
            <tag> OpenAI </tag>
            
            <tag> GPT2 </tag>
            
            <tag> fine-tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BERT需要那么多multi-head吗？</title>
      <link href="/2021/01/05/BERT%E9%9C%80%E8%A6%81%E9%82%A3%E4%B9%88%E5%A4%9Amulti-head%E5%90%97%EF%BC%9F/"/>
      <url>/2021/01/05/BERT%E9%9C%80%E8%A6%81%E9%82%A3%E4%B9%88%E5%A4%9Amulti-head%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1905.10650.pdf" target="_blank" rel="noopener">Are Sixteen Heads Really Better than One?</a></p><p>本文的核心思想是探索transformer结构中的multi-head机制是否真的需要这么多个head.</p><p>更多具体的内容可以参考：</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/75036286" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/75036286</a></p></blockquote><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>总结：本文实验还是做得非常充分的，总的来说有如下几个结论</p><ul><li>大部分head都是可以单独去掉的</li><li>几乎所有layer都可以只保留一个head（其他layer保持正常）</li><li>一些比较重要的head比较具有普适性，在不同场景中都同样重要</li><li>文中提出的head敏感度指标在启发式不同层间的剪枝时候比较有效</li><li>head层面的剪枝对transformer提速比较有限</li><li>生成问题中encoder-decoder的multi-head相对更重要</li><li>剪枝后需要训练的时长会更长，并且不宜剪枝过多，否则要么无法收敛，要么收敛到一个非常差的值</li></ul><a id="more"></a><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>在关系抽取中，探索对head随机mask是否会带来效果的提升？</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Pretrained Language Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> multi-head </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>allennlp jsonnet error</title>
      <link href="/2021/01/05/allennlp-jsonnet-error/"/>
      <url>/2021/01/05/allennlp-jsonnet-error/</url>
      
        <content type="html"><![CDATA[<p>在安装allennlp时，出现编译错误，仔细阅读报错，发现：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make: g++ <span class="built_in">command</span> not found</span><br></pre></td></tr></table></figure><p>原因是出在g++这里，从网上查阅了参考资料，<strong>适合我的解决方案是</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apt-get install aptitude</span><br><span class="line">apt-get update</span><br><span class="line">apt-get build-essential</span><br></pre></td></tr></table></figure><p>原因是allennlp要求的g++版本与现有版本不匹配，想办法升级即可。</p><p>然后安装<code>build-essential</code>，以及其他必要的编译环境，参考<a href="https://github.com/scrapy/scrapy/issues/2115" target="_blank" rel="noopener">issue</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python3 python-dev python3-dev \</span><br><span class="line">     build-essential libssl-dev libffi-dev \</span><br><span class="line">     libxml2-dev libxslt1-dev zlib1g-dev \</span><br><span class="line">     python-pip</span><br></pre></td></tr></table></figure><p>┭┮﹏┭┮，太不容易了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Successfully built jsonnet</span><br><span class="line">Installing collected packages: jsonnet, h5py, boto3, allennlp</span><br><span class="line">Successfully installed allennlp-1.3.0 boto3-1.16.48 h5py-3.1.0 jsonnet-0.17.0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> allennlp </tag>
            
            <tag> jsonnet </tag>
            
            <tag> g++ </tag>
            
            <tag> aptitude </tag>
            
            <tag> apt-get </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用BibTeX生成参考文献列表</title>
      <link href="/2021/01/05/%E4%BD%BF%E7%94%A8BibTeX%E7%94%9F%E6%88%90%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%E5%88%97%E8%A1%A8/"/>
      <url>/2021/01/05/%E4%BD%BF%E7%94%A8BibTeX%E7%94%9F%E6%88%90%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%E5%88%97%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇文章转载自：</p><p><a href="https://liam.page/2016/01/23/using-bibtex-to-generate-reference/" target="_blank" rel="noopener">https://liam.page/2016/01/23/using-bibtex-to-generate-reference/</a></p></blockquote><p>LaTeX 是一些理工专业论文排版的事实标准。既然是论文排版，就不可避免会涉及到参考文献的处理。Oren Patashnik 和 Leslie Lamport 在 1985 年开发的 BibTeX 是在 LaTeX 社区相当流行的参考文献格式化工具。</p><p>其实网络上流传的 BibTeX 教程很多，本不用我再来插一句嘴。不过这么多年来，始终有很多朋友会对几个问题反复提问。这让我感到，现有的教程恐怕是不够的。这篇文章尝试将 BibTeX 的基本用法讲解清楚，同时适当地提及一些处理流程，争取在有限的篇幅里，讲清楚 BibTeX 的来龙去脉。</p><h1 id="bst-和-bib-格式简介"><a href="#bst-和-bib-格式简介" class="headerlink" title="bst 和 bib 格式简介"></a>bst 和 bib 格式简介</h1><p>BibTeX 涉及到两种特有的辅助的文件格式：<code>bst</code> 和 <code>bib</code>。</p><p><code>bst</code> 是 (B)ibliography (ST)yle 的缩写。顾名思义，和 <code>sty</code> 文件是 style 的缩写一样，<code>bst</code> 文件控制着参考文献列表的格式。在这里说的「格式」，主要指参考文献列表中的编号、排序规则、对人名的处理（是否缩写）、月份的处理（是否缩写）、期刊名称的缩写等。</p><p><code>bib</code> 是 BibTeX 定义的「参考文献数据库」。通常，我们会按照 BibTeX 规定的格式，向 <code>bib</code> 文件写入多条文献信息。在实际使用时，我们就可以根据 <code>bib</code> 文件中定义的文献标记（label），从数据库中调取文献信息，继而排版成参考文献列表。</p><p>值得注意的是，<code>bib</code> 是一个数据库，其中的内容并不一定等于 LaTeX 排版参考文献列表时的内容。也就是说，如果 <code>bib</code> 数据库中有 10 条文献信息，并不一定说 LaTeX 排版出来的 PDF 文件中，参考文献列表里也一定有 10 条。实际排版出来的参考文献列表中有多少条文献，实际是哪几条，具体由文中使用的 <code>\cite</code> 命令（以及 <code>\nocite</code> 命令）指定。如果没有使用 <code>\cite</code> 命令调取文献信息，那么即使在 <code>bib</code> 文件中定义了文献信息，也不会展现在参考文献列表中。很多人对此误解甚深，于是经常有人问道「为什么我在 <code>bib</code> 文件里写的文献，不出现在参考文献中」之类的问题。</p><h1 id="BibTeX-工作流程"><a href="#BibTeX-工作流程" class="headerlink" title="BibTeX 工作流程"></a>BibTeX 工作流程</h1><p>介绍中提到，BibTeX 是一个参考文献格式化工具。这个定义，给 BibTeX 的用处做了良好的界定：BibTeX 不是用来排版参考文献的，更不是个排版工具，<strong>它只是根据需要，按照（bst 文件规定的）某种格式，将（bib 文件中包含的）参考文献信息，格式化 为 LaTeX 能够使用的列表信息</strong>。</p><p>清楚了 BibTeX 需要做的事情（用软件工程的话说，就是清楚了 BibTeX 的 API），我们就可以理清 BibTeX 的工作流程。</p><h3 id="知道需要哪些参考文献信息"><a href="#知道需要哪些参考文献信息" class="headerlink" title="知道需要哪些参考文献信息"></a>知道需要哪些参考文献信息</h3><p>既然 BibTeX 会<em>根据需要</em> 格式化数据，那么首先要解决的问题就是：BibTeX 如何了解此处的「需求」。</p><p>对 BibTeX 稍有了解的读者可能知道，运行 BibTeX 的命令行命令是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bibtex foo.aux # 其中后缀名 .aux 可以省略</span><br></pre></td></tr></table></figure><p>实际上，BibTeX 正是通过读取 <code>aux</code> 文件中的 <code>\citation{}</code> 标记，来确定用户需要哪些参考文献的。</p><p>举个例子，假设用户用 LaTeX 编译了以下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foo.tex\documentclass&#123;article&#125;\begin&#123;document&#125;bar\cite&#123;baz&#125;\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p>如果该文件名为 <code>foo.tex</code>，那么就会生成 <code>foo.aux</code>。其内容大约是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foo.aux\relax\citation&#123;baz&#125;</span><br></pre></td></tr></table></figure><p>在这里，<code>\relax</code> 表示休息一会儿，什么也不做；<code>\citation</code> 则是由 <code>tex</code> 文件中的 <code>\cite</code> 命令写入 <code>aux</code> 文件的标记。它说明了：用户需要标记为 <code>baz</code> 的参考文献信息。</p><p>当 BibTeX 读入 <code>aux</code> 文件的时候，它就会记录下所有 <code>\citation</code> 命令中的内容（即文献标记——label），这样就知道了用户需要哪些参考文献信息。</p><h3 id="了解文献列表格式以及读取文献数据库"><a href="#了解文献列表格式以及读取文献数据库" class="headerlink" title="了解文献列表格式以及读取文献数据库"></a>了解文献列表格式以及读取文献数据库</h3><p>当 BibTeX 清楚了用户需要哪些文献信息，接下来自然应该搞清楚用户想要什么样的格式。而知道了格式之后，就可以从数据库中抽取所需的文献信息，按照格式准备数据。</p><p>为了讲清楚这个步骤，我们对上述 LaTeX 代码做些许的修改。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">foo.tex</span><br><span class="line">\documentclass&#123;article&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">\bibliographystyle&#123;unsrt&#125;</span><br><span class="line">bar\cite&#123;baz&#125;</span><br><span class="line">\bibliography&#123;foobar&#125;</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p>同样，我们将它保存为 <code>foo.tex</code>，经由 LaTeX 编译之后得到一个 <code>foo.aux</code> 文件，其内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">foo.aux</span><br><span class="line">\relax</span><br><span class="line">\bibstyle&#123;unsrt&#125;</span><br><span class="line">\citation&#123;baz&#125;</span><br><span class="line">\bibdata&#123;foobar&#125;</span><br></pre></td></tr></table></figure><p>简单的对比，不难发现：</p><ol><li><code>foo.tex</code> 中新增的 <code>\bibliographystyle{unsrt}</code> 与 <code>aux</code> 文件中的 <code>\bibstyle{unsrt}</code> 相对应。</li><li><code>foo.tex</code> 中新增的 <code>\bibliography{foobar}</code> 与 <code>aux</code> 文件中的 <code>\bibdata{foobar}</code> 相对应。</li></ol><p>根据命令的名字，我们很容易猜测各个命令的作用。<code>tex</code> 文件中的 <code>\bibliographystyle</code> 指定了用户期待的参考文献列表格式文件，并将其写入 <code>aux</code> 文件备用，通过 <code>\bibstyle</code> 标记。与此同时，<code>\bibliography</code> 命令则用 <code>\bibdata</code> 在 <code>aux</code> 文件中记录了参考文献数据库的名字（不含扩展名）。</p><p>在这里，<code>unsrt</code> 是 unsort 的缩写，它对应着 <code>unsrt.bst</code> 文件，是大多数 TeX 发行版自带的标准格式文件之一；<code>foobar</code> 则对应着 <code>foobar.bib</code> 文件，该文件是用户自己编写或生成的参考文献数据库。</p><h3 id="实际操作看看"><a href="#实际操作看看" class="headerlink" title="实际操作看看"></a>实际操作看看</h3><p>我们假设上述 <code>foobar.bib</code> 文件有如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@BOOK&#123;baz,    </span><br><span class="line">title = &#123;Dummy Book&#125;,    </span><br><span class="line">publisher = &#123;Egypt&#125;,    </span><br><span class="line">year = &#123;321&#125;,    </span><br><span class="line">author = &#123;The King&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们在命令行执行以下操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">latex foo.tex   # .tex 可以省略</span><br><span class="line">bibtex foo.aux  # .aux 可以省略</span><br></pre></td></tr></table></figure><p>我们会发现，BibTeX 生成了两个文件：<code>foo.bbl</code> 和 <code>foo.blg</code>。其中 <code>foo.bbl</code> 的内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;thebibliography&#125;&#123;1&#125;</span><br><span class="line">\bibitem&#123;baz&#125;The King.</span><br><span class="line">\newblock &#123;\em Dummy Book&#125;.</span><br><span class="line">\newblock Egypt, 321.</span><br><span class="line">\end&#123;thebibliography&#125;</span><br></pre></td></tr></table></figure><p>显然，这就是一个标准的 LaTeX 环境。对 LaTeX 参考文献排版稍有了解的读者可能知道 <code>thebibliography</code> 环境正是 LaTeX 中手工编排参考文献时使用的环境。因此，<strong>foo.bbl 就是 BibTeX 格式化输出的结果</strong>，LaTeX 只需要将该文件的内容读入，就能在相应的位置输出格式化之后的参考文献列表了。</p><p>接下来，我们看看 <code>foo.blg</code> 的内容。<code>blg</code> 实际是 BibTeX Log 的缩写，亦即这是一个日志文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">foo.blgThis is BibTeX, Version 0.99d (TeX Live 2015)</span><br><span class="line">Capacity: max_strings=35307, hash_size=35307, hash_prime=30011</span><br><span class="line">The top-level auxiliary file: foo.aux</span><br><span class="line">The style file: unsrt.bst</span><br><span class="line">Database file #1: foobar.bib</span><br><span class="line">You&apos;ve used 1 entry,</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>我们看到，BibTeX 打出的日志文件中，记录了读入 <code>aux</code>/<code>bst</code>/<code>bib</code> 文件的情况。特别地，记录了所需的参考文献条目（entry）的数量（此处为 1）。</p><p>日志中值得注意的地方是在提到 <code>bib</code> 文件时，使用了 <code>#1</code> 的标记。既然存在 <code>#1</code>，那么合理推测也可以存在 <code>#2</code>。也就是说，BibTeX 可能支持两个或更多的 <code>bib</code> 数据库共同工作。具体如何实现，请读者自己阅读相关资料（手册或 Google 检索）后实验。</p><p>紧接着，我们再执行一次 LaTeX：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latex foo.tex</span><br></pre></td></tr></table></figure><p>首先，来看看 <code>aux</code> 文件会发生什么变化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\relax</span><br><span class="line">\bibstyle&#123;unsrt&#125;</span><br><span class="line">\citation&#123;baz&#125;</span><br><span class="line">\bibdata&#123;foobar&#125;</span><br><span class="line">\bibcite&#123;baz&#125;&#123;1&#125;</span><br></pre></td></tr></table></figure><p>相比上一次的 <code>foo.aux</code>，在读入 BibTeX 之后，LaTeX 向 <code>aux</code> 文件写入了更多的信息。这里 <code>\bibcite{baz}{1}</code> 将 <code>baz</code> 这一参考文献标记（label）与参考文献编号（数字 1）绑定起来了。</p><p>接下来，我们看看 <code>dvi</code> 文件的内容：</p><p><img src="/images/blog/2021/latex.png" alt></p><p>不难发现，由于读入了 <code>foo.bbl</code> 文件，参考文献列表已经正确展现出来了。然而，正文中依然有一个问号。</p><p>实际上，LaTeX 需要 <code>aux</code> 文件中的 <code>\bibcite</code> 命令，将参考文献标记与参考文献编号关联起来，从而在 <code>tex</code> 文件中的 <code>\cite</code> 命令位置填上正确的参考文献编号。我们注意到，在我们第二次执行 LaTeX 命令编译之前，<code>foo.aux</code> 文件中是没有这些信息的，直到编译完成，这些信息才被正确写入。因此，第二次执行 LaTeX 命令时，LaTeX 还不能填入正确的文献编号，于是就写入了一个问号作为占位符。</p><p>解决这个问题的办法也很简单——此时 <code>aux</code> 文件中已经有了需要的信息，再编译一遍就好了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latex foo.tex</span><br></pre></td></tr></table></figure><p>如果没有意外，此时的 <code>foo.dvi</code> 文件应该看起来一切正常了。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul><li>BibTeX 是一个参考文献格式化工具，它会根据需要，按照（<code>bst</code> 文件规定的）某种格式，将（<code>bib</code> 文件中包含的）参考文献信息，<em>格式化</em> 为 LaTeX 能够使用的列表信息。</li><li>正确使用 BibTeX 处理参考文献，需要先用 (Xe/PDF)LaTeX 编译 <code>tex</code> 文件，生成 <code>aux</code> 辅助文件。</li><li>执行 BibTeX 将读入 <code>aux</code> 文件，搞清楚用户需要哪些文献。</li><li>紧接着，BibTeX 根据 <code>aux</code> 文件中的内容，找到正确的 <code>bst</code> 和 <code>bib</code> 文件，并将参考文献信息格式化为 LaTeX 的 <code>thebibliography</code> 环境，作为 <code>bbl</code> 文件输出。</li><li>第二次执行 (Xe/PDF)LaTeX 将会读入新生成的 <code>bbl</code> 文件，同时更新 <code>aux</code> 文件。</li><li>此时，参考文献列表将会正常展示，但是正文中的引用标记显示为问号。</li><li>第三次执行 (Xe/PDF)LaTeX 将会读入 <code>bbl</code> 文件和更新过后的 <code>aux</code> 文件。此时，参考文献相关内容都正常显示。</li></ul><p>因此，总的来说，想要正确使用 BibTeX 协同 LaTeX 处理参考文献，需要编译四次：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(xe/pdf)latex foo.tex   <span class="comment"># 表示使用 latex, pdflatex 或 xelatex 编译，下同</span></span><br><span class="line">bibtex foo.aux</span><br><span class="line">(xe/pdf)latex foo.tex</span><br><span class="line">(xe/pdf)latex foo.tex</span><br></pre></td></tr></table></figure><h1 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h1><blockquote><p>我希望将一条文献展示在参考文献列表中，但不想在正文中用 <code>\cite</code> 命令引用，怎么办？</p></blockquote><p>首先，确保这条文献已经写入了 <code>bib</code> 文件。其次，可以在 <code>\bibliography</code> 命令之前，用 <code>\nocite{label}</code> 提示 BibTeX 调取这条文献。</p><blockquote><p>我有很多条文献，都存在这样的情况。每条文献逐一 <code>\nocite</code> 太繁琐了，有没有懒人适用的办法？</p></blockquote><p>有的。<code>\nocite{*}</code>。</p><blockquote><p>每次都要编译四次，我感觉懒癌又要发作了，有没有办法治疗？</p></blockquote><p>有的。可以尝试 LaTeXmk, TeXify 之类的自动化工具。</p><blockquote><p>我对默认提供的 <code>bst</code> 文件的格式效果不满意，哪里能找到更多的 <code>bst</code>？</p></blockquote><p>现代 TeX 发行版都提供了多种 <code>bst</code> 可供选择，每个 <code>bst</code> 文件的格式、适用范围、使用条件都不一样，需要仔细甄别。具体可以去安装目录下搜索试试。</p><blockquote><p>有没有遵循国家标准的 <code>bst</code>？</p></blockquote><p>有的，<a href="https://liam.page/2014/05/09/gbt7714-2005-bibtex-style/" target="_blank" rel="noopener">这里</a>。</p><blockquote><p>我找到的 <code>bst</code>，效果都不满意，怎么办？</p></blockquote><p>你可以在命令行执行 <code>latex makebst</code>，制作一个符合自己要求的 <code>bst</code> 文件。你需要回答大约 100 个关于参考文献列表效果的问题。</p><blockquote><p><code>bib</code> 文件怎么生成？</p></blockquote><p>你可以手写，或者用 JabRef 之类的文献工具生成。具体请自行 Google 检索，篇幅所限就不展开了。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> BibTeX </tag>
            
            <tag> cite </tag>
            
            <tag> nocite </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>conda安装环境报错 Solving environment failed with initial frozen solve</title>
      <link href="/2021/01/04/conda%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83%E6%8A%A5%E9%94%99-Solving-environment-failed-with-initial-frozen-solve/"/>
      <url>/2021/01/04/conda%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83%E6%8A%A5%E9%94%99-Solving-environment-failed-with-initial-frozen-solve/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询版本</span></span><br><span class="line">$ conda -V</span><br><span class="line"><span class="comment"># 升级</span></span><br><span class="line">$ conda update -n base conda</span><br><span class="line">Collecting package metadata (current_repodata.json): <span class="keyword">done</span></span><br><span class="line">Solving environment: \ </span><br><span class="line">The environment is inconsistent, please check the package plan carefully</span><br><span class="line">The following packages are causing the inconsistency:</span><br><span class="line"></span><br><span class="line">  - https://repo.anaconda.com/pkgs/main/linux-64::anaconda==2019.07=py37_0</span><br><span class="line">  - https://repo.anaconda.com/pkgs/main/linux-64::numba==0.44.1=py37h962f231_0</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Package Plan ##</span></span><br><span class="line"></span><br><span class="line">  environment location: /home/lab305/anaconda3</span><br><span class="line"></span><br><span class="line">  added / updated specs:</span><br><span class="line">    - conda</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The following packages will be downloaded:</span><br><span class="line">...</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>升级<code>conda</code>到最新版本后再更新全部应用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ conda update --all</span><br><span class="line">Collecting package metadata (current_repodata.json): <span class="keyword">done</span></span><br><span class="line">Solving environment: <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Package Plan ##</span></span><br><span class="line"></span><br><span class="line">  environment location: /home/lab305/anaconda3/envs/pytorch1.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The following packages will be downloaded:</span><br></pre></td></tr></table></figure><p>就可以继续使用<code>conda</code>安装应用。</p><p>但是我按照以上步骤后依然报错，后来等待soft-solving后提示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">allennlp requires python&gt;=3.6 &lt;3.7</span><br></pre></td></tr></table></figure><p>而我创建conda环境时使用的python3.7版本…</p><p>fix这个bug之后，就没有问题了！🎉</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> conda </tag>
            
            <tag> bug </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DISTRE源码阅读</title>
      <link href="/2021/01/04/DISTRE%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
      <url>/2021/01/04/DISTRE%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="Instance模型部分"><a href="#Instance模型部分" class="headerlink" title="Instance模型部分"></a>Instance模型部分</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">            sentence: Dict[str, torch.Tensor],</span></span></span><br><span class="line"><span class="function"><span class="params">            label: torch.Tensor = None)</span> -&gt; torch.Tensor:</span></span><br><span class="line">    </span><br><span class="line">    sentence_encoded = self.embedder(sentence[<span class="string">'byte_pairs'</span>])</span><br><span class="line"></span><br><span class="line">    output = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        output[<span class="string">"loss"</span>] = self._loss(sentence, sentence_encoded, label)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> self.clf_head <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        byte_pairs = sentence[<span class="string">'byte_pairs'</span>][:, :sentence_encoded.size(<span class="number">1</span>)]</span><br><span class="line">        clf_logits = self.clf_head(sentence_encoded, byte_pairs)</span><br><span class="line">        output[<span class="string">'logits'</span>] = clf_logits</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>模型很简单，主要分为两个部分，一个编码器，一个分类器。下面去看<code>_loss()</code>和<code>clf_head()</code>函数。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_loss</span><span class="params">(self, sentence: Dict[str, torch.Tensor], sentence_encoded: torch.Tensor, label: torch.Tensor)</span>:</span></span><br><span class="line">    lm_losses = <span class="number">0.</span></span><br><span class="line">    clf_losses = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [n_batch, seq_len]</span></span><br><span class="line">    byte_pairs = sentence[<span class="string">'byte_pairs'</span>][:, :sentence_encoded.size(<span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> self.lm_head <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># [n_batch, seq_len]</span></span><br><span class="line">        mask = (byte_pairs != <span class="number">0</span>).float()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (n_batch, seq_len - 1)</span></span><br><span class="line">        lm_logits = self.lm_head(sentence_encoded)</span><br><span class="line">        <span class="comment">#print('lm_logits:', lm_logits.shape)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># [n_batch * (seq_len - 1)]</span></span><br><span class="line">        byte_pairs_shifted = byte_pairs[:, <span class="number">1</span>:].contiguous().view(<span class="number">-1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># [n_batch * (seq_len - 1)]</span></span><br><span class="line">        lm_losses = nn.CrossEntropyLoss(reduce=<span class="literal">False</span>)(lm_logits, byte_pairs_shifted)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># [n_batch, seq_len - 1]</span></span><br><span class="line">        lm_losses = lm_losses.view(byte_pairs.size(<span class="number">0</span>), byte_pairs.size(<span class="number">1</span>) - <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># [n_batch, seq_len - 1]</span></span><br><span class="line">        lm_losses = lm_losses * mask[:, <span class="number">1</span>:]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># [n_batch * (seq_len - 1)]</span></span><br><span class="line">        lm_losses = lm_losses.sum(dim=<span class="number">1</span>) / torch.sum(mask[:, <span class="number">1</span>:], dim=<span class="number">1</span>)</span><br><span class="line">        lm_losses = lm_losses.mean()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> self.clf_head <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        clf_logits = self.clf_head(sentence_encoded, byte_pairs)</span><br><span class="line">        <span class="keyword">if</span> clf_logits.size(<span class="number">0</span>) != label.size(<span class="number">0</span>):</span><br><span class="line">            torch.save(sentence, <span class="string">'./sentence.pt'</span>)</span><br><span class="line">        clf_losses = nn.CrossEntropyLoss(reduce=<span class="literal">False</span>)(clf_logits, label)</span><br><span class="line">        clf_losses = clf_losses.mean()</span><br><span class="line">        </span><br><span class="line">    loss = clf_losses + self.language_model_weight * lm_losses</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>值得注意的是，在进行模型的训练时，除了关系分类的损失，还添加了预训练模型的损失，对于GPT是预测下一个单词，对于BERT是MLM。<strong>并且给language_model_weight赋予0.5的权重</strong>。</p><p>对于编码器和分类器，分别按照以下代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LanguageModelHead</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" Language Model Head for the transformer """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model)</span>:</span></span><br><span class="line">        super(LanguageModelHead, self).__init__()</span><br><span class="line">        self.n_embd = model.embed.embedding_dim</span><br><span class="line">        self.decoder = model.decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, h)</span>:</span></span><br><span class="line">        <span class="comment"># Truncated Language modeling logits (we remove the last token)</span></span><br><span class="line">        h_trunc = h[:, :<span class="number">-1</span>].contiguous().view(<span class="number">-1</span>, self.n_embd)</span><br><span class="line">        lm_logits = self.decoder(h_trunc)</span><br><span class="line">        <span class="keyword">return</span> lm_logits</span><br></pre></td></tr></table></figure><p>这里的decoder()可能是GPT独有的，在transformer提供的代码里对应到BERT模型没有相应的模块。</p><h2 id="BERT模型"><a href="#BERT模型" class="headerlink" title="BERT模型"></a>BERT模型</h2><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">  (11): BertLayer(</span><br><span class="line">    (attention): BertAttention(</span><br><span class="line">      (self): BertSelfAttention(</span><br><span class="line">        (query): Linear(in_features=768, out_features=768, bias=True)</span><br><span class="line">        (key): Linear(in_features=768, out_features=768, bias=True)</span><br><span class="line">        (value): Linear(in_features=768, out_features=768, bias=True)</span><br><span class="line">        (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">      )</span><br><span class="line">      (output): BertSelfOutput(</span><br><span class="line">        (dense): Linear(in_features=768, out_features=768, bias=True)</span><br><span class="line">        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span><br><span class="line">        (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (intermediate): BertIntermediate(</span><br><span class="line">      (dense): Linear(in_features=768, out_features=3072, bias=True)</span><br><span class="line">    )</span><br><span class="line">    (output): BertOutput(</span><br><span class="line">      (dense): Linear(in_features=3072, out_features=768, bias=True)</span><br><span class="line">      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span><br><span class="line">      (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>BERT中的一层主要由<code>BertAttention</code> <code>BertIntermediate</code> <code>BertOutput</code>构成，<font color="orange">BertIntermediate将768维的向量转化为3072维，是一种升维的操作！</font></p><p>而BERTPooler是最终<code>[CLS]</code>的表示，通过以下方式：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(pooler): BertPooler(</span><br><span class="line">      (dense): Linear(in_features=768, out_features=768, bias=True)</span><br><span class="line">      (activation): Tanh()</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>由上面的讲解可知，pooler层的输入是transformer最后一层的输出，[batch_size, seq_length, hidden_size]</p><p><strong>取出每一句的第一个单词，做全连接和激活。</strong>得到的输出可以用来分类等下游任务（即将每个句子的第一个单词的表示作为整个句子的表示）  </p><font color="red">并没有进行MaxPooling等操作，这个叫法确实有点迷惑。。。</font><p>再去看分类模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassificationHead</span><span class="params">(TaskHead)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, n_class, clf_token, dropout=<span class="number">0.</span>)</span>:</span></span><br><span class="line">        super(ClassificationHead, self).__init__()</span><br><span class="line">        self.n_embd = model.embed.embedding_dim</span><br><span class="line">        self.clf_token = clf_token</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear = nn.Linear(self.n_embd, n_class)</span><br><span class="line"></span><br><span class="line">        nn.init.normal_(self.linear.weight, std = <span class="number">0.02</span>)</span><br><span class="line">        nn.init.normal_(self.linear.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, h, x)</span>:</span></span><br><span class="line">        clf_h = h.contiguous().view(<span class="number">-1</span>, self.n_embd)</span><br><span class="line">        flat = x.contiguous().view(<span class="number">-1</span>)</span><br><span class="line">        clf_h = clf_h[flat == self.clf_token, :]</span><br><span class="line">        clf_h = self.dropout(clf_h)</span><br><span class="line">        clf_logits = self.linear(clf_h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> clf_logits</span><br></pre></td></tr></table></figure><p>就是在句子表示<code>__clf__</code>后面加了一层全连接层，<strong>没有激活，没有多层</strong>。</p><h2 id="语法糖"><a href="#语法糖" class="headerlink" title="语法糖"></a>语法糖</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> groupby</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_scopes</span><span class="params">(x)</span>:</span></span><br><span class="line">    scopes = []</span><br><span class="line">    <span class="keyword">for</span> k, g <span class="keyword">in</span> groupby(enumerate(x), <span class="keyword">lambda</span> i_x: i_x[<span class="number">1</span>]):</span><br><span class="line">        group = list(map(itemgetter(<span class="number">0</span>), g))</span><br><span class="line">        scopes.append((group[<span class="number">0</span>], group[<span class="number">-1</span>]+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> scopes</span><br></pre></td></tr></table></figure><h1 id="Bag模型部分"><a href="#Bag模型部分" class="headerlink" title="Bag模型部分"></a>Bag模型部分</h1><p>依然是从<code>forward()</code>函数入手：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">            sentence: Dict[str, torch.Tensor],</span></span></span><br><span class="line"><span class="function"><span class="params">            metadata: List[Dict[str, Any]],</span></span></span><br><span class="line"><span class="function"><span class="params">            label: torch.Tensor = None)</span> -&gt; torch.Tensor:</span></span><br><span class="line">    </span><br><span class="line">    byte_pairs = sentence[<span class="string">'byte_pairs'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.training <span class="keyword">and</span> self.entity_dropout &gt; <span class="number">0.</span>:</span><br><span class="line">        dropout_mask = get_entity_dropout_mask(byte_pairs, self.encoder_vocab[self.del1_token], self.encoder_vocab[self.del2_token], self.entity_dropout)</span><br><span class="line">        byte_pairs = byte_pairs.masked_fill(dropout_mask, self.encoder_vocab[self.mask_token])</span><br><span class="line"></span><br><span class="line">    sentence_encoded = self.embedder(byte_pairs)</span><br><span class="line">    </span><br><span class="line">    output = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    instance_ids = [md[<span class="string">'instance_id'</span>] <span class="keyword">for</span> md <span class="keyword">in</span> metadata]</span><br><span class="line">    scopes = get_scopes(instance_ids)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        output[<span class="string">"loss"</span>] = self._loss(sentence, sentence_encoded, scopes, label)</span><br><span class="line">    </span><br><span class="line">    clf_logits, clf_h = self.clf_head(sentence, sentence_encoded, scopes, label)</span><br><span class="line">    output[<span class="string">'instances'</span>] = [metadata[idx] <span class="keyword">for</span> idx, _ <span class="keyword">in</span> scopes]</span><br><span class="line">    output[<span class="string">'logits'</span>] = clf_logits</span><br><span class="line">    output[<span class="string">'clf_h'</span>] = clf_h</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>首先注意到<code>get_entity_dropout_mask()</code>函数，函数体为下文所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_entity_mask</span><span class="params">(x, del1, del2)</span>:</span></span><br><span class="line">    a = torch.arange(x.size(<span class="number">1</span>)).expand_as(x).type_as(x)</span><br><span class="line">    pos_del1 = a[x == del1].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    pos_del2 = a[x == del2].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    mask_ent1 = (<span class="number">0</span> &lt; a) &amp; (a &lt; pos_del1)</span><br><span class="line">    mask_ent2 = (pos_del1 &lt; a) &amp; (a &lt; pos_del2)</span><br><span class="line">    <span class="keyword">return</span> mask_ent1, mask_ent2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_entity_dropout_mask</span><span class="params">(x, del1, del2, dropout_probability=<span class="number">.1</span>)</span>:</span></span><br><span class="line">    mask_ent1, mask_ent2 = get_entity_mask(x, del1, del2)</span><br><span class="line">    binary_mask_ent1 = x.new(x.size(<span class="number">0</span>), <span class="number">1</span>).bernoulli_(<span class="number">1</span> - dropout_probability)</span><br><span class="line">    binary_mask_ent2 = x.new(x.size(<span class="number">0</span>), <span class="number">1</span>).bernoulli_(<span class="number">1</span> - dropout_probability)</span><br><span class="line">    <span class="keyword">return</span> (binary_mask_ent1 * mask_ent1.long() + binary_mask_ent2 * mask_ent2.long()).byte()</span><br></pre></td></tr></table></figure><p>不同于直接进行mask操作，<strong>值得注意的是，在DISTRE中引入了DROPOUT操作</strong>，这是我之前没有尝试过的。</p><p>回到<code>forward()</code>函数，里面用<code>get_scopes()</code>函数获取一个包的句子范围，也就是上面提到的语法糖部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_loss</span><span class="params">(self, sentence: Dict[str, torch.Tensor], sentence_encoded: torch.Tensor, scopes: List[Tuple[int, int]], label: torch.Tensor)</span>:</span></span><br><span class="line">    lm_losses = <span class="number">0.</span></span><br><span class="line">    clf_losses = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    bag_label_indices = [start <span class="keyword">for</span> start, _ <span class="keyword">in</span> scopes]</span><br><span class="line">    bag_label = label[bag_label_indices]  <span class="comment"># 对维度进行了一个缩减</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [n_batch, seq_len]</span></span><br><span class="line">    byte_pairs = sentence[<span class="string">'byte_pairs'</span>][:, :sentence_encoded.size(<span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> self.lm_head <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># [n_batch, seq_len]</span></span><br><span class="line">        mask = (byte_pairs != <span class="number">0</span>).float()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (n_batch, seq_len - 1)</span></span><br><span class="line">        lm_logits = self.lm_head(sentence_encoded)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># [n_batch * (seq_len - 1)]</span></span><br><span class="line">        byte_pairs_shifted = byte_pairs[:, <span class="number">1</span>:].contiguous().view(<span class="number">-1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># [n_batch * (seq_len - 1)]</span></span><br><span class="line">        lm_losses = nn.CrossEntropyLoss(reduce=<span class="literal">False</span>)(lm_logits, byte_pairs_shifted)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># [n_batch, seq_len - 1]</span></span><br><span class="line">        lm_losses = lm_losses.view(byte_pairs.size(<span class="number">0</span>), byte_pairs.size(<span class="number">1</span>) - <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># [n_batch, seq_len - 1]</span></span><br><span class="line">        lm_losses = lm_losses * mask[:, <span class="number">1</span>:]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># [n_batch * (seq_len - 1)]</span></span><br><span class="line">        lm_losses = lm_losses.sum(dim=<span class="number">1</span>) / torch.sum(mask[:, <span class="number">1</span>:], dim=<span class="number">1</span>)</span><br><span class="line">        lm_losses = lm_losses.mean()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> self.clf_head <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        clf_logits, _ = self.clf_head(sentence, sentence_encoded, scopes, label=label)</span><br><span class="line">        clf_losses = nn.CrossEntropyLoss(reduce=<span class="literal">False</span>)(clf_logits, bag_label)</span><br><span class="line">        clf_losses = clf_losses.mean()</span><br><span class="line">        </span><br><span class="line">    loss = clf_losses + self.language_model_weight * lm_losses</span><br><span class="line"></span><br><span class="line">    self.metrics[<span class="string">'accuracy'</span>](clf_logits, bag_label)</span><br><span class="line">    self.metrics[<span class="string">'not_na_accuracy'</span>](clf_logits, bag_label, mask=torch.ne(bag_label, self.na_idx))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>对于语言模型损失，Bag相对于Instance级别没有什么改变。但是对于分类损失，有添加注意力机制：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BagClassificationHead</span><span class="params">(TaskHead)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, n_class, encoder_vocab, clf_token, selector, dropout=<span class="number">0.</span>)</span>:</span></span><br><span class="line">        super(BagClassificationHead, self).__init__()</span><br><span class="line">        self.n_embd = model.embed.embedding_dim</span><br><span class="line">        self.encoder_vocab = encoder_vocab</span><br><span class="line">        self.clf_token = clf_token</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> selector == <span class="string">'average'</span>:</span><br><span class="line">            self.selector = Average(model, n_class, dropout)</span><br><span class="line">        <span class="keyword">elif</span> selector == <span class="string">'attention'</span>:</span><br><span class="line">            self.selector = Attention(model, n_class, dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f"Selector '<span class="subst">&#123;selector&#125;</span>' not supported."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sentence: Dict[str, torch.Tensor], sentence_encoded: torch.Tensor, scopes: List[Tuple[int, int]], label: torch.Tensor)</span>:</span></span><br><span class="line">        x = sentence[<span class="string">'byte_pairs'</span>][:, :sentence_encoded.size(<span class="number">1</span>)]</span><br><span class="line">        h = sentence_encoded</span><br><span class="line"></span><br><span class="line">        clf_token_idx = self.encoder_vocab[self.clf_token]</span><br><span class="line"></span><br><span class="line">        clf_h = h.contiguous().view(<span class="number">-1</span>, self.n_embd)</span><br><span class="line">        flat = x.contiguous().view(<span class="number">-1</span>)</span><br><span class="line">        clf_h = clf_h[flat == clf_token_idx, :]</span><br><span class="line">        clf_logits = self.selector(clf_h, scopes, label)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> clf_logits, clf_h</span><br></pre></td></tr></table></figure><p>按照不同的Bag向量获取方式，这里有两种selector，一种是<code>Average</code>，一种是<code>Attention</code>，去看这两个函数：</p><h2 id="Average-Selector"><a href="#Average-Selector" class="headerlink" title="Average Selector"></a>Average Selector</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Selector</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, n_class, dropout)</span>:</span></span><br><span class="line">        super(Selector, self).__init__()</span><br><span class="line">        relation_dim = model.embed.embedding_dim</span><br><span class="line"></span><br><span class="line">        self.relation_matrix = nn.Embedding(n_class, relation_dim)</span><br><span class="line">        self.bias = nn.Parameter(torch.Tensor(n_class))</span><br><span class="line">        <span class="comment">#self.attention_matrix = nn.Embedding(n_class, relation_dim)</span></span><br><span class="line">        self.init_weights()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        nn.init.xavier_uniform(self.relation_matrix.weight.data)</span><br><span class="line">        nn.init.normal(self.bias)  <span class="comment"># bias初始化没有置为0</span></span><br><span class="line">        <span class="comment">#nn.init.xavier_uniform(self.attention_matrix.weight.data)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_logits</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        logits = torch.matmul(x, torch.transpose(</span><br><span class="line">            self.relation_matrix.weight, <span class="number">0</span>, <span class="number">1</span>),) + self.bias</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, scopes=None, label=None)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Average</span><span class="params">(Selector)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, scopes=None, label=None)</span>:</span></span><br><span class="line">        scopes = scopes <span class="keyword">or</span> [(<span class="number">0</span>, x.size(<span class="number">0</span>))]</span><br><span class="line"></span><br><span class="line">        tower_repre = []</span><br><span class="line">        <span class="keyword">for</span> start, end <span class="keyword">in</span> scopes:</span><br><span class="line">            sen_matrix = x[start: end]</span><br><span class="line">            final_repre = torch.mean(sen_matrix, <span class="number">0</span>)  <span class="comment"># 取平均</span></span><br><span class="line">            tower_repre.append(final_repre)</span><br><span class="line">        stack_repre = torch.stack(tower_repre)</span><br><span class="line">        stack_repre = self.dropout(stack_repre)</span><br><span class="line">        logits = self.get_logits(stack_repre)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>在做Attention的时候，<strong>是拿所有关系与当前表示相乘</strong>。</p><h2 id="Attention-Selector"><a href="#Attention-Selector" class="headerlink" title="Attention Selector"></a>Attention Selector</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(Selector)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_attention_train_logit</span><span class="params">(self, x, query)</span>:</span></span><br><span class="line">        relation_query = self.relation_matrix(query)</span><br><span class="line">        attention_logit = torch.sum(x * relation_query, <span class="number">1</span>, <span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> attention_logit</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_attention_test_logit</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        attention_logit = torch.matmul(x, torch.transpose(self.relation_matrix.weight, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> attention_logit</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, scopes=None, label=None)</span>:</span></span><br><span class="line">        scopes = scopes <span class="keyword">or</span> [(<span class="number">0</span>, x.size(<span class="number">0</span>))]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            attention_logit = self._attention_train_logit(x, label)</span><br><span class="line"></span><br><span class="line">            tower_repre = []</span><br><span class="line">            <span class="keyword">for</span> start, end <span class="keyword">in</span> scopes:</span><br><span class="line">                sen_matrix = x[start: end]</span><br><span class="line">                attention_score = F.softmax(torch.transpose(attention_logit[start: end], <span class="number">0</span>, <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">                final_repre = torch.squeeze(torch.matmul(attention_score, sen_matrix))</span><br><span class="line">                tower_repre.append(final_repre)</span><br><span class="line">            stack_repre = torch.stack(tower_repre)</span><br><span class="line">            stack_repre = self.dropout(stack_repre)</span><br><span class="line">            logits = self.get_logits(stack_repre)</span><br><span class="line">            <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attention_logit = self._attention_test_logit(x)</span><br><span class="line"></span><br><span class="line">            tower_output = []</span><br><span class="line">            <span class="keyword">for</span> start, end <span class="keyword">in</span> scopes:</span><br><span class="line">                sen_matrix = x[start: end]</span><br><span class="line">                attention_score = F.softmax(torch.transpose(attention_logit[start: end], <span class="number">0</span>, <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">                final_repre = torch.matmul(attention_score, sen_matrix)</span><br><span class="line">                logits = self.get_logits(final_repre)</span><br><span class="line">                tower_output.append(torch.diag(F.softmax(logits, <span class="number">1</span>)))</span><br><span class="line">            stack_output = torch.stack(tower_output)</span><br><span class="line">            <span class="keyword">return</span> stack_output</span><br></pre></td></tr></table></figure><p>在训练和测试阶段有些不同，训练时使用<code>_attention_train_logit()</code>，只取1个关系；测试时使用<code>_attention_test_logit()</code>，使用所有的关系。</p><h1 id="数据加载部分"><a href="#数据加载部分" class="headerlink" title="数据加载部分"></a>数据加载部分</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DataIterator.register("bag")</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BagIterator</span><span class="params">(DataIterator)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 sorting_keys: List[Tuple[str, str]],</span></span></span><br><span class="line"><span class="function"><span class="params">                 padding_noise: float = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 biggest_batch_first: bool = False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size: int = <span class="number">32</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 instances_per_epoch: int = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_instances_in_memory: int = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 cache_instances: bool = False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 track_epoch: bool = False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 maximum_samples_per_batch: Tuple[str, int] = None)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> sorting_keys:</span><br><span class="line">            <span class="keyword">raise</span> ConfigurationError(<span class="string">"BucketIterator requires sorting_keys to be specified"</span>)</span><br><span class="line"></span><br><span class="line">        super().__init__(cache_instances=cache_instances,</span><br><span class="line">                         track_epoch=track_epoch,</span><br><span class="line">                         batch_size=batch_size,</span><br><span class="line">                         instances_per_epoch=instances_per_epoch,</span><br><span class="line">                         max_instances_in_memory=max_instances_in_memory,</span><br><span class="line">                         maximum_samples_per_batch=maximum_samples_per_batch)</span><br><span class="line">        self._sorting_keys = sorting_keys</span><br><span class="line">        self._padding_noise = padding_noise</span><br><span class="line">        self._biggest_batch_first = biggest_batch_first</span><br><span class="line"></span><br><span class="line"><span class="meta">    @overrides</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_batches</span><span class="params">(self, instances: Iterable[Instance], shuffle: bool)</span> -&gt; Iterable[Batch]:</span></span><br><span class="line">        <span class="keyword">for</span> instance_list <span class="keyword">in</span> self._memory_sized_lists(instances):</span><br><span class="line"></span><br><span class="line">            bags = &#123;&#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> instance_id, mentions <span class="keyword">in</span> groupby(</span><br><span class="line">                sorted(instance_list, key=<span class="keyword">lambda</span> instance: instance[<span class="string">'metadata'</span>][<span class="string">'instance_id'</span>]),</span><br><span class="line">                key=<span class="keyword">lambda</span> instance: instance[<span class="string">'metadata'</span>][<span class="string">'instance_id'</span>]):</span><br><span class="line"></span><br><span class="line">                bags[instance_id] = list(mentions)</span><br><span class="line"></span><br><span class="line">            shuffled_instance_ids = list(bags.keys())</span><br><span class="line">            random.shuffle(shuffled_instance_ids)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'creating new instances'</span>)</span><br><span class="line"></span><br><span class="line">            new_instances = []</span><br><span class="line">            <span class="keyword">for</span> instance_id <span class="keyword">in</span> shuffled_instance_ids:</span><br><span class="line">                mentions = bags[instance_id]</span><br><span class="line">                <span class="keyword">if</span> shuffle:</span><br><span class="line">                    random.shuffle(mentions)</span><br><span class="line">                new_instances.extend(mentions)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'creating batches'</span>)</span><br><span class="line"></span><br><span class="line">            batches = []</span><br><span class="line">            <span class="keyword">for</span> batch_instances <span class="keyword">in</span> lazy_groups_of(iter(new_instances), self._batch_size):</span><br><span class="line">                <span class="keyword">for</span> possibly_smaller_batches <span class="keyword">in</span> self._ensure_batch_is_sufficiently_small(batch_instances):</span><br><span class="line">                    batches.append(Batch(possibly_smaller_batches))</span><br><span class="line">            </span><br><span class="line">            print(<span class="string">'num batches:'</span>, len(batches))</span><br><span class="line"></span><br><span class="line">            move_to_front = self._biggest_batch_first <span class="keyword">and</span> len(batches) &gt; <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> move_to_front:</span><br><span class="line">                <span class="comment"># We'll actually pop the last _two_ batches, because the last one might not be full.</span></span><br><span class="line">                last_batch = batches.pop()</span><br><span class="line">                penultimate_batch = batches.pop()</span><br><span class="line">            <span class="keyword">if</span> move_to_front:</span><br><span class="line">                batches.insert(<span class="number">0</span>, penultimate_batch)</span><br><span class="line">                batches.insert(<span class="number">0</span>, last_batch)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'yielding from batches'</span>)</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> batches</span><br></pre></td></tr></table></figure><h2 id="group-by"><a href="#group-by" class="headerlink" title="group_by"></a>group_by</h2><p>首先根据instance_id进行<code>group_by</code>操作，而instance_id是以下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">instance_id = <span class="string">f'<span class="subst">&#123;head&#125;</span>#<span class="subst">&#123;tail&#125;</span>'</span></span><br><span class="line"><span class="keyword">if</span> label:</span><br><span class="line">    instance_id = <span class="string">f'<span class="subst">&#123;instance_id&#125;</span>#<span class="subst">&#123;label&#125;</span>'</span></span><br></pre></td></tr></table></figure><p>在实验中，作者制定了label，为下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> self.text_to_instance(sentence=sentence, head=head, tail=tail, label=relation,</span><br><span class="line">                                            head_type=head_type, tail_type=tail_type)</span><br></pre></td></tr></table></figure><p>因此，<strong>instance_id即为头尾实体加关系构成</strong>，这样，对于知识库中有多个关系的头尾实体，会分别打成多个包。<em>比如，Steve Jobs -&gt; Apple，有关系CEO和founder，则会打成两个包，包中句子完全相同</em>。</p><h2 id="creating-batches"><a href="#creating-batches" class="headerlink" title="creating batches"></a>creating batches</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'creating new instances'</span>)</span><br><span class="line"></span><br><span class="line">new_instances = []</span><br><span class="line"><span class="keyword">for</span> instance_id <span class="keyword">in</span> shuffled_instance_ids:</span><br><span class="line">    mentions = bags[instance_id]</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.shuffle(mentions)</span><br><span class="line">    new_instances.extend(mentions)</span><br></pre></td></tr></table></figure><p><strong><em>通过这一步，将同一个包中的句子挨在了一起</em></strong>。</p><font color="red">问题是`for instance_list in self._memory_sized_lists(instances)`要看一下instance_list的值和大小。</font><p>在网上找到了示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要导入模块: from allennlp.common import util [as 别名]</span></span><br><span class="line"><span class="comment"># 或者: from allennlp.common.util import lazy_groups_of [as 别名]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_create_batches</span><span class="params">(self, instances                    , shuffle      )</span>:</span></span><br><span class="line">        <span class="comment"># First break the dataset into memory-sized lists:</span></span><br><span class="line">        <span class="keyword">for</span> instance_list <span class="keyword">in</span> self._memory_sized_lists(instances):</span><br><span class="line">            <span class="keyword">if</span> shuffle:</span><br><span class="line">                random.shuffle(instance_list)</span><br><span class="line">            iterator = iter(instance_list)</span><br><span class="line">            <span class="comment"># Then break each memory-sized list into batches.</span></span><br><span class="line">            <span class="keyword">for</span> batch_instances <span class="keyword">in</span> lazy_groups_of(iterator, self._batch_size):</span><br><span class="line">                <span class="keyword">for</span> possibly_smaller_batches <span class="keyword">in</span> self._ensure_batch_is_sufficiently_small(batch_instances):</span><br><span class="line">                    batch = Batch(possibly_smaller_batches)</span><br><span class="line">                    <span class="keyword">yield</span> batch</span><br></pre></td></tr></table></figure><p>添加以下代码查看instance_list的大小：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'============================================================'</span>)</span><br><span class="line">print(<span class="string">"'instance_list' type "</span>, type(instance_list))</span><br><span class="line">print(<span class="string">"'instance_list' length "</span>, len(instance_list))</span><br><span class="line">print(<span class="string">'============================================================'</span>)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">============================================================</span><br><span class="line"><span class="string">'instance_list'</span> <span class="built_in">type</span>  &lt;class <span class="string">'list'</span>&gt;</span><br><span class="line"><span class="string">'instance_list'</span> length  522611</span><br><span class="line">============================================================</span><br></pre></td></tr></table></figure><p>可以看到，是将所有数据加载到了内存中。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><font color="orange">**总结一下，1.对句子按照(head, tail, relation)排序 2.按照batch_size来截取**</font><p>在论文中batch_size设置为8，在源代码中默认设置为16。</p><h1 id="预测部分"><a href="#预测部分" class="headerlink" title="预测部分"></a>预测部分</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Predictor.register('tre-classifier')</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TREClassifierPredictor</span><span class="params">(Predictor)</span>:</span></span><br><span class="line">    <span class="string">"""Predictor wrapper for the TREClassifier"""</span></span><br><span class="line"><span class="meta">    @overrides</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_json_to_instance</span><span class="params">(self, json_dict: JsonDict)</span> -&gt; Instance:</span></span><br><span class="line">        sentence = json_dict[<span class="string">'sentence'</span>]</span><br><span class="line">        head = json_dict[<span class="string">'head'</span>]</span><br><span class="line">        tail = json_dict[<span class="string">'tail'</span>]</span><br><span class="line">        instance = self._dataset_reader.text_to_instance(sentence=sentence, head=head, tail=tail)</span><br><span class="line">        <span class="keyword">return</span> instance</span><br><span class="line"></span><br><span class="line"><span class="meta">    @overrides</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_batch_instance</span><span class="params">(self, instances: List[Instance])</span> -&gt; List[JsonDict]:</span></span><br><span class="line">        model = self._model</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            cuda_device = model._get_prediction_device()</span><br><span class="line">            dataset = Batch(instances)</span><br><span class="line">            dataset.index_instances(model.vocab)</span><br><span class="line">            model_input = util.move_to_device(dataset.as_tensor_dict(), cuda_device)</span><br><span class="line">            outputs = model.decode(model(**model_input))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sanitize(outputs)</span><br></pre></td></tr></table></figure><p>根据训练时的模型选择，分别对应Bag2Bag或Sent2Sent预测模式。</p><h2 id="predict-batch-json"><a href="#predict-batch-json" class="headerlink" title="predict_batch_json()"></a>predict_batch_json()</h2><p>从官网的文档来看，是这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SemanticRoleLabelerPredictor</span><span class="params">(Predictor)</span>:</span></span><br><span class="line"> | ...</span><br><span class="line"> | @overrides</span><br><span class="line"> | <span class="function"><span class="keyword">def</span> <span class="title">predict_batch_json</span><span class="params">(self, inputs: List[JsonDict])</span> -&gt; List[JsonDict]</span></span><br></pre></td></tr></table></figure><p>Expects JSON that looks like <code>[{&quot;sentence&quot;: &quot;...&quot;}, {&quot;sentence&quot;: &quot;...&quot;}, ...]</code> and returns JSON that looks like</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;<span class="attr">"words"</span>: [...],</span><br><span class="line">     <span class="attr">"verbs"</span>: [</span><br><span class="line">        &#123;<span class="attr">"verb"</span>: <span class="string">"..."</span>, <span class="attr">"description"</span>: <span class="string">"..."</span>, <span class="attr">"tags"</span>: [...]&#125;,</span><br><span class="line">        ...</span><br><span class="line">        &#123;<span class="attr">"verb"</span>: <span class="string">"..."</span>, <span class="attr">"description"</span>: <span class="string">"..."</span>, <span class="attr">"tags"</span>: [...]&#125;,</span><br><span class="line">    ]&#125;,</span><br><span class="line">    &#123;<span class="attr">"words"</span>: [...],</span><br><span class="line">     <span class="attr">"verbs"</span>: [</span><br><span class="line">        &#123;<span class="attr">"verb"</span>: <span class="string">"..."</span>, <span class="attr">"description"</span>: <span class="string">"..."</span>, <span class="attr">"tags"</span>: [...]&#125;,</span><br><span class="line">        ...</span><br><span class="line">        &#123;<span class="attr">"verb"</span>: <span class="string">"..."</span>, <span class="attr">"description"</span>: <span class="string">"..."</span>, <span class="attr">"tags"</span>: [...]&#125;,</span><br><span class="line">    ]&#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DISTRE </tag>
            
            <tag> source code </tag>
            
            <tag> relation extraction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR曲线横纵坐标轴的数据来源</title>
      <link href="/2021/01/03/PR%E6%9B%B2%E7%BA%BF%E6%A8%AA%E7%BA%B5%E5%9D%90%E6%A0%87%E8%BD%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90/"/>
      <url>/2021/01/03/PR%E6%9B%B2%E7%BA%BF%E6%A8%AA%E7%BA%B5%E5%9D%90%E6%A0%87%E8%BD%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_loss, test_acc, y, y_pred, logit_list, y_hot = self.predict(sess, self.data[<span class="string">'test'</span>])</span><br></pre></td></tr></table></figure><p>查看predict()函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sess, data, wLabels=True, shuffle=False, label=<span class="string">'Evaluating on Test'</span>)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Evaluate model on valid/test data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">sess:Session of tensorflow</span></span><br><span class="line"><span class="string">data:Data to evaluate on</span></span><br><span class="line"><span class="string">wLabels:Does data include labels or not</span></span><br><span class="line"><span class="string">shuffle:Shuffle data while before creates batches</span></span><br><span class="line"><span class="string">label:Log label to be used while logging</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Returns</span></span><br><span class="line"><span class="string">-------</span></span><br><span class="line"><span class="string">losses:Loss over the entire data</span></span><br><span class="line"><span class="string">accuracies:Overall Accuracy</span></span><br><span class="line"><span class="string">y: Actual label</span></span><br><span class="line"><span class="string">y_pred:Predicted labels</span></span><br><span class="line"><span class="string">logit_list:Logit list for each bag in the data</span></span><br><span class="line"><span class="string">y_actual_hot:One hot represetnation of actual label for each bag in the data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">losses, accuracies, y_pred, y, logit_list, y_actual_hot = [], [], [], [], [], []</span><br><span class="line">bag_cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, batch <span class="keyword">in</span> enumerate(self.getBatches(data, shuffle)):</span><br><span class="line"><span class="comment"># nn_out =&gt; logits &#125; =&gt; loss</span></span><br><span class="line"><span class="comment">#           y</span></span><br><span class="line"><span class="comment"># nn_out =&gt; y_pred   &#125; =&gt; accuracy</span></span><br><span class="line"><span class="comment">#           y_actual</span></span><br><span class="line">loss, logits, accuracy = sess.run([self.loss, self.logits, self.accuracy], feed_dict = self.create_feed_dict(batch, split=<span class="string">'test'</span>))</span><br><span class="line">losses.    append(loss)</span><br><span class="line">accuracies.append(accuracy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> 只预测一个标签, 对真实标签也是只取第一个</span></span><br><span class="line">pred_ind      = logits.argmax(axis=<span class="number">1</span>)  <span class="comment"># (batch, )</span></span><br><span class="line">y_pred += pred_ind.tolist()  <span class="comment"># (batch, )</span></span><br><span class="line"></span><br><span class="line">y_actual_hot += self.getOneHot(batch[<span class="string">'Y'</span>], self.num_class).tolist()  <span class="comment"># 保留所有 gold label</span></span><br><span class="line">y      += np.argmax(self.getOneHot(batch[<span class="string">'Y'</span>], self.num_class), <span class="number">1</span>).tolist()  <span class="comment"># 只取第一个 gold label</span></span><br><span class="line"></span><br><span class="line">logit_list += logits.tolist()</span><br><span class="line">bag_cnt      += len(batch[<span class="string">'sent_num'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">self.logger.info(<span class="string">'&#123;&#125; (&#123;&#125;/&#123;&#125;):\t&#123;:.5&#125;\t&#123;:.5&#125;\t&#123;&#125;'</span>.format(label, bag_cnt, len(self.data[<span class="string">'test'</span>]), np.mean(accuracies)*<span class="number">100</span>, np.mean(losses), self.p.name))</span><br><span class="line"></span><br><span class="line">self.logger.info(<span class="string">'Test Accuracy: &#123;&#125;'</span>.format(accuracy))</span><br><span class="line"><span class="keyword">return</span> np.mean(losses), np.mean(accuracies)*<span class="number">100</span>, y, y_pred, logit_list, y_actual_hot</span><br></pre></td></tr></table></figure><a id="more"></a><font color="red">**但是这里取y的时候，也是对0-1数组取第一个为1的索引，可能会有顺序的影响！为了避免干扰，在实验中确保rel2id.json与RESIDE设置一致。**</font><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去除NA并全部拉成1维向量 #</span></span><br><span class="line">y_true   = np.array([e[<span class="number">1</span>:] <span class="keyword">for</span> e <span class="keyword">in</span> y_hot]).   reshape((<span class="number">-1</span>))</span><br><span class="line">y_scores = np.array([e[<span class="number">1</span>:] <span class="keyword">for</span> e <span class="keyword">in</span> logit_list]).reshape((<span class="number">-1</span>))</span><br><span class="line">area_pr  = average_precision_score(y_true, y_scores)</span><br><span class="line"></span><br><span class="line">self.logger.info(<span class="string">'Final results: Prec:&#123;&#125; | Rec:&#123;&#125; | F1:&#123;&#125; | Area:&#123;&#125;'</span>.format(test_prec, test_rec, test_f1, area_pr))</span><br><span class="line"><span class="comment"># Store predictions</span></span><br><span class="line">pickle.dump(&#123;<span class="string">'logit_list'</span>: logit_list, <span class="string">'y_hot'</span>: y_hot&#125;, open(<span class="string">"results/&#123;&#125;/precision_recall.pkl"</span>.format(self.p.name), <span class="string">'wb'</span>))</span><br></pre></td></tr></table></figure><p><font color="orange">值得注意的是，</font>在绘制PR曲线时，与计算F1不同，是取了所有的gold_label (one-hot向量)，并取了除NA以外的所有logits值。</p><p>但是在保存的时候，又直接保存了logits_list和y_hot，继续阅读plot_pr.py代码，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(path)</span>:</span></span><br><span class="line">preds    = pickle.load(open(path, <span class="string">'rb'</span>))</span><br><span class="line">y_hot    = np.array(preds[<span class="string">'y_hot'</span>])</span><br><span class="line">logit_list = np.array(preds[<span class="string">'logit_list'</span>])</span><br><span class="line">y_hot_new       = np.reshape(np.array([x[<span class="number">1</span>:] <span class="keyword">for</span> x <span class="keyword">in</span> y_hot]),      (<span class="number">-1</span>))</span><br><span class="line">logit_list_new  = np.reshape(np.array([x[<span class="number">1</span>:] <span class="keyword">for</span> x <span class="keyword">in</span> logit_list]), (<span class="number">-1</span>))</span><br><span class="line"><span class="keyword">return</span> y_hot_new, logit_list_new</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotPR</span><span class="params">(dataset, args)</span>:</span></span><br><span class="line">y_true, y_scores    = loadData(<span class="string">'./results/&#123;&#125;/precision_recall.pkl'</span>.format(args.name))</span><br><span class="line">precision,recall,threshold = precision_recall_curve(y_true,y_scores)</span><br><span class="line">area_under       = average_precision_score(y_true, y_scores)</span><br><span class="line">baselines_path    = <span class="string">'./baselines_pr/&#123;&#125;/'</span>.format(dataset)</span><br><span class="line">print(<span class="string">'Area under the curve: &#123;:.3&#125;'</span>.format(area_under))</span><br><span class="line"></span><br><span class="line">plt.plot(recall[:], precision[:], label=<span class="string">'RESIDE'</span>, color =<span class="string">'red'</span>, lw=<span class="number">1</span>, marker = <span class="string">'o'</span>, markevery = <span class="number">0.1</span>, ms = <span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>可以看到，<strong>与上文一致，也是除去NA后取所有数据</strong>，然后调用<code>precision_recall_curve()</code>，为了避免过多的数据，<em>可能还有截断操作</em>，对于RESIDE，作者保留了precision, recall数组的前50000个数字。</p>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Distant Supervision </tag>
            
            <tag> PR-curve </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>matplotlib.plot()</title>
      <link href="/2021/01/03/matplotlib-plot/"/>
      <url>/2021/01/03/matplotlib-plot/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Property</th><th>Value Type</th></tr></thead><tbody><tr><td>alpha</td><td>float</td></tr><tr><td>animated</td><td>[True \</td><td>False]</td></tr><tr><td>antialiased or aa</td><td>[True \</td><td>False]</td></tr><tr><td>clip_box</td><td>a matplotlib.transform.Bbox instance</td></tr><tr><td>clip_on</td><td>[True \</td><td>False]</td></tr><tr><td>clip_path</td><td>a Path instance and a Transform instance, a Patch</td></tr><tr><td>color or c</td><td>any matplotlib color</td></tr><tr><td>contains</td><td>the hit testing function</td></tr><tr><td>dash_capstyle</td><td>[<code>&#39;butt&#39;</code></td><td><code>&#39;round&#39;</code></td><td><code>&#39;projecting&#39;</code>]</td></tr><tr><td>dash_joinstyle</td><td>[<code>&#39;miter&#39;</code></td><td><code>&#39;round&#39;</code></td><td><code>&#39;bevel&#39;</code>]</td></tr><tr><td>dashes</td><td>sequence of on/off ink in points</td></tr><tr><td>data</td><td>(np.array xdata, np.array ydata)</td></tr><tr><td>figure</td><td>a matplotlib.figure.Figure instance</td></tr><tr><td>label</td><td>any string</td></tr><tr><td>linestyle or ls</td><td>[ <code>&#39;-&#39;</code></td><td><code>&#39;--&#39;</code></td><td><code>&#39;-.&#39;</code></td><td><code>&#39;:&#39;</code></td><td><code>&#39;steps&#39;</code> \</td><td>…]</td></tr><tr><td>linewidth or lw</td><td>float value in points</td></tr><tr><td>marker</td><td>[ <code>&#39;+&#39;</code></td><td><code>&#39;,&#39;</code></td><td><code>&#39;.&#39;</code></td><td><code>&#39;1&#39;</code></td><td><code>&#39;2&#39;</code></td><td><code>&#39;3&#39;</code></td><td><code>&#39;4&#39;</code> ]</td></tr><tr><td>markeredgecolor or mec</td><td>any matplotlib color</td></tr><tr><td>markeredgewidth or mew</td><td>float value in points</td></tr><tr><td>markerfacecolor or mfc</td><td>any matplotlib color</td></tr><tr><td>markersize or ms</td><td>float</td></tr><tr><td>markevery</td><td>[ None \</td><td>integer \</td><td>(startind, stride) ]</td></tr><tr><td>picker</td><td>used in interactive line selection</td></tr><tr><td>pickradius</td><td>the line pick selection radius</td></tr><tr><td>solid_capstyle</td><td>[<code>&#39;butt&#39;</code></td><td><code>&#39;round&#39;</code></td><td><code>&#39;projecting&#39;</code>]</td></tr><tr><td>solid_joinstyle</td><td>[<code>&#39;miter&#39;</code></td><td><code>&#39;round&#39;</code></td><td><code>&#39;bevel&#39;</code>]</td></tr><tr><td>transform</td><td>a matplotlib.transforms.Transform instance</td></tr><tr><td>visible</td><td>[True \</td><td>False]</td></tr><tr><td>xdata</td><td>np.array</td></tr><tr><td>ydata</td><td>np.array</td></tr><tr><td>zorder</td><td>any number</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matplotlib </tag>
            
            <tag> plot </tag>
            
            <tag> 绘图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>两个句子之间的注意力机制</title>
      <link href="/2020/12/31/%E4%B8%A4%E4%B8%AA%E5%8F%A5%E5%AD%90%E4%B9%8B%E9%97%B4%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/2020/12/31/%E4%B8%A4%E4%B8%AA%E5%8F%A5%E5%AD%90%E4%B9%8B%E9%97%B4%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h1><p>首先需要清楚，注意力机制的来源是Encoder-Decoder模型，即Seq2Seq模型，从一个句子到另一个句子，从一个序列到另一个序列。</p><h1 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h1><p>怎么理解Attention模型的物理含义呢？一般在自然语言处理应用里<strong>会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型</strong>，这是非常有道理的。</p><p>如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p><p>我们可以这样来看待Attention机制：将Source中的构成元素想象成是由一系列的数据对构成，此时给定Target中的<strong>某个元素Query</strong>，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。</p><font color="purple">可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</font><a id="more"></a><h1 id="计算方式"><a href="#计算方式" class="headerlink" title="计算方式"></a>计算方式</h1><p><img src="/images/blog/2020/attention.jpg" alt></p><h1 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h1><p>通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。</p><p>在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。</p><p>如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。</p><p><img src="/images/blog/2020/attention_example.jpg" alt></p><p>从两张图（图11、图12）可以看出，<strong>Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）</strong>。</p><p>很明显，<font color="orange">引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征</font>，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p><p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，<em>Self Attention对于增加计算的并行性也有直接帮助作用</em>。这是为何Self Attention逐渐被广泛使用的主要原因。</p><h2 id="Self-Attention需要MASK吗？"><a href="#Self-Attention需要MASK吗？" class="headerlink" title="Self-Attention需要MASK吗？"></a>Self-Attention需要MASK吗？</h2><p><strong>self-attention的出现是为了摆脱循环神经网络不能并行计算的缺点而提出的</strong>，它的设计模式可以通过当前单词去查看其输入序列中的其他单词，以此来寻找编码这个单词更好的线索。</p><p><strong>而Transformer使用的Self-attention。顾名思义即句子中的每一个词都要和该句子当中的所有词进行一个attention计算，目的是学习句子内部的词依赖关系，获取词的内部结构。因此，从这个特点当中我们可以其实也能推测出Self-attention计算的信息来源都是来源于其句子本身。</strong></p><p>故在上述attention的计算公式当中，可以看出self-attention即Q=K=V。下面用Transformer的部分代码（Tensorflow）来进一步的了解Q，K，V在self-attention中的构成（以Encoder端为例）</p><p>以下是<code>train.py</code>中的部分代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="keyword">with</span> tf.variable_scope(<span class="string">"num_blocks_&#123;&#125;"</span>.format(i)):</span><br><span class="line"><span class="number">2</span>     self.enc = multihead_attention(queries=self.enc,</span><br><span class="line"><span class="number">3</span>                                    keys=self.enc,</span><br><span class="line"><span class="number">4</span>                                    num_units=hp.hidden_units,</span><br><span class="line"><span class="number">5</span>                                    num_heads=hp.num_heads,</span><br><span class="line"><span class="number">6</span>                                    dropout_rate=hp.dropout_rate,</span><br><span class="line"><span class="number">7</span>                                    is_traing=is_traing,</span><br><span class="line"><span class="number">8</span>                                    causality=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>train.py中的 self.enc代表的就是 encoder端经过word_embedding以及position_embedding之后的句子信息,<strong>之后用X来代替</strong>。<br>根据其传入 multihead_attention 函数中的参数来看，在机器翻译领域当中，Transformer当中的queries以及Keys都是其输入信息x。</p><p>以下是<code>module.py</code>中的部分代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span><span class="params">(queries,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="number">2</span>                         keys,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="number">3</span>                         num_units=None,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="number">4</span>                         num_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="number">5</span>                         dropout_rate=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="number">6</span>                         is_training=True,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="number">7</span>                         causality=False,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="number">8</span>                         scope=<span class="string">'multihead_attention'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="number">9</span>                         reuse=None)</span>:</span></span><br><span class="line"><span class="number">10</span>                        </span><br><span class="line"><span class="number">11</span>    <span class="keyword">with</span> tf.variable_scope(scope,reuse=reuse):</span><br><span class="line"><span class="number">12</span>        <span class="keyword">if</span> num_units <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"><span class="number">13</span>            num_units = queries.get_shape().as_list[<span class="number">-1</span>]</span><br><span class="line"><span class="number">14</span></span><br><span class="line"><span class="number">15</span>        Q = tf.layers.dense(queries,num_units,activation=tf.nn.relu)</span><br><span class="line"><span class="number">16</span>        K = tf.layers.dense(keys,num_units,activation=tf.nn.relu)</span><br><span class="line"><span class="number">17</span>        V = tf.layers.dense(keys,num_units,activation=tf.nn.relu)</span><br></pre></td></tr></table></figure><p>而在module.py文件当中，我们从矩阵Q，K，V的计算公式中我们可以发现：<br>Q是将queries输入进一个节点数为num_units的前馈神经网络之后得到的矩阵<br>而K，V则是将keys输入进一个节点数为num_units的前馈神经网络之后得到的矩阵。<br>结合所有的信息，在机器翻译领域当中，Q,K,V的所有来源就是encoder端的输入X。即可以看成Q=K=V<br>具体计算过程如下图所示:</p><p><img src="/images/blog/2020/attention.png" alt></p><font color="orange">即将句子的embedding序列投入三个全连接层，分别得到Q，K，V</font>在这里也顺便提一下muilti_head的概念，Multi_head self_attention的意思就是重复以上过程多次，论文当中是重复8次，即8个Head，使用多套（WQ，WK，WV）矩阵(只要在初始化的时候多稍微变一下，很容易获得多套权重矩阵)。获得多套（Q，K，V）矩阵，然后进行attention计算时便能获得多个self_attention矩阵。self-attention之后紧接着的步骤是前馈神经网络，而前馈神经网络接受的是单个的矩阵向量，而不是多个矩阵，因此需要把计算得到的多个self-attention矩阵采用某种方式进行合并。因此文章中给出的思路是将这多个连接在一起（可能是简单就是好，奥卡姆剃刀原则）再和一个矩阵W0相乘。步骤如下图：![](/images/blog/2020/multi-head.png)综合上述说法，multi_layer_self-attention的整体计算流程如下图所示:![](/images/blog/2020/attention-procedure.png)self-attention在神经机器翻译实际的操作设计当中，***不仅仅是由上面self-attention计算公式那般设计，其中还要加入Mask操作。***其中在Encoder端和Decoder端都需要使用的Mask操作，称之为PADDING MASK。而仅仅在Decoder段使用的Mask操作，则被称之为Sequence MASK。下面将结合代码分别对其进行介绍。## PADDING MASK我们在训练的过程中，自然语言数据往往都是以Batch的形式输入进的模型，而一个batch中的每一句话不能保证长度都是一样的，所以需要使用PADDING的方式将所有的句子都补全到最长的长度，比如拿0进行填充，但是我们知道这种用0填充的位置的信息是完全没有意义的，因此我们希望这个位置不参与后期的反向传播过程。以此避免最后影响模型自身的效果，因此提出了在训练时将补全的位置给Mask掉的做法。而在Self-attention的计算当中，我们自然也不希望有效词的注意力集中在这些没有意义的位置上，因此使用了PADDING MASK的方式.PADDING MASK在attention的计算过程中处于softmax之前(图1中的opt表示optional即该层可加可不加,若是不想使用PADDING MASK操作,则直接Softmax就完事了)，**通过PADDING MASK的操作，使得补全位置上的值成为一个非常大的负数（可以是负无穷），这样的话，经过Softmax层的时候，这些位置上的概率就是0**。以此操作就相当于把补全位置的无用信息给遮蔽掉了（Mask掉了）。具体操作见如下代码（请和注释一起食用）:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span><span class="params">(queries,</span></span></span><br><span class="line"><span class="function"><span class="params">                        keys,</span></span></span><br><span class="line"><span class="function"><span class="params">                        num_units=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                        num_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        dropout_rate=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        is_traing=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                        causality=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                        scope=<span class="string">'multihead_attention'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        reuse=None)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope,reuse=reuse):</span><br><span class="line">        <span class="keyword">if</span> num_units <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            num_units = queries.get_shape().as_list[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># Q，K，V的shape均为（batch_size, max_length, hidden_size）</span></span><br><span class="line">        Q = tf.layers.dense(queries,num_units,activation=tf.nn.relu)</span><br><span class="line">        K = tf.layers.dense(keys,num_units,activation=tf.nn.relu)</span><br><span class="line">        V = tf.layers.dense(keys,num_units,activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过tf.split将Q，K，按照最后一维切分成num_heads份，然后按第一维度进行拼接，</span></span><br><span class="line"><span class="comment"># 以此达到“多头的效果”，此时的Q_就相当于num_heads个Q的拼接，其余同理。</span></span><br><span class="line">        Q_ = tf.concat(tf.split(Q,num_heads,axis=<span class="number">2</span>),axis=<span class="number">0</span>)</span><br><span class="line">        K_ = tf.concat(tf.split(K,num_heads,axis=<span class="number">2</span>),axis=<span class="number">0</span>)</span><br><span class="line">        V_ = tf.concat(tf.split(V,num_heads,axis=<span class="number">2</span>),axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 这便是图2公式中SoftMax部分的内容</span></span><br><span class="line">        A = tf.transpose(K_,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">        outputs = tf.matmul(Q_,tf.transpose(K_,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]))</span><br><span class="line">        outputs = outputs / (K_.get_shape().as_list()[<span class="number">-1</span>]**<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下是PADDING MASK的过程 不管是在Encoder的计算中还是Decoder的计算当中都会使用。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个是整个PADDING MASK的核心知识点,当输入的自然语言句子转换成embedding向量的时候.</span></span><br><span class="line"><span class="comment"># 将用0填充的位置(PADDING)进行embedding的时候全部使用0来代替:</span></span><br><span class="line"><span class="comment"># 举个例子:若一句自然语言用0填充之后的表示方式为[1,2,0,0],若embedding_size选择为2</span></span><br><span class="line"><span class="comment"># vocab_size为3.我们的lookup_table则初始化为如下的一个3*3的矩阵(第一行必须全部为0):</span></span><br><span class="line"><span class="comment"># [[0,0,0],</span></span><br><span class="line"><span class="comment"># [-0.12,0.21,0.32],</span></span><br><span class="line"><span class="comment"># [0.14,0.24,-0.32]]</span></span><br><span class="line"><span class="comment"># 那么经过tf.nn.embedding_lookup()之后 原自然语言句子就被embedding变成了如下形式</span></span><br><span class="line"><span class="comment"># [1,2,0,0]-&gt;[[-0.12,0.21,0.32],</span></span><br><span class="line"><span class="comment"># [0.14,0.24,-0.32],</span></span><br><span class="line"><span class="comment"># [0,0,0],</span></span><br><span class="line"><span class="comment"># [0,0,0]]</span></span><br><span class="line"><span class="comment"># 这样的话,通过如下的这几步的操作,便能将与key_masks为0的位置(即填充的位置)一样的output的位置变成一个特别小的一个负数.</span></span><br><span class="line"><span class="comment"># 这样,经过后期的Softmax的时候,便能将该填充位置的输出变成0,以此来防止因为填充位置的无用信息影响模型的效果</span></span><br><span class="line"><span class="comment"># 如果在最开始的embedding的同时没有使用0元素进行遮盖(即lookup_table矩阵第一行不为0,而是一些别的随机数),那么PADDING_MASK将不起作用.</span></span><br><span class="line">        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=<span class="number">-1</span>)))</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># 接下来的两步操作只是为了能够让key_masks的维度能够和outputs匹配</span></span><br><span class="line">        key_masks = tf.tile(key_masks, [num_heads, <span class="number">1</span>])</span><br><span class="line">        key_masks = tf.tile(tf.expand_dims(key_masks, <span class="number">1</span>), [<span class="number">1</span>, tf.shape(queries)[<span class="number">1</span>], <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># paddings里面的值都是非常小的负数，当key_masks矩阵的某个变量和0相等的时候，将同样位置上的outputs的值变成一个特别小的数.</span></span><br><span class="line">        paddings = tf.ones_like(outputs)*(<span class="number">-2</span>**<span class="number">32</span>+<span class="number">1</span>)</span><br><span class="line">        outputs = tf.where(tf.equal(key_masks, <span class="number">0</span>), paddings, outputs)</span><br><span class="line">        </span><br><span class="line">        outputs = tf.nn.softmax(outputs)</span><br><span class="line">        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries,axis=<span class="number">-1</span>)))</span><br><span class="line">        query_masks = tf.tile(query_masks,[num_heads,<span class="number">1</span>])</span><br><span class="line">        query_masks = tf.tile(tf.expand_dims(query_masks,<span class="number">-1</span>),[<span class="number">1</span>,<span class="number">1</span>,tf.shape(keys)[<span class="number">1</span>]])</span><br><span class="line">        outputs *= query_masks</span><br></pre></td></tr></table></figure>在进行softmax之后,源码当中又加入了一个query_mask.*原因在于若不是self-attention的情况下,query和key的第二维可能不同(两者句子长度不同)可能会出现一个填充,一个没有填充的状况出现,因此需要分情况讨论.*但是keys_mask和query_mask都是一样的目的.## Sequence MASK**Sequence Mask只在Decoder端进行**,目的是为了使得decoder不能看见未来的信息.也就是对于一个序列中的第i个token,解码的时候只能够依靠i时刻之前(包括i)的输出,而不能依赖于i时刻之后的输出.因此我们要采取一个遮盖的方法(Mask)使得其在计算self-attention的时候只用i个时刻之前的token进行计算,因为Decoder是用来做预测的,而在训练预测能力的时候,我们不能够"提前看答案",因此要将未来的信息给遮盖住.效果如下图所示:![](/images/blog/2020/我爱中国.png)图中灰色部分即为mask的部分.<font color="orange">**再次强调,sequence mask 操作只发生在decoder阶段。**</font><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">         <span class="comment"># 遮盖的方法和之前的PADDING_MASK的操作方法基本一致,不一样的只是masks矩阵中的元素不一样,masks矩阵是一个</span></span><br><span class="line"><span class="comment"># 下三角矩阵,即对角线以及对角线一下是1,对角线以上全为0.之后同样将outputs位置对于Masks矩阵为0的位置上的元素统统</span></span><br><span class="line"><span class="comment"># 替换成一个很小的负数,这样之后在经过softmax的时候就能够将mask部分的self-attention的计算值变成0.以此来达到遮盖未来信息的目的</span></span><br><span class="line">         diag_vals = tf.ones_like(outputs[<span class="number">0</span>, :, :])</span><br><span class="line">         tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()</span><br><span class="line">         masks = tf.tile(tf.expand_dims(tril, <span class="number">0</span>), [tf.shape(outputs)[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">         paddings = tf.ones_like(masks)*(<span class="number">-2</span>**<span class="number">32</span>+<span class="number">1</span>)</span><br><span class="line">         outputs = tf.where(tf.equal(masks, <span class="number">0</span>), paddings, outputs)</span><br></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p><a href="https://www.codenong.com/cs106541686/" target="_blank" rel="noopener">注意力机制的基本思想和实现原理</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
            <tag> 注意力 </tag>
            
            <tag> Self-Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ridge regression 岭回归</title>
      <link href="/2020/12/30/ridge-regression-%E5%B2%AD%E5%9B%9E%E5%BD%92/"/>
      <url>/2020/12/30/ridge-regression-%E5%B2%AD%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<p>已经有多次看到岭回归 (ridge regression) 这个名词了，因此，在这里对这个概念进行一个更深入的学习。</p><h1 id="一、一般线性回归遇到的问题"><a href="#一、一般线性回归遇到的问题" class="headerlink" title="一、一般线性回归遇到的问题"></a>一、一般线性回归遇到的问题</h1><p> 在处理复杂的数据的回归问题时，普通的线性回归会遇到一些问题，主要表现在：</p><ul><li>预测精度：这里要处理好这样一对为题，即样本的数量<img src="http://latex.codecogs.com/gif.latex?n" alt="n">和特征的数量<img src="http://latex.codecogs.com/gif.latex?p" alt="p"><ul><li><img src="http://latex.codecogs.com/gif.latex?n\gg&space;p" alt="n\gg p">时，最小二乘回归会有较小的方差</li><li><img src="http://latex.codecogs.com/gif.latex?n\approx&space;p" alt="n\approx p">时，容易产生过拟合</li><li><img src="http://latex.codecogs.com/gif.latex?n%3C&space;p" alt="n&lt; p">时，最小二乘回归得不到有意义的结果</li></ul></li><li>模型的解释能力：如果模型中的特征之间有相互关系，这样会增加模型的复杂程度，并且对整个模型的解释能力并没有提高，这时，我们就要进行特征选择。</li></ul><p>以上的这些问题，主要就是表现在模型的方差和偏差问题上，这样的关系可以通过下图说明：</p><p><img src="/images/blog/2020/regression.jpg" alt></p><p>方差指的是模型之间的差异，而偏差指的是模型预测值和数据之间的差异。我们需要找到方差和偏差的折中。</p><a id="more"></a><h1 id="二、岭回归的概念"><a href="#二、岭回归的概念" class="headerlink" title="二、岭回归的概念"></a>二、岭回归的概念</h1><p>​    在进行特征选择时，一般有三种方式：</p><ul><li>子集选择</li><li>收缩方式(Shrinkage method)，又称为正则化(Regularization)。主要包括岭回归个lasso回归。</li><li>维数缩减</li></ul><p>​    <font color="orange">岭回归(Ridge Regression)是在平方误差的基础上增加正则项</font></p><script type="math/tex; mode=display">\sum_{i=1}^n (y_i - \sum_{j=0}^p w_jx_{ij})^2 + \lambda \sum_{j=0}^p w_j^2  \quad \lambda  > 0</script><p>通过确定<img src="http://latex.codecogs.com/gif.latex?\lambda" alt="\lambda">的值可以使得在方差和偏差之间达到平衡：随着<img src="http://latex.codecogs.com/gif.latex?\lambda" alt="\lambda">的增大，模型方差减小而偏差增大。</p><p>对$w$求导，结果为：</p><script type="math/tex; mode=display">2X^T(Y-XW) - 2\lambda W</script><p>令其为0，可求得$w$的值：</p><script type="math/tex; mode=display">\hat w = (X^TX + \lambda I)^{-1}X^TY</script>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> regression </tag>
            
            <tag> 回归 </tag>
            
            <tag> ridge regression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenIE初探</title>
      <link href="/2020/12/29/OpenIE%E5%88%9D%E6%8E%A2/"/>
      <url>/2020/12/29/OpenIE%E5%88%9D%E6%8E%A2/</url>
      
        <content type="html"><![CDATA[<p>开放关系抽取，指没有给定预定义的关系本体 (ontology)，从文本或网页上直接抽取phrases，是一类无监督的任务。这篇博客对开放关系抽取进行了一个初步的了解。</p><h1 id="OpenIE"><a href="#OpenIE" class="headerlink" title="OpenIE"></a>OpenIE</h1><h2 id="ReVerb"><a href="#ReVerb" class="headerlink" title="ReVerb"></a><a href="http://reverb.cs.washington.edu/" target="_blank" rel="noopener">ReVerb</a></h2><p>是OpenIE的一个子项目，都是主要由华盛顿大学开发研制的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>OpenIE主要能够抽取一些头尾实体之间距离较短的关系短语，主要是利用语法信息和依赖信息。</p><p><img src="/images/blog/2020/openie.png" alt></p><p><strong>可以借鉴其方法，并作为资源加以利用</strong>。</p><a id="more"></a><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p>斯坦福OpenIE：</p><p><a href="https://stanfordnlp.github.io/CoreNLP/openie.html" target="_blank" rel="noopener">https://stanfordnlp.github.io/CoreNLP/openie.html</a></p><p><a href="https://nlp.stanford.edu/software/openie.html" target="_blank" rel="noopener">https://nlp.stanford.edu/software/openie.html</a></p><p>华盛顿大学OpenIE:</p><p><a href="https://github.com/allenai/openie-standalone" target="_blank" rel="noopener">https://github.com/allenai/openie-standalone</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Relation Extraction </tag>
            
            <tag> OpenIE </tag>
            
            <tag> Information Extraction </tag>
            
            <tag> RE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python调用.sh脚本</title>
      <link href="/2020/12/28/python%E8%B0%83%E7%94%A8-sh%E8%84%9A%E6%9C%AC/"/>
      <url>/2020/12/28/python%E8%B0%83%E7%94%A8-sh%E8%84%9A%E6%9C%AC/</url>
      
        <content type="html"><![CDATA[<p>在使用服务器跑程序时，为了充分利用服务器显卡的时间，希望机器可以24小时运行，因此需要用python自动的执行一些进程，来调参~</p><h1 id="os-system"><a href="#os-system" class="headerlink" title="os.system"></a>os.system</h1><p>可以使用os.system()执行bash命令，举个简单的例子，比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">print(os.system(<span class="string">"touch a.txt"</span>))</span><br></pre></td></tr></table></figure><p>会返回一个0，表示执行成功了，在当前文件夹下创建了一个新的a.txt文件</p><h1 id="python脚本"><a href="#python脚本" class="headerlink" title="python脚本"></a>python脚本</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">r1 = os.system(</span><br><span class="line">    <span class="string">"python -u run.py --description='NYT-H training with bert on sentence-level' --task_name='sent' "</span></span><br><span class="line">    <span class="string">"--model_name='sent_bert' --data_dir='./data/NYT-H/' --eval_model='best' --embedding_path='../Glove/glove.6B.50d.txt' "</span></span><br><span class="line">    <span class="string">"--overwrite_output_dir --output_dir='outputs/sent_bert_200' --seed=77 --embedding_dim=50 --main_device='cuda:0' "</span></span><br><span class="line">    <span class="string">"--device_ids='[0]' --epoch=1 --batch_size=16 --dropout_rate=0.5 --lr=5e-5 --max_len=200 --pos_dis=75 --pos_dim=5 "</span></span><br><span class="line">    <span class="string">"--save_best_model --do_eval --do_train --select_score='non_na_macro_f1' --tb_logging_step=1000 --encoder=bert "</span></span><br><span class="line">    <span class="string">"--pretrain=../BERT/bert-base-cased-nyt --split --threshold=0.9"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">r2 = os.system(</span><br><span class="line">    <span class="string">"python -u run.py --description='NYT-H training with bert on sentence-level' --task_name='sent' "</span></span><br><span class="line">    <span class="string">"--model_name='sent_bert' --data_dir='./data/NYT-H/' --eval_model='best' --embedding_path='../Glove/glove.6B.50d.txt' "</span></span><br><span class="line">    <span class="string">"--overwrite_output_dir --output_dir='outputs/sent_bert_200_sh_try' --seed=77 --embedding_dim=50 --main_device='cuda:0' "</span></span><br><span class="line">    <span class="string">"--device_ids='[0]' --epoch=10 --batch_size=16 --dropout_rate=0.5 --lr=5e-5 --max_len=200 --pos_dis=75 --pos_dim=5 "</span></span><br><span class="line">    <span class="string">"--save_best_model --do_eval --do_train --select_score='non_na_macro_f1' --tb_logging_step=1000 --encoder=bert "</span></span><br><span class="line">    <span class="string">"--pretrain=../BERT/bert-base-cased-nyt"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>第一个程序执行完之后，会紧接着执行第二个程序。<strong>如果第一个程序因为异常退出了，也不会影响第二个程序的执行~</strong>，完美🎉🎉🎉</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bash </tag>
            
            <tag> python </tag>
            
            <tag> Linux </tag>
            
            <tag> .sh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对NYT数据集二分类</title>
      <link href="/2020/12/27/%E5%AF%B9NYT%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BA%8C%E5%88%86%E7%B1%BB/"/>
      <url>/2020/12/27/%E5%AF%B9NYT%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BA%8C%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<p>在最近的实验中，考虑到远程监督数据集NYT的去噪问题，设想能否利用预训练语言模型的强大能力，对数据集的噪音进行一个预先的划分，进行二分类的尝试。</p><h1 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h1><p>以56种关系作为POS，以NA关系作为NEG，进行0-1的二分类学习，编码器采用BERT，由于类别不平衡，对负样本进行采样，不超过正样本的类别数。由于训练数据过多，训练时长太久，于是，对正负样本各取10000个，然后进行实验。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">TRAINING: <span class="number">625</span>it [<span class="number">02</span>:<span class="number">56</span>,  <span class="number">3.54</span>it/s]</span><br><span class="line">TESTING: <span class="number">403</span>it [<span class="number">00</span>:<span class="number">37</span>, <span class="number">10.72</span>it/s]</span><br><span class="line">epoch <span class="number">0</span>, loss <span class="number">0.3108509671562363</span>, accuracy <span class="number">0.8750775915580384</span></span><br><span class="line">TRAINING: <span class="number">625</span>it [<span class="number">02</span>:<span class="number">58</span>,  <span class="number">3.50</span>it/s]</span><br><span class="line">TESTING: <span class="number">403</span>it [<span class="number">00</span>:<span class="number">37</span>, <span class="number">10.69</span>it/s]</span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.2790242659515024</span>, accuracy <span class="number">0.8945530726256983</span></span><br><span class="line">TRAINING: <span class="number">573</span>it [<span class="number">02</span>:<span class="number">44</span>,  <span class="number">3.53</span>it/s]</span><br><span class="line">TRAINING: <span class="number">625</span>it [<span class="number">02</span>:<span class="number">59</span>,  <span class="number">3.49</span>it/s]</span><br><span class="line">TESTING: <span class="number">403</span>it [<span class="number">00</span>:<span class="number">37</span>, <span class="number">10.69</span>it/s]</span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.27044483414959747</span>, accuracy <span class="number">0.9058038485412787</span></span><br><span class="line">TRAINING: <span class="number">625</span>it [<span class="number">02</span>:<span class="number">58</span>,  <span class="number">3.50</span>it/s]</span><br><span class="line">TESTING: <span class="number">403</span>it [<span class="number">00</span>:<span class="number">37</span>, <span class="number">10.72</span>it/s]</span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.3372199231719427</span>, accuracy <span class="number">0.8947082557417753</span></span><br><span class="line">TRAINING: <span class="number">625</span>it [<span class="number">02</span>:<span class="number">58</span>,  <span class="number">3.50</span>it/s]</span><br><span class="line">TESTING: <span class="number">403</span>it [<span class="number">00</span>:<span class="number">37</span>, <span class="number">10.67</span>it/s]</span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.35960243281818205</span>, accuracy <span class="number">0.8975791433891993</span></span><br><span class="line">TRAINING: <span class="number">625</span>it [<span class="number">02</span>:<span class="number">58</span>,  <span class="number">3.50</span>it/s]</span><br><span class="line">TESTING: <span class="number">403</span>it [<span class="number">00</span>:<span class="number">37</span>, <span class="number">10.68</span>it/s]</span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.3635644751425287</span>, accuracy <span class="number">0.8998292985723153</span></span><br><span class="line">TRAINING: <span class="number">625</span>it [<span class="number">02</span>:<span class="number">58</span>,  <span class="number">3.51</span>it/s]</span><br><span class="line">TESTING: <span class="number">403</span>it [<span class="number">00</span>:<span class="number">37</span>, <span class="number">10.68</span>it/s]</span><br><span class="line">epoch <span class="number">6</span>, loss <span class="number">0.43025128015637093</span>, accuracy <span class="number">0.9021570453134699</span></span><br><span class="line">TRAINING: <span class="number">625</span>it [<span class="number">02</span>:<span class="number">58</span>,  <span class="number">3.50</span>it/s]</span><br><span class="line">TESTING: <span class="number">403</span>it [<span class="number">00</span>:<span class="number">37</span>, <span class="number">10.68</span>it/s]</span><br><span class="line">epoch <span class="number">7</span>, loss <span class="number">0.5122146993227276</span>, accuracy <span class="number">0.8918373680943513</span></span><br></pre></td></tr></table></figure><p>可以看到，分类准确率（accuracy）在90%左右，具体应用到实验中效果会怎么样呢？还有待实验~</p><h2 id="全量数据"><a href="#全量数据" class="headerlink" title="全量数据"></a>全量数据</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">epoch <span class="number">0</span>, loss <span class="number">0.19445358777635965</span>, accuracy <span class="number">0.9276846679081316</span></span><br><span class="line">BIMODEL.pkl saved, best accuracy <span class="number">0.9276846679081316</span></span><br><span class="line">TRAINING: <span class="number">8560</span>it [<span class="number">40</span>:<span class="number">00</span>,  <span class="number">3.57</span>it/s]</span><br><span class="line">TESTING: <span class="number">403</span>it [<span class="number">00</span>:<span class="number">36</span>, <span class="number">11.01</span>it/s]</span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.2355663737937318</span>, accuracy <span class="number">0.9197703289882061</span></span><br><span class="line">TRAINING: <span class="number">8560</span>it [<span class="number">39</span>:<span class="number">57</span>,  <span class="number">3.57</span>it/s]</span><br><span class="line">TESTING: <span class="number">403</span>it [<span class="number">00</span>:<span class="number">36</span>, <span class="number">11.00</span>it/s]</span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.29970387574681023</span>, accuracy <span class="number">0.914804469273743</span></span><br><span class="line">TRAINING: <span class="number">8560</span>it [<span class="number">40</span>:<span class="number">02</span>,  <span class="number">3.56</span>it/s]</span><br><span class="line">TESTING: <span class="number">403</span>it [<span class="number">00</span>:<span class="number">36</span>, <span class="number">11.00</span>it/s]</span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.2967500277536646</span>, accuracy <span class="number">0.9183736809435133</span></span><br></pre></td></tr></table></figure><p>可以看到，准确率可以接近93%，挺不错的~</p>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> Relation Extraction </tag>
            
            <tag> NYT </tag>
            
            <tag> Distant Supervision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二分类损失</title>
      <link href="/2020/12/27/%E4%BA%8C%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1/"/>
      <url>/2020/12/27/%E4%BA%8C%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1/</url>
      
        <content type="html"><![CDATA[<p>在用pytorch进行二分类时，最后网络的输出经过sigmoid()函数，得到形如(batch, )的tensor，该使用什么损失函数呢？</p><h1 id="BCELoss"><a href="#BCELoss" class="headerlink" title="BCELoss"></a>BCELoss</h1><p>Creates a criterion that measures the Binary Cross Entropy between the target and the output:</p><blockquote><p>Input: (N, *)</p><p>Output: (N, *)</p></blockquote><p>可以用于指导二分类</p><h1 id="L1Loss"><a href="#L1Loss" class="headerlink" title="L1Loss"></a>L1Loss</h1><p>Creates a criterion that measures the mean absolute error (MAE) between each element in the input x<em>x</em> and target y<em>y</em>.</p><blockquote><p>Input: (N, *)</p><p>Output: (N, *)</p></blockquote><p>看起来也可以</p><h1 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h1><p>个人理解是，一个是建模为分类问题，一个是建模为回归问题。看任务更适合哪一个，就采用哪一个就可以。</p>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> BCELoss </tag>
            
            <tag> L1Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch常用代码框架</title>
      <link href="/2020/12/27/pytorch%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81%E6%A1%86%E6%9E%B6/"/>
      <url>/2020/12/27/pytorch%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81%E6%A1%86%E6%9E%B6/</url>
      
        <content type="html"><![CDATA[<p>在用pytorch做训练的时候，总是会重复写一些代码，比如数据加载、训练过程等，为了避免重复造轮子，把之前一个二分类的项目放在这里，将来想要用的时候直接取，避免重复劳动🎇🎨🎃</p><h1 id="helper-py"><a href="#helper-py" class="headerlink" title="helper.py"></a>helper.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BISET</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, file, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.data = self._get_data_(file)</span><br><span class="line">        self.tokenizer = BertTokenizer.from_pretrained(config.pretrain)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[item]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_data_</span><span class="params">(file)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> json.load(open(file, encoding=<span class="string">'utf-8'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_generate_batch_</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        token2ids, masks, labels = [], [], []</span><br><span class="line">        <span class="keyword">for</span> record <span class="keyword">in</span> data:</span><br><span class="line">            id = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(record[<span class="string">'text'</span>]))</span><br><span class="line">            h_id = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(record[<span class="string">'h'</span>][<span class="string">'name'</span>]))</span><br><span class="line">            t_id = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(record[<span class="string">'t'</span>][<span class="string">'name'</span>]))</span><br><span class="line">            <span class="comment"># [CLS] + head + [unused6] + tail + [unused7] + sentence + [SEP]</span></span><br><span class="line">            representation = [<span class="number">101</span>] + h_id + [<span class="number">7</span>] + t_id + [<span class="number">8</span>] + id + [<span class="number">102</span>]</span><br><span class="line">            token2ids.append(representation)</span><br><span class="line">        max_len= min(max([len(id) <span class="keyword">for</span> id <span class="keyword">in</span> token2ids]), <span class="number">150</span>)</span><br><span class="line">        <span class="keyword">for</span> i, id <span class="keyword">in</span> enumerate(token2ids):</span><br><span class="line">            mask = ([<span class="number">1</span>] * len(id) + [<span class="number">0</span>] * max_len)[:max_len]</span><br><span class="line">            masks.append(mask)</span><br><span class="line">            token2ids[i] = (id + [<span class="number">0</span>] * max_len)[:max_len]</span><br><span class="line">            <span class="keyword">if</span> data[i][<span class="string">'relation'</span>] == <span class="string">'NA'</span>:</span><br><span class="line">                labels.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(token2ids).cuda(), torch.tensor(masks).cuda(), torch.tensor(labels).cuda()</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BI</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(config.pretrain)</span><br><span class="line">        self.dense = nn.Linear(</span><br><span class="line">            in_features=<span class="number">768</span>,</span><br><span class="line">            out_features=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.non_linear = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, token2ids, masks)</span>:</span></span><br><span class="line">        x = self.bert(token2ids, masks)</span><br><span class="line">        x = x[<span class="number">1</span>]</span><br><span class="line">        x = self.dense(x).squeeze()</span><br><span class="line">        x = self.non_linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h1 id="config-py"><a href="#config-py" class="headerlink" title="config.py"></a>config.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">config</span>:</span></span><br><span class="line">    train_data = <span class="string">'train.json'</span></span><br><span class="line">    test_data = <span class="string">'test.json'</span></span><br><span class="line">    pretrain = <span class="string">'../BERT/bert-base-cased-nyt'</span></span><br><span class="line">    epoch = <span class="number">5</span></span><br><span class="line">    lr = <span class="number">0.00001</span></span><br><span class="line">    threshold = <span class="number">0.5</span></span><br><span class="line">    batch_size = <span class="number">32</span></span><br></pre></td></tr></table></figure><h1 id="run-py"><a href="#run-py" class="headerlink" title="run.py"></a>run.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">from</span> helper <span class="keyword">import</span> BISET</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> BI</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(config)</span>:</span></span><br><span class="line">    train_set = BISET(config.train_data, config)</span><br><span class="line">    train_loader = DataLoader(train_set, shuffle=<span class="literal">True</span>, batch_size=config.batch_size, collate_fn=train_set._generate_batch_)</span><br><span class="line">    test_set = BISET(config.test_data, config)</span><br><span class="line">    test_loader = DataLoader(test_set, shuffle=<span class="literal">False</span>, batch_size=config.batch_size, collate_fn=test_set._generate_batch_)</span><br><span class="line">    model = BI(config).cuda()</span><br><span class="line">    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=<span class="number">0.01</span>)</span><br><span class="line">    loss_func = nn.BCELoss()  <span class="comment"># 二分类交叉熵损失</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(config.epoch):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> tqdm(enumerate(train_loader), desc=<span class="string">'TRAINING'</span>, ncols=<span class="number">60</span>):</span><br><span class="line">            token2ids, masks, labels = data</span><br><span class="line">            logits = model(token2ids, masks)</span><br><span class="line">            loss = loss_func(logits, labels.type(dtype=torch.float))</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        model.eval()</span><br><span class="line">        loss_array, preds_array, labels_array = [], [], []</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> tqdm(enumerate(test_loader), desc=<span class="string">'TESTING'</span>, ncols=<span class="number">60</span>):</span><br><span class="line">            token2ids, masks, labels = data</span><br><span class="line">            logits = model(token2ids, masks)</span><br><span class="line">            loss = loss_func(logits, labels.type(dtype=torch.float))</span><br><span class="line">            preds = (logits &gt; config.threshold).type(dtype=torch.int)</span><br><span class="line">            loss_array.append(loss.item())</span><br><span class="line">            preds_array += preds.tolist()</span><br><span class="line">            labels_array += labels.tolist()</span><br><span class="line">        accuracy = np.sum(np.asarray(preds_array) == np.asarray(labels_array)) / len(labels_array)</span><br><span class="line">        print(<span class="string">f'epoch <span class="subst">&#123;epoch&#125;</span>, loss <span class="subst">&#123;np.mean(loss_array)&#125;</span>, accuracy <span class="subst">&#123;accuracy&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    run(config)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 框架 </tag>
            
            <tag> 二分类 </tag>
            
            <tag> sigmoid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>将GAN用于Domain Adaptation</title>
      <link href="/2020/12/26/%E5%B0%86GAN%E7%94%A8%E4%BA%8EDomain-Adaption/"/>
      <url>/2020/12/26/%E5%B0%86GAN%E7%94%A8%E4%BA%8EDomain-Adaption/</url>
      
        <content type="html"><![CDATA[<h1 id="SimGAN"><a href="#SimGAN" class="headerlink" title="SimGAN"></a>SimGAN</h1><p><img src="/images/blog/2020/DCGAN/simgan.png" alt></p><p>理解：将输入的合成图片投入生成器中，首先编码压缩为向量，然后转置卷积，再复原为图片。然后用判别器区分图像是真实图片还是合成图片。最终目的是使判别器无法区分真实图片和合成图片。</p><h2 id="关键点："><a href="#关键点：" class="headerlink" title="关键点："></a>关键点：</h2><ol><li>输入生成器的是合成图片，不再是随机噪音（隐变量）</li><li>添加损失$l_{reg} = || \phi(\widetilde{\mathrm{x}}) - \mathrm{x}||_1$，逐像素误差。使生成的图像保留标注信息。</li></ol><h2 id="生成器结构"><a href="#生成器结构" class="headerlink" title="生成器结构"></a>生成器结构</h2><p>以gaze的实验为例，refiner网络<img src="https://private.codecogs.com/gif.latex?R_%5Ctheta" alt="R_\theta">主体是残差结构，4个残差块(residual block)叠加，每个残差块中包含两个卷积层，卷积核大小3<em>3，输出64个特征图。最后的残差块接一个卷积核大小1</em>1的卷积层，输出一张特征图作为refined synthetic image，如下图所示：左侧为自己画的具体结构图，右侧为论文中残差模块的示意图。</p><p>注意：看了一下代码，残差应该是和残差块输入进行了element-wise相加，因此有相同的通道数。所以在残差模块前必须将输入图像batch变换到64个channels。代码里是通过卷积核为3*3，无步幅(因此尺度不变)的卷积实现的，共64个卷积核，因此得到的特征图是64个。然后才输入到四个连续的残差块中。因此下图input后应更改为加入一个卷积层，然后再进入ResNet Block。</p><p><img src="/images/blog/2020/DCGAN/simganrefiner.png" alt></p><a id="more"></a><h1 id="DANN"><a href="#DANN" class="headerlink" title="DANN"></a>DANN</h1><p>Domain-Adversarial Training of Neural Networks（DANN）[1]。这篇文章可以说是最早将对抗网络的思想引入到迁移学习领域中的，并且引发了很大的反响。后续有很多知名的研究者都跟进，在这篇文章的基础上进行了一些列改进和拓展。</p><p><strong>一、简介</strong></p><p>与上一次介绍的文章不同，在这篇论文中，作者的工作重心放在了之前提到的迁移学习三个核心问题之一：How to transfer。其实作者在这篇文章中做的是域适配的工作（Domain adaptation），下面我们先简单介绍一下Domain adaptation 的主要目的是什么。</p><p>想象一个分类任务，假设现在我们手里有源域数据和目标域数据，其中源域数据是丰富并且有标记的，而目标域数据是充足但是没有标记的，但是源域和目标域的特征空间和标记空间相同。很显然，我们可以轻松的利用源域数据为源域建立一个分类器，但是由于目标域数据本身没有标记，那么我们无法通过常规方法为目标域构建分类器，这时候Domain adaptation 就可以发挥作用了，具体过程如下图所示（手绘图，希望大家能看懂）：</p><p><img src="/images/blog/2020/DCGAN/DANN.jpg" alt></p><p>从图中可以看到，源域和目标域的特征都是{ <img src="https://www.zhihu.com/equation?tex=x_1%2Cx_2" alt="[公式]"> }，并且有相同的标记{-1，+1}，但是在目标域数据的标记是不可获得的。此外，虽然源域和目标域的特征空间相同，但是两者的特征分布却不同（两者的样本并不在特征空间的相同位置）。那么我们如何做到利用源域数据来帮助目标域进行分类呢？</p><p>这里最关键的一点就是在同一个特征空间中源域和目标域的分布不同，假设两者分布相同，那么我们就可以直接利用源域分类器对目标域数据进行分类了。因此，Domain adaptation的思想就是通过消除源域和目标域的分布差异，使得源域数据和目标域数据能同时被分开。在图一中，通过映射函数 <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="[公式]"> 将源域数据和目标域数据映射到某个空间中，并且将属于+1 的样本混合在一起，属于-1 的样本混合在一起。由于源域数据有标记，所以可以利用源域数据建立分类器。又因为源域数据和目标域数据混合在一起了，所以在分开源域样本的同时目标域样本也被分开了，这样任务就完成了。</p><h2 id="二、Domain-Adversarial-Training-of-Neural-Networks-DANN"><a href="#二、Domain-Adversarial-Training-of-Neural-Networks-DANN" class="headerlink" title="二、Domain-Adversarial Training of Neural Networks(DANN)"></a><strong>二、Domain-Adversarial Training of Neural Networks(DANN)</strong></h2><p>在上面的 Domain adaptation 过程中最关键的一点就是如何做到将源域样本和目标域样本混合在一起，并且还能保证被同时分开，DANN的主要任务也是做到这一点。</p><p>首先介绍一下DANN的应用背景：源域数据充足并且有标记，目标域数据充足但是无标记，源域和目标域的特征空间和标记空间相同，任务是借助源域数据对目标域数据进行分类。</p><p>DANN的结构如下：</p><p><img src="/images/blog/2020/DCGAN/DANNNET.jpg" alt></p><p>网络分为三部分：</p><ul><li>Feature extractor(特征提取器)</li></ul><p>特征提取器的功能包括两部分：（1）提取后续网络完成任务所需要的特征。（2）将源域样本和目标域样本进行映射和混合。</p><ul><li>Label predictor (类别预测器)</li></ul><p>利用Feature extractor提取的信息对样本进行分类。</p><ul><li>Domain classifier(域分类器)</li></ul><p>判断Feature extractor提取的信息来自源域还是目标域</p><p>可见，DANN中的对抗思想蕴含在特征提取器和域分类器中。其中特征提取器的作用就像是图1 中的映射函数 <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="[公式]"> 。但是我们并不知道 <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="[公式]"> 的具体形式，也就是说我们不知道特征提取器的训练目标是什么，无法用一个网络来拟合函数 <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="[公式]"> 。因此后面添加了域分类器，用来和特征提取器进行对抗训练。</p><p>特征提取器提取的信息会传入域分类器，之后域分类器会判断传入的信息到底是来自源域还是目标域，并计算损失。在反向传播更新参数的过程中，域分类器和特征提取器中间有一个梯度反转层（Gradient reversal layer），也就是说域分类器的训练目标是尽量将输入的信息分到正确的域类别（源域还是目标域），而特征提取器的训练目标却恰恰相反（由于梯度反转层的存在），特征提取器所提取的特征（或者说映射的结果）目的是是域判别器不能正确的判断出信息来自哪一个域，因此形成一种对抗关系。可见，当域分类器不能将接收的信息正确分为源域样本还是目标域样本时，特征提取器的任务就圆满完成了，因为此时源域样本和目标域样本在某个空间中已经被混合在一起不能分开了。</p><p><strong>然而上述过程存在一个问题，我们最终的目的是对目标域样本进行分类，那么我们如何保证特征提取器提取的信息是能够用来分类的呢？假如无论输入什么样本给特征提取器，它都输出一个单位向量，这样依旧可以“骗过”域分类器，但是却无法完成后续的分类工作。</strong></p><p>这时就要靠Label predictor (类别预测器)了，因为源域样本是有标记的，所以在提取特征时不仅仅要考虑后面的域判别器的情况，还要利用源域的带标记样本进行有监督训练从而兼顾分类的准确性。</p><h2 id="感受：训练神经网络一个很重要的地方就是给网络足够的有效偏置"><a href="#感受：训练神经网络一个很重要的地方就是给网络足够的有效偏置" class="headerlink" title="感受：训练神经网络一个很重要的地方就是给网络足够的有效偏置"></a>感受：训练神经网络一个很重要的地方就是给网络足够的有效偏置</h2><p>综上，当把特征提取器、域分类器和类别预测器都训练完成后，就可以做到把源域和目标域混合在一起并且进行分类了。</p><h1 id="是否可以借鉴DANN的思想，将编码器（BERT）直接输出融合的向量，不再使用弱小的多层感知机！同时想办法将判别器改为BERT。强强碰撞！"><a href="#是否可以借鉴DANN的思想，将编码器（BERT）直接输出融合的向量，不再使用弱小的多层感知机！同时想办法将判别器改为BERT。强强碰撞！" class="headerlink" title="是否可以借鉴DANN的思想，将编码器（BERT）直接输出融合的向量，不再使用弱小的多层感知机！同时想办法将判别器改为BERT。强强碰撞！"></a>是否可以借鉴DANN的思想，将编码器（BERT）直接输出融合的向量，不再使用弱小的多层感知机！同时想办法将判别器改为BERT。强强碰撞！</h1><h1 id="MADA"><a href="#MADA" class="headerlink" title="MADA"></a>MADA</h1><p>动机：现在的基于单个判别器对抗域自适应方法在进行源域和目标域数据分布对齐时没有利用数据分布底层的多模式结构。结果，源域和目标域的数据混淆，判别结构也会混淆，这就可能导致不同分布相应的区分结构错误对齐。比如文中的例子，源域中猫这一类可能错误地和目标域中狗这一类对齐：</p><p>这里我一直没有办法很好地理解文中multimode sructures是指什么，我目前的理解是源域和目标域中每一类对应之间（比如狗和狗）的分布。</p><p>贡献：提出了一种新的方法——MADA，基于多个域判别器捕获多模式结构从而可以对不同的数据分布进行细粒度的对齐。</p><p>在进行领域自适应时主要有以下两个挑战：</p><ol><li>怎么通过最大限度地匹配不同数据分布底层的多模式结构增强正迁移（源域和目标域相同类之间尽可能对齐）。</li><li>怎么通过防止跨域不同分布之间的错误对齐减轻负迁移。</li></ol><blockquote><p>知乎：</p><p><a href="https://zhuanlan.zhihu.com/p/272738019" target="_blank" rel="noopener">Multi-Adversarial Domain Adaptation阅读总结</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
          <category> GAN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> Adversarial Training </tag>
            
            <tag> SimGAN </tag>
            
            <tag> Domain Adaptation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用BERT计算语义相似度</title>
      <link href="/2020/12/24/%E7%94%A8BERT%E8%AE%A1%E7%AE%97%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
      <url>/2020/12/24/%E7%94%A8BERT%E8%AE%A1%E7%AE%97%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<p>最近在实验中需要计算句子之间的相似度，偶然看到了一篇文章，有以下说法：</p><blockquote><p>自2018年BERT惊艳众人之后，基于预训练模型对下游任务进行微调已成为炼丹的标配。然而近两年的研究却发现，没有经过微调，直接由BERT得到的句子表示在语义文本相似性方面明显薄弱，甚至会弱于GloVe得到的表示。此篇论文中首先从理论上探索了masked language model 跟语义相似性任务上的联系，并通过实验分析了BERT的句子表示，最后提出了BERT-Flow来解决上述问题。</p></blockquote><h1 id="为什么BERT的句子Embeddings表现弱？"><a href="#为什么BERT的句子Embeddings表现弱？" class="headerlink" title="为什么BERT的句子Embeddings表现弱？"></a>为什么BERT的句子Embeddings表现弱？</h1><font color="orange">由于Reimers等人之前已实验证明 context embeddings 取平均要优于[CLS] token的embedding。因而在文章中，**作者都以最后几层文本嵌入向量的平均值来作为BERT句子的表示向量**。</font><p><a href="*https://arxiv.org/pdf/2011.05864.pdf*"><strong><em>On the Sentence Embeddings from Pre-trained Language Models</em></strong></a></p><a id="more"></a><h1 id="句子表示"><a href="#句子表示" class="headerlink" title="句子表示"></a>句子表示</h1><p>在关系抽取任务中，对不同句子表示的最终结果进行实验，实验结果为：</p><h2 id="取-CLS"><a href="#取-CLS" class="headerlink" title="取[CLS]"></a>取[CLS]</h2><p>此处采用了<code>[CLS]head[unused6]tail[unused6]sentence</code>的句子组织形式投入BERT，然后取<code>[CLS]</code>对应的向量作为句子的表示，结果为：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">precision: 0.7702</span><br><span class="line">recall: 0.8052</span><br><span class="line">F1: 0.7729</span><br></pre></td></tr></table></figure><h2 id="取平均"><a href="#取平均" class="headerlink" title="取平均"></a>取平均</h2><p>句子组织形式相同，取整个句子向量的平均作为最终向量表示，结果为：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">precision: 0.7585</span><br><span class="line">recall: 0.7693</span><br><span class="line">F1: 0.7367</span><br></pre></td></tr></table></figure><p>和上面的论述不同，我的个人实验中，取[CLS]可以取得更好的效果。</p><h2 id="取大头"><a href="#取大头" class="headerlink" title="取大头"></a>取大头</h2><p>句子的组织形式依然与上面一致，但是，取<code>[CLS]head[unused6]tail[unused6]</code>的平均作为句子的表示，最终结果为：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">precision: 0.5396</span><br><span class="line">recall: 0.6381</span><br><span class="line">F1: 0.5670</span><br></pre></td></tr></table></figure><p>取大头的形式显然没有取得很好的效果。。。。。。</p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="如何变长的截断序列？"><a href="#如何变长的截断序列？" class="headerlink" title="如何变长的截断序列？"></a>如何变长的截断序列？</h2><p>在做实验的时候，需要对序列变长截断，序列的shape为(3, 150)，想要对序列分别取前5，6，9维，感觉自己用for循环取向量一是不够优雅，代码不和谐；二是没有经过优化，担心速度会比较慢。</p><p>在询问了学长后，学到了一种新方法：o(<em>￣▽￣</em>)ブ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">result = torch.ones(<span class="number">3</span>, <span class="number">150</span>)</span><br><span class="line">c = torch.arange(<span class="number">150</span>).unsqueeze(<span class="number">0</span>).repeat(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">mask = (c &lt; torch.tensor([<span class="number">5</span>, <span class="number">6</span>, <span class="number">9</span>]).unsqueeze(<span class="number">1</span>))</span><br><span class="line"><span class="comment"># gaga = result[mask]  # 和下面那个两种方法均可</span></span><br><span class="line">gaga = result.masked_select(mask)</span><br><span class="line">print(gaga)</span><br><span class="line">print(gaga.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,</span><br><span class="line">        1., 1.])</span><br><span class="line">torch.Size([20])</span><br></pre></td></tr></table></figure><h2 id="如何取多个相同值的最后一个？"><a href="#如何取多个相同值的最后一个？" class="headerlink" title="如何取多个相同值的最后一个？"></a>如何取多个相同值的最后一个？</h2><p>对于一个序列中的多个值，我们想取值为7的最后一个的下标？怎么做呢？</p><p>使用torch.max()只会返回第一个值的下标……</p><p>我真是想了半天没想出来，太笨了，自杀了🤡，后来请教了一下别人，其实很简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tmp = torch.tensor([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br><span class="line">tmp = tmp == <span class="number">7</span></span><br><span class="line">tmp = tmp + torch.arange(<span class="number">0</span>, tmp.size(<span class="number">0</span>)) * tmp <span class="comment"># 关键点，加上下标！</span></span><br><span class="line">_, index = torch.max(tmp)</span><br><span class="line">print(index)</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span><br><span class="line">tensor(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>✨🎉🎉🎉🎉</p><h2 id="BUG"><a href="#BUG" class="headerlink" title="BUG!"></a>BUG!</h2><p>在使用torch.max()的过程中，发现了官方文档的一个bug，在官方文档中，这样写道：</p><p><a href="https://pytorch.org/docs/stable/generated/torch.max.html?highlight=torch%20max#torch.max" target="_blank" rel="noopener">https://pytorch.org/docs/stable/generated/torch.max.html?highlight=torch%20max#torch.max</a></p><blockquote><p>If there are multiple maximal values in a reduced row then the indices of the first maximal value are returned.</p></blockquote><p>即，如果存在多个最大值，返回第一个最大值对应的下标。</p><p>但是在实验过程中，发现返回的是最后一个最大值对应的下标：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">1</span>,<span class="number">3</span>], dtype=torch.int)</span><br><span class="line">_, tmp = torch.max(x, dim=<span class="number">0</span>)</span><br><span class="line">print(tmp)</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span><br><span class="line">tensor(<span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>我把这个问题提到了pytorch官方的github上，具体的细节可以去看issue #49845</p><p><a href="https://github.com/pytorch/pytorch/issues/49845" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/49845</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Pretrained Language Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> semantic similarity </tag>
            
            <tag> CLS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pip虚拟环境-使用virtualenv</title>
      <link href="/2020/12/23/pip%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83-%E4%BD%BF%E7%94%A8virtualenv/"/>
      <url>/2020/12/23/pip%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83-%E4%BD%BF%E7%94%A8virtualenv/</url>
      
        <content type="html"><![CDATA[<p>在开发Python应用程序的时候，系统安装的Python3只有一个版本：3.4。所有第三方的包都会被<code>pip</code>安装到Python3的<code>site-packages</code>目录下。</p><p>如果我们要同时开发多个应用程序，那这些应用程序都会共用一个Python，就是安装在系统的Python 3。如果应用A需要jinja 2.7，而应用B需要jinja 2.6怎么办？</p><p>这种情况下，每个应用可能需要各自拥有一套“独立”的Python运行环境。virtualenv就是用来为一个应用创建一套“隔离”的Python运行环境。</p><p>首先，我们用<code>pip</code>安装virtualenv：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install virtualenv</span><br></pre></td></tr></table></figure><p>然后，假定我们要开发一个新的项目，需要一套独立的Python运行环境，可以这么做：<a id="more"></a></p><h1 id="第一步，创建目录："><a href="#第一步，创建目录：" class="headerlink" title="第一步，创建目录："></a>第一步，创建目录：</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Mac:~ michael$ mkdir myproject</span><br><span class="line">Mac:~ michael$ <span class="built_in">cd</span> myproject/</span><br><span class="line">Mac:myproject michael$</span><br></pre></td></tr></table></figure><h1 id="第二步，创建一个独立的Python运行环境，并指定python版本为3-6，命名为pytorch："><a href="#第二步，创建一个独立的Python运行环境，并指定python版本为3-6，命名为pytorch：" class="headerlink" title="第二步，创建一个独立的Python运行环境，并指定python版本为3.6，命名为pytorch："></a>第二步，创建一个独立的Python运行环境，并指定python版本为3.6，命名为<code>pytorch</code>：</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Mac:myproject michael$ virtualenv -p /usr/bin/python3.6 pytorch</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span><br><span class="line">created virtual environment CPython3.6.8.final.0-64 <span class="keyword">in</span> 7874ms</span><br><span class="line">  creator CPython3Posix(dest=/root/venv/pytorch, clear=False, no_vcs_ignore=False, global=False)</span><br><span class="line">  seeder FromAppData(download=False, pip=bundle, wheel=bundle, setuptools=bundle, via=copy, app_data_dir=/root/.<span class="built_in">local</span>/share/virtualenv)</span><br><span class="line">    added seed packages: pip==20.3.1, setuptools==51.0.0, wheel==0.36.1</span><br><span class="line">  activators PythonActivator,FishActivator,XonshActivator,CShellActivator,PowerShellActivator,BashActivator</span><br></pre></td></tr></table></figure><p>新建的Python环境被放到当前目录下的<code>pytorch</code>目录。有了<code>pytorch</code>这个Python环境，可以用<code>source</code>进入该环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Mac:myproject michael$ <span class="built_in">source</span> pytorch/bin/activate</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span><br><span class="line">(pytorch) root@ubuntu:$</span><br></pre></td></tr></table></figure><p>注意到命令提示符变了，有个<code>(pytorch)</code>前缀，表示当前环境是一个名为<code>pytorch</code>的Python环境。</p><p>下面正常安装各种第三方包，并运行<code>python</code>命令：</p><h1 id="退出当前的pytorch环境，使用deactivate命令："><a href="#退出当前的pytorch环境，使用deactivate命令：" class="headerlink" title="退出当前的pytorch环境，使用deactivate命令："></a>退出当前的<code>pytorch</code>环境，使用<code>deactivate</code>命令：</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(pytorch) root@ubuntu:$ deactivate</span><br></pre></td></tr></table></figure><p>此时就回到了正常的环境，现在<code>pip</code>或<code>python</code>均是在系统Python环境下执行。</p><p>完全可以针对每个应用创建独立的Python运行环境，这样就可以对每个应用的Python环境进行隔离。</p><p>virtualenv是如何创建“独立”的Python运行环境的呢？原理很简单，就是把系统Python复制一份到virtualenv的环境，用命令<code>source venv/bin/activate</code>进入一个virtualenv环境时，virtualenv会修改相关环境变量，让命令<code>python</code>和<code>pip</code>均指向当前的virtualenv环境。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>virtualenv为应用提供了隔离的Python运行环境，解决了不同应用间多版本的冲突问题。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> conda </tag>
            
            <tag> pip </tag>
            
            <tag> virtualenv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对GAN再次学习</title>
      <link href="/2020/12/23/%E5%AF%B9GAN%E5%86%8D%E6%AC%A1%E5%AD%A6%E4%B9%A0/"/>
      <url>/2020/12/23/%E5%AF%B9GAN%E5%86%8D%E6%AC%A1%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<ol><li>Z是噪声，也就是G的输入，可以是高斯噪声，一般为均匀噪声；</li><li>从贝叶斯的角度上做，生成模型可以认为是都是属于ML(最大化似然）的过程，因为，ML就是找到能使样本数据的类条件概率最大化的分布参数。那如果经过训练集训练，我们找到该参数,此时我们的模型是有最大可能生成和样本训练数据具有同样分布的数据，这个也是生成模型的一个目标。当然这个方法的前提就是对数据的分布进行一个假设，然后才能求参数。比如假设数据分布是符合高斯分布的。</li></ol><p><img src="/images/blog/2020/generative_model.webp" alt></p><ol><li>GoodFellow的论文证明了Gans 全局最小点的充分必要条件是:</li></ol><script type="math/tex; mode=display">p_g = p_{real}</script><p>pg表示generate 生成数据的分布函数</p><p>pdata表示真实data的分布函数</p><ol><li>我们知道，G和D是一个对抗的过程，而这个对抗是，G不断的学习，D也不断的学习，而且需要保证两者学习速率基本一致，也就是都能不断的从对方那里学习到“知识”来提升自己。否则，就是这两者哪一个学习的过快，或过慢，以至于双方的实力不再均衡，就会导致实力差的那一方的“loss”不再能“下降”，也就不在学到“知识”。一般的对抗模型中的G和D的网络框架大小基本上是相似(可能存在较小的差异)，而且，训练的过程就是先训练G一次，再训练D一次，这也是为了稳定训练的一个保证。当然这并不能完全稳定训练，所以，对抗网络的稳定训练，依然是一个研究的热点和方向。</li><li>这是DCGAN的生成网络模型的架构 (5层反卷积)，对于LSUN，Imagenet-1k大小的数据集，我们可以使用这个架构，但并不是说对于任何数据集，都可以，比如，更大，或者更小的，那对应的卷积架构就需要进行改变。<strong>比如，对于mnist数据集，G和D的网络架构都相应地减小了，否则不能拟合</strong>，产生不了好的结果(亲测)。</li><li>因为“反卷积”存在于卷积的反向传播中。<font color="red">其中反向传播的滤波器矩阵，是前向传播(卷积)的转置</font> (<strong>如何理解？</strong>)，所以，这就是它的名字的由来。只不过我们把反向传播的操作拿到了前向传播来做，就产生了所谓的反卷积一说。但是transport-convolution只能还原信号的大小，不能还原其value，所以，不能叫做反卷积，不是真正的逆操作。</li></ol><a id="more"></a><hr><p>DCGAN之我的测试</p><p>我对自己编写的代码也进行了一些测试，修改了一些参数。得到的启发如下。</p><p>(1)如果模型太大，则会出现训练不稳定。</p><p>什么是模型过大，模型过大是同样大小的数据集的分类任务情况下，比监督训练的模型较大，如果较大的话，会出现，生成图片的不稳定。主要体现就是生成的图片，在开始的时候有趋于变优的过程，最后却变得越来越模糊。</p><p>(2)采样的噪声如果为10时，和100有何区别。</p><p>发现，采样噪声维度越小，产生的图片的共性越大。</p><p>(3)将测试采样进行变动，发现高斯和均匀分布采样基本兼容，其他是乱的。</p><p>(4)训练G和D的顺序似乎没有什么关系。(值得思考)</p><hr><p><img src="/images/blog/2020/CGAN/conditionalGAN.webp" alt></p><p>可视化的介绍</p><p>深度学习的参数可视化是了解深度学习的学习过程或者结果的方法或者工具。那一般分为三种方法。</p><p>1.权重可视化</p><p>这个比较直观，一般都是对卷积权重进行可视化。</p><p>操作流程:直接取参数，然后进行一定的归一化，然后转化为RGB图片。</p><p>2.激活值可视化</p><p>这个是对激活函数的输出进行可视化，那么就需要进行NN的前向传播。</p><p>操作流程:先进行前向传播，取某一层的激活值，然后进行归一化，最后转化为RGB图片。</p><p>3.反卷积的可视化</p><p>这篇论文介绍的可视化，是需要经过训练，然后经过反卷积 (反卷积层+反pooling层) 的输出值的可视化。详细的请看论文。</p><h1 id="对于DCGAN的实现细节以及一些tricks，可以查看："><a href="#对于DCGAN的实现细节以及一些tricks，可以查看：" class="headerlink" title="对于DCGAN的实现细节以及一些tricks，可以查看："></a>对于DCGAN的实现细节以及一些tricks，可以查看：</h1><p><a href="https://blog.csdn.net/c2a2o2/article/details/78535795" target="_blank" rel="noopener">https://blog.csdn.net/c2a2o2/article/details/78535795</a></p><h1 id="InfoGAN"><a href="#InfoGAN" class="headerlink" title="InfoGAN"></a>InfoGAN</h1><p>既然原始的噪声是杂乱无章的，那就人为地加上一些限制，于是作者把原来的噪声输入分解成两部分：一是原来的<strong>z</strong>；二是由若干个latent variables拼接而成的latent code <strong>c</strong>，这些latent variables会有一个先验的概率分布，且可以是离散的或连续的，用于代表生成数据的不同特征维度，比如MNIST实验的latent variables就可以由一个取值范围为0-9的离散随机变量（用于表示数字）和两个连续的随机变量（分别用于表示倾斜度和粗细度）构成。</p><p><font color="orange">但仅有这个设定还不够，因为GAN中Generator的学习具有很高的自由度，</font>它很容易找到一个解，使得:</p><script type="math/tex; mode=display">p_G(x|c) = p_G(x)</script><p>导致<strong>c</strong>完全不起作用。</p><p><strong>mutual information</strong></p><p>作者从信息论中得到启发，提出了基于互信息（<strong>mutual information</strong>）的正则化项。<strong>c</strong>的作用是对生成数据的分布施加影响，于是需要对这两者的关系建模，在信息论中，互信息<em>I(X;Y)</em>用来衡量“已知<em>Y</em>的情况下可获取多少有关<em>X</em>的信息”，计算公式为：</p><script type="math/tex; mode=display">I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)</script><p>其中<em>H</em>表示计算entropy，<em>H(X|Y)</em>衡量的是“给定<em>Y</em>的情况下，<em>X</em>的不确定性”。从公式可以得知，当<em>X</em>和<em>Y</em>毫无关联时，<em>H(X|Y)</em> = <em>H(X)</em>，<em>I(X;Y)</em> = 0，取得最小值；当<em>X</em>和<em>Y</em>有明确的关联时，已知<em>Y</em>时，<em>X</em>没有不确定性，因而<em>H(X|Y)</em> = 0，此时<em>I(X;Y)</em>取得最大值。</p><p>所以，现在优化目标就很明确了：要让<strong>c</strong>和Generator分布<strong>G(z, c)</strong>的互信息达到最大值。现在目标函数可改写为：</p><script type="math/tex; mode=display">\min _G \max_D  V_I(D,G) = V(D,G) - \lambda I(c;G(z,c))</script><p><strong>Variational Mutual Information Maximization</strong></p><p>上述的推导看似已经很充足了，然而上面互信息的计算涉及后验概率分布<strong>P(c|x)</strong>，而后者在实际中是很难获取的，所以需要定义一个辅助性的概率分布<strong>Q(c|x)</strong>，采用一种叫作Variational Information Maximization的技巧对互信息进行下界拟合：</p><p><img src="/images/blog/2020/DCGAN/infoGAN1.png" alt></p><p>再<strong>利用概率积分公式</strong>，可把这个下界转化成如下的形式：</p><script type="math/tex; mode=display">L_I(G,Q) = E_{c \sim P(c), x \sim G(z,c)} [\log  Q(c|x)] + H(c)</script><p>很容易在现有的框架中进行优化，当它取得最大值<em>H(c)</em>时，我们也获得了互信息的最大值，达到最优结果。</p><p>现在目标函数改为：</p><script type="math/tex; mode=display">\min_{G,Q}\max_{D} V_{\mathrm{InfoGAN}}(D,G,Q) = V(D,G) -  \lambda L_I(G,Q)</script><p><strong>网络模型概览</strong></p><p>最终的网络模型示意图为：</p><p><img src="/images/blog/2020/DCGAN/infoGAN2.png" alt></p><p>网络是基于DC-GAN（Deep Convolutional GAN）的，G和D都由CNN构成。在此基础上，Q和D共享卷积网络，然后分别通过各自的全连接层输出不同的内容：Q输出对应于fake data的<strong>c</strong>的概率分布，D则仍然判别真伪。</p><h1 id="AC-GAN"><a href="#AC-GAN" class="headerlink" title="AC-GAN"></a>AC-GAN</h1><p><img src="/images/blog/2020/CGAN/AC-GAN.png" alt></p><h1 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h1><p>怎么实现两个domain的数据迁移呢？下面以图像为例来阐述吧。</p><p>实现图像间的翻译，借助GAN，应该有两个domain的discriminator，每个discriminator单独判断各自domain的数据是否是真实数据。至于generator，图像的翻译需要将domain A的图像翻成domain B的图像，所以generator有点像autoencoder结构，只是decoder的输出不是domain A的图像，而是domain B的图像。为了充分利用两个discriminator，还应该有一个翻译回去的过程，也就是说，还有一个generator，它将domain B的数据翻译到domain A。如果你看过对偶学习（dual learning）那篇文章，你应该对此很熟悉。感觉有点绕？还是看图吧。</p><p><img src="/images/blog/2020/DCGAN/CycleGAN.png" alt></p><p>图中的虚线箭头表示『将箭头起始的图像当做箭头结束的图像，按照流程图继续走 』，什么意思呢？对于Real A，它的完整流程是这样的：$A<em>{real} \to B</em>{fake} \to A<em>{fake}$，对于Real B，它的流程是这样的：$B</em>{real} \to A<em>{fake} \to B</em>{fake}$。可以看到，无论是domain A还是domain B的图像，整个流程就是一个cycle啊！所以叫CycleGAN。整个cycle可以看成是一个autoencoder，两个generator看成是encoder和decoder。而两个discriminator则是准则。(<font color="orange">PS：这里的虚线真的给人误导，想了半天…</font>)</p><p>一般来说，两个generator的设计是这样的：</p><p><img src="/images/blog/2020/DCGAN/cg.png" alt></p><p>z在G中控制一些属性，使得生成的结果不是唯一的，可以是多种多样的。</p><p>明确了CycleGAN的流程，我们就能写出它的目标函数了：</p><p><a href="https://zhuanlan.zhihu.com/p/26995910" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26995910</a></p><p>相比于普通的GAN网络，主要是增加了一个重构误差，用于限制图像的语义偏移不要太大。<strong>举个栗子：将一张苹果的图片 → 一张橘子的漫画 → 一张香蕉的照片</strong>，整个过程都符合损失函数的限制，但是造成了语义偏移。</p><p>重构误差采用以下写法：</p><script type="math/tex; mode=display">\mathbb{E}_{x \in \mathbb{P}} ||x - G_{AB}(G_{BA}(x))||</script><p>对于generator添加重构误差项，跟对偶学习一样，能够引导两个generator更好地完成encode和decode的任务。而两个D则起到纠正编码结果符合某个domain的风格的作用。结构很简答，但是很有效。并且，你<strong>并不需要pair的数据</strong>。</p><h2 id="1-去掉重构误差，模型是否还有效？"><a href="#1-去掉重构误差，模型是否还有效？" class="headerlink" title="1. 去掉重构误差，模型是否还有效？"></a>1. 去掉重构误差，模型是否还有效？</h2><p>模型仍然有效，只是收敛比较慢，毕竟缺少了重构误差这样的强引导信息。以及，虽然实现了风格迁移，但是人物的一些属性改变了，比如可能出现『变性』、『变脸』，而姿态在转换的时候一般不出现错误。<strong>这表明，对偶重构误差能够引导模型在迁移的时候保留图像固有的属性；而对抗loss则负责确定模型该学什么，该怎么迁移</strong>。</p><h2 id="2-能不能不要完整的cycle，只做一半？"><a href="#2-能不能不要完整的cycle，只做一半？" class="headerlink" title="2. 能不能不要完整的cycle，只做一半？"></a>2. 能不能不要完整的cycle，只做一半？</h2><p>结论是不行，缺少了对偶的部分，就少了重构误差，仅仅依靠$D<em>B$来纠正$G</em>{AB}$是不够的。从讨论1来看，对偶的作用还是很大的，即使缺少了重构误差。以上面CelebA的cartoonize为例，迁移的图像全是随机噪声。这里就不放结果了。</p><h1 id="cedGAN"><a href="#cedGAN" class="headerlink" title="cedGAN"></a>cedGAN</h1><p><a href="http://www.360doc.com/content/19/0417/21/32196507_829507680.shtml" target="_blank" rel="noopener">http://www.360doc.com/content/19/0417/21/32196507_829507680.shtml</a></p><h1 id="ProGAN"><a href="#ProGAN" class="headerlink" title="ProGAN"></a>ProGAN</h1><p>主要贡献是提出了一种循序渐进的生成高质量图片的方法，</p><h2 id="StyleGAN"><a href="#StyleGAN" class="headerlink" title="StyleGAN"></a>StyleGAN</h2><p>而StyleGAN则是在此基础上添加了对各个属性的控制，例如性别、发色、是否戴眼镜等。</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/93748098" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/93748098</a></p><p><a href="https://zhuanlan.zhihu.com/p/63230738" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63230738</a></p></blockquote><p>感觉StyleGAN的实现非常复杂，有时间的时候去仔细的精读。</p><h1 id="BigGAN"><a href="#BigGAN" class="headerlink" title="BigGAN"></a>BigGAN</h1><p><strong>按照原文，总结一下 BigGAN 的贡献：</strong></p><ul><li>通过大规模 GAN 的应用，BigGAN 实现了生成上的巨大突破；</li><li>采用先验分布 z 的“截断技巧”，允许对样本多样性和保真度进行精细控制；</li><li>在大规模 GAN 的实现上不断克服模型训练问题，采用技巧减小训练的不稳定。</li></ul><p>其中一个很大的原因就是 BigGAN 如它题目 <strong><em>Large Scale GAN Training for High Fidelity Natural Image Synthesis</em></strong> 描述的 <strong>Large Scale</strong>，在训练中 Batch 采用了很大的 Batch，已经达到了 2048（我们平常训练 Batch 正常都是 64 居多），在卷积的通道上也是变大了，还有就是网络的参数变多了，在 2048 的 Batch 下整个网络的参数达到了接近 16 亿（看了一下自己还在用的 GTX 1080 突然沉默了）。</p><p>BigGAN 在先验分布 z 的嵌入上做了改进，普遍的 GAN 都是将 z 作为输入直接嵌入生成网络，而 BigGAN 将噪声向量 z 送到 G 的多个层而不仅仅是初始层。<strong>文章认为潜在空间 z 可以直接影响不同分辨率和层次结构级别的特征</strong>，对于 BigGAN 的条件生成上通过将 z 分成每个分辨率的一个块，并将每个块连接到条件向量 c 来实现，这样提供约 4％ 的适度性能提升，并将训练速度提高 18％。</p><p><img src="/images/blog/2020/DCGAN/BigGAN.jpg" alt></p><p>如左图所示将噪声向量 z 通过 split 等分成多块，然后和条件标签 c 连接后一起送入到生成网络的各个层中，对于生成网络的每一个残差块又可以进一步展开为右图的结构。可以看到噪声向量 z 的块和条件标签 c 在残差块下是通过 concat 操作后送入 BatchNorm 层，其中这种嵌入是共享嵌入，线性投影到每个层的 bias 和 weight。</p><p><strong>“截断技巧”</strong></p><p><strong>对于先验分布z，一般情况下都是选用标准正态分布 N(0,I) 或者均匀分布 U[−1,1]，文章对此存在疑惑，难道别的分布不行吗？通过实验，为了适合后续的“截断”要求，文章最终选择了 z∼N(0,I)。</strong></p><p>所谓的“截断技巧”就是通过对从先验分布 z 采样，通过设置阈值的方式来截断 z 的采样，其中超出范围的值被重新采样以落入该范围内。这个阈值可以根据生成质量指标 IS 和 FID 决定。</p><p>通过实验可以知道通过对阈值的设定，随着阈值的下降生成的质量会越来越好，但是<strong>由于阈值的下降、采样的范围变窄，就会造成生成上取向单一化，造成生成的多样性不足的问题。</strong></p><p>随着截断的阈值下降，生成的质量在提高，但是生成也趋近于单一化。所以根据实验的生成要求，权衡生成质量和生成多样性是一个抉择，往往阈值的下降会带来 IS 的一路上涨，但是 FID 会先变好后一路变差。</p><h2 id="插值"><a href="#插值" class="headerlink" title="插值"></a>插值</h2><p>为了进一步说明 G 网络并非是记住训练集，在固定 z 下通过调节条件标签 c 做插值生成，通过下图的实验结果可以发现，整个插值过程是流畅的，也能说明 G 并非是记住训练集，而是真正做到了图像生成。</p><p><img src="/images/blog/2020/DCGAN/BigGANR.jpg" alt></p><h3 id="如何对C进行插值呢？"><a href="#如何对C进行插值呢？" class="headerlink" title="如何对C进行插值呢？"></a>如何对C进行插值呢？</h3><p>C是以连续的形式实现的吗？如何实现的？是以离散的形式实现的吗？那如何进行插值呢？</p><blockquote><p>class embeddings c used for the conditional BatchNorm layers in G contain a large number of weights. Instead of having a separate layer for each embedding (Miyato et al., 2018; Zhang et al., 2018), we opt to use a shared embedding, which is linearly projected to each layer’s gains and biases</p></blockquote><p>是采用embedding的形式实现的。</p><h2 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h2><p>文章的另一大亮点是把实验的 NG 结果做了分析，把自己趟的坑和大家分享了，这个真是很良心有没有，我们截取其中一些坑分享一下：</p><ul><li>一味加深网络可能会妨碍生成的性能；</li><li>共享类的思想在控制超参数上是很麻烦的，虽然可能会提高训练速度；</li><li>WeightNorm 替换 G 中的 BatchNorm 并没有达到好的效果；</li><li>除了频谱规范化之外，尝试将 BatchNorm 添加到 D（包括类条件和无条件），但并未取的好的效果；</li><li>在 G 或 D 或两者中使用 5 或 7 而不是 3 的滤波器大小，5 的滤波器可能会有些许提升，但是计算成本也上去了；</li><li>尝试在 128×128 的 G 和 D 中改变卷积滤波器的扩张，但发现在任一网络中即使少量的扩张也会降低性能；</li><li>尝试用 G 中的双线性上采样代替最近领近的上采样，但这降低了性能。</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> 生成模型 </tag>
            
            <tag> Adversarial Training </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RESIDE 实体类型 (Entity Type) 质量考察</title>
      <link href="/2020/12/21/RESIDE-%E5%AE%9E%E4%BD%93%E7%B1%BB%E5%9E%8B-Entity-Type-%E8%B4%A8%E9%87%8F%E8%80%83%E5%AF%9F/"/>
      <url>/2020/12/21/RESIDE-%E5%AE%9E%E4%BD%93%E7%B1%BB%E5%9E%8B-Entity-Type-%E8%B4%A8%E9%87%8F%E8%80%83%E5%AF%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="Entity-Type"><a href="#Entity-Type" class="headerlink" title="Entity Type"></a>Entity Type</h1><h2 id="发现1-RESIDE更详细"><a href="#发现1-RESIDE更详细" class="headerlink" title="发现1. RESIDE更详细"></a>发现1. RESIDE更详细</h2><p>例如对于m.01wgcvn，我识别出类型为people.person，而RESIDE更全面：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">"/person/actor"</span>, <span class="string">"/person"</span>, <span class="string">"/person/artist"</span>]</span><br></pre></td></tr></table></figure><h2 id="发现2-RESIDE准确率很高"><a href="#发现2-RESIDE准确率很高" class="headerlink" title="发现2. RESIDE准确率很高"></a>发现2. RESIDE准确率很高</h2><p>随机采样我个人生成的实体类别，在people, location, organization, sports_team, religion, language中随机采样若干样本，与RESIDE比较，发现一致率在95%以上。</p><h2 id="发现3-可以补充RESIDE"><a href="#发现3-可以补充RESIDE" class="headerlink" title="发现3. 可以补充RESIDE"></a>发现3. 可以补充RESIDE</h2><p>对于m.05qv655，RESIDE为空，而我们可以正确识别出religion.religion</p><h2 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h2><font color="orange">如果RESIDE对应mid的Type为空，则对其进行补充；否则维持原状。</font><p>在对RESIDE进行补充的时候，按照以下方式映射：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    '': '', </span><br><span class="line">    'film.film_festival': '/art/film', </span><br><span class="line">    'people.family': ['/people/family', '/person'], </span><br><span class="line">    'organization.organization': '/organization',</span><br><span class="line">    'people.person': '/person', </span><br><span class="line">    'people.ethnicity': '/people/ethnicity', </span><br><span class="line">    'location.location': '/location', </span><br><span class="line">    'religion.religion': '/religion/religion', </span><br><span class="line">    'sports.sports_team': '/organization/sports_team', </span><br><span class="line">    'film.film': '/art/film', </span><br><span class="line">    'language.human_language': '/language', </span><br><span class="line">    'people.profession': '/title'</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终得到的实体类型type2id为：<a id="more"></a></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> <span class="attr">"NA"</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="attr">"/art"</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="attr">"/art/film"</span>: <span class="number">2</span>,</span><br><span class="line"> <span class="attr">"/astral_body"</span>: <span class="number">3</span>,</span><br><span class="line"> <span class="attr">"/award"</span>: <span class="number">4</span>,</span><br><span class="line"> <span class="attr">"/broadcast/tv_channel"</span>: <span class="number">5</span>,</span><br><span class="line"> <span class="attr">"/broadcast_network"</span>: <span class="number">6</span>,</span><br><span class="line"> <span class="attr">"/broadcast_program"</span>: <span class="number">7</span>,</span><br><span class="line"> <span class="attr">"/building"</span>: <span class="number">8</span>,</span><br><span class="line"> <span class="attr">"/building/airport"</span>: <span class="number">9</span>,</span><br><span class="line"> <span class="attr">"/building/dam"</span>: <span class="number">10</span>,</span><br><span class="line"> <span class="attr">"/building/hospital"</span>: <span class="number">11</span>,</span><br><span class="line"> <span class="attr">"/building/hotel"</span>: <span class="number">12</span>,</span><br><span class="line"> <span class="attr">"/building/library"</span>: <span class="number">13</span>,</span><br><span class="line"> <span class="attr">"/building/restaurant"</span>: <span class="number">14</span>,</span><br><span class="line"> <span class="attr">"/building/sports_facility"</span>: <span class="number">15</span>,</span><br><span class="line"> <span class="attr">"/building/theater"</span>: <span class="number">16</span>,</span><br><span class="line"> <span class="attr">"/chemistry"</span>: <span class="number">17</span>,</span><br><span class="line"> <span class="attr">"/education/department"</span>: <span class="number">18</span>,</span><br><span class="line"> <span class="attr">"/education/educational_degree"</span>: <span class="number">19</span>,</span><br><span class="line"> <span class="attr">"/event"</span>: <span class="number">20</span>,</span><br><span class="line"> <span class="attr">"/event/attack"</span>: <span class="number">21</span>,</span><br><span class="line"> <span class="attr">"/event/election"</span>: <span class="number">22</span>,</span><br><span class="line"> <span class="attr">"/event/military_conflict"</span>: <span class="number">23</span>,</span><br><span class="line"> <span class="attr">"/event/natural_disaster"</span>: <span class="number">24</span>,</span><br><span class="line"> <span class="attr">"/finance/stock_exchange"</span>: <span class="number">25</span>,</span><br><span class="line"> <span class="attr">"/food"</span>: <span class="number">26</span>,</span><br><span class="line"> <span class="attr">"/game"</span>: <span class="number">27</span>,</span><br><span class="line"> <span class="attr">"/geography/glacier"</span>: <span class="number">28</span>,</span><br><span class="line"> <span class="attr">"/geography/island"</span>: <span class="number">29</span>,</span><br><span class="line"> <span class="attr">"/geography/mountain"</span>: <span class="number">30</span>,</span><br><span class="line"> <span class="attr">"/god"</span>: <span class="number">31</span>,</span><br><span class="line"> <span class="attr">"/government/government"</span>: <span class="number">32</span>,</span><br><span class="line"> <span class="attr">"/government/political_party"</span>: <span class="number">33</span>,</span><br><span class="line"> <span class="attr">"/government_agency"</span>: <span class="number">34</span>,</span><br><span class="line"> <span class="attr">"/internet/website"</span>: <span class="number">35</span>,</span><br><span class="line"> <span class="attr">"/language"</span>: <span class="number">36</span>,</span><br><span class="line"> <span class="attr">"/living_thing"</span>: <span class="number">37</span>,</span><br><span class="line"> <span class="attr">"/location"</span>: <span class="number">38</span>,</span><br><span class="line"> <span class="attr">"/location/body_of_water"</span>: <span class="number">39</span>,</span><br><span class="line"> <span class="attr">"/location/bridge"</span>: <span class="number">40</span>,</span><br><span class="line"> <span class="attr">"/location/cemetery"</span>: <span class="number">41</span>,</span><br><span class="line"> <span class="attr">"/location/city"</span>: <span class="number">42</span>,</span><br><span class="line"> <span class="attr">"/location/country"</span>: <span class="number">43</span>,</span><br><span class="line"> <span class="attr">"/location/county"</span>: <span class="number">44</span>,</span><br><span class="line"> <span class="attr">"/location/province"</span>: <span class="number">45</span>,</span><br><span class="line"> <span class="attr">"/military"</span>: <span class="number">46</span>,</span><br><span class="line"> <span class="attr">"/music"</span>: <span class="number">47</span>,</span><br><span class="line"> <span class="attr">"/news_agency"</span>: <span class="number">48</span>,</span><br><span class="line"> <span class="attr">"/newspaper"</span>: <span class="number">49</span>,</span><br><span class="line"> <span class="attr">"/organization"</span>: <span class="number">50</span>,</span><br><span class="line"> <span class="attr">"/organization/airline"</span>: <span class="number">51</span>,</span><br><span class="line"> <span class="attr">"/organization/company"</span>: <span class="number">52</span>,</span><br><span class="line"> <span class="attr">"/organization/educational_institution"</span>: <span class="number">53</span>,</span><br><span class="line"> <span class="attr">"/organization/fraternity_sorority"</span>: <span class="number">54</span>,</span><br><span class="line"> <span class="attr">"/organization/sports_league"</span>: <span class="number">55</span>,</span><br><span class="line"> <span class="attr">"/organization/sports_team"</span>: <span class="number">56</span>,</span><br><span class="line"> <span class="attr">"/organization/terrorist_organization"</span>: <span class="number">57</span>,</span><br><span class="line"> <span class="attr">"/park"</span>: <span class="number">58</span>,</span><br><span class="line"> <span class="attr">"/people/ethnicity"</span>: <span class="number">59</span>,</span><br><span class="line"> <span class="attr">"/people/family"</span>: <span class="number">60</span>,</span><br><span class="line"> <span class="attr">"/person"</span>: <span class="number">61</span>,</span><br><span class="line"> <span class="attr">"/person/actor"</span>: <span class="number">62</span>,</span><br><span class="line"> <span class="attr">"/person/architect"</span>: <span class="number">63</span>,</span><br><span class="line"> <span class="attr">"/person/artist"</span>: <span class="number">64</span>,</span><br><span class="line"> <span class="attr">"/person/athlete"</span>: <span class="number">65</span>,</span><br><span class="line"> <span class="attr">"/person/author"</span>: <span class="number">66</span>,</span><br><span class="line"> <span class="attr">"/person/coach"</span>: <span class="number">67</span>,</span><br><span class="line"> <span class="attr">"/person/director"</span>: <span class="number">68</span>,</span><br><span class="line"> <span class="attr">"/person/doctor"</span>: <span class="number">69</span>,</span><br><span class="line"> <span class="attr">"/person/engineer"</span>: <span class="number">70</span>,</span><br><span class="line"> <span class="attr">"/person/monarch"</span>: <span class="number">71</span>,</span><br><span class="line"> <span class="attr">"/person/musician"</span>: <span class="number">72</span>,</span><br><span class="line"> <span class="attr">"/person/politician"</span>: <span class="number">73</span>,</span><br><span class="line"> <span class="attr">"/person/religious_leader"</span>: <span class="number">74</span>,</span><br><span class="line"> <span class="attr">"/person/soldier"</span>: <span class="number">75</span>,</span><br><span class="line"> <span class="attr">"/person/terrorist"</span>: <span class="number">76</span>,</span><br><span class="line"> <span class="attr">"/play"</span>: <span class="number">77</span>,</span><br><span class="line"> <span class="attr">"/product"</span>: <span class="number">78</span>,</span><br><span class="line"> <span class="attr">"/product/computer"</span>: <span class="number">79</span>,</span><br><span class="line"> <span class="attr">"/product/ship"</span>: <span class="number">80</span>,</span><br><span class="line"> <span class="attr">"/product/weapon"</span>: <span class="number">81</span>,</span><br><span class="line"> <span class="attr">"/rail/railway"</span>: <span class="number">82</span>,</span><br><span class="line"> <span class="attr">"/religion/religion"</span>: <span class="number">83</span>,</span><br><span class="line"> <span class="attr">"/software"</span>: <span class="number">84</span>,</span><br><span class="line"> <span class="attr">"/title"</span>: <span class="number">85</span>,</span><br><span class="line"> <span class="attr">"/transit"</span>: <span class="number">86</span>,</span><br><span class="line"> <span class="attr">"/transportation/road"</span>: <span class="number">87</span>,</span><br><span class="line"> <span class="attr">"/written_work"</span>: <span class="number">88</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Relation-Alias"><a href="#Relation-Alias" class="headerlink" title="Relation Alias"></a>Relation Alias</h1><p>经过观察，relation_alias_from_wikidata_ppdb_extended.json有不错的质量，作者应该在利用PPDB对关系短语扩展后，<strong>有一个手动筛选的过程</strong>，因此，可以直接拿来加以利用。</p>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Freebase </tag>
            
            <tag> Relation Extraction </tag>
            
            <tag> Distant Supervision </tag>
            
            <tag> RE </tag>
            
            <tag> Entity Type </tag>
            
            <tag> NER </tag>
            
            <tag> Relation Aliases </tag>
            
            <tag> Wikidata </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用预训练语言模型计算句子相似度</title>
      <link href="/2020/12/21/%E5%88%A9%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%A1%E7%AE%97%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
      <url>/2020/12/21/%E5%88%A9%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%A1%E7%AE%97%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<p>使用huggingface的BertForMaskedLM做预训练后的二次预训练，即fine-tuning，在NYT数据集上。</p><p>具体的代码实现可以参考官方的<a href="https://github.com/huggingface/transformers/tree/master/examples" target="_blank" rel="noopener">github</a>, 有很多的例子可以学习，非常不错~</p><h1 id="插曲"><a href="#插曲" class="headerlink" title="插曲"></a>插曲</h1><p>在询问学长后得知，使用wwm，即whole-word-masking，可以比较显著的提高效果。但是，在<a href="https://huggingface.co/transformers/pretrained_models.html" target="_blank" rel="noopener">huggingface的网站</a>上，目前只有bert-large-cased-whole-word-masking，考虑到对比的方法目前用到的是bert-base和gpt，而且预训练语言模型不是我们的创新点，<strong>所以最终采用bert-base-cased，暂时不用wwm</strong></p><p>另外，在训练的时候整个进度条显示的是全部训练需要花费的时间，举个栗子，如果训练epoch数为3，则显示的为3个epoch对应的所有步数。</p><p><img src="/images/blog/2020/run_mlm.png" alt></p><p>在150MB的文本语料上训练，利用两个RTX 24GB显卡，batch_size设为16，epoch为3，每张卡显存占用为11GB左右，整个训练过程耗时9个小时。</p><h1 id="run-mlm-py"><a href="#run-mlm-py" class="headerlink" title="run_mlm.py"></a>run_mlm.py</h1><p>我们采用<a href="https://github.com/huggingface/transformers/tree/master/examples/language-modeling" target="_blank" rel="noopener">run_mlm.py</a>来进行bert的fine-tuning过程。</p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>在进行MLM任务之前，是否需要对文本进行预处理呢？常见的预处理技术有转换为小写、去除标点、去除停用词、提取词干等等，<font color="orange">但是，考虑到预训练语言模型词表足够大，见过的语料足够多，能力足够强</font>，所以，<strong>不进行任何的文本预处理工作</strong>，直接丢给BERT。</p><h2 id="sh"><a href="#sh" class="headerlink" title=".sh"></a>.sh</h2><p>使用以下命令运行程序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python run_mlm.py \</span><br><span class="line">  --model_name_or_path /data/klhao/BERT/bert-base-cased \</span><br><span class="line">  --train_file nyt_corpus_train.txt \</span><br><span class="line">  --validation_file nyt_corpus_valid.txt \</span><br><span class="line">  --do_train \</span><br><span class="line">  --do_eval \</span><br><span class="line">  --output_dir output \</span><br><span class="line">  --line_by_line \</span><br><span class="line">  --max_seq_length 500</span><br></pre></td></tr></table></figure><p>程序开始愉快的运行了！~</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[INFO|trainer.py:677] 2020-12-21 19:27:58,439 &gt;&gt; ***** Running training *****</span><br><span class="line">[INFO|trainer.py:678] 2020-12-21 19:27:58,439 &gt;&gt;   Num examples = 660306</span><br><span class="line">[INFO|trainer.py:679] 2020-12-21 19:27:58,440 &gt;&gt;   Num Epochs = 3</span><br><span class="line">[INFO|trainer.py:680] 2020-12-21 19:27:58,440 &gt;&gt;   Instantaneous batch size per device = 8</span><br><span class="line">[INFO|trainer.py:681] 2020-12-21 19:27:58,440 &gt;&gt;   Total train batch size (w. parallel, distributed &amp; accumulation) = 16</span><br><span class="line">[INFO|trainer.py:682] 2020-12-21 19:27:58,440 &gt;&gt;   Gradient Accumulation steps = 1</span><br><span class="line">[INFO|trainer.py:683] 2020-12-21 19:27:58,440 &gt;&gt;   Total optimization steps = 123810</span><br></pre></td></tr></table></figure><p><strong>训练了3个epoch，最终在loss为1.40左右的时候就降不下去了，线性学习率调度器此时也将学习率降到了1e-7这个数量级。</strong></p><h1 id="run-mlm-wwm-py"><a href="#run-mlm-wwm-py" class="headerlink" title="run_mlm_wwm.py"></a>run_mlm_wwm.py</h1><p>由于pypi更新不及时，需要从git安装transformers，命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/huggingface/transformers.git</span><br><span class="line"><span class="built_in">cd</span> transformers</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p>运行命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1 python run_mlm_wwm.py --model_name_or_path ../BERT/bert-large-cased-whole-word-masking --train_file nyt_corpus.txt --do_train --output_dir output --max_seq_length 200 --num_train_epochs 5  --per_device_train_batch_size 20 --save_steps 10000 --save_total_limit 8</span><br></pre></td></tr></table></figure><p>在NYT上训练耗时24H，采用bert-large-cased-whole-word-masking。</p><p><strong>每一个checkpoint占用内存为4G左右。</strong></p><p><strong>运行结束后，最终loss降到了1.15左右，说明我们之前的预训练可能还不够充分，下面我准备调到10个epoch再次进行尝试</strong></p><h2 id="10个epoch"><a href="#10个epoch" class="headerlink" title="10个epoch"></a>10个epoch</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run_mlm_wwm.py --model_name_or_path ../BERT/bert-large-cased-whole-word-masking --train_file nyt_corpus.txt --do_train --output_dir output --max_seq_length 200 --num_train_epochs 10  --per_device_train_batch_size 20 --save_steps 10000 --save_total_limit 3</span><br></pre></td></tr></table></figure><font color="orange">尝试看是否还会由效果的提升~</font><h2 id="运行log"><a href="#运行log" class="headerlink" title="运行log"></a>运行log</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">[INFO|trainer.py:395] 2020-12-28 18:28:39,231 &gt;&gt; The following columns <span class="keyword">in</span> the training <span class="built_in">set</span> don<span class="string">'t have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: .</span></span><br><span class="line"><span class="string">[INFO|trainer.py:718] 2020-12-28 18:28:39,238 &gt;&gt; ***** Running training *****</span></span><br><span class="line"><span class="string">[INFO|trainer.py:719] 2020-12-28 18:28:39,239 &gt;&gt;   Num examples = 695059</span></span><br><span class="line"><span class="string">[INFO|trainer.py:720] 2020-12-28 18:28:39,239 &gt;&gt;   Num Epochs = 5</span></span><br><span class="line"><span class="string">[INFO|trainer.py:721] 2020-12-28 18:28:39,239 &gt;&gt;   Instantaneous batch size per device = 20</span></span><br><span class="line"><span class="string">[INFO|trainer.py:722] 2020-12-28 18:28:39,239 &gt;&gt;   Total train batch size (w. parallel, distributed &amp; accumulation) = 20</span></span><br><span class="line"><span class="string">[INFO|trainer.py:723] 2020-12-28 18:28:39,239 &gt;&gt;   Gradient Accumulation steps = 1</span></span><br><span class="line"><span class="string">[INFO|trainer.py:724] 2020-12-28 18:28:39,239 &gt;&gt;   Total optimization steps = 173765</span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 2.065851318359375, '</span>learning_rate<span class="string">': 4.985612752855869e-05, '</span>epoch<span class="string">': 0.014387247144131442&#125;</span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 2.04491845703125, '</span>learning_rate<span class="string">': 4.9712255057117375e-05, '</span>epoch<span class="string">': 0.028774494288262883&#125;</span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 2.058274658203125, '</span>learning_rate<span class="string">': 4.956838258567606e-05, '</span>epoch<span class="string">': 0.043161741432394325&#125;                                                                                           </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 2.0292010498046875, '</span>learning_rate<span class="string">': 4.942451011423475e-05, '</span>epoch<span class="string">': 0.05754898857652577&#125;                                                                                           </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 2.0228314208984375, '</span>learning_rate<span class="string">': 4.9280637642793434e-05, '</span>epoch<span class="string">': 0.07193623572065722&#125;                                                                                          </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 2.018800048828125, '</span>learning_rate<span class="string">': 4.913676517135212e-05, '</span>epoch<span class="string">': 0.08632348286478865&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 2.0110439453125, '</span>learning_rate<span class="string">': 4.89928926999108e-05, '</span>epoch<span class="string">': 0.1007107300089201&#125;                                                                                                </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 2.0043060302734377, '</span>learning_rate<span class="string">': 4.8849020228469486e-05, '</span>epoch<span class="string">': 0.11509797715305153&#125;                                                                                          </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9853199462890625, '</span>learning_rate<span class="string">': 4.870514775702817e-05, '</span>epoch<span class="string">': 0.12948522429718298&#125;                                                                                           </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9929195556640624, '</span>learning_rate<span class="string">': 4.856127528558686e-05, '</span>epoch<span class="string">': 0.14387247144131443&#125;                                                                                           </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9991339111328126, '</span>learning_rate<span class="string">': 4.8417402814145545e-05, '</span>epoch<span class="string">': 0.15825971858544585&#125;                                                                                          </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.98677978515625, '</span>learning_rate<span class="string">': 4.827353034270423e-05, '</span>epoch<span class="string">': 0.1726469657295773&#125;                                                                                              </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 2.007623291015625, '</span>learning_rate<span class="string">': 4.812965787126292e-05, '</span>epoch<span class="string">': 0.18703421287370875&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9972188720703126, '</span>learning_rate<span class="string">': 4.79857853998216e-05, '</span>epoch<span class="string">': 0.2014214600178402&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9690428466796874, '</span>learning_rate<span class="string">': 4.7841912928380284e-05, '</span>epoch<span class="string">': 0.21580870716197162&#125;                                                                                          </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.959529052734375, '</span>learning_rate<span class="string">': 4.769804045693897e-05, '</span>epoch<span class="string">': 0.23019595430610307&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9772723388671876, '</span>learning_rate<span class="string">': 4.7554167985497656e-05, '</span>epoch<span class="string">': 0.24458320145023452&#125;                                                                                          </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.956650146484375, '</span>learning_rate<span class="string">': 4.741029551405634e-05, '</span>epoch<span class="string">': 0.25897044859436597&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9764119873046875, '</span>learning_rate<span class="string">': 4.726642304261503e-05, '</span>epoch<span class="string">': 0.2733576957384974&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9451396484375, '</span>learning_rate<span class="string">': 4.7122550571173715e-05, '</span>epoch<span class="string">': 0.28774494288262886&#125;                                                                                             </span></span><br><span class="line"><span class="string">  6%|████████                                                                                                                                    | 10000/173765 [1:29:26&lt;27:18:00,  1.67it/s][INFO|trainer.py:1248] 2020-12-28 19:58:05,894 &gt;&gt; Saving model checkpoint to output/checkpoint-10000</span></span><br><span class="line"><span class="string">[INFO|configuration_utils.py:289] 2020-12-28 19:58:05,896 &gt;&gt; Configuration saved in output/checkpoint-10000/config.json</span></span><br><span class="line"><span class="string">[INFO|modeling_utils.py:814] 2020-12-28 19:58:09,676 &gt;&gt; Model weights saved in output/checkpoint-10000/pytorch_model.bin</span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.967901123046875, '</span>learning_rate<span class="string">': 4.69786780997324e-05, '</span>epoch<span class="string">': 0.3021321900267603&#125;                                                                                              </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9398819580078126, '</span>learning_rate<span class="string">': 4.683480562829108e-05, '</span>epoch<span class="string">': 0.3165194371708917&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9575858154296875, '</span>learning_rate<span class="string">': 4.669093315684977e-05, '</span>epoch<span class="string">': 0.3309066843150232&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.935358642578125, '</span>learning_rate<span class="string">': 4.6547060685408454e-05, '</span>epoch<span class="string">': 0.3452939314591546&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.920288330078125, '</span>learning_rate<span class="string">': 4.640318821396714e-05, '</span>epoch<span class="string">': 0.359681178603286&#125;                                                                                              </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9564296875, '</span>learning_rate<span class="string">': 4.6259315742525826e-05, '</span>epoch<span class="string">': 0.3740684257474175&#125;                                                                                                 </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9050736083984374, '</span>learning_rate<span class="string">': 4.611544327108451e-05, '</span>epoch<span class="string">': 0.3884556728915489&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9197130126953126, '</span>learning_rate<span class="string">': 4.59715707996432e-05, '</span>epoch<span class="string">': 0.4028429200356804&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9287261962890625, '</span>learning_rate<span class="string">': 4.5827698328201885e-05, '</span>epoch<span class="string">': 0.4172301671798118&#125;                                                                                           </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.897843017578125, '</span>learning_rate<span class="string">': 4.568382585676057e-05, '</span>epoch<span class="string">': 0.43161741432394324&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.918166015625, '</span>learning_rate<span class="string">': 4.553995338531926e-05, '</span>epoch<span class="string">': 0.4460046614680747&#125;                                                                                                </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.9367061767578124, '</span>learning_rate<span class="string">': 4.5396080913877944e-05, '</span>epoch<span class="string">': 0.46039190861220614&#125;                                                                                          </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.93353662109375, '</span>learning_rate<span class="string">': 4.525220844243663e-05, '</span>epoch<span class="string">': 0.47477915575633756&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.894266357421875, '</span>learning_rate<span class="string">': 4.510833597099532e-05, '</span>epoch<span class="string">': 0.48916640290046903&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.89986474609375, '</span>learning_rate<span class="string">': 4.4964463499553996e-05, '</span>epoch<span class="string">': 0.5035536500446005&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.89962744140625, '</span>learning_rate<span class="string">': 4.482059102811268e-05, '</span>epoch<span class="string">': 0.5179408971887319&#125;                                                                                              </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.8909339599609376, '</span>learning_rate<span class="string">': 4.467671855667137e-05, '</span>epoch<span class="string">': 0.5323281443328634&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.865968505859375, '</span>learning_rate<span class="string">': 4.4532846085230055e-05, '</span>epoch<span class="string">': 0.5467153914769948&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.881063232421875, '</span>learning_rate<span class="string">': 4.438897361378874e-05, '</span>epoch<span class="string">': 0.5611026386211262&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.879991943359375, '</span>learning_rate<span class="string">': 4.424510114234742e-05, '</span>epoch<span class="string">': 0.5754898857652577&#125;                                                                                             </span></span><br><span class="line"><span class="string"> 12%|████████████████                                                                                                                            | 20000/173765 [2:59:09&lt;24:13:30,  1.76it/s][INFO|trainer.py:1248] 2020-12-28 21:27:48,397 &gt;&gt; Saving model checkpoint to output/checkpoint-20000</span></span><br><span class="line"><span class="string">[INFO|configuration_utils.py:289] 2020-12-28 21:27:48,399 &gt;&gt; Configuration saved in output/checkpoint-20000/config.json</span></span><br><span class="line"><span class="string">[INFO|modeling_utils.py:814] 2020-12-28 21:27:52,186 &gt;&gt; Model weights saved in output/checkpoint-20000/pytorch_model.bin</span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.8942625732421876, '</span>learning_rate<span class="string">': 4.410122867090611e-05, '</span>epoch<span class="string">': 0.5898771329093891&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.889036376953125, '</span>learning_rate<span class="string">': 4.3957356199464794e-05, '</span>epoch<span class="string">': 0.6042643800535206&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.867447998046875, '</span>learning_rate<span class="string">': 4.381348372802348e-05, '</span>epoch<span class="string">': 0.618651627197652&#125;                                                                                              </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.859236328125, '</span>learning_rate<span class="string">': 4.3669611256582166e-05, '</span>epoch<span class="string">': 0.6330388743417834&#125;                                                                                               </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.8667769775390626, '</span>learning_rate<span class="string">': 4.352573878514085e-05, '</span>epoch<span class="string">': 0.6474261214859149&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.864222900390625, '</span>learning_rate<span class="string">': 4.338186631369954e-05, '</span>epoch<span class="string">': 0.6618133686300464&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.876696044921875, '</span>learning_rate<span class="string">': 4.3237993842258225e-05, '</span>epoch<span class="string">': 0.6762006157741778&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.8643328857421875, '</span>learning_rate<span class="string">': 4.309412137081691e-05, '</span>epoch<span class="string">': 0.6905878629183092&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.8401954345703124, '</span>learning_rate<span class="string">': 4.29502488993756e-05, '</span>epoch<span class="string">': 0.7049751100624406&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.83107666015625, '</span>learning_rate<span class="string">': 4.280637642793428e-05, '</span>epoch<span class="string">': 0.719362357206572&#125;                                                                                               </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.841105712890625, '</span>learning_rate<span class="string">': 4.2662503956492964e-05, '</span>epoch<span class="string">': 0.7337496043507036&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.849765625, '</span>learning_rate<span class="string">': 4.251863148505165e-05, '</span>epoch<span class="string">': 0.748136851494835&#125;                                                                                                    </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.832751220703125, '</span>learning_rate<span class="string">': 4.2374759013610336e-05, '</span>epoch<span class="string">': 0.7625240986389664&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.828905517578125, '</span>learning_rate<span class="string">': 4.223088654216902e-05, '</span>epoch<span class="string">': 0.7769113457830978&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.853523193359375, '</span>learning_rate<span class="string">': 4.208701407072771e-05, '</span>epoch<span class="string">': 0.7912985929272293&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.835915771484375, '</span>learning_rate<span class="string">': 4.1943141599286395e-05, '</span>epoch<span class="string">': 0.8056858400713608&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.845493896484375, '</span>learning_rate<span class="string">': 4.179926912784508e-05, '</span>epoch<span class="string">': 0.8200730872154922&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.833302490234375, '</span>learning_rate<span class="string">': 4.165539665640377e-05, '</span>epoch<span class="string">': 0.8344603343596236&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7969888916015626, '</span>learning_rate<span class="string">': 4.1511524184962454e-05, '</span>epoch<span class="string">': 0.848847581503755&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.813013671875, '</span>learning_rate<span class="string">': 4.136765171352114e-05, '</span>epoch<span class="string">': 0.8632348286478865&#125;                                                                                                </span></span><br><span class="line"><span class="string"> 17%|████████████████████████▏                                                                                                                   | 30000/173765 [4:28:54&lt;26:16:01,  1.52it/s][INFO|trainer.py:1248] 2020-12-28 22:57:34,191 &gt;&gt; Saving model checkpoint to output/checkpoint-30000</span></span><br><span class="line"><span class="string">[INFO|configuration_utils.py:289] 2020-12-28 22:57:34,193 &gt;&gt; Configuration saved in output/checkpoint-30000/config.json</span></span><br><span class="line"><span class="string">[INFO|modeling_utils.py:814] 2020-12-28 22:57:37,929 &gt;&gt; Model weights saved in output/checkpoint-30000/pytorch_model.bin</span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.8161768798828124, '</span>learning_rate<span class="string">': 4.122377924207983e-05, '</span>epoch<span class="string">': 0.877622075792018&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.800627197265625, '</span>learning_rate<span class="string">': 4.107990677063851e-05, '</span>epoch<span class="string">': 0.8920093229361494&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.80393505859375, '</span>learning_rate<span class="string">': 4.093603429919719e-05, '</span>epoch<span class="string">': 0.9063965700802809&#125;                                                                                              </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.8064656982421874, '</span>learning_rate<span class="string">': 4.079216182775588e-05, '</span>epoch<span class="string">': 0.9207838172244123&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.800693359375, '</span>learning_rate<span class="string">': 4.0648289356314565e-05, '</span>epoch<span class="string">': 0.9351710643685437&#125;                                                                                               </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.8068582763671874, '</span>learning_rate<span class="string">': 4.050441688487325e-05, '</span>epoch<span class="string">': 0.9495583115126751&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.8205863037109375, '</span>learning_rate<span class="string">': 4.036054441343193e-05, '</span>epoch<span class="string">': 0.9639455586568066&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.83301171875, '</span>learning_rate<span class="string">': 4.021667194199062e-05, '</span>epoch<span class="string">': 0.9783328058009381&#125;                                                                                                 </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.8173153076171875, '</span>learning_rate<span class="string">': 4.0072799470549304e-05, '</span>epoch<span class="string">': 0.9927200529450695&#125;                                                                                           </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.782671630859375, '</span>learning_rate<span class="string">': 3.992892699910799e-05, '</span>epoch<span class="string">': 1.007107300089201&#125;                                                                                              </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7521663818359374, '</span>learning_rate<span class="string">': 3.9785054527666676e-05, '</span>epoch<span class="string">': 1.0214945472333323&#125;                                                                                           </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.767458740234375, '</span>learning_rate<span class="string">': 3.964118205622536e-05, '</span>epoch<span class="string">': 1.0358817943774639&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7407720947265626, '</span>learning_rate<span class="string">': 3.949730958478405e-05, '</span>epoch<span class="string">': 1.0502690415215952&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.78615087890625, '</span>learning_rate<span class="string">': 3.9353437113342735e-05, '</span>epoch<span class="string">': 1.0646562886657267&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7433726806640626, '</span>learning_rate<span class="string">': 3.920956464190142e-05, '</span>epoch<span class="string">': 1.0790435358098582&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7276751708984375, '</span>learning_rate<span class="string">': 3.906569217046011e-05, '</span>epoch<span class="string">': 1.0934307829539895&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7605506591796876, '</span>learning_rate<span class="string">': 3.8921819699018794e-05, '</span>epoch<span class="string">': 1.107818030098121&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.75696728515625, '</span>learning_rate<span class="string">': 3.8777947227577474e-05, '</span>epoch<span class="string">': 1.1222052772422524&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7471634521484376, '</span>learning_rate<span class="string">': 3.863407475613616e-05, '</span>epoch<span class="string">': 1.136592524386384&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.72530859375, '</span>learning_rate<span class="string">': 3.8490202284694846e-05, '</span>epoch<span class="string">': 1.1509797715305154&#125;                                                                                                </span></span><br><span class="line"><span class="string"> 23%|████████████████████████████████▏                                                                                                           | 40000/173765 [5:58:08&lt;24:17:23,  1.53it/s][INFO|trainer.py:1248] 2020-12-29 00:26:47,669 &gt;&gt; Saving model checkpoint to output/checkpoint-40000</span></span><br><span class="line"><span class="string">[INFO|configuration_utils.py:289] 2020-12-29 00:26:47,672 &gt;&gt; Configuration saved in output/checkpoint-40000/config.json</span></span><br><span class="line"><span class="string">[INFO|modeling_utils.py:814] 2020-12-29 00:26:51,427 &gt;&gt; Model weights saved in output/checkpoint-40000/pytorch_model.bin</span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7199117431640625, '</span>learning_rate<span class="string">': 3.834632981325353e-05, '</span>epoch<span class="string">': 1.1653670186746468&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.74417578125, '</span>learning_rate<span class="string">': 3.820245734181222e-05, '</span>epoch<span class="string">': 1.1797542658187783&#125;                                                                                                 </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.718279296875, '</span>learning_rate<span class="string">': 3.8058584870370905e-05, '</span>epoch<span class="string">': 1.1941415129629096&#125;                                                                                               </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.739827392578125, '</span>learning_rate<span class="string">': 3.791471239892959e-05, '</span>epoch<span class="string">': 1.2085287601070411&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7394217529296876, '</span>learning_rate<span class="string">': 3.777083992748828e-05, '</span>epoch<span class="string">': 1.2229160072511727&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7399776611328126, '</span>learning_rate<span class="string">': 3.7626967456046964e-05, '</span>epoch<span class="string">': 1.237303254395304&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7402420654296875, '</span>learning_rate<span class="string">': 3.748309498460565e-05, '</span>epoch<span class="string">': 1.2516905015394355&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7499097900390626, '</span>learning_rate<span class="string">': 3.733922251316434e-05, '</span>epoch<span class="string">': 1.2660777486835668&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.731468994140625, '</span>learning_rate<span class="string">': 3.719535004172302e-05, '</span>epoch<span class="string">': 1.2804649958276983&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7096973876953125, '</span>learning_rate<span class="string">': 3.705147757028171e-05, '</span>epoch<span class="string">': 1.2948522429718299&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.715921875, '</span>learning_rate<span class="string">': 3.690760509884039e-05, '</span>epoch<span class="string">': 1.3092394901159612&#125;                                                                                                   </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7501182861328124, '</span>learning_rate<span class="string">': 3.6763732627399075e-05, '</span>epoch<span class="string">': 1.3236267372600927&#125;                                                                                           </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.75416552734375, '</span>learning_rate<span class="string">': 3.6619860155957755e-05, '</span>epoch<span class="string">': 1.338013984404224&#125;                                                                                              </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7309554443359374, '</span>learning_rate<span class="string">': 3.647598768451644e-05, '</span>epoch<span class="string">': 1.3524012315483556&#125;                                                                                            </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.7114766845703124, '</span>learning_rate<span class="string">': 3.633211521307513e-05, '</span>epoch<span class="string">': 1.366788478692487&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.71192626953125, '</span>learning_rate<span class="string">': 3.6188242741633814e-05, '</span>epoch<span class="string">': 1.3811757258366184&#125;                                                                                             </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.72509228515625, '</span>learning_rate<span class="string">': 3.60443702701925e-05, '</span>epoch<span class="string">': 1.39556297298075&#125;                                                                                                 </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.6887713623046876, '</span>learning_rate<span class="string">': 3.5900497798751186e-05, '</span>epoch<span class="string">': 1.4099502201248812&#125;                                                                                           </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.70512939453125, '</span>learning_rate<span class="string">': 3.575662532730987e-05, '</span>epoch<span class="string">': 1.4243374672690128&#125;                                                                                              </span></span><br><span class="line"><span class="string">&#123;'</span>loss<span class="string">': 1.6820177001953125, '</span>learning_rate<span class="string">': 3.561275285586856e-05, '</span>epoch<span class="string">': 1.4387247144131443&#125;</span></span><br></pre></td></tr></table></figure><h1 id="DEBUG"><a href="#DEBUG" class="headerlink" title="DEBUG"></a>DEBUG</h1><h2 id="1-Import-Error"><a href="#1-Import-Error" class="headerlink" title="1. Import Error"></a>1. Import Error</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: cannot import name <span class="string">'is_main_process'</span> from <span class="string">'transformers.trainer_utils'</span></span><br></pre></td></tr></table></figure><p>原因是安装transformer的时候一些辅助代码是没有安装的 ，有两种<a href="https://github.com/huggingface/transformers/issues/8210" target="_blank" rel="noopener">解决方案</a>：</p><ol><li>从github安装transformer，使用命令：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/huggingface/transformers.git</span><br><span class="line"><span class="built_in">cd</span> transformers</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><ol><li>直接去找到对应的函数，复制粘贴到当前文件中。</li></ol><p>为了简单起见，我采用第二种方法。</p><h2 id="2-ConnectionError-text-py"><a href="#2-ConnectionError-text-py" class="headerlink" title="2. ConnectionError: text.py"></a>2. ConnectionError: text.py</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Couldn<span class="string">'t reach https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/text/text.py</span></span><br></pre></td></tr></table></figure><p>在服务器上使用的代理问题，导致不能自动联网下载需要的py文件。</p><p>手动下载text.py文件，然后放置在当前目录下的<code>text/text.py</code>中。再次尝试运行。</p><p>Problem Solved !!!</p><h2 id="3-tensor-size"><a href="#3-tensor-size" class="headerlink" title="3. tensor size"></a>3. tensor size</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: The size of tensor a (762) must match the size of tensor b (512) at non-singleton dimension 1</span><br></pre></td></tr></table></figure><p>可以利用<code>python run_mlm.py --help</code>来查看帮助。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">max_seq_length: Optional[int] = field(</span><br><span class="line">    default=<span class="literal">None</span>,</span><br><span class="line">    metadata=&#123;</span><br><span class="line">        <span class="string">"help"</span>: <span class="string">"The maximum total input sequence length after tokenization. Sequences longer "</span></span><br><span class="line">        <span class="string">"than this will be truncated."</span></span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>原因是由于句子长度过长，超过了512的最大限制，在运行时指定<code>--max_seq_length</code>就可以解决问题。</p><h2 id="4-Evaluation"><a href="#4-Evaluation" class="headerlink" title="4. Evaluation"></a>4. Evaluation</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"run_mlm.py"</span>, line 420, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    main()</span><br><span class="line">  File <span class="string">"run_mlm.py"</span>, line 398, <span class="keyword">in</span> main</span><br><span class="line">    eval_output = trainer.evaluate()</span><br><span class="line">  File <span class="string">"/home/klhao/.conda/envs/klhao-pytorch/lib/python3.7/site-packages/transformers/trainer.py"</span>, line 1251, <span class="keyword">in</span> evaluate</span><br><span class="line">    output = self.prediction_loop(eval_dataloader, description=<span class="string">"Evaluation"</span>)</span><br><span class="line">  File <span class="string">"/home/klhao/.conda/envs/klhao-pytorch/lib/python3.7/site-packages/transformers/trainer.py"</span>, line 1353, <span class="keyword">in</span> prediction_loop</span><br><span class="line">    preds_host = logits <span class="keyword">if</span> preds_host is None <span class="keyword">else</span> nested_concat(preds_host, logits, dim=0)</span><br><span class="line">  File <span class="string">"/home/klhao/.conda/envs/klhao-pytorch/lib/python3.7/site-packages/transformers/trainer_pt_utils.py"</span>, line 49, <span class="keyword">in</span> nested_concat</span><br><span class="line">    <span class="built_in">return</span> torch.cat((tensors, new_tensors), dim=dim)</span><br><span class="line">RuntimeError: Sizes of tensors must match except <span class="keyword">in</span> dimension 1. Got 77 and 111 (The offending index is 0)</span><br></pre></td></tr></table></figure><p>原因是测试的时候对preds进行拼接，向量维度不匹配。</p><p>修改方法为在.sh加入命令, <code>--pad_to_max_length</code>，就可以确保句子长度一致。在NYT数据集上，max_length设为150就可以了。</p><p>但是，依然会出现爆显存的情况，可以修改<code>/home/klhao/.conda/envs/klhao-pytorch/lib/python3.7/site-packages/transformers/trainer.py</code>的1353行附近：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> loss <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    losses = loss.repeat(batch_size)</span><br><span class="line">    losses_host = losses <span class="keyword">if</span> losses_host <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> torch.cat((losses_host, losses), dim=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span> logits <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    logits = logits.cpu().numpy()  <span class="comment"># 减少显存</span></span><br><span class="line">    preds_host = logits <span class="keyword">if</span> preds_host <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> nested_concat(preds_host, logits, dim=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span> labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    labels = labels.cpu().numpy()  <span class="comment"># 减少显存</span></span><br><span class="line">    labels_host = labels <span class="keyword">if</span> labels_host <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> nested_concat(labels_host, labels, dim=<span class="number">0</span>)</span><br><span class="line">self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)</span><br></pre></td></tr></table></figure><p>Perfect! 完美~🎊🎗🎗🎃🎃✨🎉</p><h1 id="计算相似度"><a href="#计算相似度" class="headerlink" title="计算相似度"></a>计算相似度</h1><p>将句子转化为向量，然后计算cosine相似度，分为以下两种：</p><ol><li>relation phrase 直接投入BERT</li><li>句子取头尾实体之间以及两侧距离为k 的投入BERT</li></ol><p>取[CLS]对应的向量，计算得到cosine相似度即可~</p><h1 id="其他补充"><a href="#其他补充" class="headerlink" title="其他补充"></a>其他补充</h1><p>在Trainer.py代码中可以看到默认的optimizer和scheduler，为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_optimizer_and_scheduler</span><span class="params">(self, num_training_steps: int)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Setup the optimizer and the learning rate scheduler.</span></span><br><span class="line"><span class="string">        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the</span></span><br><span class="line"><span class="string">        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.optimizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            no_decay = [<span class="string">"bias"</span>, <span class="string">"LayerNorm.weight"</span>]</span><br><span class="line">            optimizer_grouped_parameters = [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"params"</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> self.model.named_parameters() <span class="keyword">if</span> <span class="keyword">not</span> any(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)],</span><br><span class="line">                    <span class="string">"weight_decay"</span>: self.args.weight_decay,</span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"params"</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> self.model.named_parameters() <span class="keyword">if</span> any(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)],</span><br><span class="line">                    <span class="string">"weight_decay"</span>: <span class="number">0.0</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">            ]</span><br><span class="line">            <span class="keyword">if</span> self.sharded_dpp:</span><br><span class="line">                self.optimizer = OSS(</span><br><span class="line">                    params=optimizer_grouped_parameters,</span><br><span class="line">                    optim=AdamW,</span><br><span class="line">                    lr=self.args.learning_rate,</span><br><span class="line">                    betas=(self.args.adam_beta1, self.args.adam_beta2),</span><br><span class="line">                    eps=self.args.adam_epsilon,</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.optimizer = AdamW(</span><br><span class="line">                    optimizer_grouped_parameters,</span><br><span class="line">                    lr=self.args.learning_rate,</span><br><span class="line">                    betas=(self.args.adam_beta1, self.args.adam_beta2),</span><br><span class="line">                    eps=self.args.adam_epsilon,</span><br><span class="line">                )</span><br><span class="line">        <span class="keyword">if</span> self.lr_scheduler <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.lr_scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">                self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps</span><br><span class="line">            )</span><br></pre></td></tr></table></figure><p>而线性调节器代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_linear_schedule_with_warmup</span><span class="params">(optimizer, num_warmup_steps, num_training_steps, last_epoch=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after</span></span><br><span class="line"><span class="string">    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        optimizer (:class:`~torch.optim.Optimizer`):</span></span><br><span class="line"><span class="string">            The optimizer for which to schedule the learning rate.</span></span><br><span class="line"><span class="string">        num_warmup_steps (:obj:`int`):</span></span><br><span class="line"><span class="string">            The number of steps for the warmup phase.</span></span><br><span class="line"><span class="string">        num_training_steps (:obj:`int`):</span></span><br><span class="line"><span class="string">            The total number of training steps.</span></span><br><span class="line"><span class="string">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span></span><br><span class="line"><span class="string">            The index of the last epoch when resuming training.</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lr_lambda</span><span class="params">(current_step: int)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> current_step &lt; num_warmup_steps:</span><br><span class="line">            <span class="keyword">return</span> float(current_step) / float(max(<span class="number">1</span>, num_warmup_steps))</span><br><span class="line">        <span class="keyword">return</span> max(</span><br><span class="line">            <span class="number">0.0</span>, float(num_training_steps - current_step) / float(max(<span class="number">1</span>, num_training_steps - num_warmup_steps))</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> LambdaLR(optimizer, lr_lambda, last_epoch)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Pretrained Language Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> MLM </tag>
            
            <tag> Pretrain </tag>
            
            <tag> BertForPreTraining </tag>
            
            <tag> BertForMaskedLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DISTRE debug 记录</title>
      <link href="/2020/12/20/DISTRE-debug-%E8%AE%B0%E5%BD%95/"/>
      <url>/2020/12/20/DISTRE-debug-%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="1-en-core-web-sm"><a href="#1-en-core-web-sm" class="headerlink" title="1. en_core_web_sm"></a>1. en_core_web_sm</h1><p>在Spacy中需要用到这个，但是在服务器上使用代理，会报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"/data/klhao/.conda/klhao-allennlp/lib/python3.6/site-packages/urllib3/connectionpool.py"</span>, line 696, <span class="keyword">in</span> urlopen</span><br><span class="line">    self._prepare_proxy(conn)</span><br><span class="line">  File <span class="string">"/data/klhao/.conda/klhao-allennlp/lib/python3.6/site-packages/urllib3/connectionpool.py"</span>, line 964, <span class="keyword">in</span> _prepare_proxy</span><br><span class="line">    conn.connect()</span><br><span class="line">  File <span class="string">"/data/klhao/.conda/klhao-allennlp/lib/python3.6/site-packages/urllib3/connection.py"</span>, line 366, <span class="keyword">in</span> connect</span><br><span class="line">    self._tunnel()</span><br><span class="line">  File <span class="string">"/data/klhao/.conda/klhao-allennlp/lib/python3.6/http/client.py"</span>, line 930, <span class="keyword">in</span> _tunnel</span><br><span class="line">    (version, code, message) = response._read_status()</span><br><span class="line">  File <span class="string">"/data/klhao/.conda/klhao-allennlp/lib/python3.6/http/client.py"</span>, line 280, <span class="keyword">in</span> _read_status</span><br><span class="line">    raise RemoteDisconnected(<span class="string">"Remote end closed connection without"</span></span><br><span class="line">http.client.RemoteDisconnected: Remote end closed connection without response</span><br></pre></td></tr></table></figure><p><a href="https://stackoom.com/question/3mGaI/%E7%94%B1%E4%BA%8E%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98-%E6%97%A0%E6%B3%95%E7%82%B9%E7%82%B9%E5%AE%89%E8%A3%85spacy-%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener">文档中</a>建议可以直接下载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz</span><br></pre></td></tr></table></figure><p>ok, it worked! ✨</p><a id="more"></a><h1 id="train-json"><a href="#train-json" class="headerlink" title="train.json"></a>train.json</h1><p>作者没有明确的说明采用的格式，但是可以推断出来：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">target = &#123;</span><br><span class="line">    'sentence': record['text'],</span><br><span class="line">    'head': &#123;'word': record['h']['name']&#125;,</span><br><span class="line">    'tail': &#123;'word': record['t']['name']&#125;,</span><br><span class="line">    'relation': record['relation']</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AllenNLP </tag>
            
            <tag> Pytorch </tag>
            
            <tag> DISTRE </tag>
            
            <tag> SpaCy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AllenNLP Jsonnet 模式</title>
      <link href="/2020/12/20/AllenNLP-Jsonnet-%E6%A8%A1%E5%BC%8F/"/>
      <url>/2020/12/20/AllenNLP-Jsonnet-%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>这篇文章是对官方教程的翻译：</p><p><a href="https://github.com/allenai/allennlp/blob/v0.6.1/tutorials/getting_started/using_as_a_library_pt1.md" target="_blank" rel="noopener">https://github.com/allenai/allennlp/blob/v0.6.1/tutorials/getting_started/using_as_a_library_pt1.md</a></p><p>正如前文中所提到的，Allennlp的基本使用流程是需要自定义两个文件：<code>datareader</code>；<code>model</code>;<br> 同时，如果存在已经定义好的<code>datareader</code>我们也是可以直接拿来就用。接下来要介绍的这个例子呢，主要起到了两个作用：①熟悉allennlp的基本框架和逻辑②学习如何自定义自己的模型。模型全部都定义完成了之后，需要写一个json文件用来完成对模型的基本配置。</p><p>  在这篇博客完成之后，我们将会能够构建一个对学术论文进行分类的模型，并且会使用该模型在其他的数据集上进行测试。目的是通过这篇博客让你对如何修改代码，如何训练模型，如何进行预测等等基本操作过程有一个大致的了解。在最后我们会贴出来源代码。</p><h4 id="3-2-构建自己的代码库"><a href="#3-2-构建自己的代码库" class="headerlink" title="3.2 构建自己的代码库"></a>3.2 构建自己的代码库</h4><p>  正如前面所提到的，要想自己利用<code>AllenNLP</code>实现一个模型，那么就需要写两部分的代码：DatasetReader/Model。所以呢，我们的下一步就是要把我们自己写的代码放在python能够找到的位置，这样我们的<code>AllenNLP</code>才能够调用这些模块呀。假设我们自己的代码库基础包名叫<code>my_library</code>，当然啦，你想起啥名字都行，只要这个库符合两个条件：①位于<code>python</code>的查找包的路径上②这个库里包含你写的<code>models</code>以及<code>dataset_readers</code></p><h4 id="3-3-编写你自己的DatasetReader"><a href="#3-3-编写你自己的DatasetReader" class="headerlink" title="3.3 编写你自己的DatasetReader"></a>3.3 编写你自己的DatasetReader</h4><p>  现在我们自己的代码库已经建立好啦，要开始写代码啦！当然啦，我们首先需要有一些数据。在这个教程中，我们想要预测某一篇学术论文的发表场合。具体一点说，就是给出一篇论文的标题和摘要，我们想要判断它到底是在”自然语言处理领域“，“机器学习领域”还是”人工智能领域”的论文。</p><p>  我们使用的数据是从<code>Semantic Scholar</code>搜索引擎上下载的，每篇文章都标注好了所属的领域。下载下来的数据文件是JSON格式的，每个文件都应该至少包含标题（<code>title</code>），摘要(<code>paperAbstract</code>)，以及所属领域（<code>venue</code>）这样三部分内容。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"title"</span>: <span class="string">"A review of Web searching studies and a framework for future research"</span>,</span><br><span class="line">  <span class="attr">"paperAbstract"</span>: <span class="string">"Research on Web searching is at an incipient stage. ..."</span>,</span><br><span class="line">  <span class="attr">"venue"</span>: <span class="string">"&#123;AI|ML|ACL&#125;"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>在这一部分呢，我们除了会完成文件读取的代码的编写，还会对这些代码进行测试，这是一个非常好的习惯！你们也要学会哦。我们测试的时候，会从数据集中抽出来10篇文章构成一个简单的代码测试数据集，用来测试我们代码的基本功能是不是都实现啦。</p><p>  当然啦，我们的测试代码是要采用<code>AllenNLP</code>中提供的测试接口滴。这些接口能够很好的帮我们做好测试的准备和收尸的工作。</p><p>  我们首先想一想我们需要测试的只是<code>DatasetReader</code>中的<code>read</code>函数，下面给出测试代码的写法</p><p> …………….此处省略1000字……………………..</p><p>测试文件写好啦，我们现在要开始正式的写<code>DatasetReader</code>方法啦。首先，我们需要写重写一下<code>_read</code>函数，这个函数从文本文件中获取数据，然后将数据转换成<code>tokens</code>表示的<code>instance</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_read</span><span class="params">(self, file_path)</span>:</span></span><br><span class="line">      <span class="keyword">with</span> open(cached_path(file_path), <span class="string">"r"</span>) <span class="keyword">as</span> data_file:</span><br><span class="line">          logger.info(<span class="string">"Reading instances from lines in file at: %s"</span>, file_path)</span><br><span class="line">          <span class="keyword">for</span> line_num, line <span class="keyword">in</span> enumerate(Tqdm.tqdm(data_file.readlines())):</span><br><span class="line">              line = line.strip(<span class="string">"\n"</span>)</span><br><span class="line">              <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                  <span class="keyword">continue</span></span><br><span class="line">              paper_json = json.loads(line)</span><br><span class="line">              title = paper_json[<span class="string">'title'</span>]</span><br><span class="line">              abstract = paper_json[<span class="string">'paperAbstract'</span>]</span><br><span class="line">              venue = paper_json[<span class="string">'venue'</span>]</span><br><span class="line">              <span class="keyword">yield</span> self.text_to_instance(title, abstract, venue)</span><br></pre></td></tr></table></figure><p>上面这就是<code>_read</code>函数的内容了，注意到我们在这里使用<code>open</code>函数来打开需要读取的文件。<strong>注意，open函数是把我们提供的这个路径当成网络资源(url)来访问并且读取的，这一点在后面需要下载的数据文件中起到了至关重要的作用</strong>。</p><p>  在打开了文件之后，<code>_read</code>函数的基本逻辑呢，是遍历文件中每一行数据，然后把这一行数据转换成<code>JSON</code>格式，从而提取出所需要的字段，并且利用<code>text_to_instance</code>将这些字段转换成<code>instance</code>类型的数据。<strong>注意，这个函数的返回值是一个instance而不是整个文本的instances，换句话说，这里只是教给程序如何去处理这些输入，具体的处理过程是透明的</strong>。还有一点需要提一下，这里用了一个很有趣的函数<code>Tqdm</code>，这个函数是什么？是进度条！每次调用一下就会打印一个进度条，<code>pretty neat</code>哈。下面给出<code>text_to_instance</code>的基本实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_to_instance</span><span class="params">(self, title: str, abstract: str, venue: str = None)</span> -&gt; Instance:</span></span><br><span class="line">    tokenized_title = self._tokenizer.tokenize(title)</span><br><span class="line">    tokenized_abstract = self._tokenizer.tokenize(abstract)</span><br><span class="line">    title_field = TextField(tokenized_title, self._token_indexers)</span><br><span class="line">    abstract_field = TextField(tokenized_abstract, self._token_indexers)</span><br><span class="line">    fields = &#123;<span class="string">'title'</span>: title_field, <span class="string">'abstract'</span>: abstract_field&#125;</span><br><span class="line">    <span class="keyword">if</span> venue <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        fields[<span class="string">'label'</span>] = LabelField(venue)</span><br><span class="line">    <span class="keyword">return</span> Instance(fields)</span><br></pre></td></tr></table></figure><p>首先，需要记住一点：<code>instance</code>类型实际上就是多个字段的集合。这些字段将会教给<code>AllenNLP</code>进行处理，并且传递给你的<code>model</code>。这里用到了<code>TextField</code>表示转化成了序号之后(<code>tokenized</code>)的文本数据；<code>LabelField</code>表示类别标签；此外还有很多类型的字段我们在这里暂时用不到就先不介绍了。需要额外说一句，我们在这里能够看出，<code>AllenNLP</code>在幕后做了很多工作，但是记住这些幕后工作也是可以调节的。</p><p>  其次，我们可以看到我们使用类成员变量（<code>self._tokenizer</code>等）来构造我们的<code>TextField</code>字段。我们需要简单介绍一下这里用到的两个成员变量：<code>self._tokenizer</code>/<code>self._token_indexers</code>。<code>Tokenizer</code>呢是把你的文本转换成单词啦，字母啦，比特对啦等等常见的你想要的形式。<code>TokenIndexer</code>呢则是给这些形式编个号，并且把最终的文本转换成序号表示的形式。举个例子来说，如果你的<code>token</code>是单词，那么我们强大的<code>TokenIndexer</code>可以自动的为你生成单词编号，字母编号，<code>pos_tags</code>的编号。你有可能意识到了一个问题，就是我们明明没有定义过这些成员变量，这些成员变量是从哪里来的呢！答：这些成员变量都是在<code>init</code>函数中定义的。下面我们就来看看这个函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DatasetReader.register("s2_papers")</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SemanticScholarDatasetReader</span><span class="params">(DatasetReader)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 tokenizer: Tokenizer = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 token_indexers: Dict[str, TokenIndexer] = None)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        self._tokenizer = tokenizer <span class="keyword">or</span> WordTokenizer()</span><br><span class="line">        self._token_indexers = token_indexers <span class="keyword">or</span> &#123;<span class="string">"tokens"</span>: SingleIdTokenIndexer()&#125;</span><br></pre></td></tr></table></figure><p>看一看，瞧一瞧哈，这个构造函数从参数中获取了<code>Tokenizer</code>以及<code>TokenIndexer</code>，这两个参数还都有默认值。我们只需要在调用这个构造函数的时候传递参数进去就行啦。但是<strong>注意！我们是不会自己去调用这个构造函数的，我们调用这个reader和model全都在config中完成，也就意味着我们所有的参数都将会配置文件中完成</strong>。当然啦，为了能够在配置文件中找到我们定义的这个<code>model</code>以及<code>dataset_reader</code>，我们需要给这两个类注册一个名字<code>@DatasetReader.register(&quot;s2_papers&quot;)</code>，像这样。有了这个注册，我们就能够在配置文件中使用这个类。<code>AllenNLP</code>为所有的注册的类都实现了一个<code>from_params</code>的方法，这个方法能够非常好根据配置文件中提供的信息，对应的调用构造函数，为我们构造<code>DatasetReader</code>以及<code>model</code>实例。</p><p>  就是这么简单！就是这个感觉！</p><h4 id="3-4-编写Model-代码"><a href="#3-4-编写Model-代码" class="headerlink" title="3.4 编写Model 代码"></a>3.4 编写Model 代码</h4><p>  我们现在有了可以处理数据的代码，只需要一个可以用的模型就可以跑啦。这也就是我们这一小节需要实现的东西。在这里，我们还是传统思路，先把测试文件定义好。</p><p>…………………….此处省略1000字……………………………</p><p>好啦，下面可以动真格的开始写模型啦。不过在开始写之前，我们首先需要确定一下模型的基本结构。我们有两个输入(标题和摘要)以及一个输出标签，注意这两个输入都应该已经是转换成序号啦。那么我们下一步自然就是需要把序号转换成对应的<code>embeddings</code>啦。接下来需要考虑的事情我们如何处理这些向量呢？我们的前馈神经网络该弄成几层？神经网络该是什么结构？不用怕！我们的<code>AllenNLP</code>在这里全封装，没有中间商赚差价，在这里能够让你花最少的时间完成一个基本的模型。我们先来看一看构造函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Model.register("paper_classifier")</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AcademicPaperClassifier</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab: Vocabulary,</span></span></span><br><span class="line"><span class="function"><span class="params">                 text_field_embedder: TextFieldEmbedder,</span></span></span><br><span class="line"><span class="function"><span class="params">                 title_encoder: Seq2VecEncoder,</span></span></span><br><span class="line"><span class="function"><span class="params">                 abstract_encoder: Seq2VecEncoder,</span></span></span><br><span class="line"><span class="function"><span class="params">                 classifier_feedforward: FeedForward,</span></span></span><br><span class="line"><span class="function"><span class="params">                 initializer: InitializerApplicator = InitializerApplicator<span class="params">()</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 regularizer: Optional[RegularizerApplicator] = None)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super(AcademicPaperClassifier, self).__init__(vocab, regularizer)</span><br><span class="line"></span><br><span class="line">        self.text_field_embedder = text_field_embedder</span><br><span class="line">        self.num_classes = self.vocab.get_vocab_size(<span class="string">"labels"</span>)</span><br><span class="line">        self.title_encoder = title_encoder</span><br><span class="line">        self.abstract_encoder = abstract_encoder</span><br><span class="line">        self.classifier_feedforward = classifier_feedforward</span><br><span class="line">        self.metrics = &#123;</span><br><span class="line">                <span class="string">"accuracy"</span>: CategoricalAccuracy(),</span><br><span class="line">                <span class="string">"accuracy3"</span>: CategoricalAccuracy(top_k=<span class="number">3</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        self.loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">        initializer(self)</span><br></pre></td></tr></table></figure><p>在这里，我们就像在<code>DatasetReader</code>当中一样注册一下我们的模型，方便配置文件的查找。<strong>注意，这里出现了一个奇怪的参数Vocabulary，这个参数顾名思义就是我们的数据字典，但是我们在哪里构造的呢？？？答案是不用构造！写model的时候顺手写上去就行啦，这个是Allennlp帮助我们写好的</strong>。同时这个数据字典其实是个复合字典，包括所有<code>TextField</code>的字典，以及<code>LabelField</code>自己单独的字典。然后需要介绍的参数就是<code>TextFieldEmbedder</code>为所有的<code>TextField</code>类共同建立了一个<code>embeddings</code>。</p><p>  利用这个<code>embeddings</code>以及我们输入的序号，我们就能够获得一个向量组成的序列。下一步就是对这个序列进行变化。在这里我们使用的是<code>Seq22VecEncoder</code>。这个<code>Encoder</code>可以有很多的变化，在这里我们使用的是最最简单的一种，就是<code>bag of embeddings</code>，直接求平均。当然啦，我们也可以使用什么<code>CNN</code>啦，<code>RNN</code>啦等高大上的模型，我们都有的!我们都有的！</p><p>  前馈神经网络呢也是一个预先定义好的<code>Module</code>，我们可以修改这个网络的深度宽度激活函数。<code>InitializerApplicator</code>包含着所有参数的基本初始化方法。如果你想自定义初始化，就需要时候用<code>RegularizerApplicator</code></p><p>  这些概念搞懂了之后，下面的操作就很简单啦。我们只需要像传统的<code>Pytorch</code>中的程序一样，重写一个<code>forward</code>函数就好啦。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">            title: Dict[str, torch.LongTensor],</span></span></span><br><span class="line"><span class="function"><span class="params">            abstract: Dict[str, torch.LongTensor],</span></span></span><br><span class="line"><span class="function"><span class="params">            label: torch.LongTensor = None)</span> -&gt; Dict[str, torch.Tensor]:</span></span><br><span class="line">    embedded_title = self.text_field_embedder(title)</span><br><span class="line">    title_mask = util.get_text_field_mask(title)</span><br><span class="line">    encoded_title = self.title_encoder(embedded_title, title_mask)</span><br><span class="line"></span><br><span class="line">    embedded_abstract = self.text_field_embedder(abstract)</span><br><span class="line">    abstract_mask = util.get_text_field_mask(abstract)</span><br><span class="line">    encoded_abstract = self.abstract_encoder(embedded_abstract, abstract_mask)</span><br><span class="line"></span><br><span class="line">    logits = self.classifier_feedforward(torch.cat([encoded_title, encoded_abstract], dim=<span class="number">-1</span>))</span><br><span class="line">    class_probabilities = F.softmax(logits)</span><br><span class="line"></span><br><span class="line">    output_dict = &#123;<span class="string">"class_probabilities"</span>: class_probabilities&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss = self.loss(logits, label.squeeze(<span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">for</span> metric <span class="keyword">in</span> self.metrics.values():</span><br><span class="line">            metric(logits, label.squeeze(<span class="number">-1</span>))</span><br><span class="line">        output_dict[<span class="string">"loss"</span>] = loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_dict</span><br></pre></td></tr></table></figure><p>我们首先注意到的应该是这个函数的参数。还记得我们写的<code>DatasetReader</code>吗？它构造的<code>instance</code>就包含了这样几个字段<code>Fields</code>。所以在这里，参数的名字一定要和<code>DatasetReader</code>中定义的名字保持一致。<code>AllenNLP</code>在这里将会自动的利用你的<code>DatasetReader</code>并且把数据组织成<code>batches</code>的形式，必要时还会给你<code>padding</code>一下。<strong>注意，forward函数接收的参数正是一个batch的数据</strong>。</p><p>  注意，我们要求必须把<code>labels</code>也传递给<code>forward</code>函数。这么做的主要目的是为了能够计算损失函数。在训练的时候，我们的模型会主动的去寻找这个<code>loss</code>，然后自动的反向传播回去，然后更改参数。<strong>同时我们也应该注意到，这个参数是可以为空的，这主要是为了应对prediction的情况</strong>。这个将会在后面章节中进行介绍。</p><p>  接下来，我们来看一看输入的类型。<code>label</code>很简单，他就是一个<code>[batch_size,1]</code>大小的<code>tensor</code>没什么好说的。另外两个可就有点复杂啦。如果你还记得，<code>title</code>和<code>abstract</code>两个是<code>TextField</code>类型的，然后这些<code>TextField</code>又被转换成了字典类型的。这个新的字典呢可能包括了单词id,字母array或者pos标签ID什么的。但是我们的<code>embedder</code>是不在乎你是什么鬼字典的，直接一股脑的扔进去就能够帮你完成转换过程。这就意味着我们<code>TextFieldEmbedder</code>必须和<code>TextField</code>完美的对接哇，不然不是要瞎转换啦。对接的过程又是在配置文件中完成的，在后面我们将会详细的讲解。</p><p>现在我们已经理解了模型的基本输入，来看看它的基本逻辑。模型干的第一件事就是找到<code>title</code>和<code>abstract</code>的<code>embeddings</code>,然后对这些向量进行操作。注意我们需要利用一个叫<code>masks</code>的变量来标识哪些元素仅仅是用来标识边界的，而不需要模型考虑。我们对这些向量进行了一通操作之后，生成了一个向量。然后把这个向量扔到一个前馈神经网络中就可以得到<code>class logits</code>其实就是预测为各个类的概率，有了这个概率我们就可以得到最终预测的结果。最后，如果是训练过程的话，我们还需要计算损失和评价标准。我们来看一看这两部分的代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(self, reset: bool = False)</span> -&gt; Dict[str, float]:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;metric_name: metric.get_metric(reset) <span class="keyword">for</span> metric_name, metric <span class="keyword">in</span> self.metrics.items()&#125;</span><br><span class="line">This method <span class="keyword">is</span> how the model tells the training code which metrics it<span class="string">'s computing. The second pieces of code is the decode method:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def decode(self, output_dict: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:</span></span><br><span class="line"><span class="string">        predictions = output_dict['</span>class_probabilities<span class="string">'].cpu().data.numpy()</span></span><br><span class="line"><span class="string">        argmax_indices = numpy.argmax(predictions, axis=-1)</span></span><br><span class="line"><span class="string">        labels = [self.vocab.get_token_from_index(x, namespace="labels")</span></span><br><span class="line"><span class="string">                  for x in argmax_indices]</span></span><br><span class="line"><span class="string">        output_dict['</span>label<span class="string">'] = labels</span></span><br><span class="line"><span class="string">        return output_dict</span></span><br></pre></td></tr></table></figure><p><code>decode</code>函数包括两个功能①是接收<code>forward</code>函数的返回值，并且对这个返回值进行操作，比如说算出具体是那个词啊等等。②是将数字变成字符，方便阅读。好啦，至此我们的模型已经构建好啦，现在我们可以测试啦。</p><h4 id="3-5-训练模型"><a href="#3-5-训练模型" class="headerlink" title="3.5 训练模型"></a>3.5 训练模型</h4><p>  为了训练模型，我们需要定义一个配置文件。首先来看对<code>dataset_reader</code>的定义。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"dataset_reader": &#123;</span><br><span class="line">  "type": "s2_papers"</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p> 这一部分配置应该包含着<code>DatasetReader</code>的输入参数。<code>type</code>这个字段呢，就是我们注册的自己写的<code>DatasetReader</code>的名字，剩下的参数都是直接传入到了构造函数中。在这里没有其他的参数，所以我们就会使用默认的<code>tokenizer</code>和<code>tokenIndexer</code>。</p><p>  接着看一看训练和测试的基本参数。下面这些参数定义了如何构造batch数据，如何训练模型。<code>sorting_keys</code>顾名思义是定义了我们所有<code>instance</code>的排序方式，排序可以提高效率（花在padding token上面的计算减少）；使用<code>AdaGrad</code>作为优化器，执行40个epoch，如果10个epoch没有改进的话就提前结束训练。并且这里的训练数据是从网上下载下来的，这也就是我们之前写读取文件的代码时用到了cached_path的原因。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"train_data_path"</span>: <span class="string">"https://s3-us-west-2.amazonaws.com/allennlp/datasets/academic-papers-example/train.jsonl"</span>,</span><br><span class="line">  <span class="attr">"validation_data_path"</span>: <span class="string">"https://s3-us-west-2.amazonaws.com/allennlp/datasets/academic-papers-example/dev.jsonl"</span>,</span><br><span class="line">  <span class="attr">"iterator"</span>: &#123;</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"bucket"</span>,</span><br><span class="line">    <span class="attr">"sorting_keys"</span>: [[<span class="string">"abstract"</span>, <span class="string">"num_tokens"</span>], [<span class="string">"title"</span>, <span class="string">"num_tokens"</span>]],</span><br><span class="line">    <span class="attr">"batch_size"</span>: <span class="number">64</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"trainer"</span>: &#123;</span><br><span class="line">    <span class="attr">"num_epochs"</span>: <span class="number">40</span>,</span><br><span class="line">    <span class="attr">"patience"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"cuda_device"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="attr">"grad_clipping"</span>: <span class="number">5.0</span>,</span><br><span class="line">    <span class="attr">"validation_metric"</span>: <span class="string">"+accuracy"</span>,</span><br><span class="line">    <span class="attr">"optimizer"</span>: &#123;</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"adagrad"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>最后一段配置新鲜出炉啦。<code>key</code>字段又出现啦，这是我们注册的模型名称。剩下的字段将会传递给模型的<code>from_params</code>函数。我们还定义了<code>TextFieldEmbedder</code>的配置，在这里用的是glove的向量作为我们的embeddings。当然我们也可以定义一些字母级别的embeddings,甚至可以定义完之后让他自动的卷积一下什么的，这都是可行的。在这里没有这种操作，也就不介绍啦。不过在<code>crf_tagger</code>中是存在的。</p><p>  <code>title</code>和<code>abstract</code>呢用的都是双向LSTM的模型作为<code>encoders</code>,其实就是pytorch 里面封装了一下<code>Seq2VecEncoder</code>，这样在代码中就可以给出统一的格式了。前馈神经网络可以自定义宽度深度和激活层。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">"model": &#123;</span><br><span class="line">    "type": "paper_classifier",</span><br><span class="line">    "text_field_embedder": &#123;</span><br><span class="line">      "tokens": &#123;</span><br><span class="line">        "type": "embedding",</span><br><span class="line">        "pretrained_file": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz",</span><br><span class="line">        "embedding_dim": 100,</span><br><span class="line">        "trainable": false</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    "title_encoder": &#123;</span><br><span class="line">      "type": "lstm",</span><br><span class="line">      "bidirectional": true,</span><br><span class="line">      "input_size": 100,</span><br><span class="line">      "hidden_size": 100,</span><br><span class="line">      "num_layers": 1,</span><br><span class="line">      "dropout": 0.2</span><br><span class="line">    &#125;,</span><br><span class="line">    "abstract_encoder": &#123;</span><br><span class="line">      "type": "lstm",</span><br><span class="line">      "bidirectional": true,</span><br><span class="line">      "input_size": 100,</span><br><span class="line">      "hidden_size": 100,</span><br><span class="line">      "num_layers": 1,</span><br><span class="line">      "dropout": 0.2</span><br><span class="line">    &#125;,</span><br><span class="line">    "classifier_feedforward": &#123;</span><br><span class="line">      "input_dim": 400,</span><br><span class="line">      "num_layers": 2,</span><br><span class="line">      "hidden_dims": [200, 3],</span><br><span class="line">      "activations": ["relu", "linear"],</span><br><span class="line">      "dropout": [0.2, 0.0]</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;,</span><br></pre></td></tr></table></figure><p>完啦！到了这里我们已经把自己的模型写完啦，现在可以直接在数据集上训练啦。现在呢，唯一的问题就是我们的<code>AllenNLP</code>并不认识我们的模型哇，我们必须吧它放在<code>python</code>的包路径上，然后执行时加上这么一个参数<code>--include-package my_library</code>。完整的执行命令如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">allennlp train \</span><br><span class="line">    experiments/venue_classifier.json \</span><br><span class="line">    -s /tmp/venue_output_dir \</span><br><span class="line">    --include-package my_library</span><br></pre></td></tr></table></figure><h1 id="3-6-实现预测功能"><a href="#3-6-实现预测功能" class="headerlink" title="3.6 实现预测功能"></a>3.6 实现预测功能</h1><blockquote><p>  当我们训练好模型之后，接下来需要做的就是预测了。预测的实现是本小节的主要内容。</p></blockquote><h4 id="3-6-1-创建预测器"><a href="#3-6-1-创建预测器" class="headerlink" title="3.6.1 创建预测器"></a>3.6.1 创建预测器</h4><p>  在我们前面写过的论文分类的模型当中，最核心的就是<code>forward</code>函数，这个函数长这个样子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                title: Dict[str, torch.LongTensor],</span></span></span><br><span class="line"><span class="function"><span class="params">                abstract: Dict[str, torch.LongTensor],</span></span></span><br><span class="line"><span class="function"><span class="params">                label: torch.LongTensor = None)</span> -&gt; Dict[str, torch.Tensor]:</span></span><br></pre></td></tr></table></figure><p>上面的<code>forward</code>函数已经很棒啦，其实我们以前呢都是在forward的之后，自己写个函数就叫个<code>predict</code>啥的，然后用这个去预测。但是呢，在<code>AllenNLP</code>里的解决方案是有所不同的。在这里把预测器定义成了一个类，重点重写一个<code>_json_to_instance</code>函数，这个函数呢主要工作是根据<code>json</code>格式的输入数据，生成<code>instance</code>，以及标签之类的信息，然后利用<code>forward</code>函数进行预测,最后把<code>forward</code>的返回结果和这玩意的返回结果一起返回给你。<strong>之所以要采用这种方式，主要是为了能够方便的用于展示demo</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Predictor.register('paper-classifier')</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PaperClassifierPredictor</span><span class="params">(Predictor)</span>:</span></span><br><span class="line">    <span class="string">"""Predictor wrapper for the AcademicPaperClassifier"""</span></span><br><span class="line"><span class="meta">    @overrides</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_json_to_instance</span><span class="params">(self, json_dict: JsonDict)</span> -&gt; Tuple[Instance, JsonDict]:</span></span><br><span class="line">        title = json_dict[<span class="string">'title'</span>]</span><br><span class="line">        abstract = json_dict[<span class="string">'paperAbstract'</span>]</span><br><span class="line">        instance = self._dataset_reader.text_to_instance(title=title, abstract=abstract)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># label_dict will be like &#123;0: "ACL", 1: "AI", ...&#125;</span></span><br><span class="line">        label_dict = self._model.vocab.get_index_to_token_vocabulary(<span class="string">'labels'</span>)</span><br><span class="line">        <span class="comment"># Convert it to list ["ACL", "AI", ...]</span></span><br><span class="line">        all_labels = [label_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(label_dict))]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> instance, &#123;<span class="string">"all_labels"</span>: all_labels&#125;</span><br></pre></td></tr></table></figure><p>  这段代码使用了前面写好的<code>text_to_instance</code>组成了一个只包含<code>title</code>和<code>abstract</code>两个字段的<code>instance</code>。这个函数的返回值是一个元组，包括了两个元素。第一个是返回的实例<code>instance</code>，第二个是一个字典，包含了所有的标签之类的东西，<font color="green">总之就是我们的forward的返回值没有提供的但是我们又需要的东西都放在这里就好啦</font>。<strong>在这里我们只是存储所有可能的标签，其他的东西暂时就不存啦。</strong>当然，如果你连标签都用不着，那你就放个空空的字典在这里占位就好啦。</p><h4 id="3-6-2-测试预测器"><a href="#3-6-2-测试预测器" class="headerlink" title="3.6.2 测试预测器"></a>3.6.2 测试预测器</h4><p>………………………..此处省略N个字…………………………..</p><p>经过这些测试用例的验证，我们就可以放心大胆的使用预测的功能啦。下面给出基本的使用方法。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">usage: allennlp [<span class="built_in">command</span>] predict [-h]</span><br><span class="line">                                  [--output-file OUTPUT_FILE]</span><br><span class="line">                                  [--batch-size BATCH_SIZE]</span><br><span class="line">                                  [--silent]</span><br><span class="line">                                  [--cuda-device CUDA_DEVICE]</span><br><span class="line">                                  [-o OVERRIDES]</span><br><span class="line">                                  [--include-package INCLUDE_PACKAGE]</span><br><span class="line">                                  [--predictor PREDICTOR]</span><br><span class="line">                                  archive_file input_file</span><br></pre></td></tr></table></figure><p>留心一下上面的代码有两个必须要有的参数，输入文件，以及训练好的模型。下面给出一个例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">allennlp predict \</span><br><span class="line">    tests/fixtures/model.tar.gz \</span><br><span class="line">    tests/fixtures/s2_papers.jsonl \</span><br><span class="line">    --include-package my_library \</span><br><span class="line">    --predictor paper-classifier</span><br><span class="line">When you run this it will <span class="built_in">print</span> the ten <span class="built_in">test</span> inputs and their predictions, each of <span class="built_in">which</span> looks like:</span><br><span class="line"></span><br><span class="line">prediction:  &#123;<span class="string">"all_labels"</span>: [<span class="string">"AI"</span>, <span class="string">"ACL"</span>, <span class="string">"ML"</span>], <span class="string">"logits"</span>: [0.008737504482269287, 0.22074833512306213, -0.005263201892375946], <span class="string">"class_probabilities"</span>: [0.31034138798713684, 0.38363200426101685, 0.3060266375541687], <span class="string">"label"</span>: <span class="string">"ACL"</span>&#125;</span><br></pre></td></tr></table></figure><h1 id="3-7-运行在线的demo"><a href="#3-7-运行在线的demo" class="headerlink" title="3.7 运行在线的demo"></a>3.7 运行在线的demo</h1><p>一旦你训练好了模型，并且配置好了一个预测器，，就很容易运行一个简单的web Demo啦。只需要执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python -m allennlp.service.server_simple \</span><br><span class="line">    --archive-path tests/fixtures/model.tar.gz \</span><br><span class="line">    --predictor paper-classifier \</span><br><span class="line">    --include-package my_library \</span><br><span class="line">    --title <span class="string">"Academic Paper Classifier"</span> \</span><br><span class="line">    --field-name title \</span><br><span class="line">    --field-name paperAbstract</span><br></pre></td></tr></table></figure><p>执行上面的命令将会在<code>localhost:8000</code>开启一个简单的服务，如图所示</p><p><img src="/images/blog/2020/allennlp/demo.webp" alt></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AllenNLP </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Allennlp 入门</title>
      <link href="/2020/12/19/Allennlp-%E5%85%A5%E9%97%A8/"/>
      <url>/2020/12/19/Allennlp-%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<p>在最近的实验中，对比方法里有一个需要用到AllenNLP，因此，对AllenNLP进行一个入门的了解。</p><h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>Allennlp将NLP任务处理流程中的各个阶段都做了一定程度的抽象，在软件设计上讲就是，实现了高内聚，低耦合，让我们能够专注于特定模块的逻辑，而无需其他流程的改动，极大程度上减少了工作量。</p><p>那基础且重要的处理流程有：</p><ul><li>DatasetReader：从文件中读取数据，转化为Instance集合</li><li>Model：模型主体</li><li>Iterator：迭代数据，提取batch数据</li><li>Trainer：模型训练器，并记录metric</li><li>Predictor：使用训练好的模型来预测数据</li></ul><p>以上每个流程是松耦合的，没有强关联，需要全局配置的东西非常少。比如在NLP处理中最常见的一个：num_embedding/vocab_size 需要提前全局给定，或者使用单独的配置来存储，只不过带来的问题就是这个配置在很多地方都可能会使用到，故参数的传递就又会增加代码的冗杂度，使用起来很不方便。在Allennlp中会有一个vocabulary全局存储，无需单独配置，且提供函数接口调用获取，非常简单。<a id="more"></a></p><h1 id="DatasetReader"><a href="#DatasetReader" class="headerlink" title="DatasetReader"></a>DatasetReader</h1><p>数据预处理，繁琐无聊但又少不了，而Allennlp让我们只关注于核心的数据读取，通用的东西早已封装好，核心的逻辑一个都逃不了，比如在DatasetReader我们只需要实现两个函数：_read , text_to_instance，即可完成数据预处理整个流程，其他的比如build_vocabulary, idx2word, word2idx, get_vocab_size等都帮我们做辽，就问你这个工具棒不棒，妙不妙。</p><p>别以为_read和text_to_instance函数逻辑有多复杂，其实再简单不过了：</p><ol><li>从本地读取数据</li><li>从数据中读取相关数据字段</li><li>将提取的数据转化成Instance数组</li></ol><p>示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DatasetReader.register("pos")</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PosDatasetReader</span><span class="params">(DatasetReader)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    DatasetReader for PoS tagging data, one sentence per line, like</span></span><br><span class="line"><span class="string">    The###DET dog###NN ate###V the###DET apple###NN</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, token_indexers: Dict[str, TokenIndexer] = None)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super().__init__(lazy=<span class="literal">False</span>)</span><br><span class="line">        self.token_indexers = token_indexers <span class="keyword">or</span> &#123;<span class="string">"tokens"</span>: SingleIdTokenIndexer()&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_to_instance</span><span class="params">(self, tokens: List[Token], tags: List[str] = None)</span> -&gt; Instance:</span></span><br><span class="line">        sentence_field = TextField(tokens, self.token_indexers)</span><br><span class="line">        fields = &#123;<span class="string">"sentence"</span>: sentence_field&#125;</span><br><span class="line">        <span class="keyword">if</span> tags:</span><br><span class="line">            label_field = SequenceLabelField(labels=tags, sequence_field=sentence_field)</span><br><span class="line">            fields[<span class="string">"labels"</span>] = label_field</span><br><span class="line">        <span class="keyword">return</span> Instance(fields)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_read</span><span class="params">(self, file_path: str)</span> -&gt; Iterator[Instance]:</span></span><br><span class="line">        <span class="keyword">with</span> open(file_path) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                pairs = line.strip().split()</span><br><span class="line">                sentence, tags = zip(*(pair.split(<span class="string">"###"</span>) <span class="keyword">for</span> pair <span class="keyword">in</span> pairs))</span><br><span class="line">                <span class="keyword">yield</span> self.text_to_instance([Token(word) <span class="keyword">for</span> word <span class="keyword">in</span> sentence], tags)</span><br></pre></td></tr></table></figure><p>读取的Instance集合结构如下所示：</p><p><img src="/images/blog/2020/allennlp/datasetreader.jpg" alt></p><p>需要注意这里的text_to_instance函数是<strong>重写</strong>，另外也可以将text_to_instance中的代码搬迁到_read中去，程序不会出错，只不过鉴于设计模式中的<strong>单一职责</strong>规定，建议将不同逻辑的代码使用不同函数进行隔离。</p><p>这个时候，有心人会注意到，DatasetReader 和 Pytorch中的Dataset、DataLoader的异同点是啥？</p><p>答：DatasetReader是对后两者的部分抽象融合。</p><ul><li><p>相同点</p></li><li><ul><li>都完成数据的读取与抽象的功能</li><li>都能够实现数据的懒加载（针对于大数据量的情况）</li><li>都有加速数据读取的性能的机制</li></ul></li><li><p>不同点</p></li><li><ul><li>DatasetReader合二为一，更加简单易懂</li><li>DatasetReader只需要实现一个函数即可完成所有功能（并不是说所有功能在DatasetReader中实现，而是配合其他模块一起实现）</li><li>DatasetReader能够在数据类型上支持多种不同NLP任务</li><li>其他的我都不说了，反正很简单……</li></ul></li></ul><p>说了不同点，可还没有说到其核心强大之处。我将以三点说： - 懒加载数据 - 约定胜于配置 - 不同类型的Field</p><h3 id="懒加载"><a href="#懒加载" class="headerlink" title="懒加载"></a>懒加载</h3><p>在编写_read函数的时候，使用yield关键字返回数据，DatasetReader于是对于数据的读取天然有着一种lazy generator模式，支持懒加载数据，只需要在传递lazy参数就行。</p><p>当然，对内存有所优化，在时间上就会有所消耗，这是所有算法都不能避免的，针对不同的数据集使用不同的数据加载模式。</p><h3 id="约定胜于配置"><a href="#约定胜于配置" class="headerlink" title="约定胜于配置"></a>约定胜于配置</h3><p>细心的朋友会发现，我们重写的是_read函数，这是一个私有函数，也是整个数据读取最核心的函数，Allennlp帮我们完成了read函数，并会自动调用_read函数读取数据。我们只需要实现这个函数，其他数据整理与读取进度条的展示等附带功能就会自动帮我们完成。</p><h3 id="不同类型的Field"><a href="#不同类型的Field" class="headerlink" title="不同类型的Field"></a>不同类型的Field</h3><p>DatasetReader最终返回的是Instance实例的集合，而Instance实际上是一个字典类型的数据：<code>MutableMapping[str, Field]</code>，value是Field类型的数据，常用的Field类型有：</p><ul><li>TextField</li><li>LabelField</li><li>SequenceLabelField</li><li>KnowledgeGraphField</li><li>…</li></ul><p>首先，Field的作用是存储token相关信息，不同的Field实现类能够存储不同任务下的数据结构信息，比如sentence-classification与POS/NER的数据结构就不一样。且不同实现有不同的处理方法，Allennlp给我们提供了很多内置实现，详细可以去看<a href="https://link.zhihu.com/?target=https%3A//github.com/allenai/allennlp/tree/master/allennlp/data/fields" target="_blank" rel="noopener">官方代码</a>，这里我就不一一介绍，简单介绍几个：TextField，LabelField</p><p>先看TextField，用来存储序列（tokens）数据，在初始化的时候是需要传递token_indexers，以对数据进行索引映射。</p><p>而LabelField和SequenceLabelField就不需要token_indexers，因为本身就是label标签，无需使用不同的索引映射，故使用统一默认（SingleId）的索引映射处理即可。</p><p>好了，我们知道了Instance是如何使用Field存储数据，那如何把字段塞进model.forward函数中去呢？</p><p>答：Instance中的key值与forward函数中的参数名对应。</p><p>比如以上代码中，每个Instance有sentence和label两个字段（Field），故在model.forward函数也有与之对应的参数名，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Model.register("lstm-tagger")</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LstmTagger</span><span class="params">(Model)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 其他函数我就不展示出来了</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                sentence: Dict[str, torch.Tensor],</span></span></span><br><span class="line"><span class="function"><span class="params">                label: torch.Tensor = None)</span> -&gt; Dict[str, torch.Tensor]:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>【思考题】：咳咳～～，细心的朋友可能会观察到，sentence的数据类型是<code>Dict[str, torch.Tensor]</code>，可我在Instance中sentence键上存储的就是TextField。前面也说了，TextField本身就是需要将数据转化成index，然后用pytorch包装成torch.Tensor数据类型，那为什么会是Dict[str,torch.Tensor]数据类型呢？ 【线索】：token_indexers , 思考五秒钟……</p><h2 id="token-indexers-amp-token-embedders"><a href="#token-indexers-amp-token-embedders" class="headerlink" title="token_indexers &amp; token_embedders"></a>token_indexers &amp; token_embedders</h2><p>为了讲清楚上面这个问题，我们再跟随着sentence这个字段从读取到映射成词向量这整个流程来讲解。</p><font color="orange">sentence字段的处理分为以下几个阶段： 1. tokenizer -> 分词 2. Token -> 转化为单个Token对象 3. Instance -> 转化为Instance实例 4. Iterator -> 并组装成batch模式 5. model.forward -> 塞给模型去执行 6. token_embedders -> 将idx转化成词向量 </font><ol><li><strong>tokenizer -&gt; 分词</strong></li></ol><p>这个过程是在文本读取的时候执行的。在DatasetReader初始化的时候，会将tokenizer传递到构造函数当中，没有的话就初始化一个默认分词器。这个分词器可以是一个，也可以是多个，取决于在参数列表里面传递的个数。</p><p>对于英文分词，allennlp有内置的WordTokenizer，可是中文分词的话，就需要自己手动构造一个：继承Tokenizer，然后注册。这样就可以在配置文件中通过type找到定制分词器。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love cat" -&gt; ["I", "love", "cat"]</span><br></pre></td></tr></table></figure><ol><li><strong>Token -&gt; 转化成单个Token对象</strong></li></ol><p>如果看过Token源码就会知道，其核心存储着text，text_id,分别代表着分词的<strong>文本</strong>以及<strong>索引</strong>。</p><p>这个过程比较简单，没有什么逻辑。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">["I", "love", "cat"] -&gt; [Token("I"), Token("love"), Token("cat")]</span><br></pre></td></tr></table></figure><ol><li><strong>Instance -&gt; 转化为Instance实例</strong></li></ol><p>这个过程一般是在DatasetReader的text_to_instance函数中完成，并针对不同字段转化成不同的Field。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tokens = [Token("I"), Token("love"), Token("cat")]</span><br><span class="line">token_indexers = &#123;</span><br><span class="line">    "word_token": SingleIdTokenIndexer(),</span><br><span class="line">    "character_token": TokenCharactersIndexer()</span><br><span class="line">&#125;</span><br><span class="line">instance = &#123;</span><br><span class="line">    "sentence": TextField(tokens, token_indexers)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li><strong>Iterator -&gt; 组装成batch模式</strong></li></ol><p>这个过程或许看不见，可是逻辑基本上固定，如无特殊需求，无需定制。</p><p>将Instance转化成idx的伪代码如下所示：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">instance = &#123;</span><br><span class="line">    "sentence": &#123;</span><br><span class="line">        "word_token": ["I", "love", "cat"] -&gt; torch.Tensor([23, 55, 67]),</span><br><span class="line">        "character_token": [["I"], ["l", "o", "v", "e"],["c", "a", "t"]] -&gt; torch.Tensor([[23], [34, 78, 35, 36],[13, 74, 26]])</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然而，Iterator看似简单，可还有一些细节我想与大家聊聊：</p><ul><li>在batch数据的时候，同batch中不同长度的数据是需要pad</li><li>为了pad过程的<strong>性能</strong>，可优先将长度相近的文本放置在同一个batch中</li><li>随机打乱数据</li></ul><p>Allennlp已经内置了几个DataIterator，几乎不需要你自己重写，除非你在batch的过程中，完成一些创新性的小trick。</p><p>示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> allennlp.data.iterators <span class="keyword">import</span> BucketIterator</span><br><span class="line"></span><br><span class="line">iterator = BucketIterator(batch_size=config.batch_size, </span><br><span class="line">                          biggest_batch_first=<span class="literal">True</span>,</span><br><span class="line">                          sorting_keys=[(<span class="string">"tokens"</span>, <span class="string">"num_tokens"</span>)],</span><br><span class="line">                         )</span><br><span class="line">iterator.index_with(vocab)</span><br></pre></td></tr></table></figure><ul><li>sorting_keys 能够提升padding过程效率。</li><li><p>index_with(vocab)非常重要：给token_indexers配置vocabulary。<strong>这一步千万不要给忘记了</strong>。</p></li><li><p><strong>model.forward -&gt; 模型的参数</strong></p></li></ul><p>Instance经过token_indexers转化成索引之后，由Iterator组装成batch数据，然后塞给模型的forward函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">token_embedders = &#123;</span><br><span class="line">    <span class="string">"word_token"</span>: TokenEmbedder(embedding_dim = <span class="number">23</span>),</span><br><span class="line">    <span class="string">"character_token"</span>: TokenEmbedder(embedding_dim = <span class="number">27</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">text_field_embedders = BaseTextFieldEmbedder(token_embedders)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,sentence: Dict[str,torch.Tensor])</span>:</span></span><br><span class="line">    sentence_embedding = text_field_embedders(sentence)</span><br></pre></td></tr></table></figure><p>上述伪代码很简单的，不过需要注意几点： 1）text_field_embedders参数token_embedders的关键字和token_indexers的关键字必须要保持一致。 2）多种TokenEmbedders对同一个文本分别做处理并映射到词向量后，将其拼接到一起。比如上述两个token_embedder维度为23和27，sentence_embedding的维度就为50。通过简单的几行代码就可以完成很复杂的词向量拼接的功能。(WTF? 词向量拼接很复杂吗？)</p><ol><li><strong>token_embedders -&gt; 词向量映射</strong></li></ol><p>其实如何将将文本索引映射到词向量，第五点就已经说了。其核心需要注意的就是： - token_indexers和token_embedders都是字典类型，且键值必须保持一致 - token_embedders处理后的词向量是拼接到一起<strong>（这个特性非常棒）</strong></p><p>至此，我们跟随着sentence字段从读取到映射成词向量整个流程都已经聊完了，相信都已经掌握了。</p><p>至此，我已经将模型执行之前所有的注意点都给聊完了。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>建议看看源码，因为看了源码你才会发现，Allennlp的model和module都是基于Pytorch的torch.nn.Module模块建立，所以我们可以很容易的使用Allennlp中的任何类。</p><p>为了说明Allennlp中的模型，我先与Pytorch中的模型做一个对比说明：</p><p>示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, tokens: Dict[str, torch.Tensor],</span></span></span><br><span class="line"><span class="function"><span class="params">                id: Any, label: torch.Tensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        mask = get_text_field_mask(tokens)</span><br><span class="line">        embeddings = self.word_embeddings(tokens)</span><br><span class="line">        state = self.encoder(embeddings, mask)</span><br><span class="line">        class_logits = self.projection(state)</span><br><span class="line"></span><br><span class="line">        output = &#123;<span class="string">"class_logits"</span>: class_logits&#125;</span><br><span class="line">        output[<span class="string">"loss"</span>] = self.loss(class_logits, label)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><ul><li>Allennlp中的forward函数参数有的是一个字典类型（类似于TextField指定了TokenIndexer）的数据，有的是纯torch.Tensor数据。而Pytorch中的数据格式没有限制，自由度由自己控制，但推荐是torch.Tensor数据类型。</li><li>Allennlpforward函数返回的也是一个字典类型的数据，其中最重要的就是损失函数值，必须将其存储在loss键下（这是一个<strong>约定</strong>），同时loss值的一个计算也是在forward函数中执行的。</li></ul><p>至此，我们已经了解为什么forward函数有的参数为什么是字典类型。那我这里想问一个问题：</p><p>为什么forward函数返回的值也是字典类型？ 答：因为在Trainer中有大用处，后面我会进一步讲解。</p><p>Allennlp为什么会这么好用呢？</p><p>答：因为里面将很多类似的组件，并将其抽象成模块来使用，常用的有：</p><ul><li>token_embedder</li><li>encoder</li><li>decoder</li></ul><p>示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> allennlp.nn.util <span class="keyword">import</span> get_text_field_mask</span><br><span class="line"><span class="keyword">from</span> allennlp.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> allennlp.modules.text_field_embedders <span class="keyword">import</span> TextFieldEmbedder</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaselineModel</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, word_embeddings: TextFieldEmbedder,</span></span></span><br><span class="line"><span class="function"><span class="params">                 encoder: Seq2VecEncoder,</span></span></span><br><span class="line"><span class="function"><span class="params">                 out_sz: int=len<span class="params">(label_cols)</span>)</span>:</span></span><br><span class="line">        super().__init__(vocab)</span><br><span class="line">        self.word_embeddings = word_embeddings</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)</span><br><span class="line">        self.loss = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, tokens: Dict[str, torch.Tensor],</span></span></span><br><span class="line"><span class="function"><span class="params">                id: Any, label: torch.Tensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        mask = get_text_field_mask(tokens)</span><br><span class="line">        embeddings = self.word_embeddings(tokens)</span><br><span class="line">        state = self.encoder(embeddings, mask)</span><br><span class="line">        class_logits = self.projection(state)</span><br><span class="line"></span><br><span class="line">        output = &#123;<span class="string">"class_logits"</span>: class_logits&#125;</span><br><span class="line">        output[<span class="string">"loss"</span>] = self.loss(class_logits, label)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>上面代码中，encoder是Seq2VecEncoder（基类）类型，故可更改encoder的实现，并不改变模型内部的代码，实现组件的单独替换，而我们所需要做的，只需要添加Seq2VecEncoder的派生类即可。</p><p>这就是Allennlp的强大之处：所有的流程都抽象化，具体实现只需要自己指定或实现就行。</p><h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><p>接下来就该讨论模型的<strong>训练流程</strong>了。</p><p>相比Pytorch繁杂且毫无新特性的训练过程，Keras和tensorflow框架就做的很好，只需要简单的几行代码就可以替代Pytorch多行手动编制的训练loop。</p><p>Allennlp抽象了一个Trainer，用来执行训练过程：更新梯度，保存日志文件（用tensorboard查看），保存best_model，良好的训练过程输出等等，这个训练器大大减少了我们的工作量。</p><p>示例代码如下，也是非常简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> allennlp.training.trainer <span class="keyword">import</span> Trainer</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    optimizer=optim.Adam(model.parameters(), lr=config.lr),</span><br><span class="line">    iterator=iterator,</span><br><span class="line">    train_dataset=train_ds,</span><br><span class="line">    cuda_device=<span class="number">0</span> <span class="keyword">if</span> USE_GPU <span class="keyword">else</span> <span class="number">-1</span>,</span><br><span class="line">    num_epochs=config.epochs,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure><p>配置完Trainer，只需要指定train函数即可完成训练的整个过程。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Allennlp非常好用，也有足够的定制化能力，也可以和Transformer完美结合在一起，同时也支持<strong>semantic parsing</strong>，<strong>state machine</strong>，未来也会添加更多更强大的模块。</p><p><br></p><blockquote><p>搬运自知乎：</p><p><a href="https://zhuanlan.zhihu.com/p/111563535" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/111563535</a></p></blockquote><p><br></p><h1 id="感受"><a href="#感受" class="headerlink" title="感受"></a>感受</h1><p>虽然作者介绍了很多，但整体看下来感觉繁琐而低效，很多类和模块的封装没有必要，而且学习成本较高。值得赞扬的就是对Train和Test过程的封装了，其他的实在不够有吸引力。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AllenNLP </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pip 安装路径</title>
      <link href="/2020/12/19/pip-%E5%AE%89%E8%A3%85%E8%B7%AF%E5%BE%84/"/>
      <url>/2020/12/19/pip-%E5%AE%89%E8%A3%85%E8%B7%AF%E5%BE%84/</url>
      
        <content type="html"><![CDATA[<p>今天在服务器上安装包时，提示空间已满，原因是/home目录磁盘满了。</p><h1 id="查看pip默认安装路径"><a href="#查看pip默认安装路径" class="headerlink" title="查看pip默认安装路径"></a>查看pip默认安装路径</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip show numpy</span><br></pre></td></tr></table></figure><p>结果显示如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Name: numpy</span><br><span class="line">Version: 1.19.4</span><br><span class="line">Summary: NumPy is the fundamental package <span class="keyword">for</span> array computing with Python.</span><br><span class="line">Home-page: https://www.numpy.org</span><br><span class="line">Author: Travis E. Oliphant et al.</span><br><span class="line">Author-email: None</span><br><span class="line">License: BSD</span><br><span class="line">Location: /home/klhao/.conda/envs/klhao-allennlp/lib/python3.6/site-packages</span><br><span class="line">Requires: </span><br><span class="line">Required-by: thinc, tensorboardX, spacy, scipy, msgpack-numpy</span><br></pre></td></tr></table></figure><p>可以看到，由于我们是在conda创建的虚拟环境中安装，所以pip安装的位置是conda默认的位置。为了使空间充足，需要在创建conda环境时指定安装目录。<a id="more"></a></p><h1 id="conda创建环境时指定目录"><a href="#conda创建环境时指定目录" class="headerlink" title="conda创建环境时指定目录"></a>conda创建环境时指定目录</h1><p>安装虚拟环境到指定目录命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --prefix=/data/klhao/.conda/klhao-allennlp python=3.6</span><br></pre></td></tr></table></figure><p>对于这种方式创建的环境，需要以全路径激活，命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate /data/klhao/.conda/klhao-allennlp</span><br></pre></td></tr></table></figure><p>然后，在激活虚拟环境后，再使用conda, pip就可以愉快的安装包了~🎈🎉✨</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pip </tag>
            
            <tag> pip3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TARE debug 记录</title>
      <link href="/2020/12/12/TARE-debug-%E8%AE%B0%E5%BD%95/"/>
      <url>/2020/12/12/TARE-debug-%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="1-使用iter将DataLoader转化为迭代器"><a href="#1-使用iter将DataLoader转化为迭代器" class="headerlink" title="1. 使用iter将DataLoader转化为迭代器"></a>1. 使用iter将DataLoader转化为迭代器</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loader = iter(train_loader)</span><br></pre></td></tr></table></figure><p>内存超出，报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: [enforce fail at CPUAllocator.cpp:65] . DefaultCPUAllocator: can<span class="string">'t allocate memory: you tried to allocate 800000000000 bytes. Error code 12 (Cannot allocate memory)</span></span><br></pre></td></tr></table></figure><p>原因是我在数据集长度上设为了 int(1e12) , 导致内存溢出。</p><h1 id="2-模型的-train-或-eval-状态"><a href="#2-模型的-train-或-eval-状态" class="headerlink" title="2. 模型的 train 或 eval 状态"></a>2. 模型的 train 或 eval 状态</h1><p>想要查看模型的状态，使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(model.training)</span><br></pre></td></tr></table></figure><p>当使用 model.eval() 设置模型为测试状态时，是否会将模型中的子模块自动设置为测试状态呢？(后来思考了一下，Linear() 就是 Module 子模块啊… 是我制杖了🙄)</p><p>通过下面的代码测试:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'======================='</span>)</span><br><span class="line">print(model.training)</span><br><span class="line">print(model.classifier.training)</span><br><span class="line">model.eval()</span><br><span class="line">print(model.training)</span><br><span class="line">print(model.classifier.training)</span><br><span class="line">print(<span class="string">'======================='</span>)</span><br></pre></td></tr></table></figure><p>测试结果为True, True, False, False. 说明 Pytorch 是会自动递归对子模块进行对应设置的。<a id="more"></a></p><h1 id="3-DataLoader-中-shuffle-参数的作用"><a href="#3-DataLoader-中-shuffle-参数的作用" class="headerlink" title="3. DataLoader() 中 shuffle 参数的作用"></a>3. DataLoader() 中 shuffle 参数的作用</h1><p>在 DataLoader() 类中，有shuffle参数，在官方的文档里，是这样写的：</p><blockquote><p><strong>shuffle</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <em>optional</em>) – set to <code>True</code> to have the data reshuffled at every epoch (default: <code>False</code>)</p></blockquote><p>我的疑问是 shuffle 是否会在第一个epoch对数据随机shuffle呢?</p><p>通过实验尝试一下！💪</p><p>指定不同的随机数种子，跑两次，打印第一个batch。第一次：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[   25,  1100,   258,  ...,     0,     0,     0],s]</span><br></pre></td></tr></table></figure><p>第二次：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[   73,     2,   337,  ...,     0,     0,     0],s]</span><br></pre></td></tr></table></figure><p>可以看到，是不同的数据，也就是说<strong>shuffle 是每个epoch都会进行的</strong></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
            <tag> Relation Extraction </tag>
            
            <tag> Distant Supervision </tag>
            
            <tag> TARE </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux Bash 常用经典命令</title>
      <link href="/2020/12/09/Linux-Bash-%E5%B8%B8%E7%94%A8%E7%BB%8F%E5%85%B8%E5%91%BD%E4%BB%A4/"/>
      <url>/2020/12/09/Linux-Bash-%E5%B8%B8%E7%94%A8%E7%BB%8F%E5%85%B8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="递归查找文件"><a href="#递归查找文件" class="headerlink" title="递归查找文件"></a>递归查找文件</h1><p>想要查找一个文件夹下的所有.log文件，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find 目录 -name <span class="string">"*.log"</span></span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(klhao-pytorch) klhao@Bobby:~/Adidas/outputs$ find . -name <span class="string">"*.log"</span></span><br><span class="line">====================================================================</span><br><span class="line">./sent_att_blstm_422_NYT/log.log</span><br><span class="line">./sent_pcnn_422_NYT/log.log</span><br><span class="line">./sent_cnn_422/log.log</span><br><span class="line">./sent_crcnn_422/log.log</span><br><span class="line">./sent_pcnn_422/log.log</span><br><span class="line">./sent_crcnn_422_NYT/log.log</span><br><span class="line">./bag_cnn_one_422/log.log</span><br><span class="line">./sent_att_blstm_422/log.log</span><br><span class="line">./bag_pcnn_att_422/log.log</span><br><span class="line">./sent_cnn_422_NYT/log.log</span><br></pre></td></tr></table></figure><p>如果想要将这些文件执行进一步的操作，可以使用<code>xargs</code>命令，例如删除：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find 目录 -name <span class="string">"*.log"</span> | xargs rm</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux</span><br></pre></td></tr></table></figure><p>如果想要查看具体的用户名、进程id，可以使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps aux | grep 23087</span><br><span class="line">ps aux | grep klhao</span><br></pre></td></tr></table></figure><p>嘻嘻嘻嘻😏</p><h1 id="查看系统信息"><a href="#查看系统信息" class="headerlink" title="查看系统信息"></a>查看系统信息</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/version</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Linux version 4.15.0-66-generic (buildd@lgw01-amd64-044) (gcc version 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1)) <span class="comment">#75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> bash </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Byte Pair Embedding vs Word Embedding</title>
      <link href="/2020/12/08/Byte-Pair-Embedding-vs-Word-Embedding/"/>
      <url>/2020/12/08/Byte-Pair-Embedding-vs-Word-Embedding/</url>
      
        <content type="html"><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>众所周知，BERT 在进行 tokenization 的时候，使用的是Byte Pair Embedding，后面简称为BPE；而传统的tokenization 是以 word 为单位，简称为 WE。在一些任务中，使用 BPE 会有一些不方便，比如序列标注任务，因为 BPE 会改变位置，导致处理数据非常的麻烦。但是，不采用 BERT 自带的 tokenization 方法，又会担心结果变差，影响实验效果，到底会不会呢，今天做个实验，以绝后患🤭</p><h1 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h1><p>在 IMDB 数据集上进行，这是一个二分类数据集，判断情感是正向还是负向，我们分别采用 BPE 和 WE 方法，对比实验效果的差异。<a id="more"></a></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>首先需要将训练数据和测试数据进行格式的转化，训练数据和测试数据格式一致，均为以下形式：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    [<span class="string">"The film is excellent."</span>, <span class="number">1</span>]</span><br><span class="line">    ...</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>1代表情感为正向，0代表情感为负向。</p><p>然后下面就是训练部分，由于很简单，这里就不过多解释了，直接贴源代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IMDB</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, file, mode)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.mode = mode</span><br><span class="line">        self.data = json.load(open(file, encoding=<span class="string">'utf-8'</span>))</span><br><span class="line">        self.tokenizer = BertTokenizer.from_pretrained(<span class="string">'/data/klhao/BERT/bert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[item]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        MAX_LENGTH = <span class="number">300</span></span><br><span class="line">        ids, masks, labels = [], [], []</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">            text = d[<span class="number">0</span>].lower()</span><br><span class="line">            <span class="keyword">if</span> self.mode == <span class="string">'BPE'</span>:</span><br><span class="line">                id = self.tokenizer.encode(text)</span><br><span class="line">            <span class="keyword">elif</span> self.mode == <span class="string">'WE'</span>:</span><br><span class="line">                id = self.tokenizer.convert_tokens_to_ids(text.split())</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError</span><br><span class="line">            mask = ([<span class="number">1</span>] * len(id) + [<span class="number">0</span>] * (MAX_LENGTH - len(id)))[:MAX_LENGTH]</span><br><span class="line">            id = (id + [<span class="number">0</span>] * MAX_LENGTH)[:MAX_LENGTH]</span><br><span class="line">            label = d[<span class="number">1</span>]</span><br><span class="line">            masks.append(mask)</span><br><span class="line">            ids.append(id)</span><br><span class="line">            labels.append(label)</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(ids), torch.tensor(masks), torch.tensor(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CLS</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(<span class="string">'/data/klhao/BERT/bert-base-uncased/'</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">768</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, mask)</span>:</span></span><br><span class="line">        vector = self.bert(input, mask)[<span class="number">1</span>]</span><br><span class="line">        out = self.linear(vector)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(args, model, train_loader, test_loader, optimizer)</span>:</span></span><br><span class="line">    loss_function = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> tqdm(enumerate(train_loader), ncols=<span class="number">60</span>):</span><br><span class="line">            inputs, masks, label = data[<span class="number">0</span>].cuda(), data[<span class="number">1</span>].cuda(), data[<span class="number">2</span>].cuda()</span><br><span class="line">            out = model(inputs, masks)</span><br><span class="line">            loss = loss_function(out, label)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># TEST</span></span><br><span class="line">        PREDICTION = []</span><br><span class="line">        GOLD = []</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> tqdm(enumerate(test_loader), ncols=<span class="number">60</span>):</span><br><span class="line">            inputs, masks, label = data[<span class="number">0</span>].cuda(), data[<span class="number">1</span>].cuda(), data[<span class="number">2</span>].cuda()</span><br><span class="line">            out = model(inputs, masks)</span><br><span class="line">            _, pred = torch.max(torch.softmax(out, dim=<span class="number">1</span>), dim=<span class="number">1</span>)</span><br><span class="line">            PREDICTION += pred.cpu().tolist()</span><br><span class="line">            GOLD += label.cpu().tolist()</span><br><span class="line">        accuracy = np.sum(np.array(PREDICTION) == np.array(GOLD)) / len(GOLD)</span><br><span class="line">        print(<span class="string">f'Epoch <span class="subst">&#123;epoch&#125;</span>, accuracy <span class="subst">&#123;accuracy&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--mode'</span>, default=<span class="string">'BPE'</span>, help=<span class="string">'BPE or WE'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    train_data = IMDB(<span class="string">'train.json'</span>, args.mode)</span><br><span class="line">    test_data = IMDB(<span class="string">'test.json'</span>, args.mode)</span><br><span class="line">    train_loader = DataLoader(train_data, batch_size=<span class="number">16</span>, shuffle=<span class="literal">True</span>, collate_fn=train_data.generate_batch)</span><br><span class="line">    test_loader = DataLoader(test_data, batch_size=<span class="number">16</span>, shuffle=<span class="literal">False</span>, collate_fn=test_data.generate_batch)</span><br><span class="line">    model = CLS().cuda()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">    train(args, model, train_loader, test_loader, optimizer)</span><br></pre></td></tr></table></figure><p>运行的时候只需要<code>python run.py</code>即可。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="使用-BPE"><a href="#使用-BPE" class="headerlink" title="使用 BPE"></a>使用 BPE</h2><p>1563it [11:37,  2.24it/s]<br>1563it [04:55,  5.29it/s]<br>Epoch 0, accuracy 0.89644<br>1563it [11:35,  2.25it/s]<br>1563it [04:58,  5.24it/s]<br>Epoch 1, accuracy 0.91976</p><h2 id="使用-WE"><a href="#使用-WE" class="headerlink" title="使用 WE"></a>使用 WE</h2><p>1563it [10:24,  2.50it/s]<br>1563it [03:12,  8.12it/s]<br>Epoch 0, accuracy 0.89252<br>1563it [10:23,  2.51it/s]<br>1563it [03:12,  8.10it/s]<br>Epoch 1, accuracy 0.89956<br>1563it [10:26,  2.50it/s]<br>1563it [03:12,  8.11it/s]<br>Epoch 2, accuracy 0.89328</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><font color="orange"> 对比发现，使用 BPE 确实比使用 WE 效果好，在Epoch 1，BPE已经可以达到91.976的准确率，而使用 WE 只能达到 89.956，相差2.02个百分点。而且，使用 BPE 训练速度更快，拟合更快，使用BPE一轮就提升了2个多百分点，而使用WE，提升了0.7个百分点，还不到1个百分点。 </font><font color="red"> 而且，在第三轮的时候，WE就已经过拟合了，效果上不去了！！</font><p><strong>总之，以后不能为了懒省事，直接使用 WE 方法了！可能会对结果造成很大的影响！</strong></p><h1 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h1><h2 id="是否需要手动-lower"><a href="#是否需要手动-lower" class="headerlink" title="是否需要手动 lower()"></a>是否需要手动 lower()</h2><p>在使用 huggingface 的 bert-base-uncased 的时候，需要手动将文本转化为小写形式，因为通过实验发现，使不使用 <code>lower()</code> 得到的 id 是不一样的：</p><p>使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[100, 2152, 2003, 1037, 9476, 100, 2009, 2743, 2012, 1996, 2168, 2051]</span><br></pre></td></tr></table></figure><p>不使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[100, 100, 2003, 1037, 9476, 100, 100, 2743, 2012, 1996, 2168, 2051]</span><br></pre></td></tr></table></figure><p>可以观察到不使用 <code>lower</code> 得到的100更多，也就是有更多的 UNK 字符，out-of-vocabulary，可能会影响实验效果。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Pretrained Language Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> BPE </tag>
            
            <tag> Embedding </tag>
            
            <tag> GPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于DSGAN的细节</title>
      <link href="/2020/12/07/%E5%85%B3%E4%BA%8EDSGAN%E7%9A%84%E7%BB%86%E8%8A%82/"/>
      <url>/2020/12/07/%E5%85%B3%E4%BA%8EDSGAN%E7%9A%84%E7%BB%86%E8%8A%82/</url>
      
        <content type="html"><![CDATA[<h1 id="clean数据"><a href="#clean数据" class="headerlink" title="clean数据"></a>clean数据</h1><p><a href="https://github.com/Panda0406/Adversarial-Learning-Distant-Supervision-RE/issues/2" target="_blank" rel="noopener">https://github.com/Panda0406/Adversarial-Learning-Distant-Supervision-RE/issues/2</a></p><p>DSGAN得到的干净数据分布是不平衡的，作者手动调整了生成数据的比例，去掉了一些过高的比例的类别的数据，以避免不必要的偏置。</p><blockquote><p> <a href="https://github.com/thunlp/NRE/blob/master/PCNN%2BATT/out/pr.txt" target="_blank" rel="noopener"> Their repo</a> do not perform as you present. I have tested it.</p><p> Because we remove many sentence bags of positive entity pairs, it causes the proportion change of positive set and negative set. So we correspondingly add some positive sentence sets based on the learned filter network. These positive sentence sets are the subset of cleaned positive sentence sets. This part is explained in the code.</p></blockquote><a id="more"></a><h1 id="如何划分N和P"><a href="#如何划分N和P" class="headerlink" title="如何划分N和P?"></a>如何划分N和P?</h1><p>这个问题来自issue 4:</p><p><a href="https://github.com/Panda0406/Adversarial-Learning-Distant-Supervision-RE/issues/4" target="_blank" rel="noopener">https://github.com/Panda0406/Adversarial-Learning-Distant-Supervision-RE/issues/4</a></p><blockquote><p>论文中写道关于数据集DS已经划分了N和P，这部分是数据集本身就已经提供的还是人为又自己划分的？另外N^D和N^G具体怎么划分？</p></blockquote><p>第一个问题：通过阅读作者的源代码，N是指NA数据，P是指除去NA外的其他数据。作者训练了一个CNN的二分类器，用于区分P和N，然后取N中排在前300000的样本作为负样本 (good_negative) 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_negative</span><span class="params">(embeddings, inputs, rela2sents, args)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    利用 RANK_CNN 筛选 NA 样本，用于 GAN 训练</span></span><br><span class="line"><span class="string">    Select high-quality negative samples for robust result.</span></span><br><span class="line"><span class="string">    rela2sents: 关系 r 对应的句子列表 (去除 same)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n\n############## Training Rank_CNN ##############'</span></span><br><span class="line"></span><br><span class="line">    pos_data, neg_data = list(), list()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> rela2sents.items():</span><br><span class="line">        <span class="keyword">if</span> k != <span class="number">0</span>:</span><br><span class="line">            pos_data += v</span><br><span class="line">    neg_data = rela2sents[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'positive size:'</span>, len(pos_data), <span class="string">'negative size:'</span>, len(neg_data)  <span class="comment"># nonna 和 na</span></span><br><span class="line"></span><br><span class="line">    rank_model = Rank_CNN(embeddings, args, emb_change=<span class="literal">True</span>)</span><br><span class="line">    rank_model = rank_model.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> rank_model</span><br><span class="line"></span><br><span class="line">    x = pos_data+neg_data</span><br><span class="line">    y = [<span class="number">1</span>]*len(pos_data)+[<span class="number">0</span>]*len(neg_data)</span><br><span class="line">    trainloader = generate_trainloader(inputs, x, y, args.batch_size, shuf=<span class="literal">True</span>)</span><br><span class="line">    parameters = filter(<span class="keyword">lambda</span> p: p.requires_grad, rank_model.parameters())</span><br><span class="line">    optimizer = optim.Adam(parameters, lr=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="comment">#rank_model.train()</span></span><br><span class="line">        corrects, total, accuracy = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, (x, y) <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">            logits = calculate_logits(rank_model, x, args.max_sent)</span><br><span class="line">            y = y.type(LongTensor)</span><br><span class="line">            loss = calculate_loss(logits, y)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            corrects += (torch.max(logits, <span class="number">1</span>)[<span class="number">1</span>].view(y.size()).data == y).sum()</span><br><span class="line">            total += int(y.size()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % args.log_interval == <span class="number">0</span>:</span><br><span class="line">                accuracy = <span class="number">100.0</span> * corrects / total</span><br><span class="line">                time_str = datetime.datetime.now().isoformat()</span><br><span class="line">                sys.stdout.write(<span class="string">"[rank_data] epoch %d step %d | loss : %f, accuracy: %f"</span> % (epoch, i, loss.data[<span class="number">0</span>], accuracy) + <span class="string">'\r'</span>)</span><br><span class="line">                sys.stdout.flush()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">''</span></span><br><span class="line"></span><br><span class="line">        accuracy, accuracy_non_NA = calculate_accuracy(trainloader, rank_model, args, epoch)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> accuracy_non_NA &gt; <span class="number">90.0</span>:  <span class="comment"># 这一步是否就是作者说的让模型过拟合? #</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    torch.save(rank_model.state_dict(), <span class="string">'./models/Rank.pkl'</span>)</span><br><span class="line">    GAN_train_data = deepcopy(rela2sents)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\n##############  Selecting high-quality negative samples  ##############"</span></span><br><span class="line">    <span class="comment"># 选取排在前 300000 的负样本 #</span></span><br><span class="line">    good_negative = select_negative(rank_model, rela2sents[<span class="number">0</span>], inputs, args.max_sent)</span><br><span class="line">    GAN_train_data[<span class="number">0</span>] = good_negative</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"We select %d high-quality negative sentences"</span> % (len(good_negative))</span><br><span class="line">    pickle.dump(GAN_train_data, open(<span class="string">'./data/GAN_train_data.pkl'</span>, <span class="string">'wb'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> GAN_train_data</span><br></pre></td></tr></table></figure><p>第二个问题：$N^D$ 和 $N^G$ 的划分是随机划分的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_negative_set</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""随机采样划分负样本"""</span></span><br><span class="line">    <span class="keyword">if</span> len(self.pos_data) &lt; <span class="number">10000</span>:</span><br><span class="line">        nl = <span class="number">50000</span></span><br><span class="line">        neg_sample = self.neg_data[:<span class="number">100000</span>]</span><br><span class="line">        G_neg_data = [neg_sample[<span class="number">2</span>*i] <span class="keyword">for</span> i <span class="keyword">in</span> range(nl)]</span><br><span class="line">        G_neg_data = random.sample(G_neg_data, max(<span class="number">5</span>*len(self.pos_data), <span class="number">20000</span>))</span><br><span class="line">        D_neg_data = [neg_sample[<span class="number">2</span>*i+<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(nl)]</span><br><span class="line">        D_neg_data = random.sample(D_neg_data, max(<span class="number">5</span>*len(self.pos_data), <span class="number">20000</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        nl = min(<span class="number">5</span>*len(self.pos_data), <span class="number">150000</span>)</span><br><span class="line">        neg_sample = self.neg_data[:<span class="number">2</span>*nl]</span><br><span class="line">        G_neg_data = [neg_sample[<span class="number">2</span>*i] <span class="keyword">for</span> i <span class="keyword">in</span> range(nl)]</span><br><span class="line">        D_neg_data = [neg_sample[<span class="number">2</span>*i+<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(nl)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Positive_total: %d, G_negative: %d, D_negative: %d"</span> %(len(self.pos_data), len(G_neg_data), len(D_neg_data))</span><br><span class="line"></span><br><span class="line">    self.G_neg_data = G_neg_data</span><br><span class="line">    self.D_neg_data = D_neg_data</span><br></pre></td></tr></table></figure><h1 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h1><p>由于代码是采用早期pytorch版本实现，所以依然存在Variable这样的写法。</p><blockquote><p>variable是tensor的外包装，data属性存储着tensor数据，grad属性存储关于该变量的导数，creator是代表该变量的创造者</p></blockquote><p>但是在目前采用的pytorch 1.x 版本，Variable已经和Tensor合并，所以无需过于在意~</p><h1 id="batch-size-大小"><a href="#batch-size-大小" class="headerlink" title="batch_size 大小"></a>batch_size 大小</h1><p>在对生成器、判别器进行预训练时，batch_size 设的特别大，源代码中如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = pos_data + neg_data</span><br><span class="line">y = [<span class="number">1</span>] * len(pos_data) + [<span class="number">0</span>] * len(neg_data)</span><br><span class="line">batch_size = int(len(y) / <span class="number">50</span>) + <span class="number">1</span>  </span><br><span class="line"><span class="comment"># batch_size 很大, 对于 pos=5000, neg=30000, 则 batch_size = 700 !!</span></span><br></pre></td></tr></table></figure><h2 id="不知道设置这么大的-batch-size-依据是什么？"><a href="#不知道设置这么大的-batch-size-依据是什么？" class="headerlink" title="不知道设置这么大的 batch_size 依据是什么？"></a>不知道设置这么大的 batch_size 依据是什么？</h2><p>有时间的话，对 batch_size 的实际设置的影响，进行实验探究。</p><h1 id="强化学习优化过程"><a href="#强化学习优化过程" class="headerlink" title="强化学习优化过程"></a>强化学习优化过程</h1><p>首先，为每个样本计算 logits，然后以 logits 为概率，为每个样本赋予0, 1标签，这一步是一个按概率随机采样的过程，即<code>action</code>步。</p><p>具体代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(self, model, x, random=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    为每一个样本按照概率赋予标签</span></span><br><span class="line"><span class="string">    :param model:</span></span><br><span class="line"><span class="string">    :param x:</span></span><br><span class="line"><span class="string">    :param random: 是否引入随机性</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    probs = calculate_prob(model, x, self.max_sent, self.inputs, detach=<span class="literal">True</span>)</span><br><span class="line">    actions_probs = probs[:, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> random:</span><br><span class="line">        actions = list()</span><br><span class="line">        <span class="keyword">for</span> prob <span class="keyword">in</span> probs:</span><br><span class="line">            <span class="keyword">if</span> abs(prob[<span class="number">0</span>]-prob[<span class="number">1</span>])&lt;<span class="number">0.3</span>:  <span class="comment"># 如果概率相差不大, 就按概率选择</span></span><br><span class="line">                actions.append(int(np.random.choice(np.arange(<span class="number">2</span>), p=prob)))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                actions.append(int(np.argmax(prob)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        actions = list(np.argmax(probs, axis=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> actions</span><br></pre></td></tr></table></figure><p>根据actions中的0, 1标签，可以将样本划分为high_conf_sents 和 low_conf_sents，即可以根据Generator划分高置信度样本和低置信度样本。然后用它们去训练Discriminator。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_Discriminator</span><span class="params">(self, high_conf_sents, low_conf_sents)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param high_conf_sents: 由 Generator 预测的正样本集</span></span><br><span class="line"><span class="string">    :param low_conf_sents: 由 Generator 预测的负样本集</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    bag_size = len(high_conf_sents + low_conf_sents)</span><br><span class="line">    pp = float(len(low_conf_sents) / bag_size)</span><br><span class="line">    <span class="comment"># 动态调整学习率, high_conf_sents 占比越大, 学习率越高 #</span></span><br><span class="line">    self.D_optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>] = self.D_lr * (<span class="number">1</span> - pp)</span><br><span class="line">    <span class="string">"""在这里把标签进行了置换"""</span></span><br><span class="line">    trainloader = generate_trainloader(self.inputs, low_conf_sents, [<span class="number">1</span>] * len(low_conf_sents),</span><br><span class="line">                                       batch_size=len(low_conf_sents), shuf=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(self.D_epoch):</span><br><span class="line">        self.Discriminator.train()</span><br><span class="line">        <span class="keyword">for</span> i, (x, y) <span class="keyword">in</span> enumerate(trainloader):</span><br><span class="line">            logits = calculate_logits(self.Discriminator, x, self.max_sent)</span><br><span class="line">            loss = calculate_loss(logits, y)</span><br><span class="line">            self.D_optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            self.D_optimizer.step()</span><br><span class="line"></span><br><span class="line">    self.D_optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>] = self.D_lr * pp</span><br><span class="line">    trainloader = generate_trainloader(self.inputs, high_conf_sents, [<span class="number">0</span>] * len(high_conf_sents),</span><br><span class="line">                                       batch_size=len(high_conf_sents), shuf=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(self.D_epoch):</span><br><span class="line">        self.Discriminator.train()</span><br><span class="line">        <span class="keyword">for</span> i, (x, y) <span class="keyword">in</span> enumerate(trainloader):</span><br><span class="line">            logits = calculate_logits(self.Discriminator, x, self.max_sent)</span><br><span class="line">            loss = calculate_loss(logits, y)</span><br><span class="line">            self.D_optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            self.D_optimizer.step()</span><br></pre></td></tr></table></figure><p>上面将正负样本的标签置换，用”错误“的标签去训练Discriminator，按照自然的想法，Discriminator效果会下降，而下面的<code>reward</code>的计算就要用到这个下降幅度作为指标。</p><p>在具体的reward计算过程中，步骤如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" ================================= 强化学习 =============================== """</span></span><br><span class="line"><span class="keyword">if</span> is_Training:</span><br><span class="line">    D_pos_prob_list = [self.D_pos_prob_dict[i][bag_id] <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch)]</span><br><span class="line">    D_neg_prob_list = [self.D_neg_prob_dict[i][bag_id] <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch)]</span><br><span class="line">    self.D_pos_prob_dict[epoch].append(D_pos_prob)</span><br><span class="line">    self.D_neg_prob_dict[epoch].append(D_neg_prob)</span><br><span class="line">    <span class="keyword">if</span> epoch &gt; <span class="number">0</span>:</span><br><span class="line">        reward_PD = max(D_pos_prob_list) - self.D_pos_prob_dict[epoch][bag_id]</span><br><span class="line">        reward_ND = max(D_neg_prob_list) - self.D_neg_prob_dict[epoch][bag_id]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        reward_PD = <span class="number">0.01</span></span><br><span class="line">        reward_ND = <span class="number">0.01</span></span><br><span class="line">    reward = <span class="number">100</span> * (reward_PD + reward_ND + <span class="number">0.05</span>)</span><br><span class="line">    <span class="string">""" ===================================================================== """</span></span><br></pre></td></tr></table></figure><font color="red">这里我有一个困惑，按照道理，应该希望D_pos_prob越小越好，D_neg_prob越大越好，但是作者却对两者进行了同样的处理？！</font><p>根据我的理解，reward_ND应该正好和reward_PD目标相反，即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reward_ND = self.D_neg_prob_dict[epoch][bag_id] - max(D_neg_prob_list)</span><br></pre></td></tr></table></figure><h2 id="不知道是不是一个bug"><a href="#不知道是不是一个bug" class="headerlink" title="不知道是不是一个bug?"></a>不知道是不是一个bug?</h2><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ol><li>在代码实现中，用到了deepcopy()，用于深复制一个对象：</li></ol><blockquote><p> copy.deepcopy(x[, memo])</p><pre><code> Return a deep copy of x.</code></pre></blockquote><ol><li>在将args参数赋给模型的时候，手动写可能要好多行，可以使用<code>setattr()</code>函数，按照以下写法：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> vars(args).items(): </span><br><span class="line">    setattr(self, k, v)</span><br></pre></td></tr></table></figure><ol><li>对词向量的初始化采用均匀分布：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">word2id[<span class="string">'UNK'</span>] = W_size+<span class="number">1</span></span><br><span class="line">W[W_size+<span class="number">1</span>] = np.random.uniform(<span class="number">-0.25</span>, <span class="number">0.25</span>, w_dim)  <span class="comment"># UNK</span></span><br></pre></td></tr></table></figure><ol><li>保存数组或列表：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numpy.savez(file, *args, **kwds)</span><br></pre></td></tr></table></figure><blockquote><p>Save several arrays into a single file in uncompressed .npz format.</p></blockquote><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>总而言之，DSGAN是希望利用NA数据提供监督信息，使得模型能够区分开TP和FP，整体流程可以概括如下：</p><ol><li>选取样本数大于1500的关系作为POS，训练RANK-CNN，选取难以辨别的前300,000个NA样本作为负样本</li><li>针对每个关系R：<ol><li>利用全部POS, NA预训练G和D，以二分类为目标</li><li>固定住除全连接层以外的所有参数</li><li>如果正样本过多，随机选取7800个；对负样本，选取最难区分的前5000个</li><li>用Generator生成概率，按照概率执行select_action()，并根据action将POS minibatch划分为high_conf_sents, low_conf_sents</li><li>赋予high_conf_sents标签1，low_conf_sents标签0，训练Discriminator；显然，Discriminator表现会变差，因为接受了错误的训练导向。</li><li>根据Discriminator在high_conf_sents, NA上的概率值，计算reward</li><li>训练优化器</li></ol></li><li>选取使D效果下降最多的G，并对POS二分类，实现FP的去除。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> Distant Supervision </tag>
            
            <tag> RE </tag>
            
            <tag> DSGAN </tag>
            
            <tag> noise </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows10蓝牙设备删除失败</title>
      <link href="/2020/12/07/Windows10%E8%93%9D%E7%89%99%E8%AE%BE%E5%A4%87%E5%88%A0%E9%99%A4%E5%A4%B1%E8%B4%A5/"/>
      <url>/2020/12/07/Windows10%E8%93%9D%E7%89%99%E8%AE%BE%E5%A4%87%E5%88%A0%E9%99%A4%E5%A4%B1%E8%B4%A5/</url>
      
        <content type="html"><![CDATA[<p>在win10蓝牙设备总是会出现删除失败的问题，真是坑爹…是win10的一个bug:</p><p><a href="https://answers.microsoft.com/zh-hans/windows/forum/windows_10-hardware/win10%E8%93%9D%E7%89%99%E8%AE%BE%E5%A4%87%E6%97%A0/dd641a9b-ca95-491a-8cd9-90747b36fe2c" target="_blank" rel="noopener">https://answers.microsoft.com/zh-hans/windows/forum/windows_10-hardware/win10%E8%93%9D%E7%89%99%E8%AE%BE%E5%A4%87%E6%97%A0/dd641a9b-ca95-491a-8cd9-90747b36fe2c</a></p><p>在网上找到了解决方案：</p><ol><li>下载 <a href="http://bluetoothinstaller.com/bluetooth-command-line-tools/" target="_blank" rel="noopener">修复工具</a>，一路默认选项完成安装。防止链接失效，附上 <a href="https://pan.baidu.com/s/1CJ-7laoysT8r-iPnGvB1GA" target="_blank" rel="noopener">百度网盘链接</a></li><li>打开 Powershell，命令行输入 <code>btpair -u</code>，回车执行</li><li>等待，会发现已配对的蓝牙设备 <strong>终于</strong> <strong>成功</strong> <strong>彻底</strong> 被删除了</li><li>喜极而泣</li></ol><blockquote><p><a href="https://blog.csdn.net/u014595375/article/details/85730427" target="_blank" rel="noopener">https://blog.csdn.net/u014595375/article/details/85730427</a></p></blockquote><p>呜呜呜┭┮﹏┭┮</p><p>要是都还不管用，就试一下插拔蓝牙、重启电脑、开关电源……<strong>重启大法好!</strong></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Windows </tag>
            
            <tag> win10 </tag>
            
            <tag> Bluetooth </tag>
            
            <tag> Microsoft </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>远程监督关系抽取的inference</title>
      <link href="/2020/12/06/%E8%BF%9C%E7%A8%8B%E7%9B%91%E7%9D%A3%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%9A%84inference/"/>
      <url>/2020/12/06/%E8%BF%9C%E7%A8%8B%E7%9B%91%E7%9D%A3%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%9A%84inference/</url>
      
        <content type="html"><![CDATA[<h1 id="sentence-level-or-bag-level"><a href="#sentence-level-or-bag-level" class="headerlink" title="sentence-level or bag-level?"></a>sentence-level or bag-level?</h1><p>关于远程监督关系抽取的测试阶段，网上的一个讨论值得深思：</p><p><a href="https://github.com/malllabiisc/RESIDE/issues/51" target="_blank" rel="noopener">https://github.com/malllabiisc/RESIDE/issues/51</a></p><blockquote><p>Hi <a href="https://github.com/juanluis17" target="_blank" rel="noopener">@juanluis17</a>,<br>In the distantly-supervised setting, the evaluation is performed at the bag-level only because the label on individual instances might be wrong. In both the benchmark datasets, the labels are available at the bag-level only so at the instance-level, we do not have the ground truth to evaluate the performance.</p></blockquote><p>Hi,<br>I understand that problem in distant supervision. The need to evaluate at the instance level is given because I have a dataset where I know which label is wrong. What I came up with with this implementation is to place each instance of the bag with the predicted label for the bag and thus perform the evaluation.<br>Another alternative is to use each instance as a bag.</p><h1 id="single-label-or-multi-label"><a href="#single-label-or-multi-label" class="headerlink" title="single-label or multi-label?"></a>single-label or multi-label?</h1><p>到底是单分类还是多分类，也是一个不够统一的做法，在RESIDE中，是这样操作的：</p><p><a href="https://github.com/malllabiisc/RESIDE/issues/23" target="_blank" rel="noopener">https://github.com/malllabiisc/RESIDE/issues/23</a></p><h3 id="speedcell4-commented-on-2-Apr-2019-•-edited"><a href="#speedcell4-commented-on-2-Apr-2019-•-edited" class="headerlink" title="speedcell4 commented on 2 Apr 2019 • edited"></a><strong>speedcell4</strong> commented <a href="https://github.com/malllabiisc/RESIDE/issues/23#issue-428050708" target="_blank" rel="noopener">on 2 Apr 2019</a> • edited</h3><h3 id="svjan5-commented-on-2-Apr-2019-•-edited"><a href="#svjan5-commented-on-2-Apr-2019-•-edited" class="headerlink" title="svjan5 commented on 2 Apr 2019 • edited"></a><strong>svjan5</strong> commented <a href="https://github.com/malllabiisc/RESIDE/issues/23#issuecomment-479066990" target="_blank" rel="noopener">on 2 Apr 2019</a> • edited</h3><h1 id="Old-School-PR-Curve"><a href="#Old-School-PR-Curve" class="headerlink" title="Old-School PR Curve"></a>Old-School PR Curve</h1><p>一些传统方法的PR曲线，例如Mintz, Riedel, Hoffmann,等：</p><p><a href="https://github.com/thunlp/OpenNRE/tree/4d6d321c9b369615ca07508705d921d30d8286d4/data" target="_blank" rel="noopener">https://github.com/thunlp/OpenNRE/tree/4d6d321c9b369615ca07508705d921d30d8286d4/data</a></p><p>不得不说，OpenNRE🐮🍺</p>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Relation Extraction </tag>
            
            <tag> Distant Supervision </tag>
            
            <tag> Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多分类计算 P@N 方法</title>
      <link href="/2020/12/05/%E5%A4%9A%E5%88%86%E7%B1%BB%E8%AE%A1%E7%AE%97-P-N-%E6%96%B9%E6%B3%95/"/>
      <url>/2020/12/05/%E5%A4%9A%E5%88%86%E7%B1%BB%E8%AE%A1%E7%AE%97-P-N-%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>在多分类方法中，有时候会用到P@N的评价指标，具体可以实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPscore</span><span class="params">(self, sess, data, label=<span class="string">'P@N Evaluation'</span>)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Computes P@N for N = 100, 200, and 300</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">data:Data for P@N evaluation</span></span><br><span class="line"><span class="string">label:Log label to be used while logging</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Returns</span></span><br><span class="line"><span class="string">-------</span></span><br><span class="line"><span class="string">P@100Precision @ 100</span></span><br><span class="line"><span class="string">P@200Precision @ 200</span></span><br><span class="line"><span class="string">P@300Precision @ 300</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">test_loss, test_acc, y, y_pred, logit_list, y_hot = self.predict(sess, data, label)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># 去除 NA 类别</span></span><br><span class="line">y_true   = np.array([e[<span class="number">1</span>:] <span class="keyword">for</span> e <span class="keyword">in</span> y_hot]).   reshape((<span class="number">-1</span>))</span><br><span class="line">y_scores = np.array([e[<span class="number">1</span>:] <span class="keyword">for</span> e <span class="keyword">in</span> logit_list]).reshape((<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">allprob = np.reshape(np.array(y_scores), (<span class="number">-1</span>))  <span class="comment"># 拉成一维数组</span></span><br><span class="line">allans  = np.reshape(y_true, (<span class="number">-1</span>))</span><br><span class="line">order   = np.argsort(-allprob)  <span class="comment"># 排序后的 index</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">p_score</span><span class="params">(n)</span>:</span></span><br><span class="line">corr_num = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> order[:n]:</span><br><span class="line">corr_num += <span class="number">1.0</span> <span class="keyword">if</span> (allans[i] == <span class="number">1</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">return</span> corr_num / n</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> p_score(<span class="number">100</span>), p_score(<span class="number">200</span>), p_score(<span class="number">300</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Relation Extraction </tag>
            
            <tag> P@N </tag>
            
            <tag> multi-label </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rule Mining - Wikidata PPDB WordNet</title>
      <link href="/2020/12/04/Rule-Mining-Wikidata-PPDB-WordNet/"/>
      <url>/2020/12/04/Rule-Mining-Wikidata-PPDB-WordNet/</url>
      
        <content type="html"><![CDATA[<p>使用一些额外信息，可以辅助关系抽取的进行。</p><h1 id="Wikidata"><a href="#Wikidata" class="headerlink" title="Wikidata"></a>Wikidata</h1><p>在Wikidata中，property存在alias信息，可以用来寻找关系 (属性) 的同名关系，具体可参考我之前的文章。</p><p><strong>FIGER provides a mapping from types provides by Freebase to its 113 categories.</strong></p><h1 id="PPDB"><a href="#PPDB" class="headerlink" title="PPDB"></a>PPDB</h1><p>PPDB (The Paraphase Database)，有一个在线网站：</p><p><a href="http://paraphrase.org/#/" target="_blank" rel="noopener">PPDB (paraphrase.org)</a></p><p>例如，输入短语“capital of”，可以查到19个近似短语：<a id="more"></a></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line">capital cities of</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">capital city of</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">working capital</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line">capital cities</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line">capital city</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line">, capital city of</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line">financing of</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line">capital flows</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line">of financial resources</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line">capital city ,</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">11</span></span><br><span class="line">'s capital</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">12</span></span><br><span class="line">financial resources for</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">13</span></span><br><span class="line">resources available</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">14</span></span><br><span class="line">financial flows</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">15</span></span><br><span class="line">, officials</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">16</span></span><br><span class="line">, the capital city of</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">17</span></span><br><span class="line">, capital city</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">18</span></span><br><span class="line">capital stock of</span><br><span class="line">Unknown syntactic type</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="number">19</span></span><br><span class="line">share capital of</span><br><span class="line">Unknown syntactic type</span><br></pre></td></tr></table></figure><h1 id="————————"><a href="#————————" class="headerlink" title="————————-"></a>————————-</h1><p>但是，已经有之前的工作帮我们做好了：</p><p><a href="https://github.com/malllabiisc/RESIDE" target="_blank" rel="noopener">https://github.com/malllabiisc/RESIDE</a></p><h1 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h1><p>之前还有想过使用 WordNet 做一些同义词、上位词的替换，但是想想，WordNet有点过时了 (🥶🤯)，所以就不采用这种方法了，使用<strong>预训练语言模型</strong>，蕴含丰富的语义信息，而且泛化能力更好，岂不美哉？</p>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Wikidata </tag>
            
            <tag> PPDB </tag>
            
            <tag> WordNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>绘制PR曲线，惯例做法</title>
      <link href="/2020/12/03/%E7%BB%98%E5%88%B6PR%E6%9B%B2%E7%BA%BF%EF%BC%8C%E6%83%AF%E4%BE%8B%E5%81%9A%E6%B3%95/"/>
      <url>/2020/12/03/%E7%BB%98%E5%88%B6PR%E6%9B%B2%E7%BA%BF%EF%BC%8C%E6%83%AF%E4%BE%8B%E5%81%9A%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>在远程监督关系抽取评价时，通常会采用PR曲线的方式，需要深入理解。</p><p>本文的PR绘制方法主要参考了RESIDE，有兴趣可以直接查看源码：</p><p><a href="https://github.com/malllabiisc/RESIDE" target="_blank" rel="noopener">https://github.com/malllabiisc/RESIDE</a></p><h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><p>绘制需要用到scikit-learn工具，以及matplotlib包。</p><h1 id="计算函数"><a href="#计算函数" class="headerlink" title="计算函数"></a>计算函数</h1><p>核心功能就下面几行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve, average_precision_score</span><br><span class="line"></span><br><span class="line">y_true, y_scores = loadData(<span class="string">'./results/&#123;&#125;/precision_recall.pkl'</span>.format(args.name))</span><br><span class="line">precision,recall,threshold = precision_recall_curve(y_true,y_scores)</span><br><span class="line">rea_under = average_precision_score(y_true, y_scores)</span><br><span class="line">baselines_path = <span class="string">'./baselines_pr/&#123;&#125;/'</span>.format(dataset)</span><br><span class="line">print(<span class="string">'Area under the curve: &#123;:.3&#125;'</span>.format(area_under))</span><br><span class="line"></span><br><span class="line">plt.plot(recall[:], precision[:], label=<span class="string">'RESIDE'</span>, color =<span class="string">'red'</span>, lw=<span class="number">1</span>, marker = <span class="string">'o'</span>, markevery = <span class="number">0.1</span>, ms = <span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>关于precision_recall_curve(), average_precision_score()，强烈推荐去看文档：</p><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html</a></p><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html</a></p><a id="more"></a><h2 id="precision-recall-curve"><a href="#precision-recall-curve" class="headerlink" title="precision_recall_curve()"></a>precision_recall_curve()</h2><font color="orange"> **值得注意的是y_true要求是二进制的，对于多分类问题，就需要自己将分类标签通过某种方式转化。** </font><p>处理部分的代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_true = np.array([e[<span class="number">1</span>:] <span class="keyword">for</span> e <span class="keyword">in</span> y_hot]).reshape((<span class="number">-1</span>))  <span class="comment"># 不考虑 NA 关系</span></span><br><span class="line">y_scores = np.array([e[<span class="number">1</span>:] <span class="keyword">for</span> e <span class="keyword">in</span> logit_list]).reshape((<span class="number">-1</span>))</span><br><span class="line">area_pr = average_precision_score(y_true, y_scores)</span><br></pre></td></tr></table></figure><h2 id="average-precision-score"><a href="#average-precision-score" class="headerlink" title="average_precision_score()"></a>average_precision_score()</h2><p>在sklearn中的实现中，值得注意的是：</p><p>计算公式为：</p><script type="math/tex; mode=display">AP = \sum (R_n - R_{n-1})P_n</script><blockquote><p>This implementation is not interpolated and is different from computing the area under the precision-recall curve with the trapezoidal rule [梯形公式], which uses linear interpolation and can be too optimistic.</p></blockquote><p>采用了一种矩形的近似计算方法，没有采用插值法。<strong>Anyway…直接用就可以了！</strong></p><h1 id="绘制函数"><a href="#绘制函数" class="headerlink" title="绘制函数"></a>绘制函数</h1><p>绘制主要用到matplotlib工具，函数为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">base_list = [<span class="string">'BGWA'</span>, <span class="string">'PCNN+ATT'</span>, <span class="string">'PCNN'</span>, <span class="string">'MIMLRE'</span>, <span class="string">'MultiR'</span>, <span class="string">'Mintz'</span>]</span><br><span class="line">color = [<span class="string">'purple'</span>, <span class="string">'darkorange'</span>, <span class="string">'green'</span>, <span class="string">'xkcd:azure'</span>, <span class="string">'orchid'</span>, <span class="string">'cornflowerblue'</span>]</span><br><span class="line">marker = [<span class="string">'d'</span>, <span class="string">'s'</span>, <span class="string">'^'</span>, <span class="string">'*'</span>, <span class="string">'v'</span>, <span class="string">'x'</span>, <span class="string">'h'</span>]</span><br><span class="line">plt.ylim([<span class="number">0.3</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">0.45</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, baseline <span class="keyword">in</span> enumerate(base_list):</span><br><span class="line">precision = np.load(baselines_path + baseline + <span class="string">'/precision.npy'</span>)</span><br><span class="line">recall    = np.load(baselines_path + baseline + <span class="string">'/recall.npy'</span>)</span><br><span class="line">plt.plot(recall, precision, color = color[i], label = baseline, lw=<span class="number">1</span>, marker = marker[i], markevery = <span class="number">0.1</span>, ms = <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Recall'</span>,    fontsize = <span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Precision'</span>, fontsize = <span class="number">14</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper right"</span>, prop = &#123;<span class="string">'size'</span> : <span class="number">12</span>&#125;)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plot_path = <span class="string">'./results/&#123;&#125;/plot_pr.pdf'</span>.format(args.name)</span><br><span class="line">plt.savefig(plot_path)</span><br><span class="line">print(<span class="string">'Precision-Recall plot saved at: &#123;&#125;'</span>.format(plot_path))</span><br></pre></td></tr></table></figure><p><img scr="/images/blog/2020/plot_pr.png" width="500"></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AUC </tag>
            
            <tag> PR Curve </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pip下载速度过慢的问题</title>
      <link href="/2020/12/03/pip%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6%E8%BF%87%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98/"/>
      <url>/2020/12/03/pip%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6%E8%BF%87%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>在服务器上下载tensorflow-gpu 1.15时，下载速度太慢，只有35KB/s，太慢了…啊( •̀ ω •́ )✧</p><p>可以在下载时指定下载源，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow-gpu==1.15</span><br></pre></td></tr></table></figure><p>下载速度飙升到12MB/s，起飞⛷🤺</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tsinghua </tag>
            
            <tag> pip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Wikidata aliases 利用维基数据中的同名数据</title>
      <link href="/2020/12/03/Wikidata-aliases-%E5%88%A9%E7%94%A8%E7%BB%B4%E5%9F%BA%E6%95%B0%E6%8D%AE%E4%B8%AD%E7%9A%84%E5%90%8C%E5%90%8D%E6%95%B0%E6%8D%AE/"/>
      <url>/2020/12/03/Wikidata-aliases-%E5%88%A9%E7%94%A8%E7%BB%B4%E5%9F%BA%E6%95%B0%E6%8D%AE%E4%B8%AD%E7%9A%84%E5%90%8C%E5%90%8D%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Aliases 是 Also known as 的意思，参见<a href="https://www.wikidata.org/wiki/Help:Aliases" target="_blank" rel="noopener">Help:Aliases</a> </p><p>Aliases 可以分为实体的和属性 (关系) 的，在实验中，我们主要关注关系的同名，即 <a href="https://www.wikidata.org/wiki/Special:MyLanguage/Help:Properties" target="_blank" rel="noopener">Help:Properties</a></p><p>如果不理解Wikidata的数据组成形式以及各个名称的含义，可以参考<a href="https://www.wikidata.org/wiki/Wikidata:Glossary#Aliases" target="_blank" rel="noopener">Wikidata:Glossary</a></p><p><img src="/images/blog/2020/730px-Datamodel_in_Wikidata.svg.png" alt></p><a id="more"></a><h1 id="获取-Aliases-列表"><a href="#获取-Aliases-列表" class="headerlink" title="获取 Aliases 列表"></a>获取 Aliases 列表</h1><p>首先打开<a href="https://www.wikidata.org/wiki/Help:Properties" target="_blank" rel="noopener">Help:Properties</a> 页面，可以看到<strong>Search</strong>项，提供了搜索入口。</p><p>打开Properties搜索页面，输入想要查询的关系 (属性)。举个栗子🌰，输入”capital of”，得到结果：</p><p><img src="/images/blog/2020/Aliases.png" alt></p><h1 id="利用SPARQL查询"><a href="#利用SPARQL查询" class="headerlink" title="利用SPARQL查询"></a>利用SPARQL查询</h1><p>由于我没有亲自动手尝试，所以就把Stack overflow上面的一个问题答案直接贴在这里，感兴趣可以去看<a href="https://stackoverflow.com/questions/40593452/how-to-retrieve-aliases-from-wikidata" target="_blank" rel="noopener">原页面</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SELECT distinct ?subject ?subjectLabel ?nomeLabel ?cognomeLabel ?nickname ?alternative ?subjectAltLabel WHERE &#123;</span><br><span class="line">  ?subject wdt:P31 wd:Q5.</span><br><span class="line">  ?subject p:P54 ?team .</span><br><span class="line">  ?team ps:P54 wd:Q2739 .</span><br><span class="line">  FILTER NOT EXISTS &#123; ?team pq:P582 ?end . &#125;</span><br><span class="line">  OPTIONAL &#123; ?subject wdt:P735 ?nome . &#125;</span><br><span class="line">  OPTIONAL &#123; ?subject wdt:P734 ?cognome . &#125;</span><br><span class="line">  OPTIONAL &#123; ?subject wdt:P1449 ?nickname . &#125;</span><br><span class="line">  OPTIONAL &#123; ?subject skos:altLabel ?alternative . &#125;</span><br><span class="line">  SERVICE wikibase:label &#123; bd:serviceParam wikibase:language &quot;it,en,fr&quot;. &#125;</span><br><span class="line">&#125;</span><br><span class="line">ORDER BY (?cognomeLabel)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Distant Supervision </tag>
            
            <tag> Wikidata </tag>
            
            <tag> As same as </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch用于序列卷积的Conv1d()和Conv2d()区别</title>
      <link href="/2020/11/30/Pytorch%E7%94%A8%E4%BA%8E%E5%BA%8F%E5%88%97%E5%8D%B7%E7%A7%AF%E7%9A%84Conv1d-%E5%92%8CConv2d-%E5%8C%BA%E5%88%AB/"/>
      <url>/2020/11/30/Pytorch%E7%94%A8%E4%BA%8E%E5%BA%8F%E5%88%97%E5%8D%B7%E7%A7%AF%E7%9A%84Conv1d-%E5%92%8CConv2d-%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p>对于PCNN的实现，有两种方式，一种是使用Conv1d()，一种是使用Conv2d()，到底两种方式有没有区别？今天做了个实验进行探究。</p><h1 id="1-卷积层超参数"><a href="#1-卷积层超参数" class="headerlink" title="1. 卷积层超参数"></a>1. 卷积层超参数</h1><p>Conv2d():</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">self.conv = nn.Conv2d(</span><br><span class="line">    in_channels=<span class="number">1</span>,</span><br><span class="line">    out_channels=self.filter_num,</span><br><span class="line">    kernel_size=(self.window, self.dim),</span><br><span class="line">    stride=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    bias=<span class="literal">True</span>,</span><br><span class="line">    padding=(<span class="number">1</span>, <span class="number">0</span>),  <span class="comment"># same padding</span></span><br><span class="line">    padding_mode=<span class="string">'zeros'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Conv1d():</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">self.conv1d = nn.Conv1d(</span><br><span class="line">    in_channels=self.dim,</span><br><span class="line">    out_channels=self.filter_num,</span><br><span class="line">    kernel_size=self.window,</span><br><span class="line">    stride=<span class="number">1</span>,</span><br><span class="line">    bias=<span class="literal">True</span>,</span><br><span class="line">    padding=<span class="number">1</span>,</span><br><span class="line">    padding_mode=<span class="string">'zeros'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h1 id="2-卷积过程"><a href="#2-卷积过程" class="headerlink" title="2. 卷积过程"></a>2. 卷积过程</h1><p>Conv2d():</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolution</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">    x = x.unsqueeze(dim=<span class="number">1</span>)  <span class="comment"># (N, C, H, W) &lt;- (N, 1, seq_len, dim)</span></span><br><span class="line">    x = self.conv(x)</span><br><span class="line">    x = x.view(<span class="number">-1</span>, self.filter_num, self.max_len)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># mask, remove the effect of 'PAD'</span></span><br><span class="line">    mask = mask.unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">    mask = mask.expand(mask.shape[<span class="number">0</span>], self.filter_num, mask.shape[<span class="number">-1</span>])</span><br><span class="line">    x = x.masked_fill_(mask.eq(<span class="number">0</span>), float(<span class="string">'-inf'</span>))</span><br><span class="line">    x = x.unsqueeze(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>Conv1d():</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolution1d</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">    x = x.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (N, C, L) &lt;- (N, dim, seq_len)</span></span><br><span class="line">    x = self.conv1d(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># mask, remove the effect of 'PAD'</span></span><br><span class="line">    mask = mask.unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">    mask = mask.expand(mask.shape[<span class="number">0</span>], self.filter_num, mask.shape[<span class="number">-1</span>])</span><br><span class="line">    x = x.masked_fill_(mask.eq(<span class="number">0</span>), float(<span class="string">'-inf'</span>))</span><br><span class="line">    x = x.unsqueeze(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h1 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3. 实验结果"></a>3. 实验结果</h1><p>固定随机数种子，两种不同的实现方式结果完全一样，也就是说，<font color="orange"> <strong>两种方式没有区别！</strong> </font> 终于可以放心了！</p><p>Conv2d():</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[Mon 30 Nov 2020 14:05:47]-INFO-run.py-208-1425: Epoch: 10, DEV non_na_macro_f1: 0.5618245545712591</span><br><span class="line">[Mon 30 Nov 2020 14:06:01]-INFO-evaluate.py-85-1425: ***** Running evaluation E-11_S-1513 *****</span><br><span class="line">[Mon 30 Nov 2020 14:06:01]-INFO-evaluate.py-86-1425:   Num examples = 10262</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-345-1425: ***** dev Eval results E-11_S-1513 *****</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   acc = 0.7189631650750341</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   auc = 0.6220957891050019</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   dsgt_f1 = 0.5959275473368374</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   dsgt_p = 0.6370019755980496</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   dsgt_r = 0.6031188145161877</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   macro-f1 = 0.5959275473368374</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   macro-precision = 0.6370019755980497</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   macro-recall = 0.6031188145161875</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   max_f1 = 0.7331908070550509</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   mean_prec = 0.17543470520760482</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   micro-f1 = 0.7189631650750341</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   micro-precision = 0.7189631650750341</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   micro-recall = 0.7189631650750341</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   non_na_auc = 0.7465032659070302</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   non_na_macro_f1 = 0.5959275473368374</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   non_na_macro_precision = 0.6370019755980496</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   non_na_macro_recall = 0.6031188145161877</span><br></pre></td></tr></table></figure><p>Conv1d():</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[Mon 30 Nov 2020 14:05:47]-INFO-run.py-208-1425: Epoch: 10, DEV non_na_macro_f1: 0.5618245545712591</span><br><span class="line">[Mon 30 Nov 2020 14:06:01]-INFO-evaluate.py-85-1425: ***** Running evaluation E-11_S-1513 *****</span><br><span class="line">[Mon 30 Nov 2020 14:06:01]-INFO-evaluate.py-86-1425:   Num examples = 10262</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-345-1425: ***** dev Eval results E-11_S-1513 *****</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   acc = 0.7189631650750341</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   auc = 0.6220957891050019</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   dsgt_f1 = 0.5959275473368374</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   dsgt_p = 0.6370019755980496</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   dsgt_r = 0.6031188145161877</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   macro-f1 = 0.5959275473368374</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   macro-precision = 0.6370019755980497</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   macro-recall = 0.6031188145161875</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   max_f1 = 0.7331908070550509</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   mean_prec = 0.17543470520760482</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   micro-f1 = 0.7189631650750341</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   micro-precision = 0.7189631650750341</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   micro-recall = 0.7189631650750341</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   non_na_auc = 0.7465032659070302</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   non_na_macro_f1 = 0.5959275473368374</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   non_na_macro_precision = 0.6370019755980496</span><br><span class="line">[Mon 30 Nov 2020 14:06:05]-INFO-evaluate.py-347-1425:   non_na_macro_recall = 0.6031188145161877</span><br></pre></td></tr></table></figure><h1 id="4-torch-max-和maxpooling-的区别？"><a href="#4-torch-max-和maxpooling-的区别？" class="headerlink" title="4. torch.max()和maxpooling()的区别？"></a>4. torch.max()和maxpooling()的区别？</h1><p>在卷积后往往需要进行最大池化操作，但是使用maxpooling()和直接使用torch.max()有没有什么区别呢？不想去读源码，直接做实验尝试一下~</p><p><strong>在固定随机数种子的情况下，两种方式完全一致，说明maxpooling()就是取max()的过程，没有差别。</strong></p>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> PCNN </tag>
            
            <tag> Conv1d </tag>
            
            <tag> Conv2d </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sparql查询的速度</title>
      <link href="/2020/11/26/sparql%E6%9F%A5%E8%AF%A2%E7%9A%84%E9%80%9F%E5%BA%A6/"/>
      <url>/2020/11/26/sparql%E6%9F%A5%E8%AF%A2%E7%9A%84%E9%80%9F%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<p>最近在实验过程中，需要使用python查询mid对应的实体类型，尝试了使用<a href="https://sparqlwrapper.readthedocs.io/en/latest/SPARQLWrapper.Wrapper.html?highlight=setQuery#SPARQLWrapper.Wrapper.SPARQLWrapper.setQuery" target="_blank" rel="noopener">RDFLib</a>提供的API接口，但是效率实在太低了，我<strong>查询1个实体需要用时9s左右</strong>，这样下来，查询10万个实体，就需要耗时250小时……不可接受</p><h1 id="1-使用RDFLib"><a href="#1-使用RDFLib" class="headerlink" title="1. 使用RDFLib"></a>1. 使用RDFLib</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Wrapper</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.sparql = SPARQLWrapper(<span class="string">"http://210.28.132.62:8900/sparql"</span>)</span><br><span class="line">        self.sparql.setReturnFormat(JSON)</span><br><span class="line">        self.query_string = <span class="string">"select ?o where &#123;&lt;http://rdf.freebase.com/ns/%s&gt; &lt;http://rdf.freebase.com/ns/type.object.type&gt; ?o&#125;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">query</span><span class="params">(self, mid)</span>:</span></span><br><span class="line">        self.sparql.setQuery(self.query_string % mid)</span><br><span class="line">        results =  self.sparql.query().convert()</span><br><span class="line">        data = [result[<span class="string">"o"</span>][<span class="string">"value"</span>] <span class="keyword">for</span> result <span class="keyword">in</span> results[<span class="string">"results"</span>][<span class="string">"bindings"</span>]]</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><h1 id="2-使用urllib"><a href="#2-使用urllib" class="headerlink" title="2. 使用urllib"></a>2. 使用urllib</h1><p>尝试直接使用urllib发送请求，不再用现有的封装工具：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">query</span><span class="params">(mid)</span>:</span></span><br><span class="line">    <span class="string">"""使用urllib查询SPARQL"""</span></span><br><span class="line">    params = &#123;<span class="string">'query'</span>:<span class="string">'select ?o where &#123;&lt;http://rdf.freebase.com/ns/m.071cn&gt; &lt;http://rdf.freebase.com/ns/type.object.type&gt; ?o&#125;'</span>,</span><br><span class="line">    <span class="string">'format'</span>: <span class="string">'application/sparql-results+json'</span>&#125;</span><br><span class="line">    params = bytes(urllib.parse.urlencode(params), encoding=<span class="string">'utf8'</span>)</span><br><span class="line">    result = json.loads(urllib.request.urlopen(<span class="string">'http://210.28.132.62:8900/sparql'</span>, data=params).read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line">    result = [r[<span class="string">"o"</span>][<span class="string">"value"</span>] <span class="keyword">for</span> r <span class="keyword">in</span> result[<span class="string">"results"</span>][<span class="string">"bindings"</span>]]</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>这样查询了10个实体，耗时90s……还是太慢了，并没有什么效率上的提升。</p><h1 id="3-使用grequests并发请求"><a href="#3-使用grequests并发请求" class="headerlink" title="3. 使用grequests并发请求"></a>3. 使用grequests并发请求</h1><p>估计是因为串行导致效率太低，在网上查询后，推荐<a href="https://github.com/spyoungtech/grequests" target="_blank" rel="noopener">grequests</a></p><p>改写函数如下：<a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pquery</span><span class="params">(mids)</span>:</span></span><br><span class="line">    <span class="string">"""并行查询"""</span></span><br><span class="line">    req_list = [grequests.get(</span><br><span class="line">        <span class="string">'http://210.28.132.62:8900/sparql'</span>,</span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">"query"</span>: <span class="string">"select ?o where &#123;&lt;http://rdf.freebase.com/ns/%s&gt; &lt;http://rdf.freebase.com/ns/type.object.type&gt; ?o&#125;"</span> % mid,</span><br><span class="line">            <span class="string">"format"</span>: <span class="string">"application/sparql-results+json"</span></span><br><span class="line">        &#125;) <span class="keyword">for</span> mid <span class="keyword">in</span> mids</span><br><span class="line">    ]</span><br><span class="line">    resp_list = grequests.map(req_list)</span><br><span class="line">    <span class="keyword">return</span> resp_list</span><br></pre></td></tr></table></figure><p>查询10个实体，耗时0.03s！YES✌</p><p>查询1000个实体，耗时8.49s！火箭一般的速度！🚀起飞！</p><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><p>这次经历让我体会到算法的重要性，好的算法何止是事半功倍啊！</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SPARQL </tag>
            
            <tag> urllib </tag>
            
            <tag> urllib2 </tag>
            
            <tag> urllib3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rule Mining-规则挖掘算法</title>
      <link href="/2020/11/26/Rule-Mining-%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95/"/>
      <url>/2020/11/26/Rule-Mining-%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>可以利用数据挖掘技术，自动地从文本中挖掘模式，在关系抽取中，我采用了以下步骤：</p><h1 id="1-实体类型识别"><a href="#1-实体类型识别" class="headerlink" title="1. 实体类型识别"></a>1. 实体类型识别</h1><h2 id="1-1-How-to-search-in-freebase-by-mid"><a href="#1-1-How-to-search-in-freebase-by-mid" class="headerlink" title="1.1 How to search in freebase by mid?"></a>1.1 How to search in freebase by mid?</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select ?o</span><br><span class="line">where</span><br><span class="line">&#123;</span><br><span class="line">&lt;http://rdf.freebase.com/ns/m.071cn&gt; </span><br><span class="line">    &lt;http://rdf.freebase.com/ns/type.object.type&gt;</span><br><span class="line">    ?o</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>按照上述方式，得到当前实体<code>m.071cn</code>的实体类型，列表如下：</p><p><img src="/images/blog/2020/types.png" alt></p><p>具体在python中实现时，可以采用以下方式，主要用到<a href="https://github.com/RDFLib/rdflib" target="_blank" rel="noopener">REFLib</a>包，具体可参考我的另一篇博文：</p><blockquote><p> <a href>sparql查询的速度</a></p></blockquote><a id="more"></a><h2 id="1-2-筛选"><a href="#1-2-筛选" class="headerlink" title="1.2 筛选"></a>1.2 筛选</h2><p>可以将mid的实体链接结果和使用工具 (如 Stanford Core NLP) 的命名实体识别结果相结合，得到实体类型信息。</p><p>freebase中包含的domains有：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line"> <span class="string">"visual_art"</span>,</span><br><span class="line"> <span class="string">"soccer"</span>,</span><br><span class="line"> <span class="string">"geography"</span>,</span><br><span class="line"> <span class="string">"martial_arts"</span>,</span><br><span class="line"> <span class="string">"amusement_parks"</span>,</span><br><span class="line"> <span class="string">"conferences"</span>,</span><br><span class="line"> <span class="string">"comic_strips"</span>,</span><br><span class="line"> <span class="string">"metropolitan_transit"</span>,</span><br><span class="line"> <span class="string">"comedy"</span>,</span><br><span class="line"> <span class="string">"atom"</span>,</span><br><span class="line"> <span class="string">"tv"</span>,</span><br><span class="line"> <span class="string">"dining"</span>,</span><br><span class="line"> <span class="string">"fictional_universe"</span>,</span><br><span class="line"> <span class="string">"time"</span>,</span><br><span class="line"> <span class="string">"internet"</span>,</span><br><span class="line"> <span class="string">"nytimes"</span>,</span><br><span class="line"> <span class="string">"chess"</span>,</span><br><span class="line"> <span class="string">"book"</span>,</span><br><span class="line"> <span class="string">"engineering"</span>,</span><br><span class="line"> <span class="string">"dataworld"</span>,</span><br><span class="line"> <span class="string">"library"</span>,</span><br><span class="line"> <span class="string">"projects"</span>,</span><br><span class="line"> <span class="string">"protected_sites"</span>,</span><br><span class="line"> <span class="string">"travel"</span>,</span><br><span class="line"> <span class="string">"government"</span>,</span><br><span class="line"> <span class="string">"zoos"</span>,</span><br><span class="line"> <span class="string">"tennis"</span>,</span><br><span class="line"> <span class="string">"interests"</span>,</span><br><span class="line"> <span class="string">"spaceflight"</span>,</span><br><span class="line"> <span class="string">"award"</span>,</span><br><span class="line"> <span class="string">"base"</span>,</span><br><span class="line"> <span class="string">"broadcast"</span>,</span><br><span class="line"> <span class="string">"meteorology"</span>,</span><br><span class="line"> <span class="string">"religion"</span>,</span><br><span class="line"> <span class="string">"cvg"</span>,</span><br><span class="line"> <span class="string">"computer"</span>,</span><br><span class="line"> <span class="string">"imdb"</span>,</span><br><span class="line"> <span class="string">"opera"</span>,</span><br><span class="line"> <span class="string">"distilled_spirits"</span>,</span><br><span class="line"> <span class="string">"radio"</span>,</span><br><span class="line"> <span class="string">"people"</span>,</span><br><span class="line"> <span class="string">"language"</span>,</span><br><span class="line"> <span class="string">"transportation"</span>,</span><br><span class="line"> <span class="string">"games"</span>,</span><br><span class="line"> <span class="string">"type"</span>,</span><br><span class="line"> <span class="string">"medicine"</span>,</span><br><span class="line"> <span class="string">"olympics"</span>,</span><br><span class="line"> <span class="string">"food"</span>,</span><br><span class="line"> <span class="string">"sports"</span>,</span><br><span class="line"> <span class="string">"royalty"</span>,</span><br><span class="line"> <span class="string">"wine"</span>,</span><br><span class="line"> <span class="string">"digicams"</span>,</span><br><span class="line"> <span class="string">"aviation"</span>,</span><br><span class="line"> <span class="string">"influence"</span>,</span><br><span class="line"> <span class="string">"user"</span>,</span><br><span class="line"> <span class="string">"basketball"</span>,</span><br><span class="line"> <span class="string">"finance"</span>,</span><br><span class="line"> <span class="string">"celebrities"</span>,</span><br><span class="line"> <span class="string">"music"</span>,</span><br><span class="line"> <span class="string">"comic_books"</span>,</span><br><span class="line"> <span class="string">"organization"</span>,</span><br><span class="line"> <span class="string">"education"</span>,</span><br><span class="line"> <span class="string">"skiing"</span>,</span><br><span class="line"> <span class="string">"ice_hockey"</span>,</span><br><span class="line"> <span class="string">"military"</span>,</span><br><span class="line"> <span class="string">"cricket"</span>,</span><br><span class="line"> <span class="string">"chemistry"</span>,</span><br><span class="line"> <span class="string">"automotive"</span>,</span><br><span class="line"> <span class="string">"fashion"</span>,</span><br><span class="line"> <span class="string">"boats"</span>,</span><br><span class="line"> <span class="string">"theater"</span>,</span><br><span class="line"> <span class="string">"event"</span>,</span><br><span class="line"> <span class="string">"baseball"</span>,</span><br><span class="line"> <span class="string">"freebase"</span>,</span><br><span class="line"> <span class="string">"media_common"</span>,</span><br><span class="line"> <span class="string">"common"</span>,</span><br><span class="line"> <span class="string">"bicycles"</span>,</span><br><span class="line"> <span class="string">"american_football"</span>,</span><br><span class="line"> <span class="string">"rail"</span>,</span><br><span class="line"> <span class="string">"biology"</span>,</span><br><span class="line"> <span class="string">"business"</span>,</span><br><span class="line"> <span class="string">"kp_lw"</span>,</span><br><span class="line"> <span class="string">"venture_capital"</span>,</span><br><span class="line"> <span class="string">"astronomy"</span>,</span><br><span class="line"> <span class="string">"law"</span>,</span><br><span class="line"> <span class="string">"periodicals"</span>,</span><br><span class="line"> <span class="string">"exhibitions"</span>,</span><br><span class="line"> <span class="string">"architecture"</span>,</span><br><span class="line"> <span class="string">"film"</span>,</span><br><span class="line"> <span class="string">"location"</span>,</span><br><span class="line"> <span class="string">"symbols"</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure><font color="orange"> **将一些明显错误的的过滤掉，例如user, common等，然后选取频率最高的作为实体类型。** </font><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">RANKS = [</span><br><span class="line">    (<span class="string">'people.person'</span>, <span class="string">'people.person'</span>),</span><br><span class="line">    (<span class="string">'location.location'</span>, <span class="string">'location.location'</span>),</span><br><span class="line">    (<span class="string">'organization.organization'</span>, <span class="string">'organization.organization'</span>),</span><br><span class="line">    (<span class="string">'religion.religion'</span>, <span class="string">'religion.religion'</span>),</span><br><span class="line">    (<span class="string">'film.film'</span>, <span class="string">'film.film'</span>),</span><br><span class="line">    (<span class="string">'sports.sports_team'</span>, <span class="string">'sports.sports_team'</span>),</span><br><span class="line">    (<span class="string">'people.ethnicity'</span>, <span class="string">'people.ethnicity'</span>),</span><br><span class="line">    (<span class="string">'people.family'</span>, <span class="string">'people.family'</span>),</span><br><span class="line">    (<span class="string">'language.human_language'</span>, <span class="string">'language.human_language'</span>),</span><br><span class="line">    (<span class="string">'people.profession'</span>, <span class="string">'people.profession'</span>),</span><br><span class="line">    (<span class="string">'film.film_festival'</span>, <span class="string">'film.film_festival'</span>),</span><br><span class="line">    # ================ maps ================ #</span><br><span class="line">    (<span class="string">'film.film_character'</span>, <span class="string">'people.person'</span>),</span><br><span class="line">    (<span class="string">'people.family_name'</span>, <span class="string">'people.person'</span>)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>按照上述顺序筛选和映射，实验证明效果还是不错的嘻嘻🤩</p><h1 id="2-规则体挖掘"><a href="#2-规则体挖掘" class="headerlink" title="2. 规则体挖掘"></a>2. 规则体挖掘</h1><p>规则体挖掘主要按照以下步骤进行：</p><ol><li>取头尾实体之间的字符串</li><li>去除标点，转为小写</li><li>词干提取</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> nltk.stem.porter <span class="keyword">import</span> PorterStemmer</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure><p>利用以下几个函数：</p><ol><li>取字符串</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_nested</span><span class="params">(h, t)</span>:</span></span><br><span class="line">    <span class="string">"""实体是否有重叠部分"""</span></span><br><span class="line">    <span class="keyword">if</span> (t[<span class="string">'pos'</span>][<span class="number">0</span>] &lt;= h[<span class="string">'pos'</span>][<span class="number">0</span>] &lt; t[<span class="string">'pos'</span>][<span class="number">1</span>]) <span class="keyword">or</span> (t[<span class="string">'pos'</span>][<span class="number">0</span>] &lt; h[<span class="string">'pos'</span>][<span class="number">1</span>] &lt;= t[<span class="string">'pos'</span>][<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> (h[<span class="string">'pos'</span>][<span class="number">0</span>] &lt;= t[<span class="string">'pos'</span>][<span class="number">0</span>] &lt; h[<span class="string">'pos'</span>][<span class="number">1</span>]) <span class="keyword">or</span> (h[<span class="string">'pos'</span>][<span class="number">0</span>] &lt; t[<span class="string">'pos'</span>][<span class="number">1</span>] &lt;= h[<span class="string">'pos'</span>][<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_pattern</span><span class="params">(data)</span>:</span></span><br><span class="line">    patterns = []</span><br><span class="line">    <span class="keyword">for</span> record <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> is_nested(record[<span class="string">'h'</span>], record[<span class="string">'t'</span>]):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> record[<span class="string">'h'</span>][<span class="string">'pos'</span>][<span class="number">0</span>] &lt; record[<span class="string">'t'</span>][<span class="string">'pos'</span>][<span class="number">0</span>]:</span><br><span class="line">            start = record[<span class="string">'h'</span>][<span class="string">'pos'</span>][<span class="number">1</span>]</span><br><span class="line">            end = record[<span class="string">'t'</span>][<span class="string">'pos'</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            start = record[<span class="string">'t'</span>][<span class="string">'pos'</span>][<span class="number">1</span>]</span><br><span class="line">            end = record[<span class="string">'h'</span>][<span class="string">'pos'</span>][<span class="number">0</span>]</span><br><span class="line">        body = record[<span class="string">'text'</span>][start:end]</span><br><span class="line">        patterns.append(&#123;<span class="string">'body'</span>: body, <span class="string">'relation'</span>:record[<span class="string">'relation'</span>]&#125;)</span><br><span class="line">    <span class="keyword">return</span> patterns</span><br></pre></td></tr></table></figure><ol><li>转小写，去除标点</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">punctuation</span><span class="params">(patterns)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    regex = re.compile(<span class="string">'[%s]'</span> % re.escape(string.punctuation))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(patterns)):</span><br><span class="line">        body = regex.sub(<span class="string">''</span>, patterns[i][<span class="string">'body'</span>]).strip().lower()</span><br><span class="line">        <span class="keyword">if</span> len(body) &gt; <span class="number">0</span>:</span><br><span class="line">            result.append(&#123;<span class="string">'body'</span>:body, <span class="string">'relation'</span>:patterns[i][<span class="string">'relation'</span>]&#125;)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><ol><li>利用 NLTK 词干提取</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stemming</span><span class="params">(patterns)</span>:</span></span><br><span class="line">    stemmer = PorterStemmer()</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> pattern <span class="keyword">in</span> tqdm(patterns, total=len(patterns)):</span><br><span class="line">        body = <span class="string">' '</span>.join([stemmer.stem(token) <span class="keyword">for</span> token <span class="keyword">in</span> pattern[<span class="string">'body'</span>].split()])</span><br><span class="line">        <span class="keyword">if</span> len(body) &gt; <span class="number">0</span>:</span><br><span class="line">            result.append(&#123;<span class="string">'body'</span>:body, <span class="string">'relation'</span>:pattern[<span class="string">'relation'</span>]&#125;)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h1 id="3-规则筛选和标注"><a href="#3-规则筛选和标注" class="headerlink" title="3. 规则筛选和标注"></a>3. 规则筛选和标注</h1><p>按照频率或规则覆盖的样本数，过滤规则；然后标注规则。得到规则集合。结束🎉</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rule </tag>
            
            <tag> Pattern </tag>
            
            <tag> 规则 </tag>
            
            <tag> 模式 </tag>
            
            <tag> 规则挖掘 </tag>
            
            <tag> Rule Mining </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>涨知识——GCN有向图, Inductive, Transductive</title>
      <link href="/2020/11/25/%E6%B6%A8%E7%9F%A5%E8%AF%86%E2%80%94%E2%80%94GCN%E6%9C%89%E5%90%91%E5%9B%BE-Inductive-Transductive/"/>
      <url>/2020/11/25/%E6%B6%A8%E7%9F%A5%E8%AF%86%E2%80%94%E2%80%94GCN%E6%9C%89%E5%90%91%E5%9B%BE-Inductive-Transductive/</url>
      
        <content type="html"><![CDATA[<h1 id="GCN可以处理有向图吗？"><a href="#GCN可以处理有向图吗？" class="headerlink" title="GCN可以处理有向图吗？"></a>GCN可以处理有向图吗？</h1><p>首先图神经网络分为spectral domain 和 spatial domain两大块。</p><p>spatial domain完全可以处理有向图，例如GAT; </p><p>spectral domain确实是建立在无向图的假设的下的，理论上对有向图无能为力。不过也有一些人在改进，但是理论根基放在那，能做只有从有向图中构造出对称的权值矩阵。例如这篇16年Science的工作（70+页）给出了一种使用模体（一些非常小的局部图范式，可以根据应用自定义）从有向图构造对称拉普拉斯矩阵的方法</p><p><a href="https://arxiv.org/pdf/1612.08447.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1612.08447.pdf</a></p><p>下面这篇17年的工作，就是使用模体的方法将spectral domain GCN扩展到有向图上</p><p><a href="https://arxiv.org/abs/1802.01572" target="_blank" rel="noopener">https://arxiv.org/abs/1802.01572</a></p><h1 id="Inductive-Learning-v-s-Transductive-Learning"><a href="#Inductive-Learning-v-s-Transductive-Learning" class="headerlink" title="Inductive Learning v.s. Transductive Learning"></a>Inductive Learning v.s. Transductive Learning</h1><p>根据维基百科，有以下定义：</p><blockquote><p><strong>Transduction</strong> is reasoning from observed, specific (training) cases to specific (test) cases. In contrast, <strong>induction</strong> is reasoning from observed training cases to general rules, which are then applied to the test cases.</p></blockquote><p>拆开了看，按照以下方式理解：<a id="more"></a></p><h2 id="Inductive-Learning"><a href="#Inductive-Learning" class="headerlink" title="Inductive Learning"></a>Inductive Learning</h2><p>传统的监督学习就是Inductive Learning。我们用标注的训练集训练一个机器学习模型，然后在从来没见过的测试集上预测标签。</p><h2 id="Transductive-Learning"><a href="#Transductive-Learning" class="headerlink" title="Transductive Learning"></a>Transductive Learning</h2><p>不同于Inductive Learning，Transductive Learning在训练和测试阶段都可以看到所有的数据。利用训练集进行学习，然后在测试集上预测标签。虽然测试集上的标签不可知，但是在训练时，测试集上的模式 (pattern) 和额外信息 (additional information) 还是可以利用的。</p><h2 id="区别是什么？"><a href="#区别是什么？" class="headerlink" title="区别是什么？"></a>区别是什么？</h2><p>Transduction 不构建一个预测模型。如果测试集中加入了新的数据，那么我们需要重新训练整个模型，然后在新的测试集上的预测。与此不同，Induction 构建一个预测模型，当有新的测试数据时，不需要重新训练。</p><p><img src="/images/blog/2020/inductivetransductive.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GCN </tag>
            
            <tag> Inductive </tag>
            
            <tag> Transductive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>字符串规范化——去除标点符号和词干提取</title>
      <link href="/2020/11/24/%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%A7%84%E8%8C%83%E5%8C%96%E2%80%94%E2%80%94%E5%8E%BB%E9%99%A4%E6%A0%87%E7%82%B9%E7%AC%A6%E5%8F%B7%E5%92%8C%E8%AF%8D%E5%B9%B2%E6%8F%90%E5%8F%96/"/>
      <url>/2020/11/24/%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%A7%84%E8%8C%83%E5%8C%96%E2%80%94%E2%80%94%E5%8E%BB%E9%99%A4%E6%A0%87%E7%82%B9%E7%AC%A6%E5%8F%B7%E5%92%8C%E8%AF%8D%E5%B9%B2%E6%8F%90%E5%8F%96/</url>
      
        <content type="html"><![CDATA[<h1 id="去除标点符号"><a href="#去除标点符号" class="headerlink" title="去除标点符号"></a>去除标点符号</h1><p>利用<code>string.punctuation</code>和<code>str.translate()</code>，效率高而且质量高：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">s =<span class="string">"string. With. Punctuation"</span></span><br><span class="line">s = s.translate(string.maketrans(<span class="string">""</span>, <span class="string">""</span>), string.punctuation)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line"><span class="string">'string With Punctuation'</span></span><br></pre></td></tr></table></figure><p>但是以上方法在 Python 3.4 以后就不好用了……</p><p>可以改用正则表达式实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">regex = re.compile(<span class="string">'[%s]'</span> % re.escape(string.punctuation))</span><br><span class="line">s = <span class="string">"string. With. Punctuation"</span></span><br><span class="line">s = regex.sub(<span class="string">''</span>, s)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line"><span class="string">'string With Punctuation'</span></span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="使用-NLTK-词干提取"><a href="#使用-NLTK-词干提取" class="headerlink" title="使用 NLTK 词干提取"></a>使用 NLTK 词干提取</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem.porter <span class="keyword">import</span> PorterStemmer</span><br><span class="line"></span><br><span class="line">stemmer = PorterStemmer()</span><br><span class="line">s = <span class="string">"string With Punctuation"</span></span><br><span class="line">s = <span class="string">' '</span>.join([stemmer.stem(token) <span class="keyword">for</span> token <span class="keyword">in</span> s.split()])  <span class="comment"># 一次只能处理一个token</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line"><span class="string">'string with punctuat'</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> normalization </tag>
            
            <tag> stemming </tag>
            
            <tag> punctuation </tag>
            
            <tag> string </tag>
            
            <tag> 字符串 </tag>
            
            <tag> 词干提取 </tag>
            
            <tag> 规范化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>词干提取 (stemming) 和词形还原 (lemmatization)</title>
      <link href="/2020/11/24/%E8%AF%8D%E5%B9%B2%E6%8F%90%E5%8F%96-stemming-%E5%92%8C%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F-lemmatization/"/>
      <url>/2020/11/24/%E8%AF%8D%E5%B9%B2%E6%8F%90%E5%8F%96-stemming-%E5%92%8C%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F-lemmatization/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文转载自：</p><p><a href="https://blog.csdn.net/m0_37744293/article/details/79065002" target="_blank" rel="noopener">https://blog.csdn.net/m0_37744293/article/details/79065002</a></p></blockquote><h1 id="共同点"><a href="#共同点" class="headerlink" title="共同点"></a>共同点</h1><p><strong>词形还原</strong>（lemmatization），是把一个任何形式的语言词汇还原为一般形式（能表达完整语义），而<strong>词干提取</strong>（stemming）是抽取词的词干或词根形式（不一定能够表达完整语义）。词形还原和词干提取是词形规范化的两类<br>重要方式，都能够达到有效归并词形的目的，二者既有联系也有区别。</p><ul><li>目标一致。词干提取和词形还原的目标均为将词的屈折形态或派生形态简化或归并为词干（stem）或原形的基础形式，都是一种对词的不同形态的统一归并的过程。</li><li>结果部分交叉。词干提取和词形还原不是互斥关系，其结果是有部分交叉的。一部分词利用这两类方法都能达到相同的词形转换效果。如“dogs”的词干为“dog”，其原形也为“dog”。</li><li>主流实现方法类似。目前实现词干提取和词形还原的主流实现方法均是利用语言中存在的规则或利用词典映射提取词干或获得词的原形。</li><li>应用领域相似。主要应用于信息检索和文本、自然语言处理等方面，二者均是这些应用的基本步骤。<a id="more"></a></li></ul><h1 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h1><p><strong>区别</strong></p><ul><li>在原理上，词干提取主要是采用“缩减”的方法，将词转换为词干，如将“cats”处理为“cat”，将“effective”处理为“effect”。而词形还原主要采用“转变”的方法，将词转变为其原形，如将“drove”处理为“drive”，将“driving”处理为“drive”。</li><li>在复杂性上，词干提取方法相对简单，词形还原则需要返回词的原形，需要对词形进行分析，不仅要进行词缀的转化，还要进行词性识别，区分相同词形但原形不同的词的差别。词性标注的准确率也直接影响词形还原的准确率，因此，词形还原更为复杂。</li><li>在实现方法上，虽然词干提取和词形还原实现的主流方法类似，但二者在具体实现上各有侧重。词干提取的实现方法主要利用规则变化进行词缀的去除和缩减，从而达到词的简化效果。词形还原则相对较复杂，有复杂的形态变化，单纯依据规则无法很好地完成。其更依赖于词典，进行词形变化和原形的映射，生成词典中的有效词。</li><li>在结果上，词干提取和词形还原也有部分区别。词干提取的结果可能并不是完整的、具有意义的词，而只是词的一部分，如“revival”词干提取的结果为“reviv”，“ailiner”词干提取的结果为“airlin”。而经词形还原处理后获得的结果是具有一定意义的、完整的词，一般为词典中的有效词。</li><li>在应用领域上，同样各有侧重。虽然二者均被应用于信息检索和文本处理中，但侧重不同。<font color="orange"> <strong>词干提取更多被应用于信息检索领域，如Solr、Lucene等，用于扩展检索，粒度较粗。词形还原更主要被应用于文本挖掘、自然语言处理，用于更细粒度、更为准确的文本分析和表达</strong> </font></li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>具体在应用的时候，主要看任务<strong>更强调精度还是召回</strong>，然后选择适合的方法~</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> stemming </tag>
            
            <tag> 词干提取 </tag>
            
            <tag> 词形还原 </tag>
            
            <tag> lemmatization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文搞懂NLp中的对抗训练</title>
      <link href="/2020/11/23/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82NLp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/"/>
      <url>/2020/11/23/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82NLp%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文转自：李rumor</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5MjEyNDMxOQ==&amp;mid=2247483956&amp;idx=1&amp;sn=49d079e4cd785cb4739f660d72468235&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzA5MjEyNDMxOQ==&amp;mid=2247483956&amp;idx=1&amp;sn=49d079e4cd785cb4739f660d72468235&amp;scene=21#wechat_redirect</a></p></blockquote><p><br></p><blockquote><p>本文主要串烧了FGSM, FGM, PGD, FreeAT, YOPO, FreeLB, SMART这几种对抗训练方法，希望能使各位大佬炼出的丹药更加圆润有光泽，一颗永流传</p></blockquote><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>对抗训练是一种引入噪声的训练方式，可以对参数进行正则化，提升模型鲁棒性和泛化能力。</strong></p><p>对抗训练的假设是：给输入加上扰动之后，输出分布和原Y的分布一致</p><p>有监督的数据下使用交叉熵作为损失：</p><a id="more"></a><p>半监督数据下可计算KL散度：</p><p>扰动如何得来呢？这需要对抗的思想，即<strong>往增大损失的方向增加扰动</strong></p><p>有监督下：</p><p>半监督下：</p><p>theta上面一个尖儿代表的是常数。目的是说在计算对抗扰动时虽然计算了梯度，但不对参数进行更新，<strong>因为当前得到的对抗扰动是对旧参数最优的</strong>。不理解的同学可以自己看下伪代码体会一下。</p><p>用一句话形容对抗训练的思路，就是<strong>在输入上进行梯度上升(增大loss)，在参数上进行梯度下降(减小loss)</strong>。由于输入会进行embedding lookup，所以<strong>实际的做法是在embedding table上进行梯度上升</strong>。</p><p>接下来介绍不同的方法，后续方法<strong>优化的主要方向有两点：得到更优的扰动 &amp; 提升训练速度</strong></p><h3 id="FGSM-Fast-Gradient-Sign-Method-ICLR2015"><a href="#FGSM-Fast-Gradient-Sign-Method-ICLR2015" class="headerlink" title="FGSM (Fast Gradient Sign Method): ICLR2015"></a>FGSM (Fast Gradient Sign Method): ICLR2015</h3><p>FGSM是Goodfellow提出对抗训练时的方法，假设对于输入的梯度为：</p><p>那扰动肯定是沿着梯度的方向往损失函数的极大值走：</p><h3 id="FGM-Fast-Gradient-Method-ICLR2017"><a href="#FGM-Fast-Gradient-Method-ICLR2017" class="headerlink" title="FGM (Fast Gradient Method): ICLR2017"></a>FGM (Fast Gradient Method): ICLR2017</h3><p>FSGM是每个方向上都走相同的一步，Goodfellow后续提出的FGM则是根据具体的梯度进行scale，得到更好的对抗样本：</p><p>伪代码：</p><ul><li></li><li></li><li></li><li></li><li></li><li></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于每个x:  1.计算x的前向loss、反向传播得到梯度  2.根据embedding矩阵的梯度计算出r，并加到当前embedding上，相当于x+r  3.计算x+r的前向loss，反向传播得到对抗的梯度，累加到(1)的梯度上  4.将embedding恢复为(1)时的值  5.根据(3)的梯度对参数进行更新</span><br></pre></td></tr></table></figure><h3 id="PGD-Projected-Gradient-Descent-ICLR2018"><a href="#PGD-Projected-Gradient-Descent-ICLR2018" class="headerlink" title="PGD (Projected Gradient Descent): ICLR2018"></a>PGD (Projected Gradient Descent): ICLR2018</h3><p>FGM直接通过epsilon参数一下子算出了对抗扰动，这样得到的可能不是最优的。因此PGD进行了改进，多迭代几次，慢慢找到最优的扰动。</p><p>引用[1]：</p><blockquote><p>FGM简单粗暴的“一步到位”，可能走不到约束内的最优点。PGD则是“小步走，多走几步”，如果走出了扰动半径为epsilon的空间，就映射回“球面”上，以保证扰动不要过大</p></blockquote><p>且</p><p>伪代码：</p><ul><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于每个x:  1.计算x的前向loss、反向传播得到梯度并备份  对于每步t:      2.根据embedding矩阵的梯度计算出r，并加到当前embedding上，相当于x+r(超出范围则投影回epsilon内)      3.t不是最后一步: 将梯度归0，根据1的x+r计算前后向并得到梯度      4.t是最后一步: 恢复(1)的梯度，计算最后的x+r并将梯度累加到(1)上  5.将embedding恢复为(1)时的值  6.根据(4)的梯度对参数进行更新</span><br></pre></td></tr></table></figure><p>可以看到，在循环中r是逐渐累加的，要注意的是<strong>最后更新参数只使用最后一个x+r算出来的梯度</strong>。</p><h3 id="FreeAT-Free-Adversarial-Training-NIPS2019"><a href="#FreeAT-Free-Adversarial-Training-NIPS2019" class="headerlink" title="FreeAT (Free Adversarial Training): NIPS2019"></a>FreeAT (Free Adversarial Training): NIPS2019</h3><p>从FGSM到PGD，主要是优化对抗扰动的计算，虽然取得了更好的效果，但计算量也一步步增加。对于每个样本，FGSM和FGM都只用计算两次，一次是计算x的前后向，一次是计算x+r的前后向。而PGD则计算了K+1次，消耗了更多的计算资源。因此FreeAT被提了出来，在PGD的基础上进行训练速度的优化。</p><p>FreeAT的思想是在对每个样本x连续重复m次训练，计算r时复用上一步的梯度，为了保证速度，整体epoch会除以m。r的更新公式为：</p><p>伪代码：</p><ul><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">初始化r=0对于epoch=1...N/m:  对于每个x:      对于每步m:        1.利用上一步的r，计算x+r的前后向，得到梯度        2.根据梯度更新参数        3.根据梯度更新r</span><br></pre></td></tr></table></figure><p>缺点：FreeLB指出，FreeAT的问题在于每次的r对于当前的参数都是次优的（无法最大化loss），因为当前r是由r(t-1)和theta(t-1)计算出来的，是对于theta(t-1)的最优。</p><p>注：</p><p>1.论文中提供伪代码，但源码中好像对1步输入做了归一化论文中并没有提到</p><p>2.个人认为可以把FreeAT当成执行m次的FGSM，最开始r=0，第一次更新的是x的梯度，之后开始迭代更新r，则根据x+r的梯度更新参数。但代码中有个问题是r只在最开始初始化，如果迭代到新的样本x2，也是根据上个样本的r进行更新的，这里我有些疑问，希望懂的大佬赐教下～</p><p>代码：<a href="https://github.com/mahyarnajibi/FreeAdversarialTraining/blob/d70774030871fa3207e09ce8528c1b84cd690603/main_free.py#L160" target="_blank" rel="noopener">https://github.com/mahyarnajibi/FreeAdversarialTraining/blob/d70774030871fa3207e09ce8528c1b84cd690603/main_free.py#L160</a></p><h3 id="YOPO-You-Only-Propagate-Once-NIPS2019"><a href="#YOPO-You-Only-Propagate-Once-NIPS2019" class="headerlink" title="YOPO (You Only Propagate Once): NIPS2019"></a>YOPO (You Only Propagate Once): NIPS2019</h3><p>代码：<a href="https://github.com/a1600012888/YOPO-You-Only-Propagate-Once" target="_blank" rel="noopener">https://github.com/a1600012888/YOPO-You-Only-Propagate-Once</a></p><p>YOPO的目标也是提升PGD的效率，这篇文章需要的理论知识比较雄厚，这里只简要介绍一下。</p><p>感兴趣又啃不下来原论文的同学（比如我）可以参考[9]，如有解读错误欢迎指出～</p><p>极大值原理PMP(Pontryagin’s maximum principle)是optimizer的一种，它将神经网络看作动力学系统。这个方法的优点是在优化网络参数时，层之间是解藕的。通过这个思想，我们可以想到，既然扰动是加在embedding层的，为什么每次还要计算完整的前后向传播呢？</p><p>基于这个想法，作者想复用后几层的梯度，假设p为定值：</p><p>则对r的更新就可以变为</p><p>我们可以先写出YOPO的梯度下降版本：</p><ul><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于每个样本x初始化r(1,0)对于j=1,2,...,m:  1.根据r(j,0),计算p  对于s=0,1,...,n-1:    2.计算r(j,s+1)  3.另r(j+1,0)=r(j,n)</span><br></pre></td></tr></table></figure><p>作者又提出了PMP版本的YOPO，并证明SGD的YOPO是PMP版的一种特殊形式。这样每次迭代r就只用到embedding的梯度就可以了。</p><p>引用[9]:</p><blockquote><p><strong>虽然YOPO-m-n只完成了m次完整的正反向传播，但是却实现了m*n次梯度下降</strong>。而PGD-r算法完成r次完整的正反向传播却只能实现r次梯度下降。这样看来，YOPO-m-n算法的效率明显更高，而实验也表明，<strong>只要使得m*n略大于r，YOPO-m-n的效果就能够与PGD-r相媲美</strong>。</p></blockquote><p>然而故事的反转来的太快，FreeLB指出YOPO使用的假设对于ReLU-based网络不成立：</p><blockquote><p>Interestingly, the analysis backing the extra update steps assumes a twice continuously differentiable loss, which does not hold for ReLU-based neural networks they experimented with, and thus the reasons for the success of such an algorithm remains obscure.</p></blockquote><p>别问了，问就是PMP，来跟我一起进入下一部份的学习。</p><h3 id="FreeLB-Free-Large-Batch-ICLR2020"><a href="#FreeLB-Free-Large-Batch-ICLR2020" class="headerlink" title="FreeLB (Free Large-Batch): ICLR2020"></a>FreeLB (Free Large-Batch): ICLR2020</h3><p>FreeLB认为，FreeAT和YOPO对于获得最优r (inner max)的计算都存在问题，因此提出了一种类似PGD的方法。只不过PGD只使用了最后一步x+r输出的梯度，而FreeLB取了每次迭代r输出梯度的平均值，相当于把输入看作一个K倍大的虚拟batch，由[X+r1, X+r2, …, X+rk]拼接而成。具体的公式为：</p><p>为了方便对比，再贴下论文中PGD的公式：</p><p>FreeLB和PGD主要有两点区别：</p><p>1.PGD是迭代K次r后取最后一次扰动的梯度更新参数，FreeLB是取K次迭代中的平均梯度</p><p>2.PGD的扰动范围都在epsilon内，因为伪代码第3步将梯度归0了，每次投影都会回到以第1步x为圆心，半径是epsilon的圆内，而FreeLB每次的x都会迭代，所以r的范围更加灵活，更可能接近局部最优：</p><p>FreeLB的伪代码为：</p><ul><li></li><li></li><li></li><li></li><li></li><li></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于每个x:  1.通过均匀分布初始化r，梯度g为0  对于每步t=1...K:      2.根据x+r计算前后向，累计梯度g      3.更新r  4.根据g/K更新梯度</span><br></pre></td></tr></table></figure><p>论文中还指出了很重要的一点，就是<strong>对抗训练和dropout不能同时使用</strong>，加上dropout相当于改变了网络结构，会影响r的计算。如果要用的话需要在<strong>K步中都使用同一个mask</strong>。</p><h3 id="SMART-SMoothness-inducing-Adversarial-Regularization"><a href="#SMART-SMoothness-inducing-Adversarial-Regularization" class="headerlink" title="SMART (SMoothness-inducing Adversarial Regularization)"></a>SMART (SMoothness-inducing Adversarial Regularization)</h3><p>SMART论文中提出了两个方法：</p><p>1.对抗正则 SMoothness-inducing Adversarial Regularization，提升模型鲁棒性</p><p>2.优化算法 Bregman proximal point optimization，避免灾难性遗忘</p><p>本文只介绍其中的对抗正则方法。</p><p>SMART提出了两种对抗正则损失，加到损失函数中：</p><p>第一种参考了半监督对抗训练，对抗的目标是最大化扰动前后的输出，在分类任务时loss采用对称的KL散度，回归任务时使用平方损失损失：</p><p>第二种方法来自DeepMind的NIPS2019[8]，核心思想是让模型学习到的流行更光滑，即让loss在训练数据呈线性变化，增强对扰动的抵抗能力。作者认为，如果loss流行足够平滑，那l(x+r)可以用一阶泰勒展开进行近似，因此用来对抗的扰动需要最大化l(x+r)和一阶泰勒展开的距离：</p><p>SMART的算法和PGD相似，也是迭代K步找到最优r，然后更新梯度。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>把最近的一些对抗训练方法总结出来，可以看到趋势从“优化PGD的速度”又回到了“找寻最优扰动”，个人也比较认同，训练速度慢一些对于普通模型还是可以接受的，主要还是看最终的效果有没有提升。之前自己试过FGM和PGD，FGM有轻微提升，但PGD没有，应该需要在超参数上进行调整。FreeLB和SMART在GLUE榜单上都有出现过，相信之后对抗训练也是标配了，坐等微软放出源码。</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> adversarial learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给BERT加一个loss就能稳定提升？斯坦福+Facebook最新力作！</title>
      <link href="/2020/11/23/%E7%BB%99BERT%E5%8A%A0%E4%B8%80%E4%B8%AAloss%E5%B0%B1%E8%83%BD%E7%A8%B3%E5%AE%9A%E6%8F%90%E5%8D%87%EF%BC%9F%E6%96%AF%E5%9D%A6%E7%A6%8F-Facebook%E6%9C%80%E6%96%B0%E5%8A%9B%E4%BD%9C%EF%BC%81/"/>
      <url>/2020/11/23/%E7%BB%99BERT%E5%8A%A0%E4%B8%80%E4%B8%AAloss%E5%B0%B1%E8%83%BD%E7%A8%B3%E5%AE%9A%E6%8F%90%E5%8D%87%EF%BC%9F%E6%96%AF%E5%9D%A6%E7%A6%8F-Facebook%E6%9C%80%E6%96%B0%E5%8A%9B%E4%BD%9C%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文转载自： 李rumor</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5MjEyNDMxOQ==&amp;mid=2247484387&amp;idx=1&amp;sn=7f9eae754af0c3ddf9adce7a895a977e&amp;chksm=9070a613a7072f05a31246a6b048ee3c7d65062d49b14d4145384c17a7553b44b7bf7e7ff828&amp;mpshare=1&amp;scene=24&amp;srcid=1107993ELlp55ob4rmegV26m&amp;sharer_sharetime=1604738440808&amp;sharer_shareid=03b37d2da63d394d243dc8d08e8bee5f&amp;key=ca9d6dcc40175cc992ed8e7bfa4f484ca7b3c118a0572637d8b0b30bc2d229ce920b2ec0e5e9171fdbc7ec12f00a9845e0c543fa2510c3e5329a991a3f26f6492351b9a165569d42982fcaab5d780503114221a20eca6cc7c7e862001fc3b7e04a5e02854b769b2d5f85b5f006535e2a1a1f820649d49832fabb10c5395769fe&amp;ascene=14&amp;uin=MjExODc0NDkwMg%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=AXogFzo0WzlA%2BmWvocfsXHk%3D&amp;pass_ticket=JMu0tZ8lnp%2FRuz1okqXGKHCSZV44b5P1Q0yYYVTQAtVTaM8NrMmb5T6PBnHGy2oL&amp;wx_header=0" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzA5MjEyNDMxOQ==&amp;mid=2247484387&amp;idx=1&amp;sn=7f9eae754af0c3ddf9adce7a895a977e&amp;chksm=9070a613a7072f05a31246a6b048ee3c7d65062d49b14d4145384c17a7553b44b7bf7e7ff828&amp;mpshare=1&amp;scene=24&amp;srcid=1107993ELlp55ob4rmegV26m&amp;sharer_sharetime=1604738440808&amp;sharer_shareid=03b37d2da63d394d243dc8d08e8bee5f&amp;key=ca9d6dcc40175cc992ed8e7bfa4f484ca7b3c118a0572637d8b0b30bc2d229ce920b2ec0e5e9171fdbc7ec12f00a9845e0c543fa2510c3e5329a991a3f26f6492351b9a165569d42982fcaab5d780503114221a20eca6cc7c7e862001fc3b7e04a5e02854b769b2d5f85b5f006535e2a1a1f820649d49832fabb10c5395769fe&amp;ascene=14&amp;uin=MjExODc0NDkwMg%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=AXogFzo0WzlA%2BmWvocfsXHk%3D&amp;pass_ticket=JMu0tZ8lnp%2FRuz1okqXGKHCSZV44b5P1Q0yYYVTQAtVTaM8NrMmb5T6PBnHGy2oL&amp;wx_header=0</a></p></blockquote><p>关注CV领域的小伙伴一定都记得Hinton团队在年初提出的SimCLR[1]，采用自监督的对比学习方法进行encoder的训练，各种碾压之前的模型。所以今年我一直在等某个大招，终于在20年的尾巴看到了一丝希望。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/y3eXggUiaulKxPfjTNakIdlo21VibGTDStFIxemGWIVEJYUo0CjBGCENBjtz5icxShn7huiapJpRvL9qf566fiaghbA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>今天要介绍的这篇工作来自斯坦福和Facebook AI，作者在BERT分类任务的精调阶段加入了对比学习的loss，在各个任务上都获得了很稳定的提升：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/y3eXggUiaulKxPfjTNakIdlo21VibGTDStAW2lmSQUp1HzEzk6EkXiaRZeDRTW7jPIhnuBJcPv3mgRZ3QAnm7FHsg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>上图中CE表示交叉熵，SCL表示Supervised Contrastive Learning。实话说结果并不够惊艳，用对抗学习也差不多可以做到，让我惊喜的是在Few-shot上的效果：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/y3eXggUiaulKxPfjTNakIdlo21VibGTDStS1XJA4g2Rians3ScwrPrJghn4Lbo89FTRkCdmRHs4uDWDgbhdvqnqzQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>N表示训练样本数量。可以看到N=20时QNLI上有10个点之多的提升。</p><p>下面就让我们来走近科学，看看SCL是个啥玩意儿叭～</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">论文题目：Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning论文链接：https://arxiv.org/abs/2011.01403</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h3><p>对比学习的核心思想，就是让模型学习<strong>如何将正样本和其他负样本区别开来，抓住样本的本质特征，而不是把每个细节都考虑到</strong>。拿人来举例，假如有人让你凭空画一张一美元，你可能只画成这样[2]：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/y3eXggUiaulKxPfjTNakIdlo21VibGTDStHx7nG8z5mb77mhRy4BmNYe9s6Rwqz4TcLicmaCR9gpYs8FKibvNkPiaQQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>而如果给你一张美元照着临摹，可能还能画好看点，比如这样：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/y3eXggUiaulKxPfjTNakIdlo21VibGTDStHHnFicc4QXic2IeP04iaMz2c67hx78PVCB7lLyvNmhceKBxqu6GVBBk4Q/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>所以说我们记住的，不一定是像素级别的特征，而是更高维度的。在训练模型时，也不强求它们把所有信息都编码，只要细致到可以区分数据中的不同样本就可以。</p><p>如何实现呢？这个就体现在目标函数上：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/y3eXggUiaulKxPfjTNakIdlo21VibGTDStsXfx4gvJClkpHyxzFW1sSD2RR1qre7UcaAZ6327pOicwRnG7AlAWYOg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>在自监督的情况下，对比学习利用数据增强方法，给每个输入样本输入构建另一个view 作为正例，并使用同batch下其他样本作为负例，达到拉近正例拉开负例的“对比”目的：</p><p>描述得更具体一点，就是把N个输入样本增强到2N个，然后进行2N分类（其中有2个正例2N-2个负例）。</p><p>P.S. 关于对比学习在图像领域的进展可以参考知乎@Tobias Lee的文章[3]。</p><h3 id="Supervised-Contrastive-Learning"><a href="#Supervised-Contrastive-Learning" class="headerlink" title="Supervised Contrastive Learning"></a>Supervised Contrastive Learning</h3><p>上文讲了自监督的对比学习主要是靠一个batch内的样本间相互对比，那有监督的数据如何更好利用呢？</p><p>作者就针对分类任务进行了研究。分类的核心思想就是把不同类别的样本划分开来，通常使用交叉熵作为损失函数。作者则提出了一个新的对比学习loss SCL，<strong>将同一类的样本互相作为正例，不同类别的作为负例</strong>。以此达到拉近类内样本、拉开类间距离的目的：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/y3eXggUiaulKxPfjTNakIdlo21VibGTDStZxubfzuicAGazsUOnuCue8cxQQq3fvj1UH3LickfwWOdZ0TSQia7YgsHg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>具体的损失计算方法为（右滑公示）：</p><p>其中是正确label，是归一化后的encoder输出，是一个控制类间距离的超参数，越低负例就越难分。这个式子的主要目的就是拉近正样本（同类数据）的距离。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>除了开头展示的直接提升外，作者还进行了很多分析。从SST-2数据集的[CLS] embedding来看，通过CE（左）和SCL（右）损失训练出来的encoder对正负例的区分能力确实有不少差距：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/y3eXggUiaulKxPfjTNakIdlo21VibGTDStVorFtpY4PDjG6O53WjSXVyOm8G2yQLYBuBQgMRNMiaAp8cq40G6FlYA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>同时在有噪声的训练数据上SCL鲁棒性会更强（T越高噪声越多）：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/y3eXggUiaulKxPfjTNakIdlo21VibGTDStwbicARdicmQ8VkaibVa0ee9mgyYnHo8mgcQI3ibBKkBW436uhTkINbAGCg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这篇文章目前正在投稿ICLR2021（都在arxiv上挂了还盲审啥。。），总体的改动比较简单，但对比学习的前景还是挺大的，同时加上SCL损失之后不仅对少样本的情况很有帮助，也能提升模型鲁棒性，相比于<a href="https://mp.weixin.qq.com/s?__biz=MzA5MjEyNDMxOQ==&amp;mid=2247483956&amp;idx=1&amp;sn=49d079e4cd785cb4739f660d72468235&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">对抗学习</a>的计算代价明显要小，还是比较实用的，一起立个flag，复现一波？</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/y3eXggUiaulKxPfjTNakIdlo21VibGTDStiagG58ibWjLq0rAfbhtuIbj8aR2Hib3jgKw7rmThcFGmnFWTW9yNlpIqQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> Contrastive Loss </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> 对比学习 </tag>
            
            <tag> SimCLR </tag>
            
            <tag> Feature Space </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用matplotlib绘制柱状图</title>
      <link href="/2020/11/23/%E4%BD%BF%E7%94%A8matplotlib%E7%BB%98%E5%88%B6%E6%9F%B1%E7%8A%B6%E5%9B%BE/"/>
      <url>/2020/11/23/%E4%BD%BF%E7%94%A8matplotlib%E7%BB%98%E5%88%B6%E6%9F%B1%E7%8A%B6%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="基本绘制"><a href="#基本绘制" class="headerlink" title="基本绘制"></a>基本绘制</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line">num_list = [<span class="number">1.5</span>,<span class="number">0.6</span>,<span class="number">7.8</span>,<span class="number">6</span>]</span><br><span class="line">plt.bar(range(len(num_list)), num_list)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><div align="center"> <img src="/images/blog/2020/zhu.png" alt><br><a id="more"></a></div></p><h1 id="设置标签"><a href="#设置标签" class="headerlink" title="设置标签"></a>设置标签</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line">name_list = [<span class="string">'Monday'</span>,<span class="string">'Tuesday'</span>,<span class="string">'Friday'</span>,<span class="string">'Sunday'</span>]</span><br><span class="line">num_list = [<span class="number">1.5</span>,<span class="number">0.6</span>,<span class="number">7.8</span>,<span class="number">6</span>]</span><br><span class="line">plt.bar(range(len(num_list)), num_list,color=<span class="string">'rgb'</span>,tick_label=name_list)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><div align="center"> <img src="/images/blog/2020/zhu1.png" alt></div></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matplotlib </tag>
            
            <tag> 可视化 </tag>
            
            <tag> 柱状图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NYT baseline 笔记</title>
      <link href="/2020/11/21/NYT-baseline-%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/11/21/NYT-baseline-%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Conv1d与Conv2d的区别"><a href="#1-Conv1d与Conv2d的区别" class="headerlink" title="1. Conv1d与Conv2d的区别"></a>1. Conv1d与Conv2d的区别</h1><p>之前写PCNN时都是使用Conv1d实现，但是在阅读别人的源码时发现是用的Conv2d，难道是自己搞错了？</p><h2 id="Conv2D"><a href="#Conv2D" class="headerlink" title="Conv2D"></a>Conv2D</h2><p><img src="https://pic1.zhimg.com/80/v2-10c0d7782638fb59f5746508503e4a58_720w.jpg" alt="img"></p><p>如图，输入为 <img src="https://www.zhihu.com/equation?tex=%287%2C7%2C3%29" alt="[公式]"> 的图片，卷积核的尺寸为 <img src="https://www.zhihu.com/equation?tex=%283%2C3%29" alt="[公式]"> ，卷积核个数为2，所以</p><p>参数总数<img src="https://www.zhihu.com/equation?tex=%3D3%2A3%2A3%2A2" alt="[公式]"> 。</p><p>多出来的3为图片的channel，每个卷积核会自动扩充到3维，对应每个通道。</p><h2 id="Conv1D"><a href="#Conv1D" class="headerlink" title="Conv1D"></a>Conv1D</h2><p><img src="https://pic2.zhimg.com/80/v2-0df2ad2e4d63f8c9da6940293cf8fab1_720w.jpg" alt="img"></p><p>如图，输入有两个序列，第一个序列为 <img src="https://www.zhihu.com/equation?tex=%283%2C3%29" alt="[公式]"> 的文本，卷积核的尺寸为 2，卷积核个数为1，所以</p><p>参数总数<img src="https://www.zhihu.com/equation?tex=%3D3%2A2%2A1" alt="[公式]"> 。</p><p>多出来的3为文本的词向量维度，<strong>卷积核会自动扩充到2维，对应每个隐维度</strong>。</p><p><strong>所以结论是两者是一样的，怎样用都可以，只是写法不同！</strong><a id="more"></a></p><h1 id="2-get-item"><a href="#2-get-item" class="headerlink" title="2. __get_item__()"></a>2. __get_item__()</h1><p>在继承了Dataset的数据集类，需要重写__get_item__()函数，可以传入slice()类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">( self, key )</span> :</span></span><br><span class="line">    <span class="keyword">if</span> isinstance( key, slice ) :</span><br><span class="line">        <span class="comment">#Get the start, stop, and step from the slice</span></span><br><span class="line">        <span class="keyword">return</span> [self[ii] <span class="keyword">for</span> ii <span class="keyword">in</span> xrange(*key.indices(len(self)))]</span><br><span class="line">    <span class="keyword">elif</span> isinstance( key, int ) :</span><br><span class="line">        <span class="keyword">if</span> key &lt; <span class="number">0</span> : <span class="comment">#Handle negative indices</span></span><br><span class="line">            key += len( self )</span><br><span class="line">        <span class="keyword">if</span> key &lt; <span class="number">0</span> <span class="keyword">or</span> key &gt;= len( self ) :</span><br><span class="line">            <span class="keyword">raise</span> IndexError, <span class="string">"The index (%d) is out of range."</span>%key</span><br><span class="line">        <span class="keyword">return</span> self.getData(key) <span class="comment">#Get the data from elsewhere</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> TypeError, <span class="string">"Invalid argument type."</span></span><br></pre></td></tr></table></figure><h1 id="3-pdb-set-trace"><a href="#3-pdb-set-trace" class="headerlink" title="3. pdb.set_trace()"></a>3. pdb.set_trace()</h1><p>pdb.<strong>set_trace</strong>(*,  <em>header=None</em>)</p><p>在调用本函数的堆栈帧处进入调试器。<font color="orange">用于硬编码一个断点到程序中的固定点处</font>，即使该代码不在调试状态（如断言失败时）。如果传入 <em>header</em>，它将在调试开始前被打印到控制台。</p><h1 id="4-PCNN-ONE-实现"><a href="#4-PCNN-ONE-实现" class="headerlink" title="4. PCNN + ONE 实现"></a>4. PCNN + ONE 实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, token2ids, pos1s, pos2s, mask, scopes, rel_labels, is_training=False)</span>:</span></span><br><span class="line">        <span class="string">"""相比于 PCNN 多了参数: scopes, rel_labels, is_training=False"""</span></span><br><span class="line">        x = self.input(token2ids, pos1s, pos2s)</span><br><span class="line">        x = self.convolution(x, mask)</span><br><span class="line">        x = self.piece_maxpool(x, mask)</span><br><span class="line">        x = self.tanh(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="comment"># selector</span></span><br><span class="line">        bag_rep = list()</span><br><span class="line">        <span class="keyword">for</span> ind, scope <span class="keyword">in</span> enumerate(scopes):</span><br><span class="line">            sent_rep = x[scope[<span class="number">0</span>]:scope[<span class="number">1</span>], :]</span><br><span class="line">            out = self.dense(sent_rep)</span><br><span class="line">            probs = torch.softmax(out, dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="keyword">if</span> is_training:</span><br><span class="line">                _, j = torch.max(probs[:, rel_labels[ind]], dim=<span class="number">-1</span>)  <span class="comment"># 选取最大的样本作为 bag 表示</span></span><br><span class="line">                bag_rep.append(out[j])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                row_prob, row_idx = torch.max(probs, dim=<span class="number">-1</span>)</span><br><span class="line">                <span class="keyword">if</span> row_idx.sum() &gt; <span class="number">0</span>:</span><br><span class="line">                    mask = row_idx.view(<span class="number">-1</span>, <span class="number">1</span>).expand(<span class="number">-1</span>, probs.shape[<span class="number">-1</span>])</span><br><span class="line">                    probs = probs.masked_fill_(mask.eq(<span class="number">0</span>), float(<span class="string">'-inf'</span>))</span><br><span class="line">                    row_prob, _ = torch.max(probs[:, <span class="number">1</span>:], dim=<span class="number">-1</span>)</span><br><span class="line">                    _, row_idx = torch.max(row_prob, dim=<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    _, row_idx = torch.max(row_prob, dim=<span class="number">-1</span>)</span><br><span class="line">                bag_rep.append(out[row_idx])</span><br><span class="line"></span><br><span class="line">        bag_rep = torch.stack(bag_rep)</span><br><span class="line">        <span class="keyword">return</span> bag_rep</span><br></pre></td></tr></table></figure><h1 id="5-PCNN-ATT-实现"><a href="#5-PCNN-ATT-实现" class="headerlink" title="5. PCNN + ATT 实现"></a>5. PCNN + ATT 实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, token2ids, pos1s, pos2s, mask, scopes, rel_labels, is_training=False)</span>:</span></span><br><span class="line">    x = self.input(token2ids, pos1s, pos2s)</span><br><span class="line">    x = self.convolution(x, mask)  <span class="comment"># (batch, hidden, seq_len, 1)</span></span><br><span class="line">    x = self.piece_maxpool(x, mask)</span><br><span class="line">    x = self.tanh(x)</span><br><span class="line">    x = self.dropout(x)  <span class="comment"># (batch, 3 * hidden)</span></span><br><span class="line">    <span class="comment"># selector</span></span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">        query = torch.zeros((x.size(<span class="number">0</span>))).long()</span><br><span class="line">        query.to(x.device)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(scopes)):</span><br><span class="line">            query[scopes[i][<span class="number">0</span>]:scopes[i][<span class="number">1</span>]] = rel_labels[i]</span><br><span class="line">        <span class="comment"># (batch, 3 * hidden)</span></span><br><span class="line">        att_mat = self.dense.weight[query]  <span class="comment"># 全连接层起到了 relation query 的作用 #</span></span><br><span class="line">        att_score = (x * att_mat).sum(<span class="number">-1</span>)</span><br><span class="line">        bag_rep = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(scopes)):</span><br><span class="line">            bag_mat = x[scopes[i][<span class="number">0</span>]:scopes[i][<span class="number">1</span>]]</span><br><span class="line">            softmax_att_score = F.softmax(att_score[scopes[i][<span class="number">0</span>]:scopes[i][<span class="number">1</span>]], dim=<span class="number">-1</span>)  <span class="comment"># 对 att_score 做 softmax</span></span><br><span class="line">            bag_rep.append((softmax_att_score.unsqueeze(<span class="number">-1</span>) * bag_mat).sum(<span class="number">0</span>))</span><br><span class="line">        bag_rep = torch.stack(bag_rep, <span class="number">0</span>)</span><br><span class="line">        bag_rep = self.dropout(bag_rep)</span><br><span class="line">        bag_logits = self.dense(bag_rep)  <span class="comment"># 二次用到了 dense.weight 矩阵</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> NYT </tag>
            
            <tag> PCNN </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机AI顶会审稿流程</title>
      <link href="/2020/11/20/%E8%AE%A1%E7%AE%97%E6%9C%BAAI%E9%A1%B6%E4%BC%9A%E5%AE%A1%E7%A8%BF%E6%B5%81%E7%A8%8B/"/>
      <url>/2020/11/20/%E8%AE%A1%E7%AE%97%E6%9C%BAAI%E9%A1%B6%E4%BC%9A%E5%AE%A1%E7%A8%BF%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="1-EMNLP-2020"><a href="#1-EMNLP-2020" class="headerlink" title="1. EMNLP 2020"></a>1. EMNLP 2020</h1><p><strong>NLP可解释性和模型分析主题的投稿数量有显著上升</strong>。这是ACL2020新引入的主题，收到95篇投稿。但在EMNLP投稿数字翻倍了，展现了社区在对NLP可解释性和模型分析这一主题上的兴趣增长更为迅速。</p><h2 id="审稿过程：作者即审稿"><a href="#审稿过程：作者即审稿" class="headerlink" title="审稿过程：作者即审稿"></a><strong>审稿过程：作者即审稿</strong></h2><p>在Yulan He介绍完整体数据之后，本次评审之一，墨尔本大学教授Trevor Cohn介绍了本次会议的投稿过程。</p><p>对投稿格式和审稿格式做了很多改变。<strong>本次评委会有超过3000名成员，评委工作正如往年大会一样，按照分级结构标准开展。</strong> <strong>改变了一件事就是要求所有投稿至少提名一位作者作为审稿人</strong>。随后把他们分入不同研究领域，将更有经验的审稿人编入最终审稿库。</p><p>对每位审稿人也使用他们的语义学学术资料数据来抓取他们的论文以得到他们之前的出版物记录，通过这样的方式能识别出更多的高级审稿人。</p><p><img src="/images/blog/2020/IJCAI/distri.png" alt></p><p>再次给你们展示数据库中的一个想法。上图展示了每个审稿人过去出版物的数量。发现大部分审稿人都有大量论文，他们都是研究的“行家里手”。也发现两侧有一些数据偏离的学者，可能是由于没有语义学学术资料或者可能和其他研究者重名。从这份数据，可以高精度的识别那些有大量论文并且有博士学位的个人。将他们称之为星级审稿人并且确保每一篇论文都被至少一位星级审稿人审阅。</p><p>审稿要求使用的自动化算法与ACL相似。这建立在投稿和审稿人之间的大致相似性上。论文确认之后交给匹配算法做一个受限的优化选择来保证尊重审稿人的负担限制以保证审稿质量。对审稿任务和区域主席分配都这么做并且这些任务被高级区主席团手动调整。总体上认为这项工作完成的很出色因为移除了强制性命令这个额外之举。</p><a id="more"></a><p><strong>本次论坛的其他改动是：</strong></p><p>一是对每篇投稿都包括一份<strong>再现性清单</strong>，评审必须给一个再现性清单分数并且回答反馈问卷这个再现性清单是否有用，其中78%回答有用或者某种程度上有用。根据再现性清单，投稿大体上都提供了更多的信息。ML的再现性挑战，EMNLP正在学习其他AI大会例如NeurIPS、CVPR。</p><p>二是<strong>允许作者提供被拒稿件投稿</strong>，作者要展示被拒理由并且解释你如何改正这些问题。通过此改动，349篇此类投稿论文比其他论文高录取率6%。</p><p>三是今年新增加了道德规范要求在投稿审查流程之中。</p><p>最后是审稿质量，首先是审稿人组织过程中的前期筛选和之后的分类对提高审稿质量都有帮助。也会提供详细的审稿指导，是社区中积极的反馈。当然，这也需要社区成员更多的努力完全的来提高审稿质量。</p><h1 id="2-ACL-2020"><a href="#2-ACL-2020" class="headerlink" title="2. ACL 2020"></a>2. ACL 2020</h1><h2 id="审稿表单"><a href="#审稿表单" class="headerlink" title="审稿表单"></a>审稿表单</h2><p>我们采用了EMNLP-IJCNLP 2019以及ACL 2019的审稿表单并作出了一些修改。审稿表单将由三部分组成。</p><p><strong>1、深度审稿意见</strong>：这个部分将由你来给出对于论文的综合评价并且提供作出这些评价的依据，其中包括6个小节。</p><ul><li><p><strong>核心审稿意见</strong>：<strong>这是最重要的一部分。</strong>需要给出你对于这篇论文贡献点的看法以及达成这些贡献点的完成程度。用你的观点来描述这篇论文最显著的优点和缺点。可以是两个段落（或者更长）的论述或者是一个列表。需要说明该工作是如何推进计算语言学现有的发现，并且/或者强调该工作为什么没能做出足够的贡献。</p></li><li><p><strong>接收理由</strong>：简要地从核心审稿意见中归纳出这篇论文被大会接受的理由，并且ACL社区成员会从中得到什么样的益处。你可以参考自己的审稿意见以提供更多背景和详细信息。</p></li><li><p><strong>拒绝理由</strong>：简要地归纳出如果这篇论文以现在这种形式发表将会带来什么潜在的风险或者害处。给出为了提高知识水平应该有哪些方面的改进。</p></li><li><p><strong>总体推荐分数</strong>：这里我们会要求你综合以上观点并给出你对论文的最终评价。</p></li><li><ul><li>与EMNLP 2019相同，<strong>我们将使用“带半分间隔的5分制”来打分</strong>。审稿表单里提供了每个分数段的说明。这些数字只是用简洁的方法来表达总体观点以及上述评审内容的重要性。</li><li><strong>今年与以往不同的是我们移除了3分（模棱两可）</strong>，因为我们希望审稿人能够给出对于论文的偏向性：边界线之上还是边界线之下。</li><li>最终的录用决定不仅仅依靠这些分数并且也不会用平均分来决定，是通过考虑所有审稿意见、审稿人讨论、领域主席的综合意见（meta-review）以及推荐程度。然而，将你的审稿意见与上述因素对齐是非常有必要的，这样作者才能够理解最终录用结论是怎么给出的。</li></ul></li><li><p><strong>审稿人置信度</strong>：这一部分是用来告知程序委员会和作者你对于这篇论文的评审有多大的把握，考虑你个人的专业程度、对该领域的熟悉程度以及论文的内容。</p></li><li><p><strong>作者论辩</strong>：ACL 2020会提供作者论辩环节。所以检查作者的论辩内容是否解答了你的问题以及消除了误解是非常重要的。这可能会影响到你的最终推荐分数以及核心审稿意见。如果解答了你的疑惑，请更新你的审稿意见以及推荐分数（并且在审稿意见中标明你的新结论，以便领域主席及时发现）。</p></li></ul><p><strong>2、对作者的提问以及其他反馈</strong>：因为我们有作者论辩环节，你可以向作者提出一些提问，以便他们能够答疑解惑。这里也可以写一些提升最终论文（或未来版本的论文）质量的其他建议。</p><p><strong>3、保密信息</strong>：在这一部分的内容不会被作者看到。这里我们会让你推荐该论文的展示形式（口头报告或者张贴海报）、奖项推荐、学术道德顾虑以及其他要发送给领域主席或者程序委员会主席的保密信息。</p><h2 id="ACL通用审稿准则"><a href="#ACL通用审稿准则" class="headerlink" title="ACL通用审稿准则"></a>ACL通用审稿准则</h2><p>请在审稿的时候采取平衡的方法。一方面我们希望能够通过录用一些描述完整工作的高质量的论文来形成一个可靠的大会程序，另外一方面我们也希望能拥有一个内容广泛而又有趣的大会程序。所以在审查分配给你的论文的时候，请用开放的心态去评价和推荐它们。</p><p>同时要注意，短文并不是一个“缩短的”长文。短文需要能够通过较少的篇幅来表达观点并作出聚焦的贡献。</p><p>参考ACL 2019大会，我们推荐审稿人阅读一些有经验的审稿人以及大会组织者撰写的相关指南。尤其要推荐：</p><ul><li>NLP专家们（Mirella Lapata, Marco Baroni, Yoav Artzi, Emily Bender, Joel Tetreault, Ani Nenkova, and Tim Baldwin）撰写的优秀博客，曾经被用于ACL 2017审稿推荐：<a href="https://acl2017.wordpress.com/2017/02/23/last-minute-reviewing-advice/" target="_blank" rel="noopener">https://acl2017.wordpress.com/2017/02/23/last-minute-reviewing-advice/</a></li><li>马里兰大学的Nikas Elmqvist撰写的在审稿时避免犯的问题：<a href="https://sites.umiacs.umd.edu/elm/2016/02/01/mistakes-reviewers-make/" target="_blank" rel="noopener">https://sites.umiacs.umd.edu/elm/2016/02/01/mistakes-reviewers-make/</a></li><li>NAACL 2018程序委员会主席提供的审稿样例（注意：与我们的审稿模板不太一样）：<a href="https://naacl2018.wordpress.com/2018/01/20/a-review-form-faq/" target="_blank" rel="noopener">https://naacl2018.wordpress.com/2018/01/20/a-review-form-faq/</a></li><li>这里还有一篇短建议：<a href="https://naacl2018.wordpress.com/2017/12/19/some-holiday-reviewing-advice/" target="_blank" rel="noopener">https://naacl2018.wordpress.com/2017/12/19/some-holiday-reviewing-advice/</a> </li></ul><h2 id="辅助材料"><a href="#辅助材料" class="headerlink" title="辅助材料"></a>辅助材料</h2><p>辅助材料是单独上传的一份额外的材料。辅助材料顾名思义显然是起到辅助作用的，并且你没有义务要审查这些文件。你可以把它当做像其他引用文献一样，辅助你更好地了解背景或者超出论文内容之外的一些细节信息。</p><h2 id="第二审稿人"><a href="#第二审稿人" class="headerlink" title="第二审稿人"></a>第二审稿人</h2><p>与大多数以往的NLP会议一样，你可以向他人寻求帮助。但是在撰写最终评论并且给出最终推荐分数的时候，我们希望你能够吸收第二审稿人的审稿意见，并且用你自己的语言重新撰写审稿意见以及调整相应的分数。也就是说，最终的审稿意见要反应出你对该论文的意见，并且需要你能够证明在最终审稿意见中的观点是合理的。</p><h2 id="投稿论文格式"><a href="#投稿论文格式" class="headerlink" title="投稿论文格式"></a>投稿论文格式</h2><p>程序委员会主席以及领域主席已经移除了那些违反格式要求的论文。所以你不需要担心会被分配到这些有格式问题的论文。</p><h2 id="重要日期"><a href="#重要日期" class="headerlink" title="重要日期"></a>重要日期</h2><p>所有审稿意见必须在2020年2月7日星期五（地球上任何地方的午夜时间）返回。<font color="orange"><strong>请注意，在作者论辩环节结束后，审稿人将在2月18日-2月24日期间参与讨论环节</strong></font>。你的责任重大，所以不要将审稿任务放到最后一刻才完成！</p><ul><li>1月17日-2月7日：审稿时间</li><li>2月12日-2月17日：作者论辩环节</li><li>2月18日-2月24日：审稿人讨论时间（领域主席发起讨论）</li></ul><h1 id="3-IJCAI-2021"><a href="#3-IJCAI-2021" class="headerlink" title="3. IJCAI 2021"></a>3. IJCAI 2021</h1><p><img src="/images/blog/2020/IJCAI/chair.png" alt></p><h2 id="审稿架构"><a href="#审稿架构" class="headerlink" title="审稿架构"></a>审稿架构</h2><font color="blue">What are the different roles of the program committee?</font><p><strong>PCs (Program Committee members):</strong> participate in only full-paper review phase: write reviews and participate in discussions.<br><strong>SPCs (Senior Program Committee members):</strong> participate in both summary-reject and full-paper review phases. In summary-reject phase: fill in checkbox form and (optional) write brief reviews; in full-paper review phase: write reviews and participate discussions.<br><strong>ACs (Area Chairs):</strong> participate both summary-reject and full-paper review phase. In summary-reject phase: fill in checkbox form and (optional) write brief reviews; in full-paper review phase: <font color="purple">Coordinate reviewers, write meta-reviews and make recommendations.</font><br><strong>SACs (Senior Area Chairs)</strong>: A very small group of leading experts who help cross-check the reviews, meta-reviews and recommendations of ACs.<br><strong>(A)PCC: Program Chair and Associate Program Chairs:</strong> IJCAI has a single Program Chair governing the whole program of the conference. 3 Associate Program Chairs with different expertise are appointed to help the Program Chair to cover the whole AI field.</p><p>从上往下是层级逐渐增高的~</p><p>IJCAI 2021 的 Program Chair 即 PCC 是周志华老师 (Nanjing University)</p><h2 id="保密性"><a href="#保密性" class="headerlink" title="保密性"></a>保密性</h2><font color="blue">Who sees whom?</font><p>IJCAI-21 review process will be triple blind at the level of Area Chairs (AC), Senior Program Committee members (SPC), and Program Committee members (PC), i.e., AC/SPC/PC cannot see the identities of authors and other AC/SPC/PCs, and vice versa. Author identities are also invisible to Senior Area Chairs (SACs) and vice versa.<br>During the discussion phase, AC/SPC/PCs should avoid disclosing your own identity. You can use CMT number to call other reviewers such as Meta-Reviewer1, Reviewer2, etc. To compensate for information of the hidden reviewer names, you can see the information of “how many years have you worked in Artificial Intelligence?” of other reviewers handling the same submission.</p><h2 id="IJCAI-2020-的其他改革"><a href="#IJCAI-2020-的其他改革" class="headerlink" title="IJCAI 2020 的其他改革"></a>IJCAI 2020 的其他改革</h2><ul><li><p>首先，在 IJCAI 2020 的「Call for Papers」规定中就明确表示，被 AAAI 2020、ECAI 2020、AAMAS 2020、ICCV 2020 或 ICAPS 2020 拒绝的论文在经过实质性改进之后可以重新提交给 IJCAI-PRICAI 2020，但<strong>作者必须（must）声明这是拒稿重投</strong>，而且要附上说明信。说明信要解释被拒的主要原因以及作者针对评审意见所做的改进。这份说明信要连同之前的评审意见和被拒的稿件一起提交。</p></li><li><p>而在 rebuttal 阶段，<strong>IJCAI 2020 的新审稿机制禁止作者回答问题、做出附加解释，也不能追加实验结果。</strong>这意味着审稿人会将所有论文的初次提交版本当做最终版本看待，从而让人们对于论文优劣的判断聚焦于创新点之上。</p></li><li><p>此外，IJCAI 2020 也更新了关于单人投稿数量的上限，<strong>每位作者的投稿数不能超过 6 篇</strong>，而去年的规定是不超过 10 篇。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> AI </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> CV </tag>
            
            <tag> Data Mining </tag>
            
            <tag> ML </tag>
            
            <tag> DM </tag>
            
            <tag> Artificial Intelligence </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> Natural Language Processing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR曲线的绘制</title>
      <link href="/2020/11/20/PR%E6%9B%B2%E7%BA%BF%E7%9A%84%E7%BB%98%E5%88%B6/"/>
      <url>/2020/11/20/PR%E6%9B%B2%E7%BA%BF%E7%9A%84%E7%BB%98%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="1-二分类"><a href="#1-二分类" class="headerlink" title="1. 二分类"></a>1. 二分类</h1><p>P-R曲线的生成方法：<strong>根据学习器的预测结果对样本进行排序，排在前面的是学习器认为最可能是正例的样本，排在最后的是最不可能是正例的样本，按此顺序逐个将样本作为正例预测，则每次可以计算出当前的查全率、查准率，以查全率为横轴、查准率为纵轴做图，得到的查准率-查全率曲线即为P-R曲线</strong>。</p><p>也就是说对每个样本预测其为正例的概率，然后将所有样本按预测的概率进行排序，然后依次将排序后的样本做为正例进行预测，从而得到每次预测的查全率与查准率。这个依次将样本做为正例的过程实际上就是逐步降低样本为正例的概率的域值，通过降低域值，更多的样本会被预测为正例，从而会提高查全率，相对的查准率可能降低，而随着后面负样本的增加，查全率提高缓慢甚至没有提升，精度降低会更快。</p><p>sklearn的计算过程与定义相反是按概率从小到大递增的顺序来计算查准率与查全率的，并且分别为查准率和查全率添加了1和0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.fixes <span class="keyword">import</span> signature</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="string">"P-R Curve"</span>)</span><br><span class="line">plt.title(<span class="string">'Precision/Recall Curve'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Precision'</span>)</span><br><span class="line"><span class="comment">#y_true为样本实际的类别，y_scores为样本为正例的概率</span></span><br><span class="line">y_true = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">y_scores = np.array([<span class="number">0.9</span>, <span class="number">0.75</span>, <span class="number">0.86</span>, <span class="number">0.47</span>, <span class="number">0.55</span>, <span class="number">0.56</span>, <span class="number">0.74</span>, <span class="number">0.62</span>, <span class="number">0.5</span>, <span class="number">0.86</span>, <span class="number">0.8</span>, <span class="number">0.47</span>, <span class="number">0.44</span>, <span class="number">0.67</span>, <span class="number">0.43</span>, <span class="number">0.4</span>, <span class="number">0.52</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.1</span>])</span><br><span class="line">precision, recall, thresholds = precision_recall_curve(y_true, y_scores)</span><br><span class="line"><span class="comment">#print(precision)</span></span><br><span class="line"><span class="comment">#print(recall)</span></span><br><span class="line"><span class="comment">#print(thresholds)</span></span><br><span class="line">plt.plot(recall,precision)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>但是这个只支持二分类：</p><p><img src="/images/blog/2020/pr.png" alt></p><a id="more"></a><h1 id="2-多分类"><a href="#2-多分类" class="headerlink" title="2. 多分类"></a>2. 多分类</h1><p>查中文的查了好久都没有找到，😓，还是英文内容多~</p><p>不再写了，直接看下面几个内容就够了：</p><blockquote><p>中文：</p><p><a href="https://blog.csdn.net/YE1215172385/article/details/79443552" target="_blank" rel="noopener">https://blog.csdn.net/YE1215172385/article/details/79443552</a></p><p>英文：</p><p><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html" target="_blank" rel="noopener">https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html</a></p><p><a href="https://www.scikit-yb.org/en/latest/api/classifier/prcurve.html" target="_blank" rel="noopener">https://www.scikit-yb.org/en/latest/api/classifier/prcurve.html</a></p></blockquote><p>sklearn🐮🍺！！！</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PR-Curve </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> sklearn </tag>
            
            <tag> 评估方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>陈奕迅给唱歌爱好者的十条建议</title>
      <link href="/2020/11/20/%E9%99%88%E5%A5%95%E8%BF%85%E7%BB%99%E5%94%B1%E6%AD%8C%E7%88%B1%E5%A5%BD%E8%80%85%E7%9A%84%E5%8D%81%E6%9D%A1%E5%BB%BA%E8%AE%AE/"/>
      <url>/2020/11/20/%E9%99%88%E5%A5%95%E8%BF%85%E7%BB%99%E5%94%B1%E6%AD%8C%E7%88%B1%E5%A5%BD%E8%80%85%E7%9A%84%E5%8D%81%E6%9D%A1%E5%BB%BA%E8%AE%AE/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文转载自耳帝：<a href="https://mp.weixin.qq.com/s/hjkXYs7YQBw9LoafUa-C-A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/hjkXYs7YQBw9LoafUa-C-A</a></p></blockquote><p>我是陈奕迅的歌迷，在我看来，他是一个几乎没有短板的歌手，也是我认为当今最会唱的华语流行男歌手。</p><p>首先他有一个天然“昂贵”的音色，这点跟唱功与技巧都无关，是一个人内在人格与精神格调的反映；</p><p>二是他有出色的语感与乐感，不仅演唱华语听众最喜欢的芭乐抒情曲风是一流好手，非常会表达，同时因为有极佳的律动感与节奏感，唱各种现代律动型曲风、复古风格的舞曲音乐，以及乐队化的Jam型音乐也相当出色，因为即兴能力也很出众，不像很多传统的“声乐唱功型”歌手，唱传统曲风挺好，唱现代点的曲风或是欧美化的曲风，会感觉过时又死板；</p><p>三是他有独到的声乐技巧，他的声乐技巧不是如今网络上流行讨论的那一套声乐标准，他的声乐风格是柔顺、通畅、丰满、无痕，听似简单，换个人一唱就知道不简单，即使换的那个人是公认的唱将；</p><p>四是他的演唱有“厚度”，这个厚度不是指音色的浑厚，而是有内在的支撑，比如说千禧年的一些青春时代的歌手，唱年轻化气质的音乐倒是不错，但偶尔在大型颁奖典礼上致敬一些民歌时代的经典或者有文化底蕴的老歌，就感觉像是小孩在唱大人的歌，而陈奕迅，从出道时就开始唱港乐两大代表性词人所写的深刻晦涩的词，有大量超出年轻个人体验与生活阅历的，关于时代、情感与人生体会的主题，从一开始演唱的“厚度”就在；</p><p>五是他有非常强烈的身体自由，这点我去年经常说到，身体不自由的歌手唱歌必然存在问题，而这点不仅是呈现在他如“大娱乐家”一般的舞台表演中，而且会常出现在他各种访谈的肢体语言中。</p><p>另外还有一些我个人的观察，他是一个很“通”的人，单纯又通透，关于音乐与人生都有很多自己的想法，但可能局限在语言表达与普通话上，这点不容易被内地观众所感受到，今天看到抖音宣布陈奕迅入驻了抖音，想必近期会有一些大动作，也更会在短视频时代让更多内地观众深入了解他，我收集了一些多年来他在各种访谈中谈论到的，关于唱歌与音乐上的建议与体会，想必对于年轻歌手会很受用。</p><a id="more"></a><p><strong>1、律动（Groove）很重要</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/5C1rMEYel1HfRFicsiciaQ3lviaxQUgaEZicY3OpJE3O0tRvCIxTUx58Bx4sVwicUNMsmCLmHJag5n1HRpprycvwjOqA/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="img"></p><p>虽然陈奕迅最知名的歌都是抒情歌曲居多，毕竟华语听众更注重旋律与歌词，但是其实他本人偏爱的，多是一些节奏鲜明的律动型曲风，在《中国新歌声》做导师时，节目给他的头衔是“情歌天王”，他明显很不喜欢这个称号，觉得那类歌不能让他产生本能的兴奋，而他喜欢的音乐要有强烈的Groove，了解他的都知道，陈奕迅很喜欢Funk/Disco/Soul曲风的音乐，一是这样的音乐更加乐队化，更注重编曲上的音乐性，二是这样的歌曲现场表演起来更为酣畅淋漓、自由随性，在很多访谈中，他经常会示范性地调侃自己的一些抒情芭乐很无趣，比如《婚礼的祝福》这首歌，通常不幸就成了靶子。</p><p>而律动的重要性不仅体现在音乐与编曲上，对于唱歌来说也非常重要，它会体现在唱歌的flow中，不仅包含律动感，还有唱歌时音节与句子的时值、停顿、语气、字句与旋律安排的流动，很多人以为抒情歌不需要flow，其实抒情歌更需要，因为要找到抒情慢歌的flow更难，而这类歌唱得好的歌手，比如陈奕迅，孙燕姿，莫文蔚等，都非常了解flow的重要性。我后来会感觉到，如今网络上人们讨论歌手的唱功时，对声乐的重视过高了，而非常忽视律动的重要性，大陆有很多声乐能力很强但是flow很差的歌手，无论音质再好，声量再大，听起来都很死板，而如果flow很好，则能让档次明显提升一两个台阶。如果你是一位歌手，声乐、语感、情感都不错，但演唱总是给人感觉缺乏灵动感与流动感，也许需要去了解律动的重要。</p><p><strong>2、唱歌不必要太用力，你需要融入音乐，而不是在跟音乐打架</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5C1rMEYel1HfRFicsiciaQ3lviaxQUgaEZicYappyu7W608d9geiaC4tUGa2aZoxYt8nuxBo5DzdP3qKmthtJbFAsRXw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>这是陈奕迅在采访中多次提到的观点，唱歌太用力，会丧失歌曲美感。我认为唱歌用力是唱歌选秀时代兴起的一种风潮，以《中国好声音》为代表，草根选秀非常需要表现自己，过度呈现能力，“高、猛、强”最容易直观体现，Eason多次说过，“你是在表达作品，而不是在跟音乐比高低”。</p><p>Eason说，唱歌要用看似轻松的方式来严肃地表达事物，歌手随着年龄，机能会退化，但若学会用内化的情感来处理音乐，哪怕机能退化，但是唱歌的表达却会生长。他说的这点很符合一些欧美diva们的状态，你会发现，欧美有很多天后，到了一定年龄倒嗓了，高音飙不上去，然而这种状况逼迫了歌手从机能、感官刺激，被迫转移向更丰富且细微的乐性表达，唱得四两拨千斤，开始有了大师风范。在采访中，Eason示范过早年与如今的两种演唱《夕阳无限好》的方式，明显后者更轻、更弱、更柔，但包含的情感与意蕴却更丰富，他有一个有趣的比喻，说刀子猛然地一刀插进去，和刀子慢慢地扭转进去，痛与感受的过程都被拉长，后者才更能命中歌曲的感情。</p><p><strong>3、现场的珍贵，在于犯错</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5C1rMEYel1HfRFicsiciaQ3lviaxQUgaEZicYEvZYqP5pFqIKdI0vhrzdcNVThjXV7ndl69Yjgb6gDQ36nOwg8G5sicQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>我想，这句话表达的意思，是说现场珍贵在于它的不可复制性，即使是犯错。表演者不要拿重复的、不变的、四平八稳的东西来现场表演，现场表演要有新鲜的不一样的东西，为何人们喜欢看真正的现场表演，假唱即使完美也感觉缺乏生机，歌德说，现场表演就是在舞台上走钢索，因为现场需要一种意外与未知，在到达那种未知之前，你究竟是成功还是失败，是否是按照我脑中预想的路径来走，就像进球前的一刹那给人的紧张与期待，这是现场的魅力，即使抵达的那一刻是失败的，但是它先前所调动起的情绪与刺激却已然发生，所以不要害怕犯错。</p><p>同时现场还有即兴的魅力，以及同一首歌的不同处理方式，Eason说唱现场，每一首都要追求不一样的东西，一成不变是枯燥乏味的，而一首歌根据当下的状态与心情、观众的期待与反应的不同，演绎出来的感觉必然不同，即使有错误也是有生机的错误，冒险、即兴、错误与意外，都好过味同嚼蜡的安全。</p><p><strong>4、不要做谁的接班人，歌手只需做自己</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/5C1rMEYel1HfRFicsiciaQ3lviaxQUgaEZicY9uoHT0Sic5EP5bZFWW5GbDaVcXvibHBxu4VDJxJE8ysoAs6ia2cxYicPNA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>歌迷想必都知道，Eason在九十年代末出道时，是被唱片公司当作“张学友的接班人”来打造的，当时的唱腔就有刻意往张学友靠拢的痕迹，对此Eason自己其实很疑惑，他说“我当时心想，张学友不是还在上班吗，我怎么接他的班”，在那个时候，他看所欣赏的伍佰、张震岳都各自有自己的风格，并且市场反响也很好，为什么自己就只能去走他人的路呢？</p><p>事实上，走“张学友接班人”路线的陈奕迅发展确实不顺利，他回头过来看，说那时卖不好是当然的，我都不是在做我自己。做谁的接班人，无非是用唱片行业既有的经验、现成的模式冲着销量、名气、金钱去，这本身的动机就非常功利，有次看窦文涛说的一句话，他说，在他的人生经验里，总有一种很邪的感觉，那就是你直接冲着什么去的时候，那东西反而会离你很远。比如说你某段时间里直接冲着挣钱去，那么那段时间肯定挣不到什么钱，反而是专注投入于自己真正喜欢的事情时，钱反而是一个水到渠成的事成。这点我完全理解，当你冲着什么去的时候，是偏离了自己真正安身立命的领域，心态会失衡、判断会错误、预知会偏离，同时丧失了本能的最珍贵的直觉与感知，自然不会有好的结果。</p><p><strong>5、唱歌需要投入感与集中力，歌曲的表达会随着人生阅历而丰满</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5C1rMEYel1HfRFicsiciaQ3lviaxQUgaEZicY90EMnP2LTTTWODn4jxby0MdF2jHHibXYRUXSK20wp2vwan9BPKAlBSQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>在陈奕迅二十岁出头的时候，唱了很多由港乐经典词人写得深刻晦涩、充满象征意义且多年后再看犹如时代谶语的歌，这些词是词作者的人生阅历与世情感知，而不是二十岁年轻人的人生经验，比如《时代曲》的“别人话迟极了，愿时代仍为我留了座”，《黄金时代》的“黄金广场内分手，在时代门外再聚”，《反高潮》的“灵与欲已升级了，怎可再跌落寂寥”，《我什么都没有》的“命途无奇不有曾爱惜的总要放手，难接手的又来等候”，《我的快乐时代》“长路漫漫是如何走过，宁愿让乐极忘形的我，离时代远远，没人间烟火”等等。</p><p>后来Eason也说过，那个时候他未必能领会这些歌的意思，只能用自己全然的投入理解与直觉先唱。幸好他有出众的天赋，在青涩的年纪与片面的领会中，也将这些歌完成得不偏离主旨，然而我认为最重要的就是他所说的全然的投入，词人是从俯视的角度来写这些关于时代的词，而二十岁出头的陈奕迅则是完全把自己作为时代的参与者投入其中，用一个体微缩而全然的侧面来映现出整个时代的一角，也形成了另一种诠释，而这些歌，他在十年后、二十年后再唱，有了经历与体会，有更广阔的画面的铺开与意蕴的加深，一步步地丰满了表达，如此对照来听，更是相映成趣，意味深长。</p><p><strong>6、不要排斥大众与商业化，同时保持自己的艺术化追求</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/5C1rMEYel1HfRFicsiciaQ3lviaxQUgaEZicY4J7qqTLl6sE0LGGdGjL5r01bRibsHjIPU90q2J3K0eiaOcYFfyKQHYkg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>陈奕迅曾经表达过，有很多歌曲的国语版本，最开始是被自己所反感的，他认为那是刻意要去迎合一个市场，比如说《十年》，公司因《明年今日》在粤语市场大获成功之提出要做国语版本，起初陈奕迅很排斥，觉得有种卖二手货的感觉，然而在公司的劝说之下，还是演唱了国语版本《十年》，没想到在内地大获成功，也真正地打开了他的内地市场。</p><p>后来他到内地开演唱会，在台上第一次演唱《十年》的时候，台下全场大合唱，突然令他很受触动，他开始理解到，旋律与文字、音乐与生活、歌曲与平凡个体之间，有着奇妙的化学反应，一首歌可能你自己并不重视，但你不知道它会在什么时候又与谁的生活当下一刻形成了共鸣，一位歌手拥有着无数人如此的共鸣，那也是一种幸运。像前不久，李玟和常石磊在《我们的歌》中临时合唱了李玟早年的《每一次想你》，很令人感动，这首歌李玟本人曾经并不喜欢，但常石磊说曾在他的记忆里有着重要的位置，音乐就是如此奇妙，你并不知道，你的一首歌会在谁的青春与生活里留下深刻的印记，又无意中影响了谁，千万人聚合在一起，那样的意义是难以估量的。</p><p>所以Eason说，能受到大众化与商业化的认可，是你的幸运，但是一张专辑，也必定需要有自己真正喜欢的音乐，他认为十首里面有三四首，对于一个大众化商业成功的歌手来说，已经足够。</p><p>前几天他参加了某节目的“一秒钟前奏挑战”，即放一首歌的前奏一秒钟，然后说出歌名，这个挑战难度很大，里面有他大量的二十年前的冷门歌曲，有的甚至连很多粉丝都没听过，但他可以全部答对，而且很多连发行日期监制是谁都记得一清二楚，一秒钟的前奏，无非就是一个和弦、一个单音、或者一声鼓，能精准地记得是哪首歌，说明是对自己的作品的编曲每一个部分都了如指掌，都投入了极大的心力，果然如他自己所说的，他爱自己的音乐。</p><p><strong>7、不要强调自己背后有多努力心酸，观众没有必要理解你做了什么</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5C1rMEYel1HfRFicsiciaQ3lviaxQUgaEZicY8klA2PTaXYCtFFrrict0INJ1bmjpELzRfSJxcS5o8icsPEFstkEB7CEQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>“努力人设”是娱乐圈近些年来比较流行的一种打造，尤其是在偶像明星与流量明星身上最为常见，这背后有一种隐秘的迎合群众心里的根源，大众厌恶不劳而获，尤其是对于所得远远超出平凡百姓的公众人物来说，与大众之间有着天然的阶层鸿沟，很容易引发人们在日常生活与工作中那些不公平竞争的焦虑感，这其实很正常，所以“努力人设”就是为了取巧地化解这种矛盾而形成的，尤其是对于没有作品的艺人来说，努力人设是在告诉人们，我的所得其实没有那么容易，当你表现得越不容易的时候，群众的负面情绪就多了一份抵消（本来这种抵消是要靠作品的）。然而Eason说得很对，大家买票付了钱进来，要的是一个最终的成果，至于你受了多大委屈背后有多少努力，这不在观众需要理解的范围内，强调自己背后的努力与心酸，不会给你的作品艺术有所加成，只能给粉丝增加一种苦情感的粘合，以及让自己少一些因“才不配位”而引发的负面舆论声讨，人活于世，没有谁的生活是容易的，做哪一行都是艰难的，每个人都有自己的苦衷。Eason说，你真正需要做的是，在台上全力投入，下了台后，考虑自己该如何进取。</p><p><strong>8、要有幽默感</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/5C1rMEYel1HfRFicsiciaQ3lviaxQUgaEZicYNWU9bvhib1n9Xib26otYOUqZiaOvI8dgGR80fyS4AA8ibCpasxORKJhorQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>虽然幽默感与唱歌没有直接的关联，然而任何行业，具有幽默感都是很好的一种品质，幽默感不是卖弄与刻意抖机灵，而是一种自然的流露，Eason说，幽默感使人长命，这世上需要多一些的幽默感，人的脑袋很复杂，你活得越久，阅历越多，想的事情就会越多，这时候保持有趣与单纯的童心就非常重要。你经常会看到他露出很多鬼马的表情以及有趣的妙语，同时伴随着夸张的肢体与解放的身体语言，幽默感其实是一种对生活的热情与乐观、对人与事的善意、敏捷的思维反应、能体察到事物之间特别的关联、以及放松且自在的精神状态的集合，这些特质不仅在很多行业中需要具备，而且对待生活本身就该如此。</p><p><strong>9、歌手不可能永远处于时代的巅峰，唯有唱好歌才是你的本分</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/5C1rMEYel1HfRFicsiciaQ3lviaxQUgaEZicY8Dssl5jY19cb12U8KItS1D7ib05rZPjDFDzwBYhCv7icEf28Kkr6bzEQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>近两年对于音乐行业冲击最大的新生事物想必就是抖音了，在抖音之前的几年是音乐综艺节目（2012～2017），音综给了观众一种强调唱功技术、需要感官冲击的、自证能力的、以及复杂化改编呈现的审美习惯，而抖音——在今年金曲奖颁奖典礼上陈珊妮的发言中就特别提到，它最大的改变，就是大众对于一首歌“十几秒钟决定生死”的审美习惯。</p><p>在如何面对短视频时代如此汹涌的浪潮，我想无论是老歌手还是新人都很关注，不被时代给淘汰，或者是赶上这趟飞速行驶的列车，目力所及，它带来了很多从业者的焦虑，我也常听到有歌手有诸如此类疑问，常见的有以下几种，“创作者是否应该主动增加作品的通俗性，创作更符合短视频审美与传播特性的音乐”、“在十几秒定生死的时代，还是否有必要投入全部心力做完整专辑”、“短视频音乐更新换代如此之快，究竟什么样的歌才能留下真正成为经典”“传统歌手如何在这个时代让新世代们喜欢”、“音乐行业的门槛越来越低的时候，如何才能知道自己适不适合成为歌手”。</p><p>我想这些问题，上述的几点，陈奕迅都作为一个在华语乐坛屹立二十年不倒的前辈给出了自己的答案，恰好今日抖音宣布陈奕迅已正式入驻了抖音，我觉得在上述的很多建议中，很多贴合这个时代的趋势，比如说“律动很重要”，尽管对于Eason来说更多的是音乐性上的考虑，但是如今时代的音乐律动的重要性越来越明显，在抖音上它也作为了节奏卡点与跳舞风潮的一面而被强烈体现；比如“大胆地犯错”，优点与缺点同样鲜明，是草根从短视频里脱颖而出的最凸显的特征，有人爱有人恨，这个时代人们不要完美无缺的事物，需要生命力最为鲜活生动的人与事；比如“不走他人的路，要做自己”，在新世代开始轰轰烈烈地登上舞台时，个性与强烈自我的重要性会愈发凸显，一定得有自己的鲜明辨识才能在这无边无际的短视频汪洋中脱颖而出。</p><p>同时，Eason还曾讲到的这些点也更为重要，我认为无论是处在唱片时代、音综时代、还是短视频音乐时代，这些话都同样受用。他说“不要用功利之心去做音乐，只想出名不是一个长远计划”，如今有太多人想赶上这一波短视频红利，流量至上，然后很多人得到的只是昙花一现的喧嚣，有的时候，时代的加速未必与个体的进程一致，就像陈奕迅在九十年代走张学友路线出道，被认为是“辉煌的时代，碰上了失败的个人际遇”，反而在港乐没落的时候，他才称为被寄予了拯救香港乐坛的厚望。同时他也认为，不要问自己是否适合做歌手，真正要做歌手的人其实不会这么问，那对他来说是一种天然的驱使，除此外他做不了任何事，他别无选择。Eason说对于刚入行的新人来说，不要怕吃亏，用最诚恳的心来表现自己就好，而对于老歌手来说，若不选择主动顺应时代，那么时代不会顺应你，这看个人的选择，如果有一天，当你感觉已经做尽了自己的所能，乐坛还是不能再容纳你，那也是自然的事，因为人生都有周期，听天命，尽人事，问心无愧，曾经有人听过你唱歌，其实已经比世上的绝大多数人幸运。</p><p><strong>10、音乐道路如人生，不要害怕挫折，弯路的沿途风景会更多</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5C1rMEYel1HfRFicsiciaQ3lviaxQUgaEZicY3J35ib1WgTIibbprvOPagrjt9lfbLRGPnaxsU5ovyFEM1Q5rZUn1uhZQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>在2004年，陈奕迅也曾陷入进家庭、事业、合约的多重低谷，然而回过头来看，他却很坦然，Eason说挫折每个人都会遇到，很多时候你接受了反而让自己能舒服一点，有些事越抗拒越不开心，越抗拒只会越钻牛角尖，接受了，退一步再看这些事，会发现原来人生不止有一条出路。Eason说人生若是一段从A走到B的路程，直路尽管会顺畅，可弯路能让人欣赏到沿路更多风景，在人生的各种转弯之处，你也会有更多丰富的体会，人生的目的不是顺畅，而是体验，珍惜每个当下所发生的事件与情感触动，因为Life is good。</p><p>就像最近Eason的新歌《致明日的舞》，人生如一出跌宕起伏、华丽繁复的舞蹈剧，世人的舞姿大不相同，有华丽探戈、孤单芭蕾、自由踢踏、性感爵士、热情伦巴，有准备充分的安排，也有临场发挥的即兴，构成了一个缤纷而跳动的世界；这舞蹈剧也充满着突发与意外，会跌倒、会失误、会错拍、会失控、或快或慢、或急或缓，如生活一般，未知重重，无法预知，你只有做好全面的应对；这舞蹈剧也不止你一人，你可以纵情独舞、醉梦双舞、或与众人共舞，你是自己剧中的主角，也是他人世界的风景，“跨出的步履如作画，沿途静听山海清雅”，无论是明日还是过往，他对于人生的信念与愿景都始终未变，在二十二年前《我的快乐时代》里第一句就唱着，让我有个美满旅程。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Music </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 音乐 </tag>
            
            <tag> Music </tag>
            
            <tag> Eason </tag>
            
            <tag> 陈奕迅 </tag>
            
            <tag> 唱歌 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python中的测试单元 - doctest</title>
      <link href="/2020/11/19/python%E4%B8%AD%E7%9A%84%E6%B5%8B%E8%AF%95%E5%8D%95%E5%85%83-doctest/"/>
      <url>/2020/11/19/python%E4%B8%AD%E7%9A%84%E6%B5%8B%E8%AF%95%E5%8D%95%E5%85%83-doctest/</url>
      
        <content type="html"><![CDATA[<p>使用python的doctest的模块，可以很容易的对函数进行测试~</p><font color="orange">在函数体中，在开头的注释部分，使用">>> function(...)"输入测试参数在下一行输出目标结果</font><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leaky_data</span><span class="params">(file1, file2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    查看训练集和测试集是否存在数据泄露</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; leaky_data('train.txt', 'test.txt')</span></span><br><span class="line"><span class="string">    []</span></span><br><span class="line"><span class="string">    数据集中训练集和验证集不存在重复数据</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    leaky = []</span><br><span class="line">    samples = set()</span><br><span class="line">    <span class="keyword">with</span> open(file1, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> f:</span><br><span class="line">            samples.add(json.loads(l)[<span class="string">'text'</span>])</span><br><span class="line">    <span class="keyword">with</span> open(file2, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> f:</span><br><span class="line">            record = json.loads(l)</span><br><span class="line">            sample = record[<span class="string">'text'</span>]</span><br><span class="line">            <span class="keyword">if</span> sample <span class="keyword">in</span> samples:</span><br><span class="line">                leaky.append(sample)</span><br><span class="line">    leaky = list(set(leaky))</span><br><span class="line">    <span class="keyword">return</span> leaky</span><br></pre></td></tr></table></figure><a id="more"></a><p>然后在主程序中，使用以下的写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">import</span> doctest</span><br><span class="line">    doctest.testmod()</span><br></pre></td></tr></table></figure><p>如果所有的测试都通过了，则不会有任何提示，如下图：</p><p><img src="/images/blog/2020/pass.png" alt></p><p>如果出现了不满足的情况，则会报错：</p><p><img src="/images/blog/2020/notpass.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> doctest </tag>
            
            <tag> 测试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torch.nn.CrossEntropyLoss()</title>
      <link href="/2020/11/19/torch-nn-CrossEntropyLoss/"/>
      <url>/2020/11/19/torch-nn-CrossEntropyLoss/</url>
      
        <content type="html"><![CDATA[<p><em>CLASS</em><code>torch.nn.CrossEntropyLoss</code>(<em>weight: Optional[torch.Tensor] = None</em>, <em>size_average=None</em>, <em>ignore_index: int = -100</em>, <em>reduce=None</em>, <em>reduction: str = ‘mean’</em>)<a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CrossEntropyLoss" target="_blank" rel="noopener">[SOURCE]</a></p><p>This criterion combines <code>nn.LogSoftmax()</code> and <code>nn.NLLLoss()</code> in one single class.</p><p>It is useful when training a classification problem with C classes. If provided, the optional argument <code>weight</code> should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</p><font color="orange">**The input is expected to contain raw, unnormalized scores for each class.**</font><p><em>PS: 我之前不会一直用错了吧？在用这个函数的时候是不需要自己手动Softmax()的……去做实验试一下。。。</em></p><p><br><a id="more"></a></p><p>input has to be a Tensor of size either (minibatch, C) or (minibatch, C, d_1, d_2, …, d_K) with K≥1 for the K-dimensional case (described later).</p><p>This criterion expects a class index in the range [0, C-1] as the target for each value of a 1D tensor of size <em>minibatch</em>; if <em>ignore_index</em> is specified, this criterion also accepts this class index (this index may not necessarily be in the class range).</p><p>The loss can be described as:</p><script type="math/tex; mode=display">\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right) = -x[class] + \log\left(\sum_j \exp(x[j])\right)</script><p>or in the case of the <code>weight</code> argument being specified:</p><script type="math/tex; mode=display">\text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)</script><p>The losses are averaged across observations for each minibatch. If the <code>weight</code> argument is specified then this is a weighted average:</p><script type="math/tex; mode=display">\text{loss} = \frac{\sum^{N}_{i=1} loss(i, class[i])}{\sum^{N}_{i=1} weight[class[i]]}</script><p>Can also be used for higher dimension inputs, such as 2D images, by providing an input of size (minibatch, C, d_1, d_2, …, d_K)(minibatch,C,d1,d2,…,dK) with K \geq 1K≥1 , where KK is the number of dimensions, and a target of appropriate shape (see below).</p><ul><li><p>Parameters</p><p><strong>weight</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a><em>,</em> <em>optional</em>) – a manual rescaling weight given to each class. If given, has to be a Tensor of size C<strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code><strong>ignore_index</strong> (<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) – Specifies a target value that is ignored and does not contribute to the input gradient. When <code>size_average</code> is <code>True</code>, the loss is averaged over non-ignored targets.<strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code><strong>reduction</strong> (<em>string**,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>&#39;none&#39;</code> | <code>&#39;mean&#39;</code> | <code>&#39;sum&#39;</code>. <code>&#39;none&#39;</code>: no reduction will be applied, <code>&#39;mean&#39;</code>: the weighted mean of the output is taken, <code>&#39;sum&#39;</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>&#39;mean&#39;</code></p></li><li><p>Shape:</p><p>Input: (N, C)(N,C) where C = number of classes, or (N, C, d_1, d_2, …, d_K)(N,C,d1,d2,…,dK) with K \geq 1K≥1 in the case of K-dimensional loss.Target: (N)(N) where each value is 0 \leq \text{targets}[i] \leq C-10≤targets[i]≤C−1 , or (N, d_1, d_2, …, d_K)(N,d1,d2,…,dK) with K \geq 1K≥1 in the case of K-dimensional loss.Output: scalar. If <code>reduction</code> is <code>&#39;none&#39;</code>, then the same size as the target: (N)(N) , or (N, d_1, d_2, …, d_K)(N,d1,d2,…,dK) with K \geq 1K≥1 in the case of K-dimensional loss.</p></li></ul><p>Examples:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; loss = nn.CrossEntropyLoss()</span><br><span class="line">&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span><br><span class="line">&gt;&gt;&gt; target = torch.empty(3, dtype=torch.long).random_(5)</span><br><span class="line">&gt;&gt;&gt; output = loss(input, target)</span><br><span class="line">&gt;&gt;&gt; output.backward()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> Loss Function </tag>
            
            <tag> CrossEntropy </tag>
            
            <tag> Softmax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>韩先培 - 开放式知识获取</title>
      <link href="/2020/11/18/%E9%9F%A9%E5%85%88%E5%9F%B9-%E5%BC%80%E6%94%BE%E5%BC%8F%E7%9F%A5%E8%AF%86%E8%8E%B7%E5%8F%96/"/>
      <url>/2020/11/18/%E9%9F%A9%E5%85%88%E5%9F%B9-%E5%BC%80%E6%94%BE%E5%BC%8F%E7%9F%A5%E8%AF%86%E8%8E%B7%E5%8F%96/</url>
      
        <content type="html"><![CDATA[<p>近期了解到<a href="http://www.icip.org.cn/team/hanxianpei/" target="_blank" rel="noopener">韩先培</a>是中科院软件研究所的一位老师，在Bootstrapping RE方面做了很多的工作，这也是我之前想要尝试的方向，所以后面会多留意追踪大佬的步伐。</p><p><img src="/images/blog/2020/hanxianpei.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Relation Extraction </tag>
            
            <tag> Rule </tag>
            
            <tag> Open IE </tag>
            
            <tag> Bootstraping </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> End-to-End </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>和老师赶Deadline的注意事项</title>
      <link href="/2020/11/18/%E5%92%8C%E8%80%81%E5%B8%88%E8%B5%B6Deadline%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"/>
      <url>/2020/11/18/%E5%92%8C%E8%80%81%E5%B8%88%E8%B5%B6Deadline%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇文章转载自陈怡然老师的公众号：陈老师有话说</p></blockquote><p><br></p><p>今年电子设计自动化领域和计算机体系结构领域的两大顶级会议DAC和ISCA的截稿日期放在了学校感恩节假期的前一天晚上（11月21日），加上之前一周截稿的CVPR和某个会议，结果组里的老师和同学都赶死线（deadline）赶了个天昏地暗。随着截止日期日益临近，陈老师的脾气也一天坏似一天。</p><p>把文章交上去之后，回想整个过程，觉得还是有必要写一篇文章对各位曾经、正在、或未来将会赶死线的博士生同学们说几句有关如何提高和导师一起赶文章死线效率的</p><p><strong>十大注意事项</strong>。</p><h1 id="一-请提早开始"><a href="#一-请提早开始" class="headerlink" title="|| 一 ||  请提早开始"></a><strong>|| 一 ||</strong>  <strong>请提早开始</strong></h1><p>今年有个刚来的学生在文章截稿两周前我碰到他在实验室看视频。我说你没事做了?赶紧赶文章。他说没问题，我心里有数。结果他的第一稿一直到截稿前三个小时才全部完成。我只好陪着他改到最后一分钟。由于时间有限，很多段落都只能改了改英语，对于逻辑和细节则完全无法细抠了。为什么会这样？这有两个重要的原因：</p><p>1)     <strong>人们往往以结构进度为基准来线性预测所需要的时间，而忽略细节可能带来的额外开销。</strong>比如在拥有详细图纸的情况下，建造一个大楼所需的框架需要的时间远低于装水电、装修的时间。而没有经验的学生经常以自己写一段话的时间长短来预测撰写整个论文所需要的时间，造成严重低估。实际上，<strong>撰写一篇学术论文所需的时间的绝大部分是花在文章大体框架出来之后的逻辑调整、补充数据、英文修改、甚至画图表等琐碎的细节上。</strong></p><p>2)     所有的人都是越到死线效率越高，但<strong>作为一个协作的整体，教研组最后的瓶颈实际上在导师</strong>：他/她要面对所有投稿的文章而不是你一个人！越往后，你单位时间所获得的老师的有效反馈越少。从而降低你修改的效率。</p><p>所以，<strong>尽早开始，尽早利用你和导师的空余时间</strong>，会极大的提高最终论文的质量。<a id="more"></a></p><p><br></p><h1 id="二-一定要列提纲和实验计划"><a href="#二-一定要列提纲和实验计划" class="headerlink" title="|| 二 ||  一定要列提纲和实验计划"></a><strong>|| 二 ||  一定要列提纲和实验计划</strong></h1><p>很多学生喜欢打腹稿，不喜欢把文章的提纲和实验计划列出来，更不喜欢照着提纲和计划去写，觉得多此一举，老子只要实验结果棒棒哒就好了么！</p><p><strong>列提纲和实验计划最重要的功用是为了梳理整个文章的逻辑脉络。</strong>顶级会议录取率都在20%甚至以下。所以评审论文如战场，各个评审人拿着放大镜来寻找文章中缺失的逻辑链条，比如：这个推论的逻辑是怎么来的？有充足数据支持么？有没有某个设计参数没有讨论？还缺了哪个实验结果？只要有一个这样的逻辑链条缺失，在评审会上基本就可以一锤定音的拒掉这篇文章。因此，<strong>提纲和实验计划会让你和导师从头一环环的检验整个论文中故事的逻辑链条，确保每一个结论都有足够的前提假设、讨论和实验数据支持，前后呼应，确保无懈可击。</strong></p><p>我经历过的绝大部分的写作过程中，文章各部分逻辑关系都会在一开始频繁调整，好像搭积木一样，<strong>最终找到一个最稳定，最难攻击的结构。</strong>在其之上开始写作和准备实验结果，如同做完形填空一样，既自信、又高效。</p><p><br></p><h1 id="三-请多花点时间给文章取个好名字"><a href="#三-请多花点时间给文章取个好名字" class="headerlink" title="|| 三 ||  请多花点时间给文章取个好名字"></a><strong>|| 三 ||  请多花点时间给文章取个好名字</strong></h1><p>很多人觉得给文章取名字不重要，振振有词的说只要文章写的好，实验结果充分，效果好，就一定会有好结果。</p><p>我们来想象一下你的文章是怎么被评审的：</p><p><strong>首先</strong>是大会技术委员会主席收到这些文章，然后根据文章的名字（第一次！有时候还加上文章的摘要），分配到他/她认为合适的评审人手里。有的会议还要先分到合适的track;</p><p><strong>然后</strong>再由track chair分到各个评审人手里。而这些评审人很多并不会马上看这些文章（他们也忙啊），而是根据自己的时间安排，看一下各个文章的标题（第二次！），决定哪一篇先审，哪一篇后审。人之常情，大家都会先看自己最感兴趣或者最擅长的方向的文章。而在最后评审意见提交截至日期前把自己不喜欢的文章匆匆看一下赶紧提交。</p><p><strong>最后</strong>，文章会在<font color="red">评审会上</font>被大家讨论。而会上大家做的第一件事情，就是主席读一遍文章编号和题目（第三次），提醒大家注意该文章要被讨论。</p><p><strong>在以上过程中，你的文章题目会被提出至少三次。</strong></p><p>那么现在问题来了，如果两篇文章质量差不多，一篇的题目平平无奇甚至会引起读者对内容的误解，而另一篇的题目与内容非常贴切而且吸引眼球，那么哪篇文章被分配到不合适的评审人手中的几率高？哪篇文章评审人会先读、精读、提出更有建设性的审稿意见？哪篇文章会在条件差不多的情况下被最终选中?</p><p>不用我再多说了吧？</p><p><br></p><h1 id="四-熟悉你的写作工具和template"><a href="#四-熟悉你的写作工具和template" class="headerlink" title="|| 四 ||  熟悉你的写作工具和template"></a><strong>|| 四 ||</strong>  <strong>熟悉你的写作工具和template</strong></h1><p>无论你是用word还是latex，用不用endnote，用什么环境，请熟悉他们的使用技巧。</p><p>一个排版漂亮的文章总会给人赏心悦目的感觉。所以，熟悉你的写作工具很重要。每个会议都有他们提供的template，有的好看，有的不好看，有的人可以把不好看的变得好看，有的人则可以把好看的template变丑。这些都是需要花时间来培养的经验。</p><p>不过<strong>至少，你可以做到格式一致</strong>：</p><p>所有段落的<strong>缩进相同</strong>，前后的<strong>space一致</strong>；</p><p>图和表永远在图文框里，而图文框永远放在四个角；</p><p>Title 1所有单词大写, Title 2的每个词第一个字母大写, Title 3、图的caption和表的title只有第一个单词首字母大写；</p><p>所有的reference里面<strong>所有item的文章题目、卷、页、年份格式都一致</strong>。</p><p>一份干干净净、整整齐齐、格式漂亮、花心思的draft已经向导师表明的你认真的态度和一定要中的决心。“所以，请（导师）您也一起努力吧！”</p><p>最后，无论你选择怎样的软件来管理你的references，一定要持之以恒，不要变来变去。</p><p><br></p><h1 id="五-不要把匆匆而就的第一稿给导师"><a href="#五-不要把匆匆而就的第一稿给导师" class="headerlink" title="|| 五 ||  不要把匆匆而就的第一稿给导师"></a><strong>|| 五 ||</strong>  <strong>不要把匆匆而就的第一稿给导师</strong></h1><p><strong>很多人习惯了把写完的东西未经检查就直接交给导师检查。</strong>有的时候是因为时间紧急，有的时候或许仅仅是因为懒。但<strong>请记住，千万不要这么做。</strong></p><p>我跟我的学生说，无论是他还是我，或者任何人。你把你任何写好的稿子放一周回头看，都会有一种“这是什么鬼”的感觉。所以无论如何，请把你的稿子多读几遍，确保你已经竭尽全力把它写的清楚-清楚-再清楚（重要的事情说三遍）了之后，再给你的导师看。</p><p>对于没有准备好就给我改的稿子，通常我遇到的情况有以下三种：</p><p><strong>故事的逻辑有瑕疵：</strong>提纲和实验计划只能保证整体框架的逻辑正确。并不能保证没有文字尤其是句与句之间和一句话内前后的逻辑瑕疵。如果智商没问题（这个大家都不会承认的），那通常出现这种情况唯一的原因是懒（得想），俗称想到哪里写到哪里。</p><p><strong>英语说不清楚：</strong>我每次问我的学生这个我读不懂的时候，大多数人都会说：这个我不知道怎么说（所以我胡写了一个请您来改），说的理直气壮。但你有没有try your best去写明白（比如读了别人的文章或者google了别人怎么写）还是胡乱写了一个导师是读的出来的。<strong>把不想干的脏活想都不想就留给导师，你将永远无法提高自己。</strong></p><p><strong>大量的语法和拼写的低级错误：</strong>没啥说的，这意味着你已经放弃你自己了。要么回去重新检查，要么你还是找一份更适合你的职业道路吧。</p><p><strong>无论以上哪一种，我会跟学生说：对不起，请拿回去。</strong>记住只有你自己尽力了之后，别人才会尽力帮你。</p><p><br></p><h1 id="六-用reviewer的视角去写文章"><a href="#六-用reviewer的视角去写文章" class="headerlink" title="|| 六 ||  用reviewer的视角去写文章"></a><strong>||</strong> <strong>六 ||  用reviewer的视角去写文章</strong></h1><p>文章是写给reviewer和读者看的，不是写给自己的日记。所以，要从一个reviewer的角度来组织和撰写你的文章。下面是一些你可能<strong>需要考虑的细节</strong>：</p><p><strong>文章的结构是否合理</strong>，包括各个部分长度是否合适，intro和background是不是太长？Proposed method是不是太短让人觉得理论贡献不够？实验数据是不是太短，数据是不是太少让人觉得所做的讨论不够深入细致？</p><p><strong>图、表、公式的数量</strong>是不是合适？太多让人觉得纷乱，太少让人读起来累（想象一下大段大段的枯燥英文）。<strong>位置</strong>是不是出现在他们应该出现的地方（比如不需要翻页就可以找到文中提到的图表）。这次提交还有五分钟的时候我发现有个学生有一张表竟然从来没有在正文提到过（当时两眼真的是马上一黑啊！）</p><p>是不是所有的<strong>缩写</strong>之前都定义过？是不是满篇有太多的缩写？格式是不是太紧或者太松？图的<strong>颜色</strong>是不是太过鲜艳（比如我们组的一个同学喜欢用‘豹纹“纹理的直方图<img src="https://res.wx.qq.com/mpres/htmledition/images/icon/common/emotion_panel/smiley/smiley_0.png?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">）或者太过单调（尤其不要用Excel自带的default设置）？</p><p>最后，<strong>要用reviewer般挑剔的眼光去对自己的工作提出质疑，然后在文章尽量去一一address这些疑问</strong>：评审专家在这行混了多年，你能看出来的问题，他们绝对能看出来。而你看不出来的，并不代表他们看不出来。千万不要有侥幸心理。</p><p>如果你不知道该怎么做，<strong>找个实验室其他不熟悉你工作的同学读一下</strong>，如果他读的辛苦晦涩或者发现了明显的逻辑漏洞，那说明你的文章还需要进一步的修改润色。</p><p><br></p><h1 id="七-不要不加思索的接受导师的revision"><a href="#七-不要不加思索的接受导师的revision" class="headerlink" title="|| 七 ||  不要不加思索的接受导师的revision"></a><strong>|| 七 ||  不要不加思索的接受导师的revision</strong></h1><p>我一次跟一个学生说：请把这里的AAA改成BBB。学生很快就改好给我了。我拿到后眼前一黑：他真的就在只我指出的那一行里把AAA改了BBB，而且真的就只改了这么一行！而眼看着下一段的AAA还是AAA！</p><p>我不是怀疑你不会用全文寻找-替换功能，而是想说你<strong>如何有效的在导师的反馈下迅速提高自己。</strong></p><p>导师冒着word可能会crash的风险坚持用track revision的方式改你的word文档或者坚持把pdf打出来用笔修改再让你一字一字的敲进latex不是为了为难你，而是为了逼你自己仔细思考他为什么这么修改。如果你轻轻松松的accept all revisions，那这些功夫就白费了。</p><p><strong>写文章和做科研一样，要举一反三。</strong>仅仅改正导师指出的那个错误是不够的，要学会将其推广至更普遍的情况并把这些情况都修改过来。比如你会经常发现老师把你写的一个长句子打成几个短句子。这是因为大多数中国学生用从句时容易犯语法错，而且由于语言习惯问题往往词不达意。短句子比较容易说清楚问题。那你下次再写的时候，就要在全文中都避免这种情况。而不要只是老师改了哪里，你就接受那里。</p><p>最后，如果你觉得老师的意见不对或者对你的文字理解有误（这经常出现在学生文字词不达意的情况），请直接跟老师说他哪里错了，不要憋在心里。反过来，也请对老师提出的意见不要太defensive，着急解释（我为什么会犯这个错误），还是赶紧回去修改文章最重要。Deadline前大家都忙的四脚朝天，谁也不会有心情听你的心路历程。<strong>无论老师还是学生，错了就改，在改正中提高（这个很重要），就够了。</strong>等到文章中了，再回味也不迟。</p><p>最后的最后，<strong>请保留你文章的每一个版本（最好一天一个）</strong>，然后有时间<strong>把导师修改后的最终投出稿和这些版本一一做对比</strong>，想一想导师为什么要这么修改，一定会受益匪浅。</p><p><br></p><h1 id="八-不要照抄-保留所有推导和实验细节"><a href="#八-不要照抄-保留所有推导和实验细节" class="headerlink" title="|| 八 ||  不要照抄, 保留所有推导和实验细节"></a><strong>|| 八 ||  不要照抄, 保留所有推导和实验细节</strong></h1><p>这是十大注意事项里面唯一<strong>跟学术道德有关</strong>的部分，但也是最不能碰的<strong>高压线</strong>。</p><p><strong>不要照抄别人任何文字！</strong>无论是技术的还是非技术的。很多学生觉得因为不是技术贡献，抄别人的intro和related work没关系；也有的学生觉得抄同组甚至自己之前的文章没关系。</p><p><strong>以上-都-是-错-的！</strong></p><p>陈老师不怕你文字写的烂，但每次碰到写的漂漂亮亮文辞优美的部分却胆战心惊：这怕不是抄的吧？别说，这么多年还真被我抓出过几次。</p><p><strong>所有的数据都要有出处和原始的实验记录，所有的推导都要有详细过程</strong>，即使这些细节不在文中出现，也一定要<strong>长期保留</strong>！好的文章会有很多人follow你的工作，大家会要求你的代码公开、重复你的实验，和你的结果做比较。记住，所有不能重复的实验都是骗子、都是耍流氓！<strong>开源是个好办法：</strong>既检验了你的工作的可信性，又宣传了你的工作。</p><p>如果你没有照以上做，那你就是坑导师了。我只能礼送你出组，在加上一句菩提祖师对孙悟空说的那句话：“日后你惹出祸来，不把为师说出来就行了“。</p><p><br></p><h1 id="九-保持随时待命和反应的状态"><a href="#九-保持随时待命和反应的状态" class="headerlink" title="|| 九 ||  保持随时待命和反应的状态"></a><strong>|| 九 ||  保持随时待命和反应的状态</strong></h1><p><strong>请不要闭门造车，不要在小黑屋里憋大招。</strong></p><p>很多学生很害怕被导师骂，或者觉得自己写的太烂，就希望把文章写的七七八八再给导师一个惊喜。说真的，每次我看到的时候，都只剩下了“惊“，尤其是还剩两天就截稿的时候。</p><p><strong>在整个写作的过程中保持和导师及时良好的更新与沟通是十分必要的。</strong>大多数导师的时间是碎片化的，对你的文章真的是“想起来就看一下“。如果你不能随时提供一个最新的版本，那么他对你的反馈就不可能是及时有效的。所以，建立一个Dropbox文件夹，<strong>每天把最新的版本和导师做一个更新，并提醒导师，是有效的督促导师修改和润色你的文章的方法。</strong>陈老师之前说过，导师对你科研的作用是为了让你行进在正确的方向上，在你每次跑偏的时候及时把你拉回来。这个时间间隔越长，你跑偏的可能性就越大，跑偏的距离就越长，把你拉回来所需要的力气就越大。</p><p>对于导师的意见要及时反馈（be responsive）。之前流行过一篇鸡汤，说<strong>靠谱的人只需要做到三点：凡事有交代，件件有着落，事事有回音。</strong>试想老师告诉你去修改1,2, 3, 4, 5，你回来只修改了2, 4, 5，然后他再给你1,3, 6，你再回来个1和6，那他肯定非疯了不行（但这样的学生我真的见过还不少）。还有，<strong>千万不能玩消失。</strong>我知道有的学生害怕被导师骂或者希望做完了再跟老师说而玩消失，不回老师email，电话，微信，…直到把事情做完，那导师就只有祈祷上帝你做的是他想要的这一条路了。</p><p>最后，<strong>合理的安排时间也很重要。</strong>买菜做饭陪女朋友逛街这些事情请在deadline之前的一周先做了！<strong>一个做事没有计划、不知轻重缓急的人丢掉的不仅仅是一篇论文，而是你的未来。</strong></p><p><br></p><h1 id="十-改到最后一分钟！"><a href="#十-改到最后一分钟！" class="headerlink" title="|| 十 ||  改到最后一分钟！"></a><strong>|| 十 ||  改到最后一分钟！</strong></h1><p>人在重压下激发出的爆发力是惊人的！所以，<strong>请改到最后一分钟！</strong></p><p>也许最后你已经做不了什么了（也千万别做大的修改以免忙中出错），但至少你可以<strong>仔细检查文章的每一个细节</strong>，包括<strong>拼写错误</strong>，<strong>标题的大小写</strong>是否一致，是否有<strong>遗漏的引用或者引用错误</strong>（latex上会出现一个问号“？”）<strong>图的caption的字体大小是否统一</strong>，等等。</p><p><strong>图表错误是最容易忽视的部分</strong>：我曾经有个现在已经做教授的学生在当年一篇文章提交时慌忙中贴错了一张图，结果文章虽然review获得高分，但还是被一个评审发现问题给拒掉了。我建议他把那篇评审结果打出来贴在墙上，提醒自己不要再犯这样愚蠢的错误。<strong>记得，检查到最后一分钟，不断提交修改错误后的论文直到网站关闭！</strong></p><p><strong>文章一旦开始写，就一定要努力投出去！不要停下来！</strong></p><p>虽然你是第一作者，但<strong>请记住你的身后是所有共同作者的心血和努力</strong>。如果他们不帮你完成这篇文章，他们完全可以去写另一篇文章。<strong>你的放弃是对他们生命的浪费。</strong>在一个学术领域，顶级会议往往只有1-2个。你错过了一次deadline，可能大家半年甚至一年的心血就白费了，下一次机会又是半年以后。所以一旦决定要投稿，请无论如何不要放弃，一定要努力到最后一分钟再说！</p><p>对自己说“无论如何我已经努力过了”而轻易放弃的人是没有未来的。日剧《胜者即是正义 Legal High》的台词说到：</p><p>“ 越是工作做不好的无用之人”</p><p>“ 就越是会主张自己有多努力”</p><p>“ 努力了又怎么样，结果才是全部 “</p><p><img src="/images/blog/2020/chen.webp" alt></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> IJCAI </tag>
            
            <tag> AI </tag>
            
            <tag> Deadline </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>致2021届金融毕业生</title>
      <link href="/2020/11/18/%E8%87%B42021%E5%B1%8A%E9%87%91%E8%9E%8D%E6%AF%95%E4%B8%9A%E7%94%9F/"/>
      <url>/2020/11/18/%E8%87%B42021%E5%B1%8A%E9%87%91%E8%9E%8D%E6%AF%95%E4%B8%9A%E7%94%9F/</url>
      
        <content type="html"><![CDATA[<p>不知道为什么转这篇文章，明明是学计算机的学生😀，只是觉得有一些话很有道理~</p><blockquote><p>以下文章来源于藏金畅春园 ，作者园主</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA4Njk5MTYwNQ==&amp;mid=2650996532&amp;idx=1&amp;sn=e0e9c1b9b7a44c47c12d2e49d7f91b81&amp;chksm=843668aeb341e1b857b71508eee5a74dd6466f5d09c26863ff0ca1349f7fbaf804fd6fd14f05&amp;mpshare=1&amp;scene=24&amp;srcid=1011rQqNEz7A399biLhf5V43&amp;sharer_sharetime=1602426945650&amp;sharer_shareid=03b37d2da63d394d243dc8d08e8bee5f&amp;key=42f89ebab0dc82fb05cada71e1d7f5beb2301b6710709bf3ff32c4a2dadc829f6e01a06b2e24dce94af2c1a538a3e0f96648bc7a1894c831ad095e32c51674e1ba25c1957c5449f260827a6e217d47d993190537477aa0c846dd452f60625ee638f39ffc185568fdea0bb267fe47995b2e55312df208619e0d8bb95d43b4af1b&amp;ascene=14&amp;uin=MjExODc0NDkwMg%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=AUy6AdlmxfPPQBv80yaBbqc%3D&amp;pass_ticket=JMu0tZ8lnp%2FRuz1okqXGKHCSZV44b5P1Q0yYYVTQAtVTaM8NrMmb5T6PBnHGy2oL&amp;wx_header=0" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzA4Njk5MTYwNQ==&amp;mid=2650996532&amp;idx=1&amp;sn=e0e9c1b9b7a44c47c12d2e49d7f91b81&amp;chksm=843668aeb341e1b857b71508eee5a74dd6466f5d09c26863ff0ca1349f7fbaf804fd6fd14f05&amp;mpshare=1&amp;scene=24&amp;srcid=1011rQqNEz7A399biLhf5V43&amp;sharer_sharetime=1602426945650&amp;sharer_shareid=03b37d2da63d394d243dc8d08e8bee5f&amp;key=42f89ebab0dc82fb05cada71e1d7f5beb2301b6710709bf3ff32c4a2dadc829f6e01a06b2e24dce94af2c1a538a3e0f96648bc7a1894c831ad095e32c51674e1ba25c1957c5449f260827a6e217d47d993190537477aa0c846dd452f60625ee638f39ffc185568fdea0bb267fe47995b2e55312df208619e0d8bb95d43b4af1b&amp;ascene=14&amp;uin=MjExODc0NDkwMg%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=AUy6AdlmxfPPQBv80yaBbqc%3D&amp;pass_ticket=JMu0tZ8lnp%2FRuz1okqXGKHCSZV44b5P1Q0yYYVTQAtVTaM8NrMmb5T6PBnHGy2oL&amp;wx_header=0</a></p></blockquote><p>最近金融圈里一个兄弟跟我讲了一个段子：</p><p>说一家研究所内部，每个研究小组都已经是新财富组了，所以我们不再比今年谁能上新财富，谁不能上新财富。<strong>我们比的是新财富能拿到第几名。</strong></p><p>这位好兄弟停顿了一下说道，这就叫金融圈的<strong>“内卷”。</strong></p><p>我心里想好吧，你们这也能叫内卷？内卷的结果无非就是这个首席名次很好年薪500万，那个首席名次不好，今年只能拿200万，卷来卷去也都是金融圈的高端玩家。</p><p>这根本就不叫卷好吧。</p><p>真正的“卷”存在于<strong>金融应届毕业生中。</strong></p><p>其实这种“卷”在宿舍安排方面就已经初露端倪了，财新网19日有一篇报道还挺有意思的，名字叫《记者手记|研究生扩招18.9万元，住宿难何解？》。</p><p>简而言之就是硕士扩招的太厉害了，很多学校都是几百上千的在扩招，床位不够了，<strong>学校只好把一堆研究生踢出去住。</strong></p><p>研究生们在尚未开学之前，就实实在在的初步尝到了<strong>“卷”</strong>的滋味，实在是相当的精彩。<a id="more"></a></p><p>清北就别说了，已经卷到天了，很多内部学生已经逐渐意识到，大家在金融圈子里这样卷下去，实在是太惨了。</p><p>于是很多人本科的时候就开始另辟蹊径，第二专业学搞IT，毕业找工作时金融+IT复合双料背景，估计要不了多久这种发展方向将蔚然成风，<strong>将互相之间的内卷进一步提升一个档位。</strong></p><p>因为工作关系我经常要到上海出差，私下里经常有复旦、交大、上财的粉丝约着见面聊天。普遍感觉就是比前几年的学生，这两年大家明显变得更加焦虑。</p><p>一个典型的特点就是<strong>复旦的学生也逐渐开始上财化。</strong>复旦跟上财一直都是一对CP，一个自称“财大牛逼”，一个自称“自由无用”。</p><p>复旦学生给人的感觉，的确是更加自由不羁一点。财大的学生迫于残酷的保研指标和就业压力，往往在学习和实习层面都非常的拼。<strong>但是目前来看复旦的学生内部氛围已经逐渐的跟上财接近。</strong></p><p>我并没有说复旦跟上财哪种学生的气质或者氛围更好一些，每个学校都有自己的历史和定位，特点都不一样，培养出来学生气质方面肯定会有所差别。</p><p>但是近几年我的观察结果就是，无论是清北也好、复旦也好、交大也好，上财也罢，培养出来的经管类学生已经越来越趋同，整天聊天的话题基本不离实习、工作、职业发展，以及怎么赚钱。</p><p>学生都只剩下一个标签：</p><p><strong><em>焦虑的中国金融应届生们。</em></strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/gaHPCYDlq2Hn6GCveLmQSNa4oWHSj3Jz6ia8O9tJZdFjsv6wbTaNEib8XEY1pXMC0ibWbdXJXILMHbEiaVNoicKGJeg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>大的环境已经把结论框死了：这必然是一个内卷的局。无论是清北复交，还是央财上财人大。除去很个别的神人，就业的天花板大概就处在三中一华，以及国内几大券商研究所中，<strong>而且进去的比例也并不高。</strong></p><p>其余学生常见的去处是银行、四大、二线咨询、互联网、快消、考编。对于相当一部分人来说，<strong>所谓的名校光环，与后期的就业实际相比，落差不可谓不大。</strong></p><p>以上讲了些问题，我知道相当多的金融学生，尤其是应届生正在为秋招秃头。细心的读者大爷或许发现，这个号中关于职业发展的文章占比越来越低，之后也就挑个秋招等关键时点，一年最多也就发个一两篇关于职业发展的文章。</p><p>文章数量相当少，我<strong>尽量把我所知道的，全部告诉你们，比如这篇文章。</strong></p><p> <strong><em>一、找工作请务必海王，渣一点。</em></strong></p><p>有些学生特别特别的可爱单纯，觉得公司是我家，友爱靠大家，领导慈祥体贴，同事温暖互助，我要衷心耿耿的，死心塌地的跟着公司。</p><p>对于这样的同学，我要给你送上社会主义的光荣武器，来，跟着我读三遍以下这句话：</p><p><strong><em>你跟你公司之间永远存在着一种矛盾，这种矛盾不是请客吃饭，不是嘻嘻哈哈就可以缓解和掩饰的，这种矛盾是工人阶级和资本家之间的，不可调和的根本性矛盾。</em></strong></p><p>无论你实习的公司看起来多么的义正言辞、信誓旦旦、和蔼可亲，你要记住他最终的目的，<strong>一定是充分压榨你身上的所有剩余价值，</strong>帮助自己进一步发展。你们之间就是纯粹的利用与被利用关系。</p><p><strong>一旦你被认为是没有价值的，就会被迅速的替代掉。</strong></p><p>以上这种基本就是职场铁律，<strong>更何况你从事的是人精遍地的金融行业！</strong></p><p>油嘴滑舌者甚多，当面一套背后一套者甚多，被拖着白嫖一年最终鸽掉offer的教训，实在是比比皆是。</p><p>因此你必须要学坏一点，必须要海王，必须要渣，<strong>秋招投够100家。</strong></p><p>疫苗的研发也是世界各地几十个项目组，几十个技术路径同步推进，最后看哪个项目组跑出来，就用哪个。</p><p>找工作也要这样子。如果实习领导拽着你不肯让你出去投简历面试。</p><p><strong>你会不会感冒？你家里表哥表姐能不能突然结婚了？你导师能不能突然找你做项目？</strong></p><p><strong>总是学不会，再聪明一点，记得自我保护，必要时候讲些善意谎言。</strong></p><p><strong><em>二、请从资产配置的角度来选择自己的实习机会。</em></strong></p><p>这段内容之前写过，估计很多人忘了，我再贴一次吧。</p><p>在秋招时配置一个实习留用的工作机会，再配置一个面试留用的工作机会。可以选择在研究所或者投行长期苟着别放弃，（毕竟10月份之前发offer的券商基本没有）。</p><p><strong>同时在秋招的时候疯狂找银行、公务员、金融监管机构、互联网快消等方面的工作。</strong></p><p>不排除的一个情形是，你的领导抓着你需要留用这个把柄拼命压榨你甚至画大饼，但这个时候你务必要对自己负责。你可以选择性找几个借口，比如拉肚子、发烧或者跑步腿折了，或者搬出你一年没见过的导师，说有项目让你做。</p><p><strong>千方百计多出去投简历面试，然后找一个offer保底。正所谓领导的嘴骗人的鬼，我见过太多被利用完然后鸽掉的例子了呵呵。</strong></p><p><strong><em>配置一个长期实习留用的工作机会来进攻，再配置一个面试/考试留用的工作机会来防守。风险对冲高，进可攻退可守，岂不美滋滋？</em></strong></p><p>当然我也见过完全反着来的，3月份换一家券商实习、6月份再换一家，然后7月开始的时候干脆离职，直接去秋招。</p><p>敢情你之前的券商实习就是白作贡献，用爱发电帮组里老师加廉价劳动力的杠杆吗。如果有这么乖的实习生，麻烦给我来一打（笑哭），<strong>我金融行业那群哥们正嗷嗷待哺想要白嫖实习生呢。</strong> </p><p>跟你们开个玩笑哈哈。</p><p>丢掉幻想，准备战斗，大家加油吧！</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Interview </tag>
            
            <tag> Job </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>清华大佬的秋招算法岗总结</title>
      <link href="/2020/11/18/%E6%B8%85%E5%8D%8E%E5%A4%A7%E4%BD%AC%E7%9A%84%E7%A7%8B%E6%8B%9B%E7%AE%97%E6%B3%95%E5%B2%97%E6%80%BB%E7%BB%93/"/>
      <url>/2020/11/18/%E6%B8%85%E5%8D%8E%E5%A4%A7%E4%BD%AC%E7%9A%84%E7%A7%8B%E6%8B%9B%E7%AE%97%E6%B3%95%E5%B2%97%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p>作者 | 对白</p><p>个人简介 | 清华大学，工学硕士在读</p><p>原文链接，点击文末阅读原文直达：</p><p><a href="https://zhuanlan.zhihu.com/p/296551038" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/296551038</a></p></blockquote><p><br></p><p>一晃接近三个月过去了，秋招也到了尾声，之前一直忙于写毕业论文，现在在这里想总结一些自己求职互联网大厂算法岗的面经和心得，希望帮助后来的学弟学妹们收获自己心仪的offer。</p><p>今年的算法岗求职较往年竞争也更加激烈，可以预见以后进大厂的算法岗会变得越来越难，比如美团北斗去年的准入门槛是一篇CCF A，而今年直接提升到了两篇CCF A，难度提高了一倍，加之疫情的影响，我认识的很多手握顶会的本科学弟们以及海外的同学们也加入到了找工作的大军中，因此今年算法岗的竞争堪称史上巅峰，真的是八仙过海，各显神通。不过，找工作不仅需要实力，运气和方法也缺一不可，下面我就谈谈我自己的心路历程。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>本人Top2硕士在读，非计算机科班，两篇论文在投，投递岗位的方向主要为推荐/广告/机器学习，在整个秋招的过程中，共参加了8场面试，阿里、百度、京东、美团、拼多多、快手、小红书、平安。最后侥幸获得了7家公司的offer，其中6家ssp，1家sp，薪资待遇40+W-70+W不等。</p><p>可能很多人会觉得我应该有多家大厂的实习，但其实我并没有，由于父母的工作受到了疫情的影响，导致我今年从疫情开始到7月初一直都在家里帮父母的忙，从而错过了暑期实习，看到周围的同学都拿到了大厂实习的offer，当时的我只感觉自己的秋招应该和互联网无缘了。每个人的秋招只有一次，如果不奋力一搏，又怎会知道最终的结果。抱着这个念头，我在7月初就赶紧放下了父母的工作，全身心的备战秋招。在这里，我就要讲一讲我的方法了，大部分互联网公司算法岗基本是三轮技术面+一轮HR面，技术面每轮的面试时间大致是一个小时，主要考察coding能力、基础知识和项目/竞赛。由于时间的不足，在这三个方向上我做了时间的分配，我的复习顺序是基础知识-&gt;项目/竞赛-&gt;coding能力。<a id="more"></a></p><p>1.基础知识</p><p>除了极少数公司会在一上来就要求你做一道编程题以外，大部分互联网公司都会在你的自我介绍和论文之后开始进行基础知识的考察，因此它的重要性不言而喻。基础知识的复习有两种途径，一是看书，二是看视频，这取决于你对哪一种途径接受知识的速度更快。我选择的是看书，一是因为视频不一定讲得面面俱到，二是视频质量如果不过关，很有可能某些细节的讲述是错误的。以推荐/广告岗位为例，我主要看如下书籍。</p><ul><li>周志华的西瓜书《机器学习》</li><li>李航老师的《统计机器学习》第二版</li><li>DL圣经《深度学习》，又名花书。</li><li>《百面机器学习》</li><li>《概率论与数理统计》、《线性代数》、《凸优化》</li><li>推荐/广告：《深度学习推荐系统》、《计算广告》等</li></ul><p>如果时间有限，可以直接去看第二本蓝皮书和第三本花书，这两本书一定要从头到尾仔仔细细的过一遍，因为基础知识的考察无外乎就是机器学习或深度学习里的知识。当这两本书过完一遍之后，再看西瓜书的效率就会快很多了。除此之外，第四本书可以留在你将要面试的那几天着重去学习，因为你有了前几本书的知识做铺垫，第四本书就可以当成八股文去背了。前提是一定要在你理解这些知识之后，因为单纯的死记硬背面试官其实很容易就会发现破绽，毕竟现在每个人都会背。对于本科学习过的线代和概率论，建议大家也复习一遍，因为在我的面试过程中就有面试官残忍的提问了，虽然概率不高。对于学有余力的同学，可以去看一些推荐/广告方向的工业界人士出版的书籍，这一块的知识考察往往会穿插在整个面试中，有的面试官会在最后当成开放题进行考察。</p><p>其次，基础知识的复习也很容易会遗忘，面对这个问题，我一般会将高频考点的知识写在ipad中，然后每晚会去复习一遍，这样一个月下来，基础知识应该就可以烂熟于心了，这一段的复习就可以告一段落了。</p><p>2.项目/竞赛</p><p>项目/竞赛一般会在基础知识之后进行考察，这些一定要提前准备好，写在简历里的项目一定是要自己亲自做过的，因为一旦面试官问到了项目中某一个部分的代码是如何实现而你又回答不上来的话，在他心里对你的印象就会非常减分，并且会怀疑候选人的诚信问题。所以，我的方法是将之前做过的项目重新再做一遍，包括代码部分。这一块看似会花费大量时间，实则并没有那么长。具体来说，我会先将做过的项目重新梳理一遍，画出整个项目的流程图，然后再逐行复习自己的代码。等代码复习完之后，再删掉重新写一遍，这一步骤因人而异，取决于你的记忆能力。</p><p>竞赛方面，常用的竞赛模型如GBDT、XGBoost、LightGBM、FFM、DeepFFM等一定要了解。最后，如果你的论文或项目与面试官所做的方向非常的match，你通过这一面的概率也将会大很多。</p><p>3.coding能力</p><p>代码能力是计算机专业学生的基础能力，求职技术方向的同学，无论是测试、开发或算法，互联网公司在这一块的考察都是重中之重。一般而言，大厂在每一轮的技术面中，至少会出一道编程题，多的会直接上三道编程题让你做（我就遇到了…），难度主要集中在easy和medium，少数丧心病狂（褒义词）的面试官会出hard题。而考察范围已是圈内公开的秘密，就在《剑指offer》和Leetcode上，因此刷题成为了大家求职路上必须要迈过的一道坎，这个坎没有人可以帮到你，只有靠你自己。我在硕士阶段也没有刻意的去刷过题，只有本科时刷过洛谷，下面是我Leetcode账号上提交次数的统计：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/nW2ZPfuYqSJh4l4lBnLYbAN7PVarFvVRcXWRCQB1dEbc4GgibJGibzibluMRCYgTX0v398v4H7GyRTcm5b5Az4mGg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>我是从8月初开始集中在Leetcode上刷题，一直刷到了10月份，《剑指offer》也是在Leetcode上刷的，平均每个月刷题数是300。可能很多人都觉得300道根本不可能完成，确实，如果你按照Leecode官方的题号顺序去刷根本不可能，但如果是按照题目类型（标签）去刷，每天刷10道，一个月也就完成了300道。一开始可能会很慢，但基本上每个类型的题目当你刷够20道以后，都可以总结出该类题目的代码模板，所以前期会慢一点，越往后则会发现刷题速度越来越快，可以理解为先苦后甜。而对于刷题的方法，可以分为三轮进行：</p><p>a.第一轮：优先效率。当一道题花了十五分钟去思考也没有思路时，则果断选择直接看答案。</p><p>b.第二轮：培养思路。你需要对以前做过的每一道题都要有一个大致的映像，并且知道解题的方法是什么。这一轮其实最难度过，因为人对事物的遗忘是有规律的，而我们需要想办法客服这个规律。在这里推荐一下我的方法。我在刷完每一道题之后，会在ipad上注明题号以及题目名称，然后将该题的解题思路写下来，最后还会重写一遍代码。这样，以天和周为单位不断地复习之前的题目，就可以做到以后遇见它们时可以迅速反映出是用什么方法解决这道题的。</p><p>c.第三轮：完善思路。当我们顺利度过了第二轮之后，第三轮则需要学习每道题目的多种解法，比如TopK问题可以用快排变形/堆/二叉查找树/计数排序四种方法解决。在我的面试过程中，有些面试官会在你AC一道题后还会要求写出最优解，这往往决定你的面评是否能达到较高级别，也就是能否拿到sp以上级别的offer，并且一道题如果学会了多种解法，也会让你加深对这道题的理解。因此，我建议大家要学会用多种解法解决一道题，并且要培养出能快速AC的能力。</p><p>PS：在这里，我要推荐一下自己免费加入的Leetcode每日打卡和竞赛群，圈内俗称残酷群。国服前一百位的选手在该群都有好几十人。规则很简单，每日完成题主发布的Leetcode题号以及每周在美服上打Leetcode周赛，但周赛排名靠后的小伙伴会要求发红包，这个机制也是为了督促大家快速成长。所以秋招不是刷题之路的终点，而仅仅只是开始。</p><h2 id="面试投递"><a href="#面试投递" class="headerlink" title="面试投递"></a>面试投递</h2><p>在完成了基础知识-&gt;项目/竞赛-&gt;coding能力的复习后，时间节点也移动到了八月中下旬，这时很多公司的秋招正式批即将开始。由于我错过了提前批的投递，而提前批往往都没有笔试，所以正式批只能先参加笔试再进行面试。经过血与泪的磨练后，在这里给大家的建议是，有提前批投提前批，无提前批投特殊计划，重要的事情说三百遍。原因有两点：1、提前批的竞争压力小，投的人少，先拿到offer的概率很大；2、正式批的笔试题一般有一定难度，且刷人主要看AC题目的数量，比较残酷。而且据我身边同学的例子，有很多人笔试完之后公司就杳无音信了，要问就是在筛选，实际上你已经妥妥变成了备胎（有恋爱经验的人都懂），然后在公司的池子里欢快的游动。除此之外，若感觉自己还没有准备好，则可以等待几天再投递，但千万不要错过提前批的截止时间。</p><p>所以提前批的面试能把握的还是尽量要把握，哪怕你觉得自己只准备了60%，但万一成功了呢？因为面试通过这个事情，实力与运气都需要， 在你的实力和别人差距不大的情况下，实力不够，也可以运气来凑不是吗。</p><h2 id="部分面经"><a href="#部分面经" class="headerlink" title="部分面经"></a>部分面经</h2><p>以下面经均为各个公司的正式批，由于时间有点久远，有些考点已经不记得了。</p><blockquote><p>京东</p></blockquote><p>一面：</p><p>1、介绍论文、项目，很详细。包括每一个环节是怎么实现的，损失函数是如何设计的，模型是如何训练的等等。</p><p>2、代码题：Leetcode 713：乘积小于k的子数组；Leetcode 297：二叉树的序列化和反序列化。</p><p>3、GBDT、XGBoost、LigthGBM的区别与联系。</p><p>4、送入LR前，如何处理数据（特征工程）。</p><p>二面：</p><p>1、介绍论文、项目，很详细。</p><p>2、Transformer中的Scaled Dot-Product Attention为什么要缩放（两点）。</p><p>3、Transformer中的Position Embedding是怎么实现的？为什么？</p><p>4、bagging和boosting与偏差和方差的关系以及原因。</p><p>5、如何解决数据不平衡的问题。</p><p>6、假设检验的两类错误。</p><p>7、MSE、MAE与贝叶斯估计的区别。</p><p>8、为什么快排比堆排快？</p><p>9、口述算法题：对一个商品的价格、数量、购买人数进行分次排序，不改变之前的排序结果。</p><p>三面：</p><p>由于一、二面表现好，三面直接过了。</p><blockquote><p>美团：</p></blockquote><p>一面：</p><p>1、介绍论文和项目，很详细。</p><p>2、代码题：面试题17.24：最大子矩阵；Leetcode 695：岛屿的最大面积。</p><p>3、介绍一下DSSM。</p><p>4、开放题：关于双塔模型的应用。</p><p>二面：</p><p>1、介绍论文和项目，以及论文中的涉及到的baseline模型。</p><p>2、代码题：面试题01.08：零矩阵，写出两种解法。</p><p>3、XGBoost 如果损失函数没有二阶导，该怎么办。</p><p>4、聊天+反问。</p><p>三面：</p><p>1、介绍论文和项目，非常详细。</p><p>2、介绍实习，说说实习印象中最深的点。</p><p>3、开放题：部门中的某个应用场景你会怎么解决。</p><p>4、职业规划，个人希望做的方向。</p><blockquote><p>拼多多：</p></blockquote><p>一面：</p><p>1、介绍论文和项目。</p><p>2、AUC是如何实现的，它对均匀正负样本采样是否敏感，并用代码实现。</p><p>3、BERT与ALBERT的区别。</p><p>4、介绍一下DKN模型。</p><p>二面：</p><p>1、介绍论文和项目。</p><p>2、过拟合如何解决+具体方法追问。</p><p>3、代码题：Leetcode 42：接雨水。</p><p>4、知识图谱表示学习有哪些模型。</p><p>5、聊天+反问。</p><blockquote><p>阿里：</p></blockquote><p>一面：</p><p>1、介绍论文和项目。</p><p>2、代码题：Leetcode 382：链表随机节点，并口述蓄水池采样算法的推导。</p><p>3、概率题：将一根木棍分成三段，求这三段构成三角形的概率。</p><p>4、开放题：一个超级大文件，每一行有一个 ip 地址，内存有限，如何找出其中重复次数最多的 ip 地址。</p><p>二面：</p><p>1、介绍论文和项目。</p><p>2、论文后续可能提升的点以及想法探讨。</p><p>3、说一说Graph Embedding和GNN的区别。</p><p>4、代码题：Leetcode 23：合并K个升序链表。</p><p>5、聊天+反问。</p><p>三面：</p><p>1、介绍论文和项目。</p><p>2、介绍一下最能体现自己工程能力的项目。</p><p>3、如何提高推荐线上的性能。</p><p>4、强化学习在推荐中的应用及探讨。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后，我想谈一谈自己对于面试的看法。无论是竞赛、论文还是大厂实习，这些都是为了方便你拿到面试的资格，哪怕你三者都没有，但只要公司愿意向你发起面试，你都应该好好努力。而最终是否能顺利拿到offer以及offer的等级，主要还是取决于你的面评，一个非常好的面评是帮助你拿下大厂sp以上offer至关重要的因素，因此切忌面试紧张，如果你紧张，可以心中默念欧米豆腐。</p><p>祝愿大家都能拿到自己心仪的offer！</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Interview </tag>
            
            <tag> Job </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谈谈未来：NLP路在何方？</title>
      <link href="/2020/11/18/%E8%B0%88%E8%B0%88%E6%9C%AA%E6%9D%A5%EF%BC%9ANLP%E8%B7%AF%E5%9C%A8%E4%BD%95%E6%96%B9%EF%BC%9F/"/>
      <url>/2020/11/18/%E8%B0%88%E8%B0%88%E6%9C%AA%E6%9D%A5%EF%BC%9ANLP%E8%B7%AF%E5%9C%A8%E4%BD%95%E6%96%B9%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p> 本文转载自：“夕小瑶的卖萌屋”</p><p>这是一个很萌很干货的公众号，强烈安利一下~</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247500560&amp;idx=1&amp;sn=f80675e02c36ff1c99eb2adb5a3c9205&amp;chksm=970febc6a07862d06a375a70ba21a63200dc0b8521d9655b48b4c37cfe00e8972bcb37644afc&amp;scene=126&amp;sessionid=1605625565&amp;key=1e61c84483e13f9471ee0797d016c58ad2c5650c2da9838f5a72d9628e9968cb2de25800764d4886b46fc6369be40d19db24e806b3df2775b2735adb3b07026fd4f9a606ea6808c12cf5c315c448e9320c0d6c3398f8042d2a272a48f2b1b74a3c08daf500b18c90b33a9389de250c682340459bc12840bc2a10de67ec55aa35&amp;ascene=1&amp;uin=MjExODc0NDkwMg%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=AUEDy2LXhDbW1ZdKANZsG%2Fo%3D&amp;pass_ticket=JMu0tZ8lnp%2FRuz1okqXGKHCSZV44b5P1Q0yYYVTQAtVTaM8NrMmb5T6PBnHGy2oL&amp;wx_header=0" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247500560&amp;idx=1&amp;sn=f80675e02c36ff1c99eb2adb5a3c9205&amp;chksm=970febc6a07862d06a375a70ba21a63200dc0b8521d9655b48b4c37cfe00e8972bcb37644afc&amp;scene=126&amp;sessionid=1605625565&amp;key=1e61c84483e13f9471ee0797d016c58ad2c5650c2da9838f5a72d9628e9968cb2de25800764d4886b46fc6369be40d19db24e806b3df2775b2735adb3b07026fd4f9a606ea6808c12cf5c315c448e9320c0d6c3398f8042d2a272a48f2b1b74a3c08daf500b18c90b33a9389de250c682340459bc12840bc2a10de67ec55aa35&amp;ascene=1&amp;uin=MjExODc0NDkwMg%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=AUEDy2LXhDbW1ZdKANZsG%2Fo%3D&amp;pass_ticket=JMu0tZ8lnp%2FRuz1okqXGKHCSZV44b5P1Q0yYYVTQAtVTaM8NrMmb5T6PBnHGy2oL&amp;wx_header=0</a></p></blockquote><p><strong>CMU、华盛顿大学、南加州大学、MIT、MILA、密歇根大学、爱丁堡大学、DeepMind、伯克利、Apple</strong>…如果我说来自这些地方的dalao共同发表了一篇文章，你相信么？但别惊讶，在即将召开的EMNLP’20的长文列表里，我们就真找到了这样一篇“奇文”。一篇论文引得众星云集，那解决的必然不是小问题。这不，作者也很贴心地把他们所希望解决的问题斜体独行地放在了论文的首栏里——</p><p><strong>Where is NLP going？</strong></p><p>……</p><p>在未来的这几分钟里，让我们暂时放下自己正在改的模型、正在写的论文和正在追的SOTA，重拾自然语言处理的初心，跟随大佬们的脚步，去畅想一下未来的NLP究竟是什么样的吧。</p><p><strong>论文题目</strong>：</p><p>《Experience Grounds Language》</p><p><strong>论文链接</strong>：</p><p><em><a href="https://arxiv.org/abs/2004.10151" target="_blank" rel="noopener">https://arxiv.org/abs/2004.10151</a></em></p><p>Arxiv访问慢的小伙伴也可以在【<strong>夕小瑶的卖萌屋</strong>】订阅号后台回复关键词【<strong><em>1117</em></strong>】下载论文PDF~</p><h2 id="NLP，到底该怎么搞？"><a href="#NLP，到底该怎么搞？" class="headerlink" title="NLP，到底该怎么搞？"></a>NLP，到底该怎么搞？</h2><p>这是每一个NLP人都希望探索的终极问题。在经历了21世纪初的神经语言模型、2013年word2vec算法、2018年的预训练模型等等的里程碑过后，当今的NLP已经在许多任务上取得了令人欣喜的效果。但是，在欣喜于一个个子任务的突破之后，我们也该停下来思考我们每个人在初识NLP时的那个问题：如何才能让机器<strong>真正地</strong>理解人类语言呢？</p><p>本文提出了未来NLP的发展方向：只靠文本，是学不会语言的；学会语言，需要的是“<strong>语言之外的事件</strong>”和“<strong>社会环境</strong>”。这样虚无缥缈的两个词，隐含的却是未来NLP所需要添加的潜在的新组件。</p><p>为了更加具象，作者引入了“<strong>世界范围</strong>”的概念，英文名称World Scope，简称<strong>WS</strong>（不觉得和作者王苏有点关系么（逃））</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/IVz4ZzrQibgTFQmNH95NicHuicZqKFlMK2jkapjAo00uZS6tuIGrjUvKUuPsicKDW7ozpVia6pm4gw4njoCkbfPJN9g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>那么这五个世界分别表示什么，又象征着NLP的发展到了哪个阶段呢？现在，让我们把NLP系统想象成自家孩子，看看咱家宝贝儿是怎么一步步从过去只会总结文本模式到未来能够能动地改变世界的吧~（为了方便，我们就叫她<strong>N宝</strong>）<a id="more"></a></p><h2 id="WS1：少量语料的世界——当系统学会表示"><a href="#WS1：少量语料的世界——当系统学会表示" class="headerlink" title="WS1：少量语料的世界——当系统学会表示"></a>WS1：少量语料的世界——当系统学会表示</h2><p><img src="https://mmbiz.qpic.cn/mmbiz_png/IVz4ZzrQibgTFQmNH95NicHuicZqKFlMK2jFSGWkRbJIr0m3D1J4EYTFHM0YQYEiaibITElsRssicqNXiayZ6uaicq2b2Q/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>N宝终于拿到了她能接触到的第一个语料！此时的她，看的多半是类似于Penn Treebank的经典数据集，而她既没有容量很大的大脑（指模型），又接触不到其他东西（指感知和大量语料），于是研究者费尽心机地思考如何让她用少量文本也能学到些什么。这时的发展，正是集中在<strong>文本表示</strong>上。</p><p>所谓的“<strong>含义</strong>”（meaning）到底在哪里呢？一个很直观的想法是认为含义隐藏在文本的<strong>语法结构</strong>中，于是早期的NLP方法大都采用了诸如语法的分析结构。</p><p>但慢慢地，人们发现，文本的含义还有另外的表示方法。20世纪末-21世纪初，Elman和Bengio等人证明了<strong>向量表示</strong>可以捕获语法和语义信息；21世纪初，利用基于互信息的层次聚类表示方法和隐马尔科夫链生成词类别的方法证明了一个词的<strong>上下文</strong>隐含了这个词的含义；同样是21世纪初，以隐狄利克雷分布模型LDA为代表的主题生成模型证明了获取含义需要<strong>大量的</strong>上下文信息。正是基于以上的观察，才有了近年来诸如word2vec和GloVe的词向量表示，以及ELMo、GPT和BERT等等的上下文预训练表示。</p><p>然而，关于文本表示，有一个一直以来的矛盾，伴随着<strong>符号主义</strong>和<strong>连接主义</strong>的争论走到了今天——把词表示为符号，我们就可以利用一个词的字典释义，从而用其他词表示它，这种“以词释词”的方法服从直觉，解释性一流；然而，把词表示为向量，我们就能够利用诸如神经网络的“连接主义”系统进行处理，这种“以数释词”的方法难以解释，但架不住它好用。</p><p>这样的符号主义/连接主义争论经常会在当今的各大人工智能论坛见到，而在连接主义大行其道的当下，能在顶会论文见到这样的争论实在难得啊(=・ω・=)</p><h2 id="WS2：文本的世界——当系统学会阅读"><a href="#WS2：文本的世界——当系统学会阅读" class="headerlink" title="WS2：文本的世界——当系统学会阅读"></a>WS2：文本的世界——当系统学会阅读</h2><p><img src="https://mmbiz.qpic.cn/mmbiz_png/IVz4ZzrQibgTFQmNH95NicHuicZqKFlMK2jRNV7jo1G5vbq3cQujPsicTRTdCk8XjrpRAG2ocpl76yvwV8jX2k0f3A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>不是N宝不愿意上网，是多年前她的小脑瓜实在是处理不了网络上这么多纷繁复杂的信息。然而，多亏了专做N宝大脑的黄老板（黄仁勋：？）和革新了N宝大脑的Transformer结构（谷歌：？），有了<strong>增强算力</strong>和<strong>模型加持</strong>的N宝终于开眼看到了更广阔的的世界——非结构的，多语言的，跨领域的，无标签的，单拿出哪个都很让人兴奋吧，但<strong>BERT全都要！</strong></p><p>以BERT为代表的基于Transformer的预训练语言模型在众多下游任务上的优异表现，在寥寥两三年时间里把NLP领域的排行榜屠了个遍。在我们为新诞生的预训练模型欢呼雀跃时，内心也难免会为它们越来越庞大的体积和“饭量”感到触目惊心。</p><p>从训练语料来说，2013年的word2vec使用了16亿个token，一年后的GloVe使用了8400亿个token，而BERT直接吃下了整个维基百科+一万多本书。从模型参数上来说，从2018年ELMo的到GPT-3的也不过只用了两年时间。</p><p>更重要的是，这类预训练模型的效果存在明显的<strong>边际效益递减</strong>：对于16年提出的词预测任务LAMBADA[1]，从15亿参数的GPT-2，到170亿参数的TuringNLG，提升甚微；到了1750亿参数的GPT-3终于有了8个点的提升，但背后的多出来的算力开销，<strong>它值吗？</strong></p><p>更重要的是，这类预训练模型很难解决许多更难的NLP任务、例如<strong>较难的共指解析</strong>（“我把车停在了那个小停车场，因为它足够[小/大]了。”）。之所以称之为“较难的”，是因为它们是经过精心选择的处于数据分布尾端的共指关系。如果N宝没停过车，她怎么会知道这个问题的答案不是从前半句里简单地提取出那个“小”字呢？解决这个问题的关键，在于<strong>经验</strong>。论文标题中的“Experience”，为未来可能的发展指明了方向。</p><p>这时，我们终于意识到，再怎么非结构多语言跨领域无标签的文本，也终究是文本；再往预训练语言模型砸嘛尼，也不一定能换来真正智能的N宝。N宝不缺文本了，她只是需要<strong>更系统地</strong>感知这个世界而已。</p><h2 id="WS3：视觉与声觉的世界——当系统学会感知"><a href="#WS3：视觉与声觉的世界——当系统学会感知" class="headerlink" title="WS3：视觉与声觉的世界——当系统学会感知"></a>WS3：视觉与声觉的世界——当系统学会感知</h2><p><img src="https://mmbiz.qpic.cn/mmbiz_png/IVz4ZzrQibgTFQmNH95NicHuicZqKFlMK2j9iae4QNCuxVcvz1OE9sR91XYic7Tq8EuInic5ySRza63Lbm08nszEnsxA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>N宝不再只是一头扎进书海里的书呆子了，她终于拥有了能看到世界的<strong>眼睛</strong>、听到世界的<strong>耳朵</strong>和触摸世界的<strong>双手</strong>，尽管眼睛耳朵和手也都是机器学习模型。但是，如果不看不听不碰的话，她怎么能理解“动如脱兔”、“噤若寒蝉”的真意，怎么体会到打工人钢铁般的意志（不）</p><p>这多出来的感知究竟是什么？是人类在进行决策时的<strong>多重依据</strong>，也是人们在认识世界时达成的<strong>共识</strong>，同时也是语言学证明的人类在学习语言时<strong>必需的外部输入</strong>。海伦·凯勒学习语言的故事脍炙人口，缺失视觉的辅助尚且如此，剥夺所有感官后，学到的语言还会是一样的吗？</p><p>文章引用了一种表示人类知识的方法：<strong>Frames and Scripts</strong>[2]。这种方法在上个世纪80年代被用来表示人类知识。通俗来讲，这一方法将人类世界的静态组成和动态动作流程利用类似于面向对象编程的方式进行建模：Frames利用类图构建事物之间的关联关系，而Scripts利用流程图构建一系列动作的发生过程。但即使成功表示了类别之间的关联关系，类别中的各个属性、流程图中的各个行为和条件依然没有和现实产生对应。大框架有了，细节却面临了同样的问题，因此，这种建模依然很片面。这恰恰说明了多模态对于理解知识的重要性。</p><p>既然是多模态，那自然要提及其中涉及的每个领域向多模态发展的努力。这其中，计算机视觉（CV）和自然语言处理（NLP）的结合自然是发展最多的一个。</p><p>计算机视觉领域已经提前意识到了与自然语言处理交互的重要性，并提出了一系列可以复用的模型，而计算机视觉领域也在近几年来开始解决<strong>视觉问答VQA</strong>、<strong>视觉推理</strong>和<strong>视频翻译</strong>等等CV+NLP的交互任务。这些多模态任务的标准数据集可以支持大规模<strong>视觉+文本</strong>、甚至<strong>视觉+文本+语音Transformer</strong>模型的训练。</p><p>NLP领域的发展同样支撑了多模态的应用，由于CV领域广泛采用的ImageNet[3]分类采用了WordNet[4]描述上下位词关系的层次分类，在加入了WordNet中每个概念的图像信息后，我们甚至可以在概念的<strong>向量表示</strong>中学习到仅利用文本<strong>无法获得的特征</strong>。比如，WordNet中“人”是一系列不同职业的上位词，其中包括“消防员”，“医生”等等；单纯凭借文本难以捕捉这些类别的区别；但在加入了“人”、“消防员”、“医生”的图片进行多模态学习后，我们可以利用像素级的掩码精确地获得不同类别的具体差异，甚至可以将自然语言描述拓展到从未见过的类别中，学习到新类别的特征…</p><p>这正是<strong>零次学习</strong>（Zero-shot learning）的想法，利用一段对未知类别的描述，让模型理解在训练过程中没有见过的类别的特征。对于文本的单一模态学习，用文本描述文本是WS1的想法；而多模态学习通过添加<strong>额外的感知方法</strong>，让零次学习的效果得到了大幅提升。那么问题来了，多模态之后，N宝又要做些什么呢？</p><h2 id="WS4：行为的世界——当系统学会试错"><a href="#WS4：行为的世界——当系统学会试错" class="headerlink" title="WS4：行为的世界——当系统学会试错"></a>WS4：行为的世界——当系统学会试错</h2><p><img src="https://mmbiz.qpic.cn/mmbiz_png/IVz4ZzrQibgTFQmNH95NicHuicZqKFlMK2jpMxFrhRRtrVuYibr5ibQj0ZFm0gcGvtZGDwmdkvRWlYIKibhiaPg7wLFGQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>N宝对世界观察了许久，她能读能看能听，我们感觉她好像理解了这个世界。但实际上，她对这个世界似懂非懂。</p><p>在她眼中，词语不过是一串数字或是像素组成的特征而已，每个名词概念到底隐含着什么<strong>内在属性</strong>，每个动作到底会带来什么<strong>影响</strong>，每个形容词到底描述了些什么<strong>特点</strong>，N宝都不懂。被动的学习已经满足不了她了，她想用她的感知去<strong>主动地</strong>理解语言背后的含义。当N宝有了行动的能力，她就有了具身，有了和外界<strong>互动</strong>的条件。</p><p>试想一下，对于“橘子更像是棒球还是香蕉？”这样的问题，你会作何回答？</p><p>WS1系统会认为橘子和香蕉经常出现在<strong>类似的上下文</strong>里，所以橘子和香蕉更像；WS2系统会认为橘子和棒球<strong>都是圆形的</strong>，但说不上来棒球和橘子的质地和大小；WS3系统会了解到橘子、棒球和香蕉的<strong>外表</strong>，所以同样会认为橘子和棒球更像，却说不清楚棒球、橘子和香蕉的软硬程度的重量。只有当系统能够接触到这些物体并产生<strong>互动</strong>时，它才会更加系统地回答，橘子和棒球具有相似的材质和重量，但橘子和香蕉具有相同的软硬程度和用途。</p><p>实际上，人类在学习知识时在不断地与外界产生互动并获得反馈，而这些<strong>持续的反馈</strong>构成了我们学习这个世界时的监督信号。这些信号甚至产生于我们学习语言之前，那么问题来了，这些婴儿时期产生的反馈究竟形成了什么呢？</p><p>对于人来说，这些反馈形成了我们的<strong>直觉和常识</strong>，而这些内容正是我们在日常交流时不会使用语言直接表述的<strong>隐含内容</strong>。对于机器来说，这些试错过程中得到的反馈形成的可能是“<strong>先于语言</strong>”<strong>的表示</strong>（pre-linguistic representations），它们可以被用来作为NLP系统泛化的基础。在语言学上已经证明，孩子从书本上学的东西很难被她们直接搬到现实生活中加以利用。我们利用大量的参数，希望用统计学的方法另辟蹊径地实现生物进化的成果，但缺少了与真实世界的交互，或许我们离这个目标确实遥远。</p><p>在WS4的世界，我们就需要借助<strong>机器人学</strong>领域的研究成果了。尽管从现在看，利用机器人学的成果远比利用CV的成果困难，但为了实现真正理解语言的目标，NLP的研究者应该同样关注机器人学的发展。随着动作空间的加大，NLP系统就能够学会更多的指令，让智能家居和智能机器人不再是现在这样仅靠指令集操作的机器，而是成为真正能应用在任何场景下真正的智能系统。</p><h2 id="WS5：社会的世界——当系统学会能动"><a href="#WS5：社会的世界——当系统学会能动" class="headerlink" title="WS5：社会的世界——当系统学会能动"></a>WS5：社会的世界——当系统学会能动</h2><p><img src="https://mmbiz.qpic.cn/mmbiz_png/IVz4ZzrQibgTFQmNH95NicHuicZqKFlMK2jlBfNm38POUY9VpWXialILeV1XxZtvWz92Yhb2qC88O0EfUpafOkC4iaA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>能动和能动并不是一个意思——WS4的能动是“会动”，而WS5的能动是“<strong>主观能动性</strong>”的能动。N宝的成长目标是要造福社会的，而人类社会的可是很复杂的。她要在与人打交道的过程中体现出她行为的<strong>目的性</strong>，让她真正能够实现人工智能系统的使命。到这个阶段，N宝就已经成为一个<strong>持久存在的，具有特定社会属性和经验的智能代理</strong>了。</p><p>NLP系统一直以来都是人工智能领域里最受关注的领域之一，毕竟图灵测试就是以对话系统为基础的测试。但是，在进行图灵测试时，人经常会受到<strong>框架效应</strong>（Frame effect）的影响：当聊天机器人表明自己以英语作为第二语言或是表现出弱势时，人自然会大幅降低对对方的期望，让原本真实性不高的回应也看起来像是真人一样。</p><p>那么，为什么说WS5对于语言学习至关重要呢？</p><p>首先，从说话者的角度，语言要<strong>产生作用</strong>。</p><p>从哲学上讲，语言的功能（Function）是含义的来源；从语言学上讲，基于使用的语言学习理论表明，<strong>有用的语言构建</strong>是一切的基础。这些理论在近年来开始关注语言在人类的起源和发展过程中起到的作用，表明了语言对于社会生活的重要性。</p><p>WS1-4逐步地扩展了语言含义的组成，逐渐地，语言可以由结果转变为起因，从单纯的数据转变为有用的信息。当下，NLP系统生成的语句只能以一种与社会隔离开的方式被被动的评价，而要做到衡量NLP系统对社会的影响，必须<strong>主动地</strong>让NLP系统参与到诸如<strong>谈判，合作，情感支持</strong>等等语言活动中来，让NLP系统能够推断人的情感状态和行为的社会效益。</p><p>当下的语言模型利用上下文构建每个词的释义。但实际上，一个词的含义需要被放在<strong>特定的语言和社会环境下</strong>进行综合考量。正比如，“大失所望”的词典意思是不令人满意，可是，只有在孩子学习语言时说出过或是听见过那句“你让我大失所望”时，她才能真正懂得这个词对人来说多么有分量。一个词的含义远不止词本身的意思：它最丰富的表达蕴含在了它<strong>对外界产生的影响</strong>之中。</p><p>其次，从聆听者的角度而言，语言要成为<strong>了解对方想法的工具</strong>。</p><p>“想法”并不局限于一句话本身的意思，而更多地指对方的需求，意图，感情，知识和身份。对“想法”的研究被称为“<strong>心智理论</strong>”（Theory of Mind）。这一理论被建模为<strong>讲者-听者模型</strong>（Speaker-listener model），从计算角度而言，又被进一步发展为“<strong>理性言语行为模型</strong>”[5]（Rational speech act model, RSA，一种基于贝叶斯推断的有效沟通建模）。</p><p>对交流的理解只用静态的数据集是远远不够的。对于同一个样本的标注，不同的标注者可能提供不同的标注方法，这就会引入伪关系和偏差。<strong>动态且灵活的评价</strong>可能会解决这个问题，但如何保持一个NLP系统的身份，如何面对外界可能带来的变化依旧需要进一步研究。</p><p>那么，怎样让NLP系统拥有能够<strong>在社会环境下</strong>理解语言的能力呢？</p><p>首先，如果单纯利用一个诸如神经网络的通用的函数拟合器来给文本做分类，它可能单纯利用了文本中的语法语义信息，却永远不会认为文本中出现的人、事物和因果关系是<strong>真实存在</strong>的。这需要我们向模型中引入足够的<strong>归纳偏置</strong>（Inductive bias）来解决这一问题。其次，基于交叉熵的损失函数使得NLP系统不够关注数据分布的尾端，导致<strong>出现较少的事件被忽视了</strong>。最后，由于现有的系统依然无法达成像人类一样的<strong>归纳能力</strong>，NLP系统的零次学习能力依然有待提高。因此，WS1-4的数据无论再大，以目前的系统设计也难以让NLP系统学到足够丰富的知识来降低模型的困惑度。</p><p>最后，从社会环境的角度而言，语言是用在<strong>人际交流</strong>中的，所以语言本身就携带着<strong>地位、身份、意图</strong>和其他一系列的变量，但我们当下所使用的基于众包的数据标签并没有考虑这一系列对社会生活至关重要的信息。所以，对于生成模型而言，为了考量模型与社会之间的交互性，需要给予模型一个<strong>社会地位及身份</strong>，将其置身于特定场景中来进行评价。</p><p>但是，社会交流中存在那么多变量，该怎么进行标注呢？我们需要跳出这个圈子：训练-验证-测试集的划分以及基于对比的评价方式限制了我们的想象力。我们的终极目标，是让NLP系统通过<strong>参与到社会当中</strong>进行学习，让用户与系统自由交流，使得系统在<strong>探索与试错</strong>中逐渐达成对其身份的<strong>社会语言学构建</strong>。当模型能够在测试过程中能够与人进行交互，我们便可以窥视到模型的决策边界，加深对模型的了解了。</p><h2 id="那么，要怎么进入下一个WS中呢？"><a href="#那么，要怎么进入下一个WS中呢？" class="headerlink" title="那么，要怎么进入下一个WS中呢？"></a>那么，要怎么进入下一个WS中呢？</h2><p>好问题~实际上，现在已经有很多研究在探索WS3-5的需求了。作者在文章中给出了4个这样的研究方向：</p><ul><li><strong>第二语言习得</strong>（Second language acquisition）：不同的国家虽然语言不同，却有着类似的社会模型，其中包括类似的物体指代（例如动物，水果…）和人的内在状态（例如快乐，饥饿…）。现有的研究已经开始向神经机器翻译模型引入这种相似性了：ACL’20的一篇论文[6]利用了WS3的图像信息作为增强双语对应关系的枢纽，未来会发展为利用WS4的模拟世界信息，以及最终走向WS5的真实世界信息。</li><li><strong>指代消解</strong>（Coreference resolution）和<strong>词义消歧</strong>（Word sense disambiguation）：无论是确定文本中代词对应的名词还是探究一个词在文本中的确切意思，都最终需要对心智理论的探索，通过对听者需求和经验的建模综合地完成任务，而非简单地通过文本寻找到与代词最接近的名词，或是用局部的文本信息确定词义。类似TextWorld[7]的WS4虚拟环境为进一步探索这两个问题提供了新的可能。</li><li><strong>新词学习</strong>（Novel word learning）：人对于物体的描述可能不仅局限于语言，有时还会加入肢体语言配合形容物体的形状或大小，这需要WS3系统进行多模态的感知；此外，在描述新的物体时，我们不仅会描述它的外观，还会描述它的功能，这需要WS4系统对动作和功能的认识。例如，在描述手风琴时，我们会说它“背着像吉他，但弹着像钢琴”。手风琴与吉他和钢琴的相似性仅体现在使用动作上，这种动作上的描述只有更高级的系统才能够认识。</li><li><strong>冒犯性语言</strong>（Personally charged language）：每个人都有自己不愿意听到的话。比如，“笨蛋”这个词对于不同的人有着不同的理解：有些人可能认为这样的说法是开玩笑，无伤大雅；但有些人会认为这是对自己努力的否定，从而受到伤害。只有当系统走向WS5，获得了社会交往的知识，才能明白在不同环境和条件下人的情感究竟如何。</li></ul><h2 id="看了这么多，这篇文章究竟想说什么？"><a href="#看了这么多，这篇文章究竟想说什么？" class="headerlink" title="看了这么多，这篇文章究竟想说什么？"></a>看了这么多，这篇文章究竟想说什么？</h2><p>作者王苏在阅读这篇几乎不包含任何数据和公式的文章时，体会到的吃力感完全不亚于任何一篇充斥着公式的文章。许多哲学和语言学概念在近年来很少被提及，甚至一部分概念根本查不到相关的中文翻译，只好结合维基百科和一些查得到的讲义来努力理解。这也难怪，毕竟这篇文章是众多领域大佬从NLP、CV、语言学、哲学和机器人学等等不同的角度为NLP的未来规划的前行路线。</p><p>然而，文中所说的许多东西虽然目前已经有工作开始了相关的探索。虽然诸如“具身”、“社会属性”等等名词看起来和现在的NLP社区不怎么沾边儿，而且这些名词实在是过于虚无缥缈，这也恰好给予了研究者充分的想象空间，让每一个目标得以用不同的方法实现。例如，WS4的“试错”概念和强化学习有着千丝万缕的联系，而WS5的社会属性又不由得让我们想起了微软亚研院致力于提升智商+情商的微软小冰[8]（小冰的论文对于研究对话系统的同学非常值得一读，大推荐）。</p><p>所以，在为越来越大的模型和计算开销发愁之余，换个角度来看看我们所在的领域，以大局观看看我们的发展阶段，思考思考踏入未来需要学习和发展什么样的技术，也许就能实现弯道超车呢~</p><p>要跟紧潮流鸭！加油吧，NLP人(= · ω ·=)</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 未来 </tag>
            
            <tag> 畅想 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可解释性 (interpretability)</title>
      <link href="/2020/11/18/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7-interpretability/"/>
      <url>/2020/11/18/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7-interpretability/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文是对TACL2019《Analysis Methods in Neural Language Processing: A Survey》的翻译和整理。</p></blockquote><p>随着模型精度的提高，可解释性却不断下降，下图形象的表明了这一点：</p><p><div align="center"> <img src="/images/blog/2020/interpretability.webp" alt></div></p><h3 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h3><p>NLP领域发展迅猛，其模型分析方法也正逐渐火热。为什么要研究NLP的可解释性？某种程度上，这一问题又落入更大的范畴——为什么要研究机器学习模型的可解释性？其支持者认为，在于增加ML系统的可靠性、可信度、公平性和安全性等。此外，知其所以然也有助于改进机器学习模型的效果。</p><p>而在NLP领域，这一问题需要联系其发展历程来解答：</p><p>早期的NLP工作通常涉及特征工程，这些手工特征的含义是易于理解的——形态特性，词法类别，句法类别，语义关系等等。理论上，人们可以通过观察模型对这些特征赋予的权重来更好地理解模型在做什么。而对于现在普遍的端到端的（比如，从词向量输入到文本分类的输出）神经网络模型，理解和分析它们的工作不是那么直观。但是，既然这些模型结构能够捕捉文本的特征，那么早期的关于语言学概念的分析工作也应当适用于现在的NLP研究。</p><p>本文尝试分析近年来NLP可解释领域的工作，概括当下的主流方向，并针对当前工作的不足提出一些可能的未来方向。本文将围绕以下几方面展开：</p><ul><li>语义特征学习</li><li>可视化方法</li><li>挑战集</li><li>对抗样本</li><li>解释模型预测</li><li>其他研究</li></ul><a id="more"></a><h2 id="1-语义特征学习"><a href="#1-语义特征学习" class="headerlink" title="1. 语义特征学习"></a>1. 语义特征学习</h2><p>现代NLP模型通常是端到端的，没有显式编码语义特征，所以人们很想知道模型到底都学到了什么。</p><h3 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h3><p>最常见的方法就是将模型的激活输出和语义特征分类进行关联。</p><p>即，固定一个训练好的模型（例如在机器翻译任务）的权重，将其用于编码语料并训练一个分类器做一些语义特征分类（例如词性分类等）。分类结果的好坏反映了模型学习表示的好坏。</p><p>以上这一方法具有多个名字，如“辅助分类任务”（auxiliary prediction tasks），“诊断分类器”（diagnostic classifiers），以及“探针实验”（probing tasks）等。目前已有的工作总结如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/GNpj5fw72EqiccKOqVfMxVaHMdiaIOIFpc1PT4Synn2yedUnW8gWGnDdABcdzrL0H4KoUkxH9sYfypuibHNhhDkAQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/GNpj5fw72EqiccKOqVfMxVaHMdiaIOIFpcaOIgRsk1OJnoVqVttqeDKrFXgoEwAN6dUxOMoOmw71B8UthS0m36Wg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>举一个神经机器翻译（NMT）任务的例子：</p><blockquote><p>Xing Shi, Inkit Padhi, and Kevin Knight. 2016b. Does String-Based Neural MT Learn Source Syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526–1534, Austin, Texas. Association for Computational Linguistics.</p></blockquote><p>首先训练两个模型：英译法和法译英模型，然后将模型中的编码器部分单独在语料上运行获得中间状态，并介入逻辑回归分类器用于不同语法特征分类。作者实验得出结论：NMT的编码器能够学习词语层面和句子层面重要的语法信息，并且进一步通过比较不同编码层的表示发现：在局部特征更多地体现在底层编码结果中，而高层的结构则学到句子的全局或者摘要的信息。</p><p>其他的寻找模型结构和语义特征关联的实验包括分析注意力权重和指代词分辨的关联，以及直接和间接计算模型激活层输出和语义特征的相关系数。</p><h3 id="语义特征"><a href="#语义特征" class="headerlink" title="语义特征"></a>语义特征</h3><p>上述实验涵盖了许多有用的语义特征，从基本特征如句子长度，词语位置，词语存在性，以及简单词序，形态学结构到语法和语义结构的特征，此外还有音节、口音和风格等语音特征等等。</p><p>尽管难以将这些分布广泛的工作加以整合一统，可以看出<strong>神经网络可以学到数量可观的语言学信息</strong>。对于频繁出现的特征，模型学习得很好；而对于少见的特征，对模型而言则较难学习。有工作发现LSTM可以捕捉到大部分情况的主语-动词的共现关系，而在更难的例子下需要一些直接的监督信息。</p><p>另一个逐渐浮现出来的主题是研究模型学习的文本表示的<strong>层次化性质</strong>。上文中提到的NMT模型的发现就能说明这一点，同时在RNN模型中也能看到语法信息出现在层次化的表示中。</p><p>此外，一些研究者（Williams等，2018；Maillard和Clark，2018）发现使用隐式树状结构训练的模型在自然语言推理（Natural Language Inference，NLI）任务中比使用语法标注树的模型表现要好。更进一步说，这些模型中的树并没有按照现有的语法理论构建语法树，这也对神经网络学习语法信息的重要性带来疑问。</p><h3 id="模型的相关结构"><a href="#模型的相关结构" class="headerlink" title="模型的相关结构"></a>模型的相关结构</h3><p>在这一领域，模型的部分结构被用于探究是否具有对语义信息的学习能力，包括：</p><ul><li>词向量</li><li>句向量</li><li>RNN的隐藏状态和门控激活输出</li><li>句子到句子模型的注意力权重</li></ul><p>对CNN模型的研究工作较少。在语音领域和多模态相关的工作等，请参考上面的表。</p><h3 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h3><p><strong>上述的分类方法可以证明模型可以捕捉一定的语义信息，然而这不一定能证明模型利用了这些信息进行推理</strong>。例如，Vanmassenhove等（2017）研究了NMT（以及基于短语的统计机器翻译）中的方面信息。他们训练了NMT句子编码向量的分类器，发现他们可以在90％的时间内准确预测时态。但是，在评估输出翻译时，他们发现只有79％的结果具有正确的时态。他们将此结果解释为“在解码过程中丢失了部分方面信息”。与此相关的是，Cífka和Bojar（2018）在翻译质量（BLEU）和表示质量（分类任务）方面比较了各种NMT模型的性能。他们发现两者之间存在负相关关系，这表明高质量的系统可能未在学习某些句子的含义。相反，Artetxe等（2018）指出，词嵌入包含不同的语言信息，可以通过对学习的嵌入进行线性变换来发现它。他们的结果提出了另一种解释，表明“嵌入模型能够对不同的语言信息进行编码，但对这些信息的表达方式有限制。”</p><p>从方法论的角度来看，大多数相关的分析工作都与互相关（correlation）有关：<strong>具有语言属性的神经网络组件之间的关联程度如何</strong>？可能缺乏一种<strong>因果关系</strong>的度量：语言属性的编码如何影响系统输出。Giulianelli等（2018）在这个问题上取得了一些进展。他们根据RNN隐藏状态和门控在不同时间步长预测了数字一致性。然后，他们根据预测和正确标签之间的差异，通过更改隐藏的激活来干预模型如何处理句子。这种提高的一致性预测准确性，并且效果在句子过程中持续存在，表明此信息对模型有影响。但是，他们没有报告对整体模型质量的影响，例如通过测量困惑度。因果推理（causal inference）的方法可能会为其中一些问题提供新的思路。</p><h2 id="2-可视化方法"><a href="#2-可视化方法" class="headerlink" title="2. 可视化方法"></a>2. 可视化方法</h2><p>可视化工具，不仅在语言领域，在其他领域都是是一类分析神经网络的重要工具。</p><ul><li>早期的工作可视化了语言建模任务训练的<strong>RNN中的隐藏单元激活输出</strong>，并观察到它们如何与某些语法关系（例如相关性）相对应（Elman，1991）。</li><li><strong>注意力机制</strong>一出来，人们很自然就想到将其可视化用做解释工具。注意力的可视化工作包括：NMT（Bahdanau等人，2014），NLI（Rocktäschel等人，2016; Yin等人，2016），摘要（Rush等人，2015），机器翻译的后编辑（Jauregi Unanue等人， 2018）等。</li><li>计算各种<strong>显著性度量</strong>（saliency mearures）以将预测归因于输入要素，然后可以在选定的示例中可视化重要或显著特征（Li等人，2016a; Aubakirova和Bansal，2016; Sundararajan等人，2017; Arras等人，2017a，b; Ding等人，2017; Murdoch等人，2018; Mudrakarta等人，2018; Montavon等人，2018; Godin等人，2018）。显著性也可以针对中间值而不是输入特征进行计算（Ghaeini等人，2018）。</li></ul><blockquote><p>下图是NMT的attention可视化工作的一个实例。说到注意力可视化，我读到有工作（《Attention Interpretability Across NLP Tasks》）对比了不同任务的attention weights，得出一个有趣的结论：在single sentence任务（如句子分类）的attention可解释性不好，而在sentence-pair任务（如翻译和推理）上attention是有效的机制，其可解释性也和模型效果正相关。</p></blockquote><p><img src="https://mmbiz.qpic.cn/mmbiz_png/GNpj5fw72EqiccKOqVfMxVaHMdiaIOIFpcbV4nlTytz3tmMiaWYT8NnOnnibcohj9qMHPviafCNE9ngoursyhCvSnXw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>一种有启发性的可视化技术是对神经网络激活进行聚类，并将它们与某种语言特性进行比较。早期的工作将RNN激活聚集在一起，表明它们按词汇类别进行组织（Elman，1989，1990）。其他人也遵循了类似的技术。</p><p>同时本领域也出现了一些用于可视化神经网络的在线工具：</p><ul><li><strong>LSTMVis</strong>（Strobelt等人，2018b）可视化RNN激活，重点是跟踪隐藏状态动态。</li><li><strong>Seq2Seq-Vis</strong>（Strobelt等，2018a）可视化基于注意力的seq2seq模型中的不同模块，目的是检查模型决策和测试替代决策。</li><li><strong>Rikters</strong>（2018）提出了另一个专注于比较注意力比对的工具。它还根据注意权重的分布提供翻译置信度评分。</li><li><strong>NeuroX</strong>（Dalvi等，2019b）是一种用于发现和分析单个神经元的工具，专注于机器翻译。</li></ul><h3 id="评估方式"><a href="#评估方式" class="headerlink" title="评估方式"></a>评估方式</h3><p>如同在可解释性方面的许多工作中一样，<strong>评估可视化质量非常困难，并且通常仅限于定性示例</strong>。</p><p>Singh等（2018）展示了人类通过两种解释方法生成的输入词的层次聚类，并要求他们评估哪种方法更准确，或者他们更信任哪种方法。其他人报告了在对话建模（Freeman等人，2018）和医疗代码预测任务（Mullenbach等人，2018）中对注意力可视化进行的人类评估。</p><p>上述开放源代码工具的可用性有望鼓励用户在其常规研发周期中利用可视化。然而其可视化效果如何仍有待观察。</p><h2 id="3-挑战集"><a href="#3-挑战集" class="headerlink" title="3. 挑战集"></a>3. 挑战集</h2><p>NLP中的大多数基准数据集均来自文本语料库，反映了语言现象的自然频率分布。尽管在实践中对于平均情况下评估系统性能很有用，但此类数据集可能无法捕获更广泛的现象。</p><p>另一种评估框架由挑战集构成，也称为测试套件（test suites），已经在NLP中使用了很长时间（Lehmann等，1996），尤其是评估机器翻译系统（King和Falkedal，1990；Isahara，1995；Koh等，2001）。</p><p>Lehmann等（1996年）指出了测试套件的几个关键特性：<strong>系统性，对数据的控制，否定数据的包含以及穷举性</strong>。</p><blockquote><p>这一领域因为统计NLP系统的大规模量化评估方法的流行而沉寂了一段时间。</p></blockquote><p>挑战数据集可以按照任务，语言现象，语言，规模，构造方法，以及评估方法等标准进行分类，如下表：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/GNpj5fw72EqiccKOqVfMxVaHMdiaIOIFpcjyyJPH0NA0gO0cBCz8XoicKGg7wLUK8v98sxfcd1wl5yUJN94gyFOeA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h3 id="按任务分类"><a href="#按任务分类" class="headerlink" title="按任务分类"></a>按任务分类</h3><p>挑战集中针对最多的任务是自然语言推断（NLI）和机器翻译（MT）：一方面这些任务的模型多，另一方面这些任务涉及各种语言水平；而其他一些高级任务（如阅读理解或问题解答）并未得到足够的重视，它们也可能从精心设计的挑战集中获益。</p><p><strong>挑战集主要的工作旨在通过将嵌入模型在单词或句子对上计算的相似性，与人类相似性判断相关联，来评估嵌入模型的质量</strong>。</p><p>包含此类相似性评分的数据集通常用于评估单词嵌入（Finkelstein等，2002；Bruni等，2012；Hill等，2015）或句子嵌入，这些数据集中有许多在粗糙的级别上评估相似性。还有一些数据集提供了更细粒度的评估：例如，一些数据集专用于特定词类，如动词（Gerz等，2016）或稀有词（Luong等，2013），或评估句子嵌入中的成分知识（Marelli等，2014）等；还有数据集收集了多语言和跨语言版本（Leviant和Reichart，2015年; Cer等人，2017年）。</p><p>尽管这些数据集被广泛使用，但这种评估由于其主观性以及与下游绩效的可疑相关性而受到批评（Faruqui et al，2016）。</p><h3 id="按语言现象分类"><a href="#按语言现象分类" class="headerlink" title="按语言现象分类"></a>按语言现象分类</h3><p>挑战集的主要目标之一是<strong>评估模型处理特定语言现象的能力</strong>。</p><p>早期的研究强调穷举性（Cooper等，1996；Lehmann等，1996），而最近的研究则倾向于关注一些感兴趣的特性：例如，Sennrich（2017）为MT评估引入了一个挑战集，该挑战集着重于5个属性：主语一致，名词短语一致，动词结构，极性和音译。对形态学的MT挑战集略为详细阐述，包括14种形态学特征（Burlot和Yvon，2017年）。其他挑战集涵盖了更多种语言属性：例如，在Cooper等人中扩展类别（1996年），针对NLI的GLUE分析集涵盖了四个粗略类别（词汇语义，谓语-自变量结构，逻辑和知识）中的30多种现象。在MT评估中，Burchardt等人（2017）使用覆盖120种现象的大型测试套件报告了结果，部分基于Lehmann等（1996），Isabelle等（2017）和Isabelle and Kuhn（2018）为MT评估准备了挑战集，涵盖了词素句法，句法和词法层面的细粒度现象。</p><p>通常，以编程方式构建的数据集倾向于涵盖较少的细粒度语言属性，而手动构建的数据集则表示更多种现象。</p><h3 id="按语言分类"><a href="#按语言分类" class="headerlink" title="按语言分类"></a>按语言分类</h3><p>不幸的是，在许多NLP工作中，大多数挑战集都是英语。这种情况在MT评估中要好一些，因为自然而然所有数据集都具有其他语言。显然，非英语语言在MT任务还有更多的发展空间。但是，也许在其他任务上更迫切的需要大型非英语数据集来开发流行的NLP任务的神经模型。</p><h3 id="按规模分类"><a href="#按规模分类" class="headerlink" title="按规模分类"></a>按规模分类</h3><p>现有的挑战集的大小差异很大：手工构建的数据集较小，典型大小为数百个，少见上万；自动构建的数据集要大得多，范围从数千到近十万（Sennrich，2017），甚至超过一百万个样本（Linzen等人，2016）。在后一种情况下，作者认为需要如此大的测试集才能获得足够的罕见情况表示。</p><h3 id="按构造方法分类"><a href="#按构造方法分类" class="headerlink" title="按构造方法分类"></a>按构造方法分类</h3><p>挑战集一般要么由编程方式创建，要么通过手工制作特定示例以手动创建。</p><p>通常，半自动方法用于编译示例的初始列表，这些示例由注释者手动验证。具体方法还影响语言使用的种类以及示例的自然性或人工/综合性。目前主流方案如下：</p><ul><li><strong>通过修改或从现有数据集中提取示例来构造数据集</strong>。例如，Sanchez等（2018）和Glockner等人（2018年）从SNLI中提取了示例（Bowman等人，2015年），并替换了特定词（如上位词，同义词和反义词），然后进行了手动验证。Linzen等（2016）使用启发式方法从原始文本中提取了主语-动词一致的示例，从而形成了一个大型数据集。Gulordava等（2018）将其扩展到其他协议现象，但他们依赖于树库中可用的句法信息，从而导致数据集更小。</li><li>一些挑战集<strong>利用现有的测试套件作为示例的直接来源</strong>（Burchardt等，2017）或<strong>搜索相似的自然存在的示例</strong>（Wang等，2018a）。</li><li>Sennrich（2017）引入了一种<strong>通过对比翻译对评估NMT系统</strong>的方法，模型估计反映特定语言特性的两种候选翻译的概率。Sennrich通过应用简单的试探法（例如更改性别和数字以引起前后不一致）以编程方式生成了此类对，从而得到了接近10万个示例的大规模挑战集。作者扩展了该框架以评估其他属性，但通常需要更复杂的生成方法，例如使用形态分析仪/生成器（Burlot和Yvon，2017）或更多人工参与生成（Bawden等，2018）或验证（Rios Gonzales等，2017）。</li><li>最后，一些研究<strong>定义了模板用于捕获了某些语言属性并使用单词列表将其实例化</strong>（Dasgupta等，2018；Rudinger等，2018；Zhao等，2018a）。基于模板的生成具有提供更多控制（例如用于获得特定词汇分布）的优势，但这是以牺牲示例的自然程度为代价的。</li></ul><h3 id="按评估方式分类"><a href="#按评估方式分类" class="headerlink" title="按评估方式分类"></a>按评估方式分类</h3><p>通常，评估模型用其在挑战集示例上的性能来评估，要么使用与第一阶段用于评估系统相同的度量标准，要么通过代理进行评估，如Sennrich（2017）的对比对评估中那样。</p><p>自动评估指标的代价较低，可以大规模计算。但是，可能会缺少某些方面的效果。因此，一些研究报告了人类对其挑战集的评估，例如在MT中（Isabelle等，2017; Burchardt等，2017）。</p><p>此外，根据模型在挑战集上的表现来判断模型的质量可能很棘手。一些作者强调，他们希望在“超出正常运行能力”的极端或困难情况下测试系统（Naik等人，2018）。但是，是否应该期望系统在特殊选择的情况下（相对于一般情况）表现良好，可能取决于目标：为了更好地看待结果，可以将同一任务的模型绩效与人类绩效进行比较（Gulordava等人，2018）。</p><h2 id="4-对抗样本"><a href="#4-对抗样本" class="headerlink" title="4. 对抗样本"></a>4. 对抗样本</h2><p>要了解模型，还需要了解其失败。尽管机器学习系统在许多任务上都取得了成功，但它们也对恶意攻击或对抗性示例非常敏感（Szegedy等，2014；Goodfellow等，2015）。在视觉领域，即使人类无法分辨输入图像的微小变化，也可能导致分类错误。</p><p>对抗样本的目标在于，对给定的模型 <em>f</em> 和输入样本 x，找到样本 x’，它被模型预测为不同的类别，同时保持和原始样本具有最小的距离：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/GNpj5fw72EqiccKOqVfMxVaHMdiaIOIFpcgicuwGbxaMapMxPFYIcmjmicFKZSQRwt7iat2n0ibeG9SjNno16j3ArNJQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>在CV领域，样本改变通常是图像像素，计算两个图片向量的差值很简单，因为数值连续所以可以通过计算相对输入的梯度来计算得到改变后的样本。</p><p>在NLP领域，输入是离散的（例如单词序列），这带来了两个问题：</p><ul><li>如何测量原始示例和对抗示例 x 和 x’ 之间的距离？</li><li>如何将最小化这个距离表述为优化问题？（因为这需要计算改变相对于离散输入的梯度）</li></ul><p>本文按照对抗是否可以已知模型、攻击的目的性、修改单位、攻击的任务对对抗方法进行分类，如下表：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/GNpj5fw72EqiccKOqVfMxVaHMdiaIOIFpcRqks0M0Chm5E8KYiaolADIYzT0ZibA1rxQgiaGdiaiaJNMLQneeSC0D1HMA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h3 id="按对抗是否已知模型分类"><a href="#按对抗是否已知模型分类" class="headerlink" title="按对抗是否已知模型分类"></a>按对抗是否已知模型分类</h3><p>对抗模型可以通过访问模型参数（也称为白盒攻击）来生成对抗性示例，也可以通过使用黑盒攻击来获得对抗性示例（Papernot等，2016a，2017；Narodytska and Kasiviswanathan，2017；Liu等，2017） 。</p><p><strong>白盒攻击</strong>很难适应文本世界，因为它们通常需要对输入计算梯度，在文本情况下可能是离散的。包括如下方法：</p><ul><li><strong>对输入词的嵌入计算梯度，并扰动嵌入</strong>。由于这可能会导致一个不对应任何单词的向量，因此人们可以搜索嵌入给定词典中的最近邻单词（Papernot等人，2016b）；Cheng等（2018）将这个想法扩展到seq2seq模型。</li><li><strong>对输入词的嵌入计算梯度，以识别和排序要修改的词</strong>（Samanta和Mehta，2017；Liang等，2018）。Ebrahimi等（2018b）开发了一种替代方法，该方法通过在向量空间中表示文本编辑操作（例如，一个二进制向量来指定单词中的哪些字符将被更改），并沿着该向量用导数来近似损失的变化。</li></ul><p>考虑到为文本生成白盒对抗性示例的困难性，许多研究都致力于黑盒示例。主要工作如下：</p><ul><li><strong>受文本编辑启发</strong>（自然的或由人类普遍产生的，例如错别字，拼写错误等）（Sakaguchi等人，2017；Heigold等人，2018；Belinkov和Bisk，2018）。Gao等（2018）定义了评分函数以识别要修改的词语。函数不需要访问模型内部，但是需要模型预测分数。在识别了重要标记之后，它们使用常见的编辑操作来修改字符。</li><li>Zhao等（2018c）使用<strong>生成对抗网络</strong>（GAN）（Goodfellow et al，2014）来最小化输入的潜在表示和对抗性示例之间的距离，<strong>并在潜在空间中进行扰动</strong>。由于潜在表示不需要来自受攻击的模型，因此这是黑盒攻击。</li><li>Alzantot等（2018年）开发了一种有趣的<strong>基于人口的遗传算法</strong>，可通过保留原始句子的修饰语群体并评估每一代的修饰语适合度来制作文本分类的对抗性示例。他们不需要访问模型参数，但是会使用预测分数。Kuleshov等人提出了类似的想法（2018）。</li></ul><h3 id="按攻击的目的性分类"><a href="#按攻击的目的性分类" class="headerlink" title="按攻击的目的性分类"></a>按攻击的目的性分类</h3><p>对抗性攻击可以分为定向攻击和非定向攻击（Yuan等人，2017）。</p><p>其中，定向攻击指定了特定的错误类别 <img src="https://mmbiz.qpic.cn/mmbiz_svg/Ib5852jAyb82RBBvOrn3mh8z9ibBibZ1Mh3DPibCAh2zgdXYVCXvaySlvHLgibGujNBVJQv3t5M2icicicrLgFTN6ZOGSx1WicUlwYXF/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"> ，而非目标攻击仅关心预测的类别是错误的， <img src="https://mmbiz.qpic.cn/mmbiz_svg/Ib5852jAyb82RBBvOrn3mh8z9ibBibZ1MhSJUgbQeBnJafzqv7D54GETp9EIsuGSm1oQ61aicaO3mx0J8fe7cdibbia5a4URBF6DD/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"> 。定向攻击很难生成，因为它们通常需要了解模型参数，即，它们是白盒攻击。大多数对抗示例都是非针对性的，而一些针对性的攻击包括Liang等 （2018a）和Chen等（2018a），后者指定了在图像字幕模型中生成的单词或字幕。其他定向攻击模型则在攻击seq2seq模型时将特定单词指定为省略，替换或包含（Cheng等人，2018; Ebrahimi等人，2018a）。</p><p>在NLP中产生定向攻击的方法可能会从其他领域的对抗性攻击中获得更多启发。例如，在攻击恶意软件检测系统中，多项研究在黑盒情况下开发了针对性攻击（Yuan等人，2017）。Zhao等人提出了针对MT的黑匣子针对性攻击（2018c），他使用对抗性正则化自动编码器将句子映射到连续空间后，使用GAN来搜索对Google MT系统的攻击（Zhao等人，2018b）。</p><h3 id="按修改的单位分类"><a href="#按修改的单位分类" class="headerlink" title="按修改的单位分类"></a>按修改的单位分类</h3><p>对抗文本示例的大部分工作都涉及字符和/或单词级别的修改，其他转换包括添加句子或文本块（Jia and Liang，2017）或生成具有所需句法结构的复述（Iyyer等人，2018）。在图像字幕中，Chen等人（2018a）在输入图像中修改像素以对字幕文本产生针对性的攻击。</p><h3 id="按对抗任务分类"><a href="#按对抗任务分类" class="headerlink" title="按对抗任务分类"></a>按对抗任务分类</h3><p>通常，大多数在NLP中对抗示例的工作都集中在相对高级的语言理解任务上，例如文本分类（包括情感分析）和阅读理解，而文本生成的工作主要集中在MT。</p><p>除了形态标记（Heigold等人，2018）和拼写校正（Sakaguchi等人，2017）等少数工作，对抗样本却很少针对更底层的语言处理任务。</p><h3 id="对抗样本的连贯性和改变量的评估方式"><a href="#对抗样本的连贯性和改变量的评估方式" class="headerlink" title="对抗样本的连贯性和改变量的评估方式"></a>对抗样本的连贯性和改变量的评估方式</h3><p>在对抗图像样本中，通过测量像素空间中的距离来测量扰动是相当简单的，可以使用某些正则化方法，或采用与人类感知更好相关的替代措施（Rozsa等，2016）。呈现对抗图像与源图像之间没有明显差异的视觉效果也令人信服。</p><p>而在文本域中，距离的测量不是那么简单，人类甚至可以感觉到文本的很小变化。因此，对攻击的评估相当棘手。一些研究对对抗样本施加了限制，使其具有少量的编辑操作（Gao等人，2018）。其他人则以不同的方式确保句法或语义上的连贯性，例如通过单词相似性或句子相似性过滤替换（Alzantot等人，2018；Kuleshov等人，2018），或使用同义词和其他单词表（Samanta和Mehta，2017；Yang等，2018）。</p><p>一些人报告了人类是否可以正确地将对抗性示例分类（Yang等人，2018），但这并未表明变化的可感知性。更多信息丰富的人研究评估了对抗性示例与原始示例的语法相似性或相似性（Zhao等人，2018c；Alzantot等人，2018）。考虑到在文本中产生难以察觉的变化的固有困难，需要更多这样的评估。</p><h2 id="5-解释模型预测"><a href="#5-解释模型预测" class="headerlink" title="5. 解释模型预测"></a>5. 解释模型预测</h2><blockquote><p>这一部分是关于Explainability，和Interpretability有些微妙的区别。</p></blockquote><p>解释特定的预测被认为是可解释工作的一个迫切的痛点（Lipton，2016年），它被认为增加了机器学习系统的可靠性（Doshi-Velez等人，2017年）。但是，解释为什么深度非常深，高度非线性的神经网络做出一定的预测并非易事。</p><ul><li>一种解决方案是<strong>要求模型生成与主要预测一起的解释</strong>（Zaidan等，2007；Zhang等，2016），但是这种方法需要人工注释解释，这可能很难收集。</li><li>一种替代方法是<strong>使用部分输入作为解释</strong>。例如，Lei等（2016）定义了一个生成器，该生成器学习文本片段上的分布作为证明预测的合理依据，并根据情感分析进行评估。Alvarez-Melis和Jaakkola（2017）通过扰动输入并找到最相关的关联，在序列到序列的学习场景中发现了输入-输出关联。Gupta和Schütze（2018）研究了如何在RNN中积累信息以进行预测，以及预测分数与重要输入段的关联峰值。</li><li>由于这些方法使用输入段来解释预测，因此它们对网络中发生的内部计算没有太大的启发。</li></ul><p>目前，尽管这一块工作对可解释领域整体具有公认的重要性，我们在NLP中解释神经网络的预测的能力仍然有限。</p><h2 id="6-其他研究"><a href="#6-其他研究" class="headerlink" title="6. 其他研究"></a>6. 其他研究</h2><p>这里包含一些杂项。</p><ul><li><strong>擦除或者隐藏部分神经网络组件</strong>（例如词嵌入的部分维度，隐层单元，甚至整个词）的效果对比实验（Li等人，2016b; Feng等人，2018; Khandelwal等人，2018 ; Bau等人，2018）。Li等（2016b）消除了单词嵌入或隐藏状态下的特定维度，并计算了分配给不同标签的概率的变化。他们的实验揭示了词嵌入模型之间的有趣差异，在某些模型中，信息更多地集中在部分维度上。他们还发现，信息在隐藏层中的分布比在输入层中的分布多，并且在情感分析任务中删除了整个单词以找到那些重要的单词。</li><li><strong>通过定义插入任务来解释词嵌入</strong>，其中人类需要根据词嵌入维度的差异来选择插入词（Murphy等人，2012; Fyshe等人，2015; Faruqui等人，2015）。在这种工作中，如果人类能够更好地识别出插入词，那么词嵌入模型可能被认为更具解释性。由于高维表示的评估成本高昂，因此考虑了其他自动评估方式（Park等人，2017；Senel等人，2018）。</li><li>关于神经网络的一项悠久传统是<strong>评估和分析其学习不同形式语言的能力</strong>（Das等，1992; Casey，1996; Gers和Schmidhuber，2001;Bodén和Wiles，2002; Chalup和Blair，2003）。。这种趋势一直持续到今天，包括对现代结构及其可学习的形式语言的研究（Weiss等人，2018; Bernardy，2018; Suzgun等人，2019），或它们拥有的形式属性（Chen等人，2018b）。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>分析神经网络已成为NLP研究的热门话题。我们已经强调了特定于语言的分析方面：</p><ul><li>在神经网络中捕获了哪些语言信息，它们成功捕获了哪些现象以及在哪里失败了。</li><li>许多分析方法是大型机器学习社区的通用技术，例如通过显着性度量进行可视化或通过对抗性示例进行评估；但是即使是那些有时也需要非凡的改编才能与文本输入一起使用。</li><li>一些方法更特定于该领域，但在其他领域可能被证明是有用的。挑战集或测试套件就是这种情况。</li></ul><p>在整个调查过程中，我们确定了当前分析工作中的一些局限性或不足之处：</p><ul><li>使用辅助分类任务来识别神经网络捕获的语言属性已成为标准做法，但是缺乏其与原始任务之间联系的理论基础，和更好的经验性考虑。</li><li>分析工作的评估通常是有限的或定性的，尤其是在可视化技术方面。为了确定不同方法的成功，需要更新的评估形式。</li><li>除了提供可视化之外，在解释神经网络模型的预测方面所做的工作相对较少。随着公众对解释机器学习系统中算法选择的需求不断增加（Doshi-Velez和Kim，2017年; Doshi-Velez等人，2017年），迫切需要在这一方向上取得进展。</li><li>大多数分析工作都集中在英语上，特别是在为各种任务构建挑战集时，但MT由于其固有的多语言特性而除外。随着领域的发展和成熟，开发其他语言的资源和评估方法很重要。</li><li>除NLI和MT外，还需要更多挑战集来评估其他任务。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Interpretability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对偶图</title>
      <link href="/2020/11/17/%E5%AF%B9%E5%81%B6%E5%9B%BE/"/>
      <url>/2020/11/17/%E5%AF%B9%E5%81%B6%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<p>今天组会上偶然听到“对偶图”这个词，就再次回顾一下相关的定义~</p><p>一个图G=(V,E)，若能将其画在平面上，且任意两条边的交点只能是G的顶点，则称G可嵌入平面，或称G是可平面的。可平面图在平面上的一个嵌入称为一个平面图。</p><p>由平面图的边包围而成，其中不含图的顶点。也称为面。包围面R的所有边组成的回路称为该面的边界，回路长度称为该面的度，记为deg(R)。具有相同边界的面称为相邻面。由平面图的边包围且无穷大的面称为外部面。一个平面图有且只有一个外部面。</p><p>利用欧拉公式和数学归纳法可以证明平面图G的所有面的度之和等于其边数|E|的2倍。</p><p>下面我们引入对偶图，设有平面图G=(V, E)，满足下列条件的图G’= (V’, E’) 称为图G的对偶图：G的任一面$R_i$对应且仅对应一点$V_i^{‘}$；对G的面$R_i$和$R_j$的共同边$E_k$，画一条边$E_k^{‘} = (V_i^{‘}, V_j^{‘})$且只与$E_k$交于一点；若$E_k$完全处于某个面$R_i$中，则$V_i^{‘}$有一自环$E_k^{‘}$，如下图G’是G的对偶图：</p><p><div align="center"> <img src="/images/blog/2020/dual.png" alt></div></p><p>对偶图可以应用到<strong>最大流</strong>中，具体的以后再看吧~</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithms </tag>
            
            <tag> Dual Graph </tag>
            
            <tag> Graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于NYT(Riedel et al. 2010)数据集</title>
      <link href="/2020/11/17/%E5%85%B3%E4%BA%8ENYT-Riedel-et-al-2010-%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
      <url>/2020/11/17/%E5%85%B3%E4%BA%8ENYT-Riedel-et-al-2010-%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p>NYT10 (Riedel et al. 2010) 是远程监督关系抽取必不可少的数据集，对于数据集的一些疑问以及探索，在这里记录，也供大家参考🤭</p><h1 id="1-OpenNRE提供的数据集和Riedel提供的数据集是否一致？"><a href="#1-OpenNRE提供的数据集和Riedel提供的数据集是否一致？" class="headerlink" title="1. OpenNRE提供的数据集和Riedel提供的数据集是否一致？"></a>1. OpenNRE提供的数据集和Riedel提供的数据集是否一致？</h1><p>Riedel原始数据集在这里：<a href="http://iesl.cs.umass.edu/riedel/ecml/" target="_blank" rel="noopener">http://iesl.cs.umass.edu/riedel/ecml/</a></p><p>采用的是protocol buffer的形式，不能直接阅读，需要自己用工具处理成text形式。</p><p>OpenNRE处理后的数据集：<a href="https://github.com/thunlp/OpenNRE/tree/master/benchmark" target="_blank" rel="noopener">https://github.com/thunlp/OpenNRE/tree/master/benchmark</a></p><p>这两个数据集有没有什么不一致呢？清华在处理的时候有没有加一些特殊的操作呢？这是我一直疑问的一个点，因为要开展实验，必须确保数据集的一致性，才可以和前人的工作进行比较。</p><p><a href="https://github.com/thunlp/OpenNRE/issues/17" target="_blank" rel="noopener">https://github.com/thunlp/OpenNRE/issues/17</a></p><blockquote><p>train.txt and test.txt are labeled by distant supervision. As for details, you can refer to the issue <a href="https://github.com/thunlp/OpenNRE/issues/15" target="_blank" rel="noopener">#15</a><br>The train/test data are exactly same with the data in (Riedel et al. 2010) .<br>Because test data is smaller than train and the unbalance of relation truth numbers exists, the total number of relations in test.txt is smaller than train.txt</p></blockquote><p>OpenNRE提供的数据和原始数据是<strong>完全一致</strong>的。👍 <a id="more"></a></p><h1 id="2-NYT关系类型探究"><a href="#2-NYT关系类型探究" class="headerlink" title="2. NYT关系类型探究"></a>2. NYT关系类型探究</h1><h2 id="2-1-层级关系"><a href="#2-1-层级关系" class="headerlink" title="2.1 层级关系"></a>2.1 层级关系</h2><p>NYT关系类型天然存在层级，如/location/country/capital, /location/province/capital等</p><h2 id="2-2-重复关系"><a href="#2-2-重复关系" class="headerlink" title="2.2 重复关系"></a>2.2 重复关系</h2><p>NYT是由远程监督生成，噪音是其最大的问题，关系类型中就有很多重复。</p><p>/location/de_state/capital [德国: 州]</p><p>/location/fr_region/capital [法国: 地区]</p><p>/location/in_state/administrative_capital [印度: 邦]</p><p>/location/in_state/judicial_capital [印度: 邦]</p><p>/location/in_state/legislative_capital [印度: 邦]</p><p>/location/it_region/capital [意大利: 区]</p><p>/location/jp_prefecture/capital [日本: 县]</p><p>/location/br_state/capital [巴西: 州]</p><p>/location/cn_province/capital [中国: 省]</p><p>/location/mx_state/capital [墨西哥: 州]</p><p>/location/province/capital [加拿大: 省]</p><p>/location/us_state/capital [美国: 州]</p><p>表达的都是同一种关系类型……</p><h1 id="3-如何绘制PR曲线？"><a href="#3-如何绘制PR曲线？" class="headerlink" title="3. 如何绘制PR曲线？"></a>3. 如何绘制PR曲线？</h1><p>这里主要学习了：<a href="https://github.com/Spico197/NYT-H/issues/1" target="_blank" rel="noopener">https://github.com/Spico197/NYT-H/issues/1</a></p><blockquote><p>抱歉代码里可能有点乱，后面会重构一下。这其实是个PRC绘制的坑，PRC是有很多种绘制方法的。我在编码的时候也是边尝试边写的，所以包含了一些没有删掉的过程。PRC根据是否包含NA关系以及是否包含所有的预测输出，可以分为4种。</p><ul><li>不包含NA，取最大预测输出：修改<code>evaluate.py</code>中第166行的<code>for</code>语句，改为最大概率/logits位置的结果。</li><li>不包含NA，取全部预测输出：我们最终在论文中汇报的是OpenNRE绘制方案的结果，即DSGT上的PRC：<code>prc_opennre.json</code>和MAGT上的PRC：<code>b_prc_opennre.json</code>。如README中所述，这种PRC的绘制方案是将所有logits/概率 进行排序（假如有N个instance，每个instance预测输出21个非NA关系的概率，那么就会有N*21个点）。</li><li>包含NA，取最大预测输出：<code>prc.json</code>文件中的PRC结果是对每个instance的输出结果，只取最大logits/概率 进行排序，最终只有N个点（DSGT）。论文中未汇报。</li><li>包含NA，取全部预测输出：去除<code>evaluate.py</code>文件第167行的<code>if</code>条件。</li></ul><p>其它的几个文件：</p><ul><li><code>prc_bag2sent.json</code>：bag2sent track的DSGT PRC结果，论文中未汇报。</li><li><code>prc_skprc_mine.json</code>：早期使用sklearn求解PRC的一个尝试，需要将多分类预测结果先转换为二分类结果（对/错），论文中未汇报。</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Relation Extraction </tag>
            
            <tag> NYT </tag>
            
            <tag> Distant Supervision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对比学习 (Contrastive Learning) 相关进展梳理</title>
      <link href="/2020/11/14/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-Contrastive-Learning-%E7%9B%B8%E5%85%B3%E8%BF%9B%E5%B1%95%E6%A2%B3%E7%90%86/"/>
      <url>/2020/11/14/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-Contrastive-Learning-%E7%9B%B8%E5%85%B3%E8%BF%9B%E5%B1%95%E6%A2%B3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文转载自：</p><p><a href="https://zhuanlan.zhihu.com/p/141141365" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/141141365</a></p></blockquote><p><br></p><p>最近深度学习两巨头 Bengio 和 LeCun 在 ICLR 2020 上点名 Self-Supervised Learning（SSL，自监督学习） 是 AI 的未来，而其的代表的 Framework 便是 Contrastive Learning（CL，对比学习）。 另一巨头 Hinton 和 Kaiming 两尊大神也在这问题上隔空过招，MoCo、SimCLR、MoCo V2 打得火热，这和 BERT 之后，各大公司出 XL-Net、RoBerta 刷榜的场景何其相似。本篇文章，将会从对比学习的背后的直觉原理出发，介绍其框架，并且对目前的一些相关的工作进行简要介绍，希望能够为感兴趣的同学提供一些帮助。</p><h2 id="Motivation-amp-Framework"><a href="#Motivation-amp-Framework" class="headerlink" title="Motivation &amp; Framework"></a><strong>Motivation &amp; Framework</strong></h2><p>很多研究者认为，深度学习的本质就是做两件事情：<strong>Representation Learning（表示学习）和 Inductive Bias Learning（归纳偏好学习）</strong>。目前的一个趋势就是，学好了样本的表示，在一些不涉及逻辑、推理等的问题上，例如判断句子的情感极性、识别图像中有哪些东西，AI 系统都可以完成非常不错；而涉及到更高层的语义、组合逻辑，则需要设计一些过程来辅助 AI 系统去分解复杂的任务，<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1904.12584" target="_blank" rel="noopener">ICLR 19</a> 的一篇 oral 就是做的类似的事情。因为归纳偏好的设计更多的是 任务相关的，复杂的过程需要非常精心的设计，所以很多工作都开始关注到表示学习上，NLP 最近大火的预训练模型，例如 BERT，就是利用大规模的语料预训练得到文本的好的表示。那么，CV 领域的 BERT 是什么呢？答案已经呼之欲出，就是对比学习。</p><p>基于此，也就意味着<strong>表示学习算法并不一定要关注到样本的每一个细节，只要学到的特征能够使其和其他样本区别开来就行</strong>，这就是对比学习和对抗生成网络（GAN）的一个主要不同所在。</p><ul><li>如何定义目标函数？最简单的一种就是上面提到的内积函数，另外一中 triplet 的形式就是 <img src="https://www.zhihu.com/equation?tex=l+%3D+max%280%2C+%CE%B7+%2B+s+%28x%2C+x%5E%2B%29+%E2%88%92+s+%28x%2C+x%5E%E2%88%92%29%29" alt="[公式]"> ，直观上理解，就是希望正例 pair 和负例 pair 隔开至少 <img src="https://www.zhihu.com/equation?tex=%5Ceta" alt="[公式]"> 的距离，这一函数同样可以写成另外一种形式，让正例 pair 和负例 pair 采用不同的 <img src="https://www.zhihu.com/equation?tex=s" alt="[公式]"> 函数，例如，<img src="https://www.zhihu.com/equation?tex=s%28x%2C+x%5E%2B%29+%3D+%5C%7C+%5Cmax+%280%2C+f%28x%29-f%28x%5E%2B%29%5C%7C" alt="[公式]"> ，<img src="https://www.zhihu.com/equation?tex=s%28x%2C+x%5E%2B%29+%3D+%5C%7C+%5Cmax+%28%5Ceta%2C+f%28x%29-f%28x%5E-%29%5C%7C" alt="[公式]">。</li><li>如何构建正例和负例？针对不同类型数据，例如图像、文本和音频，如何合理的定义哪些样本应该被视作是 <img src="https://www.zhihu.com/equation?tex=x%5E%2B" alt="[公式]">，哪些该被视作是 <img src="https://www.zhihu.com/equation?tex=x%5E-" alt="[公式]">，；如何增加负例样本的数量，也就是上面式子里的 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]">？这个问题是目前很多 paper 关注的一个方向，因为虽然自监督的数据有很多，但是<strong>设计出合理的正例和负例 pair，并且尽可能提升 pair 能够 cover 的 semantic relation，才能让得到的表示在 downstream task 表现的更好</strong>。</li></ul><p><br></p><p>接下来，就会介绍一下 MoCo、SimCLR 以及 Contrasitve Predictive Coding（CPC） 这三篇文章，在构建对比样例中的一些核心观点。<a id="more"></a></p><h2 id="Contrastive-Pair"><a href="#Contrastive-Pair" class="headerlink" title="Contrastive Pair"></a><strong>Contrastive Pair</strong></h2><h3 id="MoCo"><a href="#MoCo" class="headerlink" title="MoCo"></a><strong>MoCo</strong></h3><p><img src="https://pic2.zhimg.com/80/v2-2c7e1ebab1ef558f5e1cb04817f96d51_720w.jpg" alt="img"></p><p>论文标题：Momentum Contrast for Unsupervised Visual Representation Learning</p><p>论文来源：CVPR 2020</p><p>论文链接：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1911.05722" target="_blank" rel="noopener">https://arxiv.org/abs/1911.05722</a></p><p>代码链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/moco" target="_blank" rel="noopener">https://github.com/facebookresearch/moco</a></p><p><img src="https://pic4.zhimg.com/80/v2-bfb035f24f9d0f8f2f7dd38bdab2a703_720w.jpg" alt="img"></p><p>前面提到了，样本数量对于学习到的样本质量有很大的影响。MoCo 做的事情很简单，就是把负例样本的 encoder <img src="https://www.zhihu.com/equation?tex=f%28%5Ccdot%29" alt="[公式]">和 mini-batch 大小解耦。也就是说，原先在算目标函数的时候，负例样本对也会为 loss 产生贡献，因为也就会有梯度回传给对应的 encoder，那么这样在实现的时候，样本数量必然会受到 batch size 的限制，从而影响学习到表示的质量。</p><p>为此，Memory Bank 提出我把所有样本的表示都存起来，然后每次随机采样，这样就可以认为我的负例样本理论上可以达到所有样本的数量，具体的做法就是每一轮来 encode 一次所有的变量，显然，这样很吃内存，并且得到的表示也和参数更新存在一定的滞后。</p><p>MoCo 则改善了上述的两个缺点，一方面，用一个 queue 来维护当前的 negative candidates pool，queue 有着进出的动态更新机制，一方面能够和 Mini-batch 解耦，queue size 可以设置的比较大，另外一方面也就不用对所有样本做类似预处理的进行编码；对于负例样本的参数，采用 Momentum update 的方式，来把正例 encoder 的参数<img src="https://www.zhihu.com/equation?tex=%5Ctheta_q" alt="[公式]"> copy 给负例 encoder <img src="https://www.zhihu.com/equation?tex=%5Ctheta_k" alt="[公式]">：</p><p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_k+%3D+m+%5Ctheta_k+%2B+%281-m%29+%5Ctheta_q+%5C%5C" alt="[公式]"></p><p>三种方式的示意图也在这一小节的开头给出了，可以清楚的看到三种方式的区别。<strong>这种对比画图的方式对于说明问题很有帮助，可以在论文中进行尝试</strong>。</p><h3 id="SimCLR"><a href="#SimCLR" class="headerlink" title="SimCLR"></a><strong>SimCLR</strong></h3><p><img src="https://pic4.zhimg.com/80/v2-b86b329fbbde88a83d9af0eb686950b3_720w.jpg" alt="img"></p><p>论文标题：A Simple Framework for Contrastive Learning of Visual Representations</p><p>论文链接：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2002.05709" target="_blank" rel="noopener">https://arxiv.org/abs/2002.05709</a></p><p>代码链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/google-research/simclr" target="_blank" rel="noopener">https://github.com/google-research/simclr</a></p><p>MoCo 刚说完样本数量对于对比学习很重要，这边 SimCLR 就从另外一个角度，说构建负例的方式（图像上，就是对于图像的 transformation）也很重要，探究了 transformation 、batch-size 大小等对于学习到的表示的影响，并且把这个框架用下面这张图来说明：</p><p><img src="https://pic1.zhimg.com/80/v2-79f91dcbda0e03df0c386bf9a93d1ba8_720w.jpg" alt="img"></p><p>文章主要得出了下面几个结论：</p><ul><li>对于样本进行变化，即构建正例和负例的 transformation 对于结果至关重要；</li><li>用 entropy loss 的 Contrastive Learning，可以通过 normalize representation embedding 以及 temperature adjustment 提点；</li><li>在计算 loss 之前，让表示再过一个 non-linear hard 能大幅提升效果，即上面框架图中的 <img src="https://www.zhihu.com/equation?tex=g%28%5Ccdot%29" alt="[公式]">；</li><li>大 batch-size 对于 CL 的增益比 Supervised Learning 更大。</li></ul><p>其中最后一个结论，和 MoCo 的初衷是符合的，并且作者虽说不用 Memory-bank，但是 SimCLR 尝试的 bsz 也达到了令人发指的 8192，用了 128 块 TPU，又是算力党的一大胜利。MoCo v2 也是利用了上面的第一点和第三点，在 MoCo 基础上得到了进一步的提升，然后作者还也明确的点名了 SimCLR，称不需要使用那么大的 batch size 也能超过它，可能这就是神仙打架吧。</p><h3 id="CPC"><a href="#CPC" class="headerlink" title="CPC"></a><strong>CPC</strong></h3><p><img src="https://pic1.zhimg.com/80/v2-155b87baf119fcb3e4f7fcf45c1d1de0_720w.jpg" alt="img"></p><p>论文标题：Representation Learning with Contrastive Predictive Coding</p><p>论文链接：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1807.03748" target="_blank" rel="noopener">https://arxiv.org/abs/1807.03748</a></p><p>代码链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/davidtellez/contrastive-predictive-coding" target="_blank" rel="noopener">https://github.com/davidtellez/contrastive-predictive-coding</a></p><p>前面讨论的两篇文章主要集中在图像数据上，那么对于文本、音频这样的数据，常见的裁剪、旋转等变换操作就无法适用了，并且，因为其数据本身的时序性，设计合理的方法来把这一点考虑进去是至关重要的。Contrastive Predictive Coding（CPC） 这篇文章就提出，可以利用一定窗口内的 <img src="https://www.zhihu.com/equation?tex=x_t" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_%7Bt%2Bk%7D" alt="[公式]"> 作为 Positive pair，并从输入序列之中随机采样一个输入 <img src="https://www.zhihu.com/equation?tex=x_%7Bt%2A%7D" alt="[公式]"> 作为负例，下图说明了 CPC 的工作过程：</p><p><img src="https://pic2.zhimg.com/80/v2-795d7581c01c2d35ae154b34a80b2e3d_720w.jpg" alt="img"></p><p>为了把历史的信息也加入进去，作者提出可以在 <img src="https://www.zhihu.com/equation?tex=z_t" alt="[公式]"> 上额外增加一个自递归模型，例如 GRU，来在表示之中融入时序关系，得到相应的 <img src="https://www.zhihu.com/equation?tex=c_t" alt="[公式]"> 来进行对比学习。在下游任务中，既可以使用 <img src="https://www.zhihu.com/equation?tex=z_t" alt="[公式]"> 也可以使用 <img src="https://www.zhihu.com/equation?tex=c_t" alt="[公式]"> ，又或者是二者的融合，可以根据任务需要来进行灵活的选择。</p><h2 id="Theory-amp-Application"><a href="#Theory-amp-Application" class="headerlink" title="Theory &amp; Application"></a><strong>Theory &amp; Application</strong></h2><p>接下来，会简要的讨论几篇关于对比学习的理论和应用类的文章：</p><h3 id="ICML-2019"><a href="#ICML-2019" class="headerlink" title="ICML 2019"></a><strong>ICML 2019</strong></h3><p><img src="https://pic3.zhimg.com/80/v2-307760d8dd68b99f6e40a2f7d36c62f2_720w.jpg" alt="img"></p><p>论文标题：A Theoretical Analysis of Contrastive Unsupervised Representation Learning</p><p>论文来源：ICML 2019</p><p>论文链接：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1902.09229" target="_blank" rel="noopener">https://arxiv.org/abs/1902.09229</a></p><p>这篇文章发表在 ICML 2019 上，对比学习这一框架虽然在直觉上非常 make sense，但是理论上为什么得到的表示就能够在 downstream 例如 classification 上表现良好？</p><p>这篇文章通过定义 latent class 以及样本和 latent class 的距离入手，推导出了二分类情况下的 loss bound，保证了其的泛化性能。文章提出了一个改进算法就是进行 block 处理，不再直接优化各个 pair 的 inner product，而是转而优化 positive block以及 negative block 的内积：</p><p><img src="https://www.zhihu.com/equation?tex=+%5Cbegin%7Barray%7D%7Bl%7D+L%5E%7Bb+l+o+c+k%7D%28f%29%3A%3D+%5C%5C+%5Cmathbb%7BE%7D%5Cleft%5B%5Cell%5Cleft%28f%28x%29%5E%7BT%7D%5Cleft%28%5Cfrac%7B%5Csum_%7Bi%7D+f%5Cleft%28x_%7Bi%7D%5E%7B%2B%7D%5Cright%29%7D%7Bb%7D-%5Cfrac%7B%5Csum_%7Bi%7D+f%5Cleft%28x_%7Bi%7D%5E%7B-%7D%5Cright%29%7D%7Bb%7D%5Cright%29%5Cright%29%5Cright%5D+%5Cend%7Barray%7D%5C%5C" alt="[公式]"></p><p>文章在后续的实验上也验证了这一方法会优于内积方法。</p><h3 id="NIPS-2017"><a href="#NIPS-2017" class="headerlink" title="NIPS 2017"></a><strong>NIPS 2017</strong></h3><p><img src="https://pic3.zhimg.com/80/v2-3e09dd4db86352a77c0589dd21fea066_720w.jpg" alt="img"></p><p>论文标题：Contrastive Learning for Image Captioning</p><p>论文来源：NIPS 2017</p><p>论文链接：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1710.02534" target="_blank" rel="noopener">https://arxiv.org/abs/1710.02534</a></p><p>代码链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/doubledaibo/clcaption_nips2017" target="_blank" rel="noopener">https://github.com/doubledaibo/clcaption_nips2017</a></p><p>这篇文章希望通过使用对比学习来解决 image captioning 中标题文本<strong>可区别性</strong>的问题，即尽可能让标题描述和唯一的一张图片对应，而不是笼统而又模糊的可能和多张图片对应。作者引入对比学习，把对应的图像和标题作为正例 pair <img src="https://www.zhihu.com/equation?tex=%28c%2C+I%5E%2B%29" alt="[公式]">，并把其中的图像随机采样得到负例 pair <img src="https://www.zhihu.com/equation?tex=%28c%2C+I%5E-%29" alt="[公式]">，并且在已有的 sota 模型上优化 <img src="https://www.zhihu.com/equation?tex=p%28c%7CI%5E%2B%29+-+p%28c%7CI%5E-%29" alt="[公式]">，提升生成的 caption 的效果。</p><h3 id="ICLR-2020"><a href="#ICLR-2020" class="headerlink" title="ICLR 2020"></a><strong>ICLR 2020</strong></h3><p><img src="https://pic4.zhimg.com/80/v2-e0a52b992f6424411deba5bd5341f2fb_720w.jpg" alt="img"></p><p>论文标题：Contrastive Learning of Structured World Models</p><p>论文来源：ICLR 2020</p><p>论文链接：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1911.12247" target="_blank" rel="noopener">https://arxiv.org/abs/1911.12247</a></p><p>代码链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/tkipf/c-swm" target="_blank" rel="noopener">https://github.com/tkipf/c-swm</a></p><p>前面提到，表示学习能够较好的解决一些简单的任务，但是理解物体之间的关系以及建模其间的交互关系不单单需要好的表示，同样需要一个好的归纳偏好。这篇文章就是通过利用 state set 来表示世界中各个物体的状态，并且利用图神经网络来建模其之间的交互，再进一步地利用对比学习来提升性能，下图给出了模型的示意图：</p><p><img src="https://pic3.zhimg.com/80/v2-a349c7e19a4f2690073a6fefde343de6_720w.jpg" alt="img"></p><p>这里的对比学习是从 TransE 架构迁移而来，具体地，在 TransE 中，我们会希望一个三元组 <img src="https://www.zhihu.com/equation?tex=%28e_t%2C+r_t%2C+o_t%29" alt="[公式]"> 的能够让 <img src="https://www.zhihu.com/equation?tex=H+%3D+d%28+f%28e_t%29+%2B+g%28r_t%29%2C+f%28o_t%29%29" alt="[公式]"> 尽可能的小，即 <img src="https://www.zhihu.com/equation?tex=e_t" alt="[公式]"> 的表示加上 relation <img src="https://www.zhihu.com/equation?tex=r_t" alt="[公式]"> 的表示和 <img src="https://www.zhihu.com/equation?tex=o_t" alt="[公式]"> 的表示尽可能地接近，而迁移到世界模型中，就是要将 entity 换成物体的 state，relation 换成 action，即经过图卷积后的得到的新的表示，通过下面的式子进行优化：</p><p><img src="https://www.zhihu.com/equation?tex=%5CDelta+z_%7Bt%7D%3DT%5Cleft%28z_%7Bt%7D%2C+a_%7Bt%7D%5Cright%29%3D%5Coperatorname%7BGNN%7D%5Cleft%28%5Cleft%5C%7B%5Cleft%28z_%7Bt%7D%5E%7Bk%7D%2C+a_%7Bt%7D%5E%7Bk%7D%5Cright%29%5Cright%5C%7D_%7Bk%3D1%7D%5E%7BK%7D%5Cright%29+%5C%5C" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=d%5Cleft%28z_%7Bt%7D%2BT%5Cleft%28z_%7Bt%7D%2C+a_%7Bt%7D%5Cright%29%2C+z_%7Bt%2B1%7D%5Cright%29%2B%5Cmax+%5Cleft%280%2C+%5Cgamma-d%5Cleft%28%5Ctilde%7Bz%7D_%7Bt%7D%2C+z_%7Bt%2B1%7D%5Cright%29%5Cright%29+%5C%5C" alt="[公式]"></p><p>这里的 <img src="https://www.zhihu.com/equation?tex=%5Ctilde+z_t" alt="[公式]"> 是从 experience buffer 中采样得到的负例样本，文章在后续多物体交互环境的模拟实验中验证了其方法的优越性。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h2><p>本文介绍了关于对比学习背后的动机，以及一系列在图像、文本上的一些工作，在计算机视觉领域，其习得的表示能够很好地在下游任务泛化，甚至能够超过监督学习的方法。</p><p>回过头来看，预训练模型从 ImageNet 开始，后来这一思想迁移到 NLP，有了 BERT 等一系列通过自监督的预训练方法来学习表示，后来这一想法又反哺了计算机视觉领域，引出了诸如 MoCo、SimCLR 等工作，在一系列分割、分类任务上都取得了惊人的表现。那么，这一思想会不会又再次和 NLP 结合，碰撞出新的火花呢，让我们拭目以待。</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Contrastive Learning </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>什么是对比学习 (Constrastive Learning) ?</title>
      <link href="/2020/11/14/%E4%BB%80%E4%B9%88%E6%98%AF%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-Constrastive-Learning/"/>
      <url>/2020/11/14/%E4%BB%80%E4%B9%88%E6%98%AF%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-Constrastive-Learning/</url>
      
        <content type="html"><![CDATA[<p>近年来，自我监督模式的成功可以归因于研究者对对比学习这一自我监督学习范式的重新兴趣。例如，人类可以辨认出野外的物体，即使我们不记得物体的确切样子。 我们通过记住高层特征而忽略微观层面的细节来做到这点。所以，现在的问题是，<strong>我们是否可以建立一个不关注像素级细节，只编码足以区分不同对象的高级特征的表示学习算法</strong>？通过对比学习，研究人员正试图解决这一问题。 最近，甚至连Google的<font color="orange"><strong>SimCLR</strong></font>也展示了对比学习的含义，我们将在本文的最后简要介绍这一点。</p><h1 id="对比学习的基本原则"><a href="#对比学习的基本原则" class="headerlink" title="对比学习的基本原则"></a>对比学习的基本原则</h1><p><div align="center"> <img src="/images/blog/2020/constrastive.png" alt></div></p><p>对比学习是一种为ML模型描述相似和不同事物的任务的方法。利用这种方法，可以训练机器学习模型来区分相似和不同的图像。 对比学习的内部工作可以表述为一个分数函数，它是衡量两个特征之间相似度的一个尺度。</p><script type="math/tex; mode=display">\mathrm{score}(f(x), f(x^+)) >> \mathrm{score}(f(x), f(x^-))</script><ul><li>x+是与x相似的数据点，称为阳性样本</li><li>x−是与x不同的数据点，称为阴性样本 </li></ul><p>在此基础上，可以构建一个softmax分类器来正确地对正样本和负样本进行分类。在最近引入的<a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html" target="_blank" rel="noopener">SimCLR</a>框架中也可以找到这种技术的类似应用。</p><font color="blue">PS: SimCLR是Hinton在2019年提出的工作，最近在不同的地方反复看到四五次了，一定要找个时间了解一下。</font><a id="more"></a><h1 id="应用对比学习"><a href="#应用对比学习" class="headerlink" title="应用对比学习"></a>应用对比学习</h1><p>谷歌推出了一个名为“SimCLR”的框架，使用对比学习。该框架首先学习未标记数据集上图像的一般表示，然后针对给定的分类任务，使用带标签图像的小数据集对其进行微调。 <strong>通过同时最大化同一图像的不同版本或视图之间的一致性，并通过对比学习缩小差异来学习基本表征。</strong> 当使用这个对比目标更新神经网络的参数时，相应视图的表示相互“吸引”，而非对应视图的表示相互“排斥”。 在<a href="https://amitness.com/2020/03/illustrated-simclr/" target="_blank" rel="noopener">这个博客</a>中，对原始论文作了更细致的解释。</p><p>程序如下： </p><ol><li>首先，从原始图像生成一定大小的批处理数据（Batch），比如N </li><li>对于这个批处理中的每个图像，应用一个随机变换函数来获得一对两个图像 </li><li>一对图像中的每个增强图像都通过一个编码器来获得图像表示。</li><li>两个增强图像的表示然后通过一个非线性密集层，然后是一个ReLU，然后是另一个密集层。这些图像被传递到一系列这些层上，以应用非线性变换并将其投影到表示中 </li><li>对于批处理中的每个增强图像，获取一个嵌入向量。</li></ol><p>现在，图像的两个增强版本之间的相似性是用余弦相似性来计算的。SimCLR使用“NT-Xent损失”（标准化的温度标度交叉熵损失），即所谓的对比损失</p><p><img src="/images/blog/2020/pair.png" alt></p><p>首先，逐个获取批处理中的增广对。然后应用一个softmax函数来计算这两个图像相似的概率。</p><p><img src="/images/blog/2020/softmax.png" alt></p><p>如上所示，softmax函数可用于计算两个增强cat图像的相似性，并将批处理中的所有剩余图像作为不同的图像（负对）进行采样。 </p><p>基于这种损失，编码器和投影头的表示会随着时间的推移而改进，并且得到的表示将相似的图像放在更近的空间中。 SimCLR的实验结果表明，它在ImageNet上的性能优于以往的自监督学习方法。</p><h1 id="自监督学习"><a href="#自监督学习" class="headerlink" title="自监督学习"></a>自监督学习</h1><p><strong>自监督学习有什么优势呢？</strong></p><p>目前机器学习主流的方法大多是监督学习方法，这类方法依赖人工标注的标签，这会带来一些缺陷：</p><ul><li><strong>数据本身提供的信息远比稀疏的标签更加丰富</strong>，因此使用有监督学习方法训练模型需要大量的标签数据，并且得到的模型有时候是“脆弱”的</li><li>有监督学习通过标签训练得到的模型往往只能学到一些任务特定的知识，而不能学习到一种通用的知识，因此有监督学习学到的特征表示<strong>难以迁移</strong>到其他任务</li></ul><p>而自监督学习能够很好地避免上面的问题，因为自监督学习直接使用数据本身来提供监督信息来指导学习。</p><p>当前自监督学习可以被大致分为两类：</p><ol><li>Generative Methods</li><li>Contrastive Methods</li></ol><p>Generative Methods（生成式方法）这类方法以自编码器为代表，主要关注pixel label的loss。举例来说，在自编码器中对数据样本编码成特征再解码重构，这里认为重构的效果比较好则说明模型学到了比较好的特征表达，而重构的效果通过pixel label的loss来衡量。</p><p>Contrastive Methods（对比式方法）这类方法则是通过将数据分别与正例样本和负例样本在特征空间进行对比，来学习样本的特征表示。Contrastive Methods主要的难点在于如何构造正负样本。</p><p><strong>那么，Contrastive Methods有什么优势呢？</strong></p><p>这里举一个例子来说明，Epstein在2016年做了一个实验：让受试者画出美元的图像，越详细越好，下图是受试者画出来的美元图像。</p><p><img src="/images/blog/2020/dollar.jpg" alt></p><p>左图是受试者根据印象画出来的美元，右图则是让受试者照着美元画出来的。可以看到，左图画出来的美元图虽然不够详细，但已经充分具备一张美元的判别性信息，因此我们可以把它识别成美元。事实上，我们并不需要见到一张美元所有详细的信息，而仅仅通过一些关键的特征就可以识别出来。</p><p>同理，相比起Generative Methods需要对像素细节进行重构来学习到样本特征，Contrastive Methods只需要在特征空间上学习到区分性。因此Contrastive Methods不会过分关注像素细节，而能够关注抽象的语义信息，并且相比于像素级别的重构，优化也变得更加简单。</p><h3 id="对比学习一般泛式"><a href="#对比学习一般泛式" class="headerlink" title="对比学习一般泛式"></a>对比学习一般泛式</h3><p>对任意数据 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> ，对比学习的目标是学习一个编码器 <img src="https://www.zhihu.com/equation?tex=f+" alt="[公式]"> 使得：</p><p><img src="https://www.zhihu.com/equation?tex=score%28f%28x%29%2Cf%28x%5E%7B%2B%7D%29%29%3E%3Escore%28f%28x%29%2Cf%28x%5E%7B-%7D%29%29" alt="[公式]"></p><p>其中 <img src="https://www.zhihu.com/equation?tex=x%5E%7B%2B%7D" alt="[公式]"> 是和 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 相似的正样本， <img src="https://www.zhihu.com/equation?tex=x%5E%7B-%7D" alt="[公式]"> 是和 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 不相似的负样本，score是一个度量函数来衡量样本间的相似度。</p><p>如果用向量内积来计算两个样本的相似度，则对比学习的损失函数可以表示成：</p><p><img src="https://www.zhihu.com/equation?tex=L_%7BN%7D%3D-%5Cmathbb%7BE%7D_%7BX%7D%5Cleft%5B+%5Clog%5Cfrac%7B%5Cexp%5Cleft%28+f%28x%29%5E%7BT%7Df%28x%5E%7B%2B%7D%29+%5Cright%29%7D%7B%5Cexp%5Cleft%28+f%28x%29%5E%7BT%7Df%28x%5E%7B%2B%7D%29+%5Cright%29%2B%5Csum_%7Bj%3D1%7D%5E%7BN-1%7D%7B%5Cexp%5Cleft%28+f%28x%29%5E%7BT%7Df%28x_%7Bj%7D%5E%7B-%7D%29+%5Cright%29%7D%7D+%5Cright%5D" alt="[公式]"></p><p>其中对应样本 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 有1个样本和N-1个负样本。可以发现，这个形式类似于交叉熵损失函数，学习的目标就是让 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 的特征和正样本的特征更相似，同时和N-1个负样本的特征更不相似。</p><p>在对比学习的相关文献中把这一损失函数称作InfoNCE损失。也有一些其他的工作把这一损失函数称为<a href="https://link.zhihu.com/?target=https%3A//papers.nips.cc/paper/6200-improved-deep-metric-learning-with-multi-class-n-pair-loss-objective" target="_blank" rel="noopener">multi-class n-pair loss</a>或者<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1809.01812" target="_blank" rel="noopener">ranking-based NCE</a>。</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Constrative Learning </tag>
            
            <tag> Constrative </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Contrastive Loss</title>
      <link href="/2020/11/13/Contrastive-Loss/"/>
      <url>/2020/11/13/Contrastive-Loss/</url>
      
        <content type="html"><![CDATA[<blockquote><p>部分内容参考以下文章：</p><p><a href="https://zhuanlan.zhihu.com/p/93917636" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/93917636</a></p><p><a href="https://zhuanlan.zhihu.com/p/149748513" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/149748513</a></p></blockquote><p><br></p><h1 id="1-什么是对比损失？"><a href="#1-什么是对比损失？" class="headerlink" title="1. 什么是对比损失？"></a>1. 什么是对比损失？</h1><p>最近Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie 与 Ross Girshick在图像处理任务上使用自监督预训练从图像中提取特征，在7个下游任务中超过监督学习的特征提取表现。使用的核心技术是对比损失(Contrastive Loss Function)，如何理解这个对比损失呢？这篇文章记录个人的直观理解。</p><p>先说大家熟悉的损失函数，categorical crossentropy, 它计算了类概率与标准答案的距离。softmax 用作最后一层的激活函数，计算归一化的类概率。每个类的标准答案是 one-hot 表示。比如在猫，狗，猪三分类任务中, one-hot 表示为，</p><p>=(1, 0, 0)</p><p>=(0, 1, 0)</p><p>=(0, 0, 1)</p><p>这对应3维空间沿 x-轴， y-轴，z-轴的3个单位基向量。</p><p>这样做没有任何问题。但是如果我们添加更多类别，比如额外添加 树，花和草，将这6种生物放在6维空间的单位基向量上，softmax 函数会将他们一视同仁，不会认为猫和狗之间的距离应该小于猫与草的距离，虽然前2者同属动物。</p><p>最直接的想法是我们假设存在一个损失函数，它满足如下的基本准则</p><ol><li>近似样本之间的距离越小越好</li><li>不似样本之间的距离越大越好</li></ol><p>如果一个损失函数只要求满足条件 1，那么相当于说我只要同一个类的样本都放在同一个点即可。神经网络将猫都放在(1, 0, 0)之后，就算将狗和猪也放在（1，0， 0），仍然不会破坏条件1。这样，神经网络只要对所有样本作出一样的预测就可以了。</p><p>如果再加上条件2，是不是就好了呢？优化的时候，神经网络将猫放在(1, 0, 0)，第一轮将狗放在（0，1， 0），第二轮将狗放在（0，2， 0），然后一轮轮训练下去，狗的坐标被放的越来越远，不相似样本之间的距离越来越大，但训练的目标却仿佛永远无法达到 。。。</p><p><strong>这是因为训练目标没有边界。</strong>来到 <strong>LeCun 和他的合作者们在2006年提出的对比损失（Contrastive Loss Function)</strong>。这个损失函数定下的基本准则是：<a id="more"></a></p><ol><li>近似样本之间的距离越小越好。</li><li>不似样本之间的距离如果小于m，则通过互斥使其距离接近m。</li></ol><p>第一个准则不变，第二个准则里加入一个超参数 m，使得训练目标有了边界。文章对第二个准则有个形象的解释，就像长度为m的弹簧，如果它被压缩，则会因为斥力恢复到长度m。</p><p>写成公式就是：</p><script type="math/tex; mode=display">L(W,Y,\hat{X_1}, \hat{X}_2) = (1-Y)\frac{1}{2}(D_W)^2 + (Y)\frac{1}{2}\{\max(0, m-D_w)\}^2</script><font color="gray">其中 W 是网络权重；Y是成对标签，如果X1，X2这对样本属于同一个类，Y=0，属于不同类则 Y=1。Dw 是 X1 与 X2 在潜变量空间的欧几里德距离。当Y=0，调整参数最小化X1与X2之间的距离。当Y=1，如果X1与X2之间距离大于m，则不做优化（省时省力）；如果 X1 与 X2 之间的距离小于 m, 则增大两者距离到m。</font><font color="orange">感觉这里有点像Hinge Loss，都是给损失增加了一个阈值限制</font><p>比较有意思的是那些属于不同类，但两两距离天生大于m的样本对。 LeCun 的对比损失完全忽视这些样本对，大大减少了计算量。另一方面，因为未对那些样本做限制，在对相似样本做聚类的时候，不同的类收缩到的高维空间点没有受到太多人为限制的影响。至于这些被收缩到一起以及被推开的样本点，它们在潜变量空间的分布是否能反映数据本身的统计规律，可能还需要额外的考虑。</p><p>对比损失提供了一个不同类别样本点之间距离的下界 m。如果想让神经网络学习到动物是同一个大类，植物是另一个大类，走监督学习路线可能需要设计 <strong>Hierarchical Contrastive Loss</strong>。目前的非监督学习，不知道是否已经做到了这一点。</p><h1 id="2-对比损失与交叉熵损失"><a href="#2-对比损失与交叉熵损失" class="headerlink" title="2. 对比损失与交叉熵损失"></a>2. 对比损失与交叉熵损失</h1><p>现有的分类模型，最后一层一般为linear + softmax，若将之前的特征视为$f(X_i)$，linear权重视为$W$，则交叉熵损失公式为：</p><script type="math/tex; mode=display">P(y=c|X_i) = \frac{\exp(W_c^T f(X_i))}{\sum_p \exp(W_p^Tf(X_i))}</script><p>这里的$W_c, W_p$可以看作$W$矩阵的不同行。</p><p>在介绍对比损失之前，先介绍non-parametric classification（非参数分类）问题，对于上式，<strong>每个权重向量事实上代表了每一类样本其特征值的模板</strong>（根据向量乘法我们知道越相似的两个向量其内积越大）。到这里，实际上我们发现，现有的分类问题实际上可以看作<strong>通过一系列深度网络提取特征，然后依据大量的样本学习到每一类样本的模板。在测试的阶段则将这个学到的特征模板去做比对。</strong><font color="orange">有趣的观点！</font></p><p>所谓非参数样本分类，则是将每个计算出的样本特征作为模板，</p><script type="math/tex; mode=display">P(y=c|X_i) = \frac{\exp(f(X_c)^T f(X_i))}{\sum_p \exp(f(X_p)^Tf(X_i))}</script><p>理解就是<font color="purple"><strong>计算样本之间的相似度（距离）作为损失</strong></font>。</p><p>当下对比损失在非监督学习中应用很广泛，同样对于数据集D，此时我们没有标签，在对比学习框架下，通常大家以每张图片作为一个单独的语义类别，并且有以下假设：</p><p>对于原始图片X，做不同的变换得到A(X), B(X)，此时对比损失希望A(X), B(X)之间的距离小于与其他图片间的距离。</p><script type="math/tex; mode=display">\mathrm{dist}(A(X), B(X)) < \mathrm{dist}(A(X), X^{'})</script><p>在实际操作中，假设使用cosine距离，并且已经特征归一化，则优化上式等价于优化下式中的softmax距离:</p><script type="math/tex; mode=display">P = \frac{\exp(f(A(X))^T f(B(X))}{\sum_i \exp(f(A(X))^Tf(X_i))}</script><p>不难发现，这与交叉熵损失有着相似的形式，<em>对比损失可以看作添加了变换不确定性的非参数分类损失</em>。这里的非参数，个人理解可能是指没有上面的$W$矩阵。</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Contrastive Loss </tag>
            
            <tag> CrossEntropy Loss </tag>
            
            <tag> Loss Function </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IJCAI 2021 投稿安排</title>
      <link href="/2020/11/13/IJCAI-2021-%E6%8A%95%E7%A8%BF%E5%AE%89%E6%8E%92/"/>
      <url>/2020/11/13/IJCAI-2021-%E6%8A%95%E7%A8%BF%E5%AE%89%E6%8E%92/</url>
      
        <content type="html"><![CDATA[<p>IJCAI（国际人工智能联合会议）曾被誉为人工智能界的女神，但因近几年的审稿翻车事件，女神高不可攀的形象，似乎跌落了一些，<strong>组委会不得不做出紧急反应</strong>，开始改革。IJCAI新提出的快速淘汰机制，2020第一轮审稿就拒绝了将近一半，最后的录取率又一次探了底，直降到<strong>12.6%</strong>，被称为“灭霸式审稿”也不为过。</p><p>更让人感慨唏嘘的是，一场突来的全球疫情，使得本该线下交流的大会，被迫延期到2021年1月，大家都不禁猜测，组委会在此形势下，会不会干脆取消下一届的举办。但就在近日，官网更新了IJCAI 2021的会议安排，终于打消了大家的疑虑。</p><p>根据官方声明，<strong>这次会议计划于2021.8.21-26期间，在加拿大蒙特利尔召开</strong>，最受投稿者关注的论文接收流程，也已经公布，下面将介绍一下行程安排。</p><p><strong>【IJCAI 2021重要时间节点】</strong></p><font color="orange">*以下为官方时间（UTC-12），非北京时间（UTC+8），两者相差20个小时*</font><p><div align="center"> <img src="/images/blog/2020/ijcai.jpg" alt></div></p><p>对应于北京时间：<a id="more"></a></p><ul><li>论文提交开放：2021.1.1</li><li>摘要提交截止：2021.1.13 19:59</li><li>投稿者信息截止：2021.1.15 19:59</li><li>完整论文提交截止：2021.1.19 19:59</li><li>摘要拒绝通知：2021.2.6</li><li>投稿者反馈期限：2021.3.21-25</li><li>论文接收通知：2021.4.26</li></ul><p>关于论文格式、排版和模版等，可参考官方指引：<a href="https://www.ijcai.org/authors_kit" target="_blank" rel="noopener">https://www.ijcai.org/authors_kit</a></p><p>论文提交地址：<a href="https://cmt3.research.microsoft.com/User/Login?ReturnUrl=%2FIJCAI2021" target="_blank" rel="noopener">https://cmt3.research.microsoft.com/User/Login?ReturnUrl=%2FIJCAI2021</a></p><p><strong>更多信息大家可以进入官网进一步了解</strong>：<a href="https://ijcai-21.org/cfp/" target="_blank" rel="noopener">https://ijcai-21.org/cfp/</a></p><p><br></p><h1 id="最后，希望能成功把女神抱回家～"><a href="#最后，希望能成功把女神抱回家～" class="headerlink" title="最后，希望能成功把女神抱回家～"></a>最后，希望能成功把女神抱回家～</h1>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IJCAI </tag>
            
            <tag> conference </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fastText原理及实践</title>
      <link href="/2020/11/12/fastText%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E8%B7%B5/"/>
      <url>/2020/11/12/fastText%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p><a href="https://github.com/facebookresearch/fastText" target="_blank" rel="noopener">fastText</a>是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：<br>1、fastText在保持高精度的情况下加快了训练速度和测试速度<br>2、fastText不需要预训练好的词向量，fastText会自己训练词向量<br>3、fastText两个重要的优化：Hierarchical Softmax、N-gram</p><h1 id="2-模型架构"><a href="#2-模型架构" class="headerlink" title="2. 模型架构"></a>2. 模型架构</h1><p>word2vec将上下文关系转化为多分类任务，进而训练逻辑回归模型，这里的类别数量|V|词库大小。通常的文本数据中，词库少则数万，多则百万，在训练中直接训练多分类逻辑回归并不现实。word2vec中提供了两种针对大规模多分类问题的优化手段， negative sampling 和hierarchical softmax。在优化中，negative sampling 只更新少量负面类，从而减轻了计算量。hierarchical softmax 将词库表示成前缀树，从树根到叶子的路径可以表示为一系列二分类器，一次多分类计算的复杂度从|V|降低到了树的高度<a id="more"></a></p><h1 id="3-层次化softmax"><a href="#3-层次化softmax" class="headerlink" title="3. 层次化softmax"></a>3. 层次化softmax</h1><p>在标准的softmax中，计算一个类别的softmax概率时，我们需要对所有类别概率做归一化，在这类别很大情况下非常耗时，因此提出了分层softmax(Hierarchical Softmax),思想是根据类别的频率构造霍夫曼树来代替标准softmax，通过分层softmax可以将复杂度从N降低到logN，下图给出分层softmax示例：</p><p><div align="center"> <img src="/images/blog/2020/hsoftmax.png" alt></div></p><p>在层次softmax模型中，叶子结点的词没有直接输出的向量，而非叶子节点都有相应的输出在模型的训练过程中，通过Huffman编码，构造了一颗庞大的Huffman树，同时会给非叶子结点赋予向量。我们要计算的是目标词w的概率，这个概率的具体含义，是指从root结点开始随机走，走到目标词w的概率。因此在途中路过非叶子结点（包括root）时，需要分别知道往左走和往右走的概率。例如到达非叶子节点n的时候往左边走和往右边走的概率分别是：</p><script type="math/tex; mode=display">p(n, left) = \sigma (\theta_n^T \cdot h) \\p(n, right) = 1 - \sigma(\theta_n^T \cdot h)</script><p>其中$\theta$是非叶子节点的向量表示，h是隐藏层。逐层累乘得到对应单词(叶子节点)的概率。</p><h1 id="4-n-gram特征"><a href="#4-n-gram特征" class="headerlink" title="4. n-gram特征"></a>4. n-gram特征</h1><p>使用n-gram有如下优点<br>1、为罕见的单词生成更好的单词向量：根据字符级别的n-gram来说，即是这个单词出现的次数很少，但是组成单词的字符和其他单词有共享的部分，因此这一点可以优化生成的单词向量<br>2、在词汇单词中，即使单词没有出现在训练语料库中，仍然可以从字符级n-gram中构造单词的词向量<br>3、n-gram可以让模型学习到局部单词顺序的部分信息, 如果不考虑n-gram则便是取每个单词，这样无法考虑到词序所包含的信息，即也可理解为上下文信息，因此通过n-gram的方式关联相邻的几个词，这样会让模型在训练的时候保持词序信息</p><h1 id="5-fastText实践"><a href="#5-fastText实践" class="headerlink" title="5. fastText实践"></a>5. fastText实践</h1><p>推荐官网教程 <a href="https://fasttext.cc/docs/en/unsupervised-tutorial.html" target="_blank" rel="noopener">https://fasttext.cc/docs/en/unsupervised-tutorial.html</a></p><h2 id="5-1-训练词向量"><a href="#5-1-训练词向量" class="headerlink" title="5.1. 训练词向量"></a>5.1. 训练词向量</h2><p>为了计算词向量，我们需要一个大的文本语料库，根据语料库的不同，单词向量也将捕捉到不同的信息，在本教程中，我们关注Wikipedia的文章，当然也可以考虑其他语料库来源，例如新闻活着Webcrawl，下载Wikipedia语料库执行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2</span><br></pre></td></tr></table></figure><p>数据集已经取到了，现在我们可以使用如下的简单命令在上述数据集上训练我们的词向量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir result</span><br><span class="line">$ ./fasttext skipgram -input data/fil9 -output result/fil9</span><br></pre></td></tr></table></figure><p>分解上述命令：./fasttext使用skipgram模型调用二进制fastText可执行文件，当然也可以使用cbow模型，-input表示输入数据路径，-output表示训练的词向量模型所在路径，当fastText运行时，屏幕会显示进度和估计的完成时间。</p><p>fil9.bin文件是一个二进制文件，它存储了整个fastText模型，随后可以进行加载，fil9.vec文件是一个包含单词向量的文本文件，每一行对应词汇表中的每个单词，可通过如下命令查看fil9.vec中的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ head -n 4 result/fil9.vec</span><br><span class="line">218316 100</span><br><span class="line">the -0.10363 -0.063669 0.032436 -0.040798 0.53749 0.00097867 0.10083 0.24829 ...</span><br><span class="line">of -0.0083724 0.0059414 -0.046618 -0.072735 0.83007 0.038895 -0.13634 0.60063 ...</span><br><span class="line">one 0.32731 0.044409 -0.46484 0.14716 0.7431 0.24684 -0.11301 0.51721 0.73262 ...</span><br></pre></td></tr></table></figure><h1 id="6-其他"><a href="#6-其他" class="headerlink" title="6. 其他"></a>6. 其他</h1><p>目前fastText仅仅可运行在CPU上，但这也是其优势所在，fastText的目的便是要成为一个高效的CPU上的分类模型，可以允许模型在没有GPU的情况下构建</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fastText </tag>
            
            <tag> embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关系抽取数据集</title>
      <link href="/2020/11/11/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
      <url>/2020/11/11/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇文章部分引用以下博客：</p><p><a href="https://blog.csdn.net/qq_41372972/article/details/104677655" target="_blank" rel="noopener">https://blog.csdn.net/qq_41372972/article/details/104677655</a></p><p><a href="https://blog.csdn.net/CSDN_wujian/article/details/100136621" target="_blank" rel="noopener">https://blog.csdn.net/CSDN_wujian/article/details/100136621</a></p></blockquote><p><br></p><p>最近由于实验需要，收集整理了关系抽取方向的数据集，主要包括SemEval、Wiki80、NYT10。目前来说全监督的关系抽取任务一般在SemEval上做，远程监督的关系抽取任务一般在NYT10上做。</p><h1 id="SemEval"><a href="#SemEval" class="headerlink" title="SemEval"></a>SemEval</h1><h2 id="数据集来源"><a href="#数据集来源" class="headerlink" title="数据集来源"></a>数据集来源</h2><p>SemEval数据集来自于2010年的国际语义评测大会中Task 8:” Multi-Way Classification of Semantic Relations Between Pairs of Nominals “</p><h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>任务：对于给定了的句子和两个做了标注的名词，从给定的关系清单中选出最合适的关系。<br>数据集中一共包含9+1个关系，各类数据的占比如下图所示：</p><p><img src="/images/blog/2020/semeval.png" alt></p><p>数据官网：<a href="http://semeval2.fbk.eu/semeval2.php?location=tasks#T11" target="_blank" rel="noopener">http://semeval2.fbk.eu/semeval2.php?location=tasks#T11</a></p><a id="more"></a><h1 id="Wiki80"><a href="#Wiki80" class="headerlink" title="Wiki80"></a>Wiki80</h1><h2 id="数据集来源-1"><a href="#数据集来源-1" class="headerlink" title="数据集来源"></a>数据集来源</h2><p>根据OpenNRE上的原文（We also provide a new dataset Wiki80, which is derived from FewRel.）来看Wiki80是由清华发布的数据集FewRel上提取的。</p><h2 id="数据集介绍-1"><a href="#数据集介绍-1" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>任务：对于给定了的句子和两个做了标注的名词，从给定的关系清单中选出最合适的关系。<br>数据集中一共包含80中关系，经统计各个关系个数均为700，合计56000个样本。</p><div class="table-container"><table><thead><tr><th>关系</th><th>个数</th></tr></thead><tbody><tr><td>place served by transport hub</td><td>700</td></tr><tr><td>mountain range</td><td>700</td></tr><tr><td>religion</td><td>700</td></tr><tr><td>participating team</td><td>700</td></tr><tr><td>contains administrative territorial entity</td><td>700</td></tr><tr><td>head of government</td><td>700</td></tr><tr><td>country of citizenship</td><td>700</td></tr><tr><td>original network</td><td>700</td></tr><tr><td>heritage designation</td><td>700</td></tr><tr><td>performer</td><td>700</td></tr><tr><td>participant of</td><td>700</td></tr><tr><td>position held</td><td>700</td></tr><tr><td>has part</td><td>700</td></tr><tr><td>location of formation</td><td>700</td></tr><tr><td>located on terrain feature</td><td>700</td></tr><tr><td>architect</td><td>700</td></tr><tr><td>country of origin</td><td>700</td></tr><tr><td>publisher</td><td>700</td></tr><tr><td>director</td><td>700</td></tr><tr><td>father</td><td>700</td></tr><tr><td>developer</td><td>700</td></tr><tr><td>military branch</td><td>700</td></tr><tr><td>mouth of the watercourse</td><td>700</td></tr><tr><td>nominated for</td><td>700</td></tr><tr><td>movement</td><td>700</td></tr><tr><td>successful candidate</td><td>700</td></tr><tr><td>followed by</td><td>700</td></tr><tr><td>manufacturer</td><td>700</td></tr><tr><td>instance of</td><td>700</td></tr><tr><td>after a work by</td><td>700</td></tr><tr><td>member of political party</td><td>700</td></tr><tr><td>licensed to broadcast to</td><td>700</td></tr><tr><td>headquarters location</td><td>700</td></tr><tr><td>sibling</td><td>700</td></tr><tr><td>instrument</td><td>700</td></tr><tr><td>country</td><td>700</td></tr><tr><td>occupation</td><td>700</td></tr><tr><td>residence</td><td>700</td></tr><tr><td>work location</td><td>700</td></tr><tr><td>subsidiary</td><td>700</td></tr><tr><td>participant</td><td>700</td></tr><tr><td>operator</td><td>700</td></tr><tr><td>characters</td><td>700</td></tr><tr><td>occupant</td><td>700</td></tr><tr><td>genre</td><td>700</td></tr><tr><td>operating system</td><td>700</td></tr><tr><td>owned by</td><td>700</td></tr><tr><td>platform</td><td>700</td></tr><tr><td>tributary</td><td>700</td></tr><tr><td>winner</td><td>700</td></tr><tr><td>said to be the same as</td><td>700</td></tr><tr><td>composer</td><td>700</td></tr><tr><td>league</td><td>700</td></tr><tr><td>record label</td><td>700</td></tr><tr><td>distributor</td><td>700</td></tr><tr><td>screenwriter</td><td>700</td></tr><tr><td>sports season of league or competition</td><td>700</td></tr><tr><td>taxon rank</td><td>700</td></tr><tr><td>location</td><td>700</td></tr><tr><td>field of work</td><td>700</td></tr><tr><td>language of work or name</td><td>700</td></tr><tr><td>applies to jurisdiction</td><td>700</td></tr><tr><td>notable work</td><td>700</td></tr><tr><td>located in the administrative territorial entity</td><td>700</td></tr><tr><td>crosses</td><td>700</td></tr><tr><td>original language of film or TV show</td><td>700</td></tr><tr><td>competition class</td><td>700</td></tr><tr><td>part of</td><td>700</td></tr><tr><td>sport</td><td>700</td></tr><tr><td>constellation</td><td>700</td></tr><tr><td>position played on team / speciality</td><td>700</td></tr><tr><td>located in or next to body of water</td><td>700</td></tr><tr><td>voice type</td><td>700</td></tr><tr><td>follows</td><td>700</td></tr><tr><td>spouse</td><td>700</td></tr><tr><td>military rank</td><td>700</td></tr><tr><td>mother</td><td>700</td></tr><tr><td>member of</td><td>700</td></tr><tr><td>child</td><td>700</td></tr><tr><td>main subject</td><td>700</td></tr><tr><td>合计</td><td>56000</td></tr></tbody></table></div><p>Ps:这里56000个是val与train一起统计的</p><p>Wiki80 文件夹中共包含3个文件:</p><p>Wiki80_rel2id.json : 关系及其索引的对照表，合计80个关系，和Semeval中的不同，这里面的关系不包含实体的前后关系。</p><p>Wiki80_train.txt &amp; wiki80_val.txt : trian(50400个样本)、val(5600个样本)合计56000个样本。</p><p>数据集中不包含测试集</p><p>样本格式：<br>例子：{“token”: [“Vahitahi”, “has”, “a”, “territorial”, “airport”, “.”], “h”: {“name”: “territorial airport”, “id”: “Q16897548”, “pos”: [3, 5]}, “t”: {“name”: “vahitahi”, “id”: “Q1811472”, “pos”: [0, 1]}, “relation”: “place served by transport hub”}</p><p>样本的格式同semeval中的几乎一致，但是在头实体和尾实体中加入了id这一属性。</p><p><strong><em>Wiki80数据集采用人工精标，不包含噪声</em></strong></p><p>数据来源：<a href="https://github.com/thunlp/OpenNRE/tree/master/benchmark" target="_blank" rel="noopener">https://github.com/thunlp/OpenNRE/tree/master/benchmark</a><br>数据参考：<a href="https://opennre-docs.readthedocs.io/en/latest/get_started/benchmark" target="_blank" rel="noopener">https://opennre-docs.readthedocs.io/en/latest/get_started/benchmark</a></p><h1 id="NYT10"><a href="#NYT10" class="headerlink" title="NYT10"></a>NYT10</h1><h2 id="数据集来源："><a href="#数据集来源：" class="headerlink" title="数据集来源："></a>数据集来源：</h2><p>NYT10是在基于远程监督的关系抽取任务上最常用的数据集，NYT10数据集来自于10年的论文Modeling Relations and Their Mentions withoutLabeled Text，是由NYT corpus 同Freebase远程监督得到：</p><p><img src="/images/blog/2020/NYT.png" alt></p><h2 id="数据集介绍-2"><a href="#数据集介绍-2" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>任务：对于给定了的句子和两个做了标注的名词，从给定的关系清单中选出最合适的关系。<br>数据集中一共包含52+1（包括NA）个关系，各个关系在样本中的分布如下：</p><div class="table-container"><table><thead><tr><th>relations</th><th>size_of_train</th><th>size_of_test</th></tr></thead><tbody><tr><td>/location/fr_region/capital</td><td>1</td><td>0</td></tr><tr><td>/location/cn_province/capital</td><td>2</td><td>0</td></tr><tr><td>/location/in_state/administrative_capital</td><td>4</td><td>0</td></tr><tr><td>/base/locations/countries/states_provinces_within</td><td>0</td><td>1</td></tr><tr><td>/business/company/founders</td><td>901</td><td>95</td></tr><tr><td>/people/person/place_of_birth</td><td>4053</td><td>162</td></tr><tr><td>/people/deceased_person/place_of_death</td><td>2422</td><td>68</td></tr><tr><td>/location/it_region/capital</td><td>22</td><td>0</td></tr><tr><td>/people/family/members</td><td>4</td><td>0</td></tr><tr><td>/people/profession/people_with_this_profession</td><td>2</td><td>0</td></tr><tr><td>/location/neighborhood/neighborhood_of</td><td>9275</td><td>68</td></tr><tr><td>NA</td><td>385664</td><td>166004</td></tr><tr><td>/location/in_state/legislative_capital</td><td>4</td><td>0</td></tr><tr><td>/sports/sports_team/location</td><td>294</td><td>10</td></tr><tr><td>/people/person/religion</td><td>202</td><td>6</td></tr><tr><td>/location/in_state/judicial_capital</td><td>3</td><td>0</td></tr><tr><td>/business/company_advisor/companies_advised</td><td>2</td><td>8</td></tr><tr><td>/people/family/country</td><td>6</td><td>0</td></tr><tr><td>/time/event/locations</td><td>4</td><td>4</td></tr><tr><td>/business/company/place_founded</td><td>648</td><td>20</td></tr><tr><td>/location/administrative_division/country</td><td>7286</td><td>424</td></tr><tr><td>/people/ethnicity/included_in_group</td><td>7</td><td>0</td></tr><tr><td>/location/br_state/capital</td><td>4</td><td>2</td></tr><tr><td>/location/mx_state/capital</td><td>1</td><td>0</td></tr><tr><td>/location/province/capital</td><td>39</td><td>11</td></tr><tr><td>/people/person/nationality</td><td>9733</td><td>723</td></tr><tr><td>/business/person/company</td><td>7336</td><td>302</td></tr><tr><td>/business/shopping_center_owner/shopping_centers_owned</td><td>1</td><td>0</td></tr><tr><td>/business/company/advisors</td><td>9</td><td>8</td></tr><tr><td>/business/shopping_center/owner</td><td>1</td><td>0</td></tr><tr><td>/location/country/languages_spoken</td><td>0</td><td>3</td></tr><tr><td>/people/deceased_person/place_of_burial</td><td>24</td><td>9</td></tr><tr><td>/location/us_county/county_seat</td><td>110</td><td>23</td></tr><tr><td>/people/ethnicity/geographic_distribution</td><td>86</td><td>136</td></tr><tr><td>/people/person/place_lived</td><td>8907</td><td>450</td></tr><tr><td>/business/company/major_shareholders</td><td>328</td><td>46</td></tr><tr><td>/broadcast/producer/location</td><td>71</td><td>0</td></tr><tr><td>/location/us_state/capital</td><td>798</td><td>39</td></tr><tr><td>/broadcast/content/location</td><td>8</td><td>0</td></tr><tr><td>/business/business_location/parent_company</td><td>19</td><td>0</td></tr><tr><td>/location/jp_prefecture/capital</td><td>2</td><td>0</td></tr><tr><td>/film/film/featured_film_locations</td><td>18</td><td>2</td></tr><tr><td>/people/place_of_interment/interred_here</td><td>24</td><td>9</td></tr><tr><td>/location/de_state/capital</td><td>7</td><td>0</td></tr><tr><td>/people/person/profession</td><td>10</td><td>0</td></tr><tr><td>/business/company/locations</td><td>19</td><td>0</td></tr><tr><td>/location/country/capital</td><td>8883</td><td>553</td></tr><tr><td>/location/location/contains</td><td>66721</td><td>2793</td></tr><tr><td>/people/person/ethnicity</td><td>148</td><td>13</td></tr><tr><td>/location/country/administrative_divisions</td><td>7286</td><td>424</td></tr><tr><td>/people/person/children</td><td>622</td><td>30</td></tr><tr><td>/film/film_location/featured_in_films</td><td>18</td><td>2</td></tr><tr><td>/film/film_festival/location</td><td>4</td><td>0</td></tr><tr><td>合计</td><td>522043</td><td>172448</td></tr></tbody></table></div><p>NYT10文件夹中包含4个文件：</p><p>Nyt10_rel2id.json : 包含53个关系及其各自对应的索引</p><p>Nyt10_train.txt : 包含466876个样本</p><p>Nyt10_val.txt : 包含55167个样本</p><p>Nyt10_test.txt : 包含172448个样本</p><p><strong><em>Ps:NYT10的数据集是通过远程监督得到的，所以样本的是根据包的形式分布的及含有相同实体的数据集分布在一起。</em></strong></p><p>样本格式：<br>例子：<br>{“text”: “Hundreds of bridges were added to the statewide inventory after an earthquake in 1994 in Northridge , a suburb of Los Angeles .”, “relation”: “/location/neighborhood/neighborhood_of”,“h”:{“id”:”/guid/9202a8c04000641f800000000008fe6d”, “name”: “Northridge”, “pos”: [89, 99]}, “t”: {“id”: “/guid/9202a8c04000641f80000000060b2879”, “name”: “Los Angeles”, “pos”: [114, 125]}}</p><p>与Wiki80的样本格式相似，区别在于NYT10的文本没有进行标记处理。</p><p><strong><em>NYT10数据集采用远程监督得到，包含噪声。</em></strong></p><p>数据来源：<a href="https://github.com/thunlp/OpenNRE/tree/master/benchmark" target="_blank" rel="noopener">https://github.com/thunlp/OpenNRE/tree/master/benchmark</a></p><p><br></p><h1 id="另外比较常用的数据集：TACRED、ACE-2005官网上下载均需要LDC账号。如有大佬愿意提供，不胜感谢！"><a href="#另外比较常用的数据集：TACRED、ACE-2005官网上下载均需要LDC账号。如有大佬愿意提供，不胜感谢！" class="headerlink" title="另外比较常用的数据集：TACRED、ACE 2005官网上下载均需要LDC账号。如有大佬愿意提供，不胜感谢！"></a><font color="orange">另外比较常用的数据集：TACRED、ACE 2005官网上下载均需要LDC账号。如有大佬愿意提供，不胜感谢！</font></h1><p><br></p><h1 id="关系抽取实时SOTA模型榜单"><a href="#关系抽取实时SOTA模型榜单" class="headerlink" title="关系抽取实时SOTA模型榜单"></a>关系抽取实时SOTA模型榜单</h1><p><a href="http://nlpprogress.com/english/relationship_extraction.html" target="_blank" rel="noopener">http://nlpprogress.com/english/relationship_extraction.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Relation Extraction </tag>
            
            <tag> NYT </tag>
            
            <tag> RE </tag>
            
            <tag> SemEval </tag>
            
            <tag> TACRED </tag>
            
            <tag> FewRel </tag>
            
            <tag> Wiki80 </tag>
            
            <tag> ACE2005 </tag>
            
            <tag> DocRED </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode-2-动态规划与背包问题</title>
      <link href="/2020/11/10/LeetCode-2-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B8%8E%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"/>
      <url>/2020/11/10/LeetCode-2-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B8%8E%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="1-将一个数分解为2的幂次"><a href="#1-将一个数分解为2的幂次" class="headerlink" title="1. 将一个数分解为2的幂次"></a>1. 将一个数分解为2的幂次</h2><p>即将一个数表示为1, 2, 4, … 的和的形式。</p><p><strong>分析</strong>: </p><ol><li>若n为奇数，则n的表示形式中必然含有一个+1，除去+1后剩下的即为n-1的表示形式，即f(n) = f(n-1)</li><li>若n为偶数，第一，若n的表示形式中含有1，则含有至少两个1，除去这两个+1，则为f(n-2)；若n的表示形式中不含有1，则均为偶数，因此，等价于除以2的形式，即f(n/2)</li></ol><p>综上，动态规划公式为：</p><script type="math/tex; mode=display">f(n) = \left \{\begin{align}f(n-1) & \quad & n为奇数\\ f(n-2) + f(n/2) & \quad & n为偶数\end{align}\right.</script><p>用C++实现：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">long</span> <span class="keyword">long</span> dp[<span class="number">1000001</span>];</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    dp[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    dp[<span class="number">2</span>] = <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">3</span>; i &lt;= <span class="number">1000000</span>; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(i&amp;<span class="number">1</span>) <span class="comment">// 按位操作</span></span><br><span class="line">            dp[i] = dp[i<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            dp[i] = dp[i<span class="number">-2</span>] + dp[i/<span class="number">2</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%lld\n"</span>, dp[n]%<span class="number">1000000000</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="2-分金币问题"><a href="#2-分金币问题" class="headerlink" title="2. 分金币问题"></a>2. 分金币问题</h2><p><strong>问题描述：</strong></p><p>有一堆各种面值的硬币，将所有硬币分成两堆，使得两堆的总值之差尽可能小。</p><p><strong>分析：</strong></p><p>就是0/1背包问题。将给出个n个硬币的面值之和记作sum，以sum/2为体积，求出sum/2下背包的最大值。</p><p>用C++实现：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> dp[maxm],a[maxm];</span><br><span class="line"><span class="keyword">int</span> n,m;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> t; <span class="built_in">cin</span>&gt;&gt;t;</span><br><span class="line">    <span class="keyword">while</span>(t--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cin</span>&gt;&gt;n;</span><br><span class="line">        <span class="keyword">int</span> sum=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">memset</span>(dp,<span class="number">0</span>,<span class="keyword">sizeof</span>(dp));</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=n;i++) <span class="built_in">cin</span>&gt;&gt;a[i],sum+=a[i];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=n;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=sum/<span class="number">2</span>;j&gt;=a[i];j--)</span><br><span class="line">            &#123;</span><br><span class="line">                dp[j] = max(dp[j],dp[j-a[i]]+a[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt; sum - <span class="number">2</span>*dp[sum/<span class="number">2</span>] &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DP </tag>
            
            <tag> Algorithms </tag>
            
            <tag> 动态规划 </tag>
            
            <tag> 背包问题 </tag>
            
            <tag> Dynamic Programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch的几种坐标选择函数</title>
      <link href="/2020/11/09/pytorch%E7%9A%84%E5%87%A0%E7%A7%8D%E5%9D%90%E6%A0%87%E9%80%89%E6%8B%A9%E5%87%BD%E6%95%B0/"/>
      <url>/2020/11/09/pytorch%E7%9A%84%E5%87%A0%E7%A7%8D%E5%9D%90%E6%A0%87%E9%80%89%E6%8B%A9%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>在pytorch中，有多种选择函数，能够明确区分它们之间的区别，才能更好的运用✅</p><h2 id="1-torch-masked-select"><a href="#1-torch-masked-select" class="headerlink" title="1. torch.masked_select()"></a>1. torch.masked_select()</h2><blockquote><p>Returns a new 1-D tensor which indexes the <code>input</code> tensor according to the boolean mask <code>mask</code> which is a BoolTensor.</p></blockquote><p>返回的是1维的向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vectors = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 想要选择第一个和第三个向量</span></span><br><span class="line">masks = torch.tensor([<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>], dtype=torch.bool).unsqueeze(<span class="number">1</span>)</span><br><span class="line">r = torch.masked_select(vectors, masks)</span><br><span class="line">print(r)</span><br><span class="line">print(r.shape)</span><br><span class="line"></span><br><span class="line">================================================================</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">torch.Size([<span class="number">6</span>])</span><br></pre></td></tr></table></figure><h2 id="2-torch-index-select"><a href="#2-torch-index-select" class="headerlink" title="2. torch.index_select()"></a>2. torch.index_select()</h2><p>沿着张量的某个<code>dim</code>方向，按照<code>index</code>规定的选取指定的低一维度张量元素整体，在拼接成一个张量。</p><p>先简单看两个示例：<br><strong>示例1</strong>：沿着<code>dim=0</code>的方向进行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="comment"># a为tensor([[1, 2, 3],</span></span><br><span class="line"><span class="comment">#        [4, 5, 6]])</span></span><br><span class="line"></span><br><span class="line">b = torch.index_select(a, dim=<span class="number">0</span>, index=torch.tensor([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]))</span><br><span class="line"><span class="comment"># b为tensor([[1, 2, 3],</span></span><br><span class="line"><span class="comment">#        [4, 5, 6],</span></span><br><span class="line"><span class="comment">#        [1, 2, 3],</span></span><br><span class="line"><span class="comment">#        [4, 5, 6]])</span></span><br></pre></td></tr></table></figure><p><img src="/images/blog/2020/select/1.png" alt></p><p>显然，对于二维张量，<code>dim=0</code>意味着按照<code>index</code>的编号选取指定的行，拼接成目标张量。其返回值仍保持和原始张量相同的<code>ndim</code>。<a id="more"></a></p><p><strong>示例2</strong>：沿着<code>dim=1</code>的方向进行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="comment"># a为tensor([[1, 2, 3],</span></span><br><span class="line"><span class="comment">#        [4, 5, 6]])</span></span><br><span class="line"></span><br><span class="line">b = torch.index_select(a, dim=<span class="number">1</span>, index=torch.tensor([<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line"><span class="comment"># b为tensor([[2, 2],</span></span><br><span class="line"><span class="comment">#          [5, 5]])</span></span><br></pre></td></tr></table></figure><p><img src="/images/blog/2020/select/2.png" alt></p><p>对于二维张量，<code>dim=1</code>意味着按照<code>index</code>的编号选取指定的列，拼接成目标张量。其返回值仍保持和原始张量相同的<code>ndim</code>。</p><p>根据上述两个例子，可见<code>index_select</code>的作用间接明了，即选取某个<code>dim</code>上的若干个元素，将其拼接为目标张量。其中<code>index</code>为一个一维张量，表明该<code>dim</code>上做选取的具体元素，返回张量与原张量的<code>ndim</code>一致。</p><h2 id="3-torch-gather"><a href="#3-torch-gather" class="headerlink" title="3. torch.gather()"></a>3. torch.gather()</h2><p>相较于<code>index_select</code>，<code>gather</code>就显得让人难以理解的多。个人理解，其操作相当于用于沿着张量的某个<code>dim</code>方向，按照<code>index</code>规定的选取指定元素，构成该为维度上的每个子张量，最后拼接成一个张量。</p><p>是不是还是令人费解？我们先以两个2维张量的例子来说明：</p><p><strong>示例1</strong>：沿着<code>dim=1</code>的方向进行选择</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="comment"># a为tensor([[1, 2, 3],</span></span><br><span class="line"><span class="comment">#        [4, 5, 6]])</span></span><br><span class="line"></span><br><span class="line">b = torch.gather(input=a, dim=<span class="number">1</span>, index=torch.tensor([[<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]]))</span><br><span class="line"><span class="comment"># 返回值为 tensor([[3, 1, 3, 2],</span></span><br><span class="line"><span class="comment">#         [5, 5, 4, 4]])</span></span><br></pre></td></tr></table></figure><p>其操作过程可参照下图：</p><p><img src="/images/blog/2020/select/3.png" alt></p><p>由上图可见，<code>dim=1</code>表示在二维张量中，以行为单位，对每行中的元素，按照<code>index</code>的索引号进行选取，再拼接到一起。从张量<code>shape</code>上看，其在<code>dim=0</code>上保持一致，对<code>dim=1</code>进行了放大或缩小。</p><p>对于更一般的张量，<code>gather</code>的过程可理解为沿着<code>dim</code>维的<code>size</code>，对各个子张量进行选取和重新的拼接，因此其返回值和原始张量的<code>ndim</code>是相同的。</p><p><strong>示例2</strong>：沿着<code>dim=0</code>的方向进行选择</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="comment"># a为tensor([[1, 2, 3],</span></span><br><span class="line"><span class="comment">#        [4, 5, 6]])</span></span><br><span class="line"></span><br><span class="line">b = torch.gather(input=a, dim=<span class="number">0</span>, index=torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]))</span><br><span class="line"><span class="comment"># 返回值为 tensor([[1, 5, 3],</span></span><br><span class="line"><span class="comment">#        [4, 2, 6],</span></span><br><span class="line"><span class="comment">#        [1, 2, 3],</span></span><br><span class="line"><span class="comment">#        [4, 5, 6]])</span></span><br></pre></td></tr></table></figure><p>其操作过程可参照下图：</p><p><img src="/images/blog/2020/select/4.png" alt></p><p>对于二维张量，其操作过程与<code>dim=1</code>相反，即以行为单位，对每列中的元素，按照<code>index</code>的索引号进行选取，再拼接到一起。从张量<code>shape</code>上看，其在<code>dim=1</code>上保持一致，对<code>dim=0</code>进行了放大或缩小。</p><p><strong>示例3</strong>：三维张量的例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment"># a为tensor([[[ 0,  1,  2,  3],</span></span><br><span class="line"><span class="comment">#         [ 4,  5,  6,  7],</span></span><br><span class="line"><span class="comment">#        [ 8,  9, 10, 11]],</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#        [[12, 13, 14, 15],</span></span><br><span class="line"><span class="comment">#         [16, 17, 18, 19],</span></span><br><span class="line"><span class="comment">#         [20, 21, 22, 23]]])</span></span><br><span class="line"></span><br><span class="line">b = torch.gather(a, dim=<span class="number">2</span>, index=torch.tensor([[[<span class="number">2</span>], [<span class="number">1</span>], [<span class="number">0</span>]], [[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># b为tensor([[[ 2],</span></span><br><span class="line"><span class="comment">#            [5],</span></span><br><span class="line"><span class="comment">#            [8]],</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#        [[13],</span></span><br><span class="line"><span class="comment">#        [18],</span></span><br><span class="line"><span class="comment">#        [23]]])</span></span><br></pre></td></tr></table></figure><p>简单解释下，其选取的<code>dim=2</code>，即沿着三维张量最内层的张量进行元素选取和拼接，其只选取了一次。因此，上述操作可理解为每个最内层选取一个元素。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensor </tag>
            
            <tag> pytorch </tag>
            
            <tag> gather </tag>
            
            <tag> masked_select </tag>
            
            <tag> index_select </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch中expand()与repeat()的区别</title>
      <link href="/2020/11/09/PyTorch%E4%B8%ADexpand-%E4%B8%8Erepeat-%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2020/11/09/PyTorch%E4%B8%ADexpand-%E4%B8%8Erepeat-%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p>二者都是用来扩展某维的数据的尺寸。</p><p><strong>一、expand()</strong><br>       返回当前张量在某维扩展更大后的张量。扩展（expand）张量不会分配新的内存，只是在存在的张量上创建一个新的视图（view），<strong>只能扩展为1的维度</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">print(<span class="string">'扩展前a的shape:'</span>, a.shape)</span><br><span class="line">a = a.expand(<span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">print(<span class="string">'扩展后a的shape:'</span>, a.shape)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">扩展前a的shape: torch.Size([<span class="number">4</span>])</span><br><span class="line">扩展后a的shape: torch.Size([<span class="number">8</span>, <span class="number">4</span>])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure><p><br></p><p><strong>二、repeat()</strong> <a id="more"></a><br>       沿着特定的维度重复这个张量，和expand()不同的是，这个函数拷贝张量的数据:</p><pre><code>     &lt;font color=&#39;green&#39;&gt;参数是指每个维度重复的次数，例如(1, 3, 1)表示第二维重复三次，其余维度不变。&lt;/font&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">print(<span class="string">'repeat前a的shape:'</span>, a.shape)</span><br><span class="line">a = a.repeat(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">print(<span class="string">'repeat后a的shape:'</span>, a.shape)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">repeat前a的shape: torch.Size([<span class="number">4</span>])</span><br><span class="line">repeat后a的shape: torch.Size([<span class="number">3</span>, <span class="number">8</span>])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensor </tag>
            
            <tag> pytorch </tag>
            
            <tag> expand </tag>
            
            <tag> repeat </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python random模块的随机采样函数</title>
      <link href="/2020/11/08/Python-random%E6%A8%A1%E5%9D%97%E7%9A%84%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7%E5%87%BD%E6%95%B0/"/>
      <url>/2020/11/08/Python-random%E6%A8%A1%E5%9D%97%E7%9A%84%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>choice(seq): 从seq序列中（可以是列表，元组，字符串）随机取一个元素返回</p><p><code>choices</code>(<em>population</em>, <em>weights=None</em>, ***, <em>cum_weights=None</em>, <em>k=1</em>)：从population中进行k次<strong>有放回随机选取</strong>，每次选取一个元素（注意会出现同一个元素多次被选中的情况），weights是相对权重值，population中有几个元素就要有相对应的weights值，cum_weights是累加权重值，例如，相对权重〔10, 5, 30, 5〕相当于累积权重〔10, 15, 45, 50〕。在内部，在进行选择之前，相对权重被转换为累积权重，因此提供累积权重节省了工作。返回一个列表。</p><p>sample(<em>population</em>, <em>k</em>)：从population中<strong>无放回取样</strong>，一次取k个，返回一个k长的列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以像这样使用：sample(range(<span class="number">10000000</span>), k=<span class="number">60</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> random </tag>
            
            <tag> python </tag>
            
            <tag> 随机 </tag>
            
            <tag> choice </tag>
            
            <tag> sample </tag>
            
            <tag> 采样 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hinge Loss 理解</title>
      <link href="/2020/11/07/Hinge-Loss-%E7%90%86%E8%A7%A3/"/>
      <url>/2020/11/07/Hinge-Loss-%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文转载自<a href="https://blog.csdn.net/hustqb/article/details/78347713" target="_blank" rel="noopener">https://blog.csdn.net/hustqb/article/details/78347713</a></p></blockquote><p>在机器学习中，<strong>hinge loss</strong>作为一个<strong>损失函数(loss function)</strong>，通常被用于<strong>最大间隔算法(maximum-margin)</strong>，而最大间隔算法又是SVM(支持向量机 support vector machines)用到的重要算法(注意：SVM的学习算法有两种解释：1. 间隔最大化与拉格朗日对偶；2. Hinge Loss)。</p><p>  <strong>Hinge loss专用于二分类问题</strong>，标签值y=±1，预测值$\hat y \in R$。该二分类问题的目标函数的要求如下：<br>  <font color="orange">当$\hat y$大于等于+1或者小于等于-1时，都是分类器确定的分类结果，此时的损失函数loss为0；</font>而当预测值$\hat y$ ∈(−1,1)时，分类器对分类结果不确定，loss不为0。显然，$\hat y$ =0时，loss达到最大值。</p><p>  如果你想到了一个可以定义这种loss的函数，那说明有成为数学家的潜质。想不到的话就乖乖的往下看：hinge loss出场。</p><p>​        <strong>对于输出y=±1，当前$\hat y$ 的损失为</strong>：</p><script type="math/tex; mode=display">l(y) = \max(0, 1-y \cdot \hat y)</script><p>上式是Hinge loss在二分类问题的的变体，可以看做双向Hinge loss。难以理解的话，可以先看单方向的hinge loss。以y=+1，为例。当y⩾1时，loss为0，否则loss线性增大。函数图像如下所示：</p><p><div align="center"> <img src="/images/blog/2020/hinge1.png" alt></div></p><p><br></p><h2 id="hinge-loss-在SVM中的应用"><a href="#hinge-loss-在SVM中的应用" class="headerlink" title="hinge loss 在SVM中的应用"></a>hinge loss 在SVM中的应用<a id="more"></a></h2><p>SVM在简单情况下（线性可分情况下）使用的就是一个最大间隔算法。几何意义如下图所示（实心的数据点就是该类别的支持向量），最大化分离超平面到两个类别的支持向量之间的距离 。</p><p><div align="center"> <img src="/images/blog/2020/hinge2.jpg" alt></div></p><p>​        线性可分SVM的预测值，其中和b都是分类器通过样本学习到的参数。正如前面所说，y^∈R。如果分离超平面在如上图所示的位置（这是最大分割情况）并且支持向量与分割平面之间的距离=1，每个y=1的样本其，每个y=−1的样本其，每个点的Hinge loss为0，整体loss作为平均值，也等于0。 如果分割超平面误分类，则Hinge loss大于0。Hinge loss驱动分割超平面作出调整。 如果分割超平面距离支持向量的距离小于1，则Hinge loss大于0，且就算分离超平面满足最大间隔，Hinge loss仍大于0</p><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>Hinge Loss是一个凸函数(convex function)，适用所有的机器学习凸优化方法。</p><p><strong>平滑</strong></p><p>为了解决Hinge Loss的优化问题，现在有两种平滑策略：</p><script type="math/tex; mode=display">l(y) =  \left \{\begin {aligned}\frac{1}{2} - ty  \quad ty \leq 0 \\\frac{1}{2}(1-ty)^2 \quad  0 < ty < 1 \\0 \quad 1 \leq ty\end {aligned}\right.</script><p>第二种：</p><script type="math/tex; mode=display">l(y) = \frac{1}{2\gamma} \max(0, 1-ty)^2</script><p>其中通常取$\gamma = 2$</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>再强调一下，使用Hinge loss的分类器的$\hat y$∈R。|$\hat y$ |越大，说明样本点离分割超平面越远，即该样本点很容易被分类。但是，我们在选择合适的损失函数进行优化时，没必要关注那些离超平面很远的样本。为此，我们可以通过对距分离超平面的距离选择一个阈值，来过滤这些离超平面很远的样本。<strong>这就是Hinge loss的精髓，式中的1就是我们选择的阈值，这个可以作为一个超参数。通过一个max(0, )函数，忽略值过高的情况</strong>。</p><p><font color="orange"><strong>即对于简单的样本，损失为0；对于困难样本，才有损失。而难易程度是以指定的hinge控制的。</strong></font> 可参考西瓜书130页。</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 损失函数 </tag>
            
            <tag> Hinge Loss </tag>
            
            <tag> 最大间隔算法 </tag>
            
            <tag> maximum-margin </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>聚类损失的实现</title>
      <link href="/2020/11/05/%E8%81%9A%E7%B1%BB%E6%8D%9F%E5%A4%B1%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
      <url>/2020/11/05/%E8%81%9A%E7%B1%BB%E6%8D%9F%E5%A4%B1%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<p>在最近的实验中，需要添加聚类损失，尝试将不同类别样本在特征空间中的距离拉远；同类样本之间距离拉近。如何使用PyTorch实现这一损失，我借鉴了<a href="https://github.com/INK-USC/NERO" target="_blank" rel="noopener">NERO</a>中的实现。</p><p>吐槽：用惯了pytorch，使用tensorflow先定义计算图，再用tf.Session()运行是真的不习惯，而且麻烦……</p><p><br></p><p>首先定义相似度矩阵，shape=(batch, batch)，这里直接给出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">tau = <span class="number">1.</span></span><br><span class="line">sim = tf.constant([</span><br><span class="line">    [<span class="number">0.8</span>, <span class="number">0.3</span>, <span class="number">0.6</span>],</span><br><span class="line">    [<span class="number">0.3</span>, <span class="number">0.9</span>, <span class="number">0.5</span>],</span><br><span class="line">    [<span class="number">0.6</span>, <span class="number">0.5</span>, <span class="number">0.8</span>]</span><br><span class="line">], dtype=tf.float64)</span><br></pre></td></tr></table></figure><p>然后定义关系，使用one-hot表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rels = tf.constant([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">], dtype=tf.float64)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">neg_idxs = tf.matmul(rels, tf.transpose(rels, [<span class="number">1</span>, <span class="number">0</span>]))  <span class="comment"># neg_idxs[i][j] = 1 表示两个样本类别相同</span></span><br><span class="line">pat_pos = tf.square(tf.maximum(tau - sim, <span class="number">0.</span>))  <span class="comment"># (batch, batch)</span></span><br><span class="line">pat_pos = tf.reduce_max(pat_pos - (<span class="number">1</span> - neg_idxs) * <span class="number">1e30</span>, axis=<span class="number">1</span>)  <span class="comment"># 找到了最不相似的正邻居</span></span><br><span class="line">pat_neg = tf.square(tf.maximum(sim, <span class="number">0.</span>))</span><br><span class="line">pat_neg = tf.reduce_max(pat_neg - <span class="number">1e30</span> * neg_idxs, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># l_sim = tf.reduce_sum(self.weight * (pat_pos + pat_neg), axis=0)</span></span><br><span class="line">l_sim = tf.reduce_sum(pat_pos + pat_neg, axis=<span class="number">0</span>)  <span class="comment"># 聚类损失</span></span><br></pre></td></tr></table></figure><a id="more"></a><blockquote><p>neg_idxs:</p><p>[[1, 0, 1]</p><p>[0, 1, 0]</p><p>[1, 0, 1]]</p><p>pat_pos:</p><p>[[0.04, 0.49, 0.16],</p><p>[0.49, 0.01, 0.25],</p><p>[0.16, 0.25, 0.04]]</p><p>pat_pos:</p><p>[0.16, 0.01, 0.16]</p><p>pat_neg同理与pat_pos</p></blockquote><p>聚类损失：（可以加权求和）</p><blockquote><p>l_sim=0.92</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 损失函数 </tag>
            
            <tag> cluster loss </tag>
            
            <tag> metric loss </tag>
            
            <tag> 聚类损失 </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode-1-素数分解</title>
      <link href="/2020/11/04/LeetCode-1-%E7%B4%A0%E6%95%B0%E5%88%86%E8%A7%A3/"/>
      <url>/2020/11/04/LeetCode-1-%E7%B4%A0%E6%95%B0%E5%88%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>有10^8个村庄排在一条公路上，依次编号为0~10^8-1，相邻村庄距离为1，其中有n个村庄居住着牛牛，居住牛牛的村庄依次为a0~an-1，保证a0=0.</p><p>现在需要 建设车站，有两个要求：</p><ol><li>每个有牛牛的村庄必须修建车站</li><li>相邻车站距离必须为1或者某个质数</li></ol><p>现给出n和a数组，求需要建设的车站的最小数量。</p><h2 id="2-分析"><a href="#2-分析" class="headerlink" title="2. 分析"></a>2. 分析</h2><p>需要解决两个关键问题：</p><ol><li>给出一个数的最少的素数分解</li><li>判断一个数是否为素数</li></ol><h2 id="3-解决"><a href="#3-解决" class="headerlink" title="3. 解决"></a>3. 解决</h2><h3 id="3-1-问题1"><a href="#3-1-问题1" class="headerlink" title="3.1 问题1"></a>3.1 问题1</h3><p>哥德巴赫猜想：任一大于2的整数都可以写作三个质数之和。</p><p><a href="https://baike.baidu.com/item/%E5%93%A5%E5%BE%B7%E5%B7%B4%E8%B5%AB%E7%8C%9C%E6%83%B3/72364?fr=aladdin" target="_blank" rel="noopener">哥德巴赫猜想的欧拉版本</a>：任一大于2的偶数都可以写成两个质数之和。</p><p>其实，也有一部分奇数可以用两个素数的和表示，大多数的奇数无法用两个素数的和表示，例如15=2+13，而23、25等则无法用两素数的和表示。</p><p><br></p><p><font color="orange">emmm，这样看来，不能直接利用哥德巴赫猜想解决问题。</font>但是，由于<strong>哥德巴赫猜想的欧拉版本</strong>已经被计算机验证到了很大的天文数字，所以可以减少我们一半的工作量。如果两个相邻车站间距离为偶数，我们有理由相信只需要在其中安插一个车站，就可以把距离变为两个质数；因此，只需要考虑两个相邻车站间距离为奇数的情况。</p><p><br></p><p>根据哥德巴赫猜想，奇数可以最少可以被拆分为两个或三个质数的和。没有更好的办法，<strong>只能通过遍历的方式，看能否被分为两个质数之和</strong>，如果不能，则必然可以被分为三个质数之和。而判断一个数是否为素数，就要通过3.2节的方法。<a id="more"></a></p><h3 id="3-2-问题2"><a href="#3-2-问题2" class="headerlink" title="3.2 问题2"></a>3.2 问题2</h3><p>具体为，把该数字1/2以内的数字都和自己整除一下，看看有没有其他因子。</p><p>但是这种思路虽然简单，在实践时却出现了超时问题：</p><p><img src="/images/blog/2020/leetcode/timeout.PNG" alt></p><p>显然，需要一种更加有技巧的算法，这时，就是需要“素数筛选法”发挥作用的时候了。</p><p><br></p><h4 id="素数筛选法"><a href="#素数筛选法" class="headerlink" title="素数筛选法"></a>素数筛选法</h4><p>开一个大小为n的数组，依次用2筛掉4,6,8,10…，用3筛掉6,9,12,…，依次进行，代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取range范围内的所有素数</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span>* <span class="title">get_prime</span><span class="params">(<span class="keyword">int</span> range)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">bool</span> *flags = <span class="keyword">new</span> <span class="keyword">bool</span>[range]; <span class="comment">// true means not prime</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; range; ++i) &#123;</span><br><span class="line">flags[i] = <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; range; ++i) &#123;</span><br><span class="line"><span class="keyword">if</span> (flags[i])</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = i + i; j &lt; range; j += i) &#123;</span><br><span class="line">flags[j] = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> flags;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="主体程序"><a href="#主体程序" class="headerlink" title="主体程序"></a>主体程序</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> station = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">bool</span> *primes = get_prime(<span class="number">10000000</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">station += <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span> (i + <span class="number">1</span> &lt; n) &#123;</span><br><span class="line"><span class="keyword">int</span> distance = a[i + <span class="number">1</span>] - a[i];</span><br><span class="line"><span class="keyword">if</span> (distance &lt;= <span class="number">2</span>);  </span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (!primes[distance]); <span class="comment">// 质数</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (distance % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">station += <span class="number">1</span>; <span class="comment">// 哥德巴赫猜想:偶数</span></span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">bool</span> split2 = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">2</span>; j &lt;= distance / <span class="number">2</span>; ++j) &#123;  <span class="comment">// 哥德巴赫猜想:奇数</span></span><br><span class="line"><span class="keyword">if</span> (!primes[j] &amp;&amp; !primes[distance - j]) &#123;</span><br><span class="line">split2 = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (split2)</span><br><span class="line">station += <span class="number">1</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">station += <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; station &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><p>再次提交，发现依然超时，😥，百思不得其解，在网上查阅后，发现自己对奇数的处理过于复杂，<strong>奇数如果表示为两个数之和，则必然为奇+偶，若想要只有两个质数相加，则偶数必然为2，因此只需要判断减去2后是否为质数</strong>。</p><p>修改<code>哥德巴赫猜想：奇数</code>部分代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (!primes[distance - <span class="number">2</span>]) <span class="comment">// 哥德巴赫猜想:奇数</span></span><br><span class="line">station += <span class="number">1</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">station += <span class="number">2</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但是还是没通过……😭</p><p>在网上查到了一个，看起来很简单，可能是我想的复杂了？</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">is_prime</span><span class="params">(<span class="keyword">int</span> a)</span></span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; a; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(a % i == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">work</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span>* a, <span class="keyword">int</span> aLen)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span> &amp;&amp; n==<span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            aLen = n;</span><br><span class="line">            <span class="keyword">return</span> aLen;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n<span class="number">-1</span>; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> len = a[i+<span class="number">1</span>]-a[i];</span><br><span class="line">            <span class="keyword">if</span>(is_prime(len) == <span class="number">1</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                result++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(len % <span class="number">2</span> == <span class="number">0</span> || is_prime(len<span class="number">-2</span>))</span><br><span class="line">            &#123;</span><br><span class="line">                result+=<span class="number">2</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                result+=<span class="number">3</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        aLen = result;</span><br><span class="line">        <span class="keyword">return</span> aLen;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>学到了很多东西。另外，补充一个题外话，在C++中计时，观察程序运行时间：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;ctime&gt;</span></span></span><br><span class="line"><span class="keyword">clock_t</span> time_start = clock();</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="keyword">clock_t</span> time_end = clock();</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"time use:"</span> &lt;&lt; <span class="number">1000</span> * (<span class="keyword">long</span> <span class="keyword">long</span>)(time_end - time_start) / (<span class="keyword">double</span>)CLOCKS_PER_SEC &lt;&lt; <span class="string">"ms"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><p>今天就学到这里，下班了下班了嘻嘻🤭😄</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
            <tag> DP </tag>
            
            <tag> 动态规划 </tag>
            
            <tag> 背包问题 </tag>
            
            <tag> Algothrim </tag>
            
            <tag> 素数 </tag>
            
            <tag> 哥德巴赫猜想 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>向量可视化</title>
      <link href="/2020/11/02/%E5%90%91%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
      <url>/2020/11/02/%E5%90%91%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>近期在实验中，需要对特征空间的向量进行可视化，观察向量的空间分布是否符合预期，对向量可视化进行了一些简单的探索。</p><h2 id="1-使用tensorboard"><a href="#1-使用tensorboard" class="headerlink" title="1. 使用tensorboard"></a>1. 使用tensorboard</h2><p>使用tensorboard可以对向量进行可视化，可以动态的拖拽、缩放，非常的方便。使用tensorboard可视化主要用到下面的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualisation</span><span class="params">(words, embeddings, log_path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param words: 词list</span></span><br><span class="line"><span class="string">    :param embeddings: np.array 词向量矩阵</span></span><br><span class="line"><span class="string">    :param log_path:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Set up a logs directory, so Tensorboard knows where to look for files</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(log_path):</span><br><span class="line">        os.makedirs(log_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save Labels separately on a line-by-line manner.</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(log_path, <span class="string">'metadata.tsv'</span>), <span class="string">"w"</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> subwords <span class="keyword">in</span> words:</span><br><span class="line">            f.write(<span class="string">"&#123;&#125;\n"</span>.format(subwords))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save the weights we want to analyse as a variable. Note that the first</span></span><br><span class="line">    <span class="comment"># value represents any unknown word, which is not in the metadata, so</span></span><br><span class="line">    <span class="comment"># we will remove that value.</span></span><br><span class="line">    weights = tf.Variable(embeddings)</span><br><span class="line">    <span class="comment"># Create a checkpoint from embedding, the filename and key are</span></span><br><span class="line">    <span class="comment"># name of the tensor.</span></span><br><span class="line">    checkpoint = tf.train.Checkpoint(embedding=weights)</span><br><span class="line">    checkpoint.save(os.path.join(log_path, <span class="string">"embedding.ckpt"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set up config</span></span><br><span class="line">    config = projector.ProjectorConfig()</span><br><span class="line">    embedding = config.embeddings.add()</span><br><span class="line">    <span class="comment"># The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`</span></span><br><span class="line">    embedding.tensor_name = <span class="string">'embedding/.ATTRIBUTES/VARIABLE_VALUE'</span></span><br><span class="line">    embedding.metadata_path = <span class="string">'metadata.tsv'</span></span><br><span class="line">    projector.visualize_embeddings(log_path, config)</span><br></pre></td></tr></table></figure><p>主要参考了TensorFlow的官方指南<a href="https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin" target="_blank" rel="noopener">https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin</a></p><p>这里<code>embedding.tensor_name = &#39;embedding/.ATTRIBUTES/VARIABLE_VALUE&#39;</code>是不能随意改动的，不明白为什么…不过暂时不再深入探究了，不是关键问题。</p><p><br></p><p>补充：关于<code>/.ATTRIBUTES/VARIABLE_VALUE/</code>问题，<a href="https://github.com/tensorflow/tensorboard/issues/2471" target="_blank" rel="noopener">https://github.com/tensorflow/tensorboard/issues/2471</a></p><blockquote><p>The tensor name change is a bit trickier. I looked into this a bit, and<br>it’s due to changes in the TensorFlow checkpoint code itself, as you<br>suggest; the tokens <a href="https://github.com/tensorflow/tensorflow/blob/5b4fe5470852d1aea737b194e03727cdedddebca/tensorflow/python/training/tracking/graph_view.py#L45-L48" target="_blank" rel="noopener"><code>.ATTRIBUTES</code></a> and <a href="https://github.com/tensorflow/tensorflow/blob/5b4fe5470852d1aea737b194e03727cdedddebca/tensorflow/python/training/tracking/base.py#L40-L44" target="_blank" rel="noopener"><code>VARIABLE_VALUE</code></a> are<br>hard-coded, and you need to supply them on any checkpoint-related<br>methods (e.g., <code>read_tensor</code>).</p></blockquote><p>也就是说，由于TensorFlow的一些原因，必须加上这个名称后缀，就这样做就是了，没有什么值得探究。</p><p><br></p><p>可视化效果如图：</p><p><img src="/images/blog/2020/visual/feature_space.png" alt></p><p>并不像预期的那样不同的类别泾渭分明😥甚至看不出明显的界限，全部混杂在了一起。猜想原因可能是有两个：</p><ul><li>训练不充分，学习效果不够好</li><li>可视化的数据太多<a id="more"></a></li></ul><p><strong>针对第一点，可以采取的做法是调整神经网络，增加训练轮数；修改Loss函数，增加聚类损失，迫使特征空间中不同类别的向量距离拉远，同类别的向量距离拉近。</strong>这里留在后续的实验进行尝试探索。</p><p>针对第二点，每个类别随机筛选10个样本，再次查看特征空间：</p><p><img src="/images/blog/2020/visual/feature_10.png" alt></p><p>左侧是真实数据的分布，右侧是生成的伪数据的分布，在上方，还有一些数据点，可以看作异常数据。对其进行分析，可以得到以下几个结论：</p><font color="green">* 真实数据的分布中，有些类别学习效果较好，例如天蓝色的点，几乎都集中在左下方。但是整体仍然没有达到预期的效果，不同类别之间重叠依然过大。* 在生成的伪数据上，分布不如真实数据，更加挤在一团。* 真实数据和伪数据之间泾渭分明，不知道是否是一种好现象？按照理论分析，生成的伪数据应该可以逼近真实数据的分布，是只需要形似？还是需要两个特征空间能够糅合在一起呢？</font><p><br></p><p>除去上述实验，我还对通用的glove词向量进行了实验，用PCA降维到3D空间后，不同语义的词能够很好地划分开界限。</p><p><img src="/images/blog/2020/visual/3d.png" alt></p><h2 id="2-使用matplotlib"><a href="#2-使用matplotlib" class="headerlink" title="2. 使用matplotlib"></a>2. 使用matplotlib</h2><p>直接上代码，简单粗暴😀，主要是用到sklearn中的PCA降维，然后用pyplot绘制出点图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wanted_word = [<span class="string">'man'</span>, <span class="string">'woman'</span>, <span class="string">'human'</span>, <span class="string">'boy'</span>, <span class="string">'girl'</span>, <span class="string">'son'</span>, <span class="string">'daughter'</span>, <span class="string">'child'</span>,</span><br><span class="line">         <span class="string">'car'</span>, <span class="string">'plane'</span>, <span class="string">'railway'</span>, <span class="string">'boat'</span>,</span><br><span class="line">         <span class="string">'tree'</span>, <span class="string">'flower'</span>, <span class="string">'grass'</span>, <span class="string">'leaf'</span>,</span><br><span class="line">         <span class="string">'cow'</span>, <span class="string">'sheep'</span>, <span class="string">'dog'</span>, <span class="string">'cat'</span>, <span class="string">'frog'</span>, <span class="string">'puppy'</span>, <span class="string">'pussy'</span>, <span class="string">'horse'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">word2id = <span class="string">'word2id_semeval.json'</span></span><br><span class="line">embedding = <span class="string">'glove_mat_semeval.npy'</span></span><br><span class="line">word2id = json.load(open(word2id, encoding=<span class="string">'utf-8'</span>))</span><br><span class="line">matrix = np.load(embedding)</span><br><span class="line">key = []</span><br><span class="line">value = []</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word2id:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> wanted_word:</span><br><span class="line">        key.append(word)</span><br><span class="line">        value.append(matrix[word2id[word]].tolist())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">result = pca.fit_transform(value)</span><br><span class="line"><span class="comment"># 可视化展示</span></span><br><span class="line">pyplot.scatter(result[:, <span class="number">0</span>], result[:, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(key):</span><br><span class="line">    pyplot.annotate(word, xy=(result[i, <span class="number">0</span>], result[i, <span class="number">1</span>]))</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>结果如下如所示，很直观吧？🤭</p><p><img src="/images/blog/2020/visual/glove_embedding.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensor </tag>
            
            <tag> embedding </tag>
            
            <tag> tensorboard </tag>
            
            <tag> visualisation </tag>
            
            <tag> PCA </tag>
            
            <tag> T-SNE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch loss.backward()的理解</title>
      <link href="/2020/11/01/pytorch-loss-backward-%E7%9A%84%E7%90%86%E8%A7%A3/"/>
      <url>/2020/11/01/pytorch-loss-backward-%E7%9A%84%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>部分引用以下文章：</p><p><a href="https://blog.csdn.net/douhaoexia/article/details/78821428" target="_blank" rel="noopener">https://blog.csdn.net/douhaoexia/article/details/78821428</a></p><p><a href="https://blog.csdn.net/a845717607/article/details/104598278" target="_blank" rel="noopener">https://blog.csdn.net/a845717607/article/details/104598278</a></p><p><a href="https://zhuanlan.zhihu.com/p/84890656" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/84890656</a></p></blockquote><p><br></p><p>最近一直在用pytorch做GAN相关的实验。<br>这两天遇到了一些模型参数寻优的问题，才发现自己对pytorch的自动求导和寻优功能没有深刻理解，导致无法灵活的进行实验。于是查阅资料，同时自己做了一点小实验，做了一些总结，虽然好像都是一些显而易见的结论，但是如果不能清晰的理解，对于实验复杂的网络模型程序会造成困扰。</p><ul><li><p>相关标志位/函数</p><ul><li>1<ul><li>requires_grad</li><li>volatile</li><li>detach()/detach_()</li></ul></li><li>2<ul><li>retain_graph</li><li>retain_variables</li><li>create_graph</li></ul></li></ul><p>前三个标志位中，最关键的就是 requires_grad，另外两个都可以转化为 requires_grad 来理解。<br>后三个标志位，与计算图的保持与建立有关系。其中 retain_variables 与 retain_graph等价，retain_variables 在pytorch 新版本中被取消掉。</p></li><li><p>requires_grad 的含义及标志位说明</p><ul><li>如果对于某Variable 变量 x ，其 <code>x.requires_grad == True</code> , 则表示 它可以参与求导，也可以从它向后求导。<br>默认情况下，一个新的Variables 的 requires_grad 和 volatile 都等于 False 。</li><li><code>requires_grad == True</code> 具有传递性，如果：<br><code>x.requires_grad == True</code> ，<code>y.requires_grad == False</code> ， <code>z=f(x,y)</code><br>则， <code>z.requires_grad == True</code></li><li>凡是参与运算的变量（包括 输入量，中间输出量，输出量，网络权重参数等），都可以设置 requires_grad 。</li><li><code>volatile==True</code> 就等价于 <code>requires_grad==False</code> 。 <code>volatile==True</code> 同样具有传递性。一般只用在inference过程中。若是某个过程，从 x 开始 都只需做预测，不需反传梯度的话，那么只需设置<code>x.volatile=True</code> ,那么 x 以后的运算过程的输出均为 <code>volatile==True</code> ,即 <code>requires_grad==False</code> 。<br>虽然inference 过程不必backward(),所以requires_grad 的值为False 或 True，对结果是没有影响的，但是对程序的运算效率有直接影响；所以使用<code>volatile=True</code> ,就不必把运算过程中所有参数都手动设一遍<code>requires_grad=False</code> 了，方便快捷。</li><li><code>detach()</code> ，如果 x 为中间输出，<code>x&#39; = x.detach</code> 表示创建一个与 x 相同，但<code>requires_grad==False</code> 的variable, (实际上是把x’ 以前的计算图 grad<em>fn 都消除了)，x’ 也就成了叶节点。原先反向传播时，回传到x时还会继续，而现在回到x’处后，就结束了，不继续回传求到了。另外值得注意, x (variable类型) 和 x’ (variable类型)都指向同一个Tensor ,即 x.data<br>而`detach</em>()` 表示不创建新变量，而是直接修改 x 本身。</li><li><code>retain_graph</code> ，每次 backward() 时，默认会把整个计算图free掉。一般情况下是每次迭代，只需一次 forward() 和一次 backward() ,前向运算forward() 和反向传播backward()是成对存在的，一般一次backward()也是够用的。但是不排除，由于自定义loss等的复杂性，需要一次forward()，多个不同loss的backward()来累积同一个网络的grad,来更新参数。于是，若在当前backward()后，不执行forward() 而可以执行另一个backward()，需要在当前backward()时，指定保留计算图，即backward(retain_graph)。</li><li>create_graph ，这个标志位用于自动增加梯度对应的计算图，可用于计算二阶导数以及更高阶导数。<a id="more"></a></li></ul></li><li>反向求导 和 权重更新<ul><li>求导和优化（权重更新）是两个<strong>独立</strong>的过程，只不过优化时一定需要对应的已求取的梯度值。所以求得梯度值很关键，而且，经常会累积多种loss对某网络参数造成的梯度，一并更新网络。</li><li>反向传播过程中，肯定需要整个过程都链式求导。虽然中间参数参与求导，但是却可以不用于更新该处的网络参数。参数更新可以只更新想要更新的网络的参数。</li><li>如果obj是函数运算结果，且是标量，则 obj.backward() （注意，backward()函数中没有填入任何tensor值, 就相当于 <code>backward(torch.tensor([1]))</code> ）。</li><li>对于继承自 nn.Module 的某一网络 net 或网络层，定义好后,发现 默认情况下，net.paramters 的 requires_grad 就是 True 的（虽然只是实验证明的，还未从源码处找到证据），这跟普通的Variable张量不同。因此，当<code>x.requires_grad == False</code> , <code>y = net(x)</code> 后, 有 <code>y.requires_grad == True</code> ;但值得注意，虽然nn.xxloss和激活层函数，是继承nn.Module的，但是这两种并没有网络参数，就更谈不上 paramters.requires_grad 的值了。所以类似这两种函数的输出，其requires_grad只跟输入有关，不一定是 True .</li></ul></li><li>计算图相关<ul><li>计算图就是模型 前向forward() 和后向求梯度backward() 的流程参照。</li><li>能获取回传梯度(grad)的只有计算图的叶节点。注意是获取，而不是求取。中间节点的梯度在计算求取并回传之后就会被释放掉，没办法获取。想要获取中间节点梯度，可以使用 register_hook （钩子）函数工具。当然， register_hook 不仅仅只有这个作用。</li><li>只有标量才能直接使用 backward()，即<code>loss.backward()</code> , pytorch 框架中的各种nn.xxLoss()，得出的都是minibatch 中各结果 平均/求和 后的值。如果使用自定义的函数，得到的不是标量，则backward()时需要传入 grad_variable 参数，这一点详见博客 <a href="https://sherlockliao.github.io/2017/07/10/backward/" target="_blank" rel="noopener">https://sherlockliao.github.io/2017/07/10/backward/</a> 。</li><li>经常会有这样的情况：<br>x1 —&gt; |net1| —&gt; y1 —&gt; |net2| —&gt; z1 , net1和net2是两个不同的网络。x1 依次通过 两个网络运算，生成 z1 。比较担心一次性运算后，再backward(),是不是只更新net1 而不是net1、net2都更新呢？<br>类比 x2 —&gt; |f1| —&gt; y2 —&gt; |f2| —&gt; z2 , f1 、f2 是两个普通的函数，<code>z2=f2(y2)</code> , <code>y2=f1(x2)</code> 。<br>按照以下代码实验</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">w1 = torch.Tensor([<span class="number">2</span>]) <span class="comment">#认为w1 与 w2 是函数f1 与 f2的参数</span></span><br><span class="line">w1 = Variable(w1,requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.Tensor([<span class="number">2</span>])</span><br><span class="line">w2 = Variable(w2,requires_grad=<span class="literal">True</span>)</span><br><span class="line">x2 = torch.rand(<span class="number">1</span>)</span><br><span class="line">x2 = Variable(x2,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y2 = x2**w1            <span class="comment"># f1 运算</span></span><br><span class="line">z2 = w2*y2+<span class="number">1</span>           <span class="comment"># f2 运算</span></span><br><span class="line">z2.backward()</span><br><span class="line">print(x2.grad)</span><br><span class="line">print(y2.grad)</span><br><span class="line">print(w1.grad)</span><br><span class="line">print(w2.grad)</span><br></pre></td></tr></table></figure><p>发现 x2.grad，w1.grad，w2.grad 是个值 ，但是 y2.grad 却是 <code>None</code>, 说明x2,w1,w2的梯度保留了，y2 的梯度获取不到。实际上，仔细想一想会发现，x2,w1,w2均为叶节点。<strong>在这棵计算树中 ，x2 与w1 是同一深度(底层)的叶节点，y2与w2 是同一深度，w2 是单独的叶节点，而y2 是x2 与 w1 的父节点，所以只有y2没有保留梯度值，</strong> 印证了之前的说法。同样这也说明，<strong>计算图本质就是一个类似二叉树的结构。</strong></p><p><img src="/images/blog/2020/gradtree.png" alt></p><p>那么对于 两个网络，会是怎么样呢？ 我使用pytorch 的cifar10 例程，稍作改动做了实验。把例程中使用的一个 Alexnet 拆成了两个net ——— net1 和 net2 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(itertools.chain(net1.parameters(), net2.parameters()),lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>) <span class="comment"># 这里 net1 和net2 优化的先后没有区别 ！！</span></span><br><span class="line">   <span class="comment">#</span></span><br><span class="line">   optimizer.zero_grad() <span class="comment">#将参数的grad值初始化为0</span></span><br><span class="line">   <span class="comment">#</span></span><br><span class="line">   <span class="comment"># forward + backward + optimize</span></span><br><span class="line">   outputs1 = net1(inputs)            <span class="comment">#input 未置requires_grad为True，但不影响</span></span><br><span class="line">   outputs2 = net2(outputs1)</span><br><span class="line">   loss = criterion(outputs2, labels) <span class="comment">#计算损失</span></span><br><span class="line">   loss.backward()                    <span class="comment">#反向传播      </span></span><br><span class="line">   <span class="comment">#     </span></span><br><span class="line">   print(<span class="string">"inputs.requires_grad:"</span>)</span><br><span class="line">   print(inputs.requires_grad)        <span class="comment"># False</span></span><br><span class="line">   print(<span class="string">"the grad of inputs:"</span>)</span><br><span class="line">   print(inputs.grad)                 <span class="comment"># None</span></span><br><span class="line">   </span><br><span class="line">   print(<span class="string">"outputs1.requires_grad:"</span>)</span><br><span class="line">   print(outputs1.requires_grad)      <span class="comment"># True</span></span><br><span class="line">   print(<span class="string">"the grad of outputs1:"</span>)        </span><br><span class="line">   print(outputs1.grad)               <span class="comment"># None     </span></span><br><span class="line">   <span class="comment"># </span></span><br><span class="line">   print(<span class="string">"the grad of net1:"</span>)</span><br><span class="line">   print(net1.conv1.bias.grad)        <span class="comment"># no-None</span></span><br><span class="line">   print(<span class="string">"the grad of net2:"</span>)</span><br><span class="line">   print(net2.fc3.bias.grad)          <span class="comment"># no-None</span></span><br><span class="line">   <span class="comment">#</span></span><br><span class="line">   optimizer.step() <span class="comment">#用SGD更新参数</span></span><br></pre></td></tr></table></figure><p>后缀注释就是打印的结果。可以看出，只有网络参数的grad是直接可获取的。<em>而且是两个网络都可以获取grad 值</em>，获取grad后，当然就可以更新网络的参数了，两个网络都是可以更新的。</p><p>类比上边例子的解释，<strong>两个网络其实就是处在叶节点的位置，只不过深度不同。同理，网络内部的运算，每一层网络权重参数其实也是处在叶节点上，只不过在树中的深度不同罢了，前向运算时按照二叉树的结构，不断生成父节点。</strong></p><p>(事实上，原先是以为 <em>网络</em> 与 普通函数不同，因为它具有register_xx_hook()这个类函数工具，所以认为它可以默认保存权重参数的grad来用于更新，后来才明白，本质上与普通函数的参数一样，都是处在叶节点，就可以保存参数的grad，至于register_xx_hook()，看来是另做它用，或者说用register_xx_hook()可以记录甚至更改中间节点的grad值）</p><p>一些特殊的情况：</p><ul><li>把网络某一部分参数，固定，不让其被训练。可以使用requires_grad.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> sub_module.parameters():</span><br><span class="line">p.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>可以这样理解，因为是叶节点（而不是中间节点），所以不求grad（grad为’None’），也不会影响网络的正常反向传播。</p><p>但是要注意，如果你是这样写的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad() 清空过往梯度；</span><br><span class="line">   loss1.backward(retain_graph=<span class="literal">True</span>) 反向传播，计算当前梯度；</span><br><span class="line">   loss2.backward(retain_graph=<span class="literal">True</span>) 反向传播，计算当前梯度；</span><br><span class="line">   optimizer.step() 根据梯度更新网络参数</span><br></pre></td></tr></table></figure><p>那么你可能会出现内存溢出的情况，并且，每一次迭代会比上一次更慢，越往后越慢（因为你的梯度都保存了，没有free）<br>解决的方法，当然是这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad() 清空过往梯度；</span><br><span class="line">   loss1.backward(retain_graph=<span class="literal">True</span>) 反向传播，计算当前梯度；</span><br><span class="line">   loss2.backward() 反向传播，计算当前梯度；</span><br><span class="line">   optimizer.step() 根据梯度更新网络参数</span><br></pre></td></tr></table></figure><p>即：最后一个backward()不要加retain_graph参数，这样每次更新完成后会释放占用的内存，也就不会出现越来越慢的情况了。</p><p>这里有人就会问了，我又没有这么多 loss，怎么还会出现这种错误呢？这里可能是因为，你用的模型本身有问题，LSTM和GRU都会出现这样的问题，问题存在与hidden unit，这个东东也参与了反向传播，所以导致了有多个backward()，<br>这里其实我也挺费解，为什么存在多个backward()呢？难道是，我的LSTM网络是N to N，即输入N和，输出N个，然后和N个label进行计算loss，再进行回传，这里，可以思考一下BPTT，即，如果是N to 1，那么梯度更新需要时间序列所有的输入以及隐藏变量计算梯度，然后从最后一个向前传，所以只有一个backward()， 而N to N 以及 N to M 都会出现多个loss需要进行backward()的情况，如果还是两个方向（一个从输出到输入，一个沿着时间）一直进行传播，那么其实会有重叠的部分。</p><p>为了更好的理解梯度回传的实现，准备找个时间读一下pytorch中CNN层和LSTM层的源代码💪，下一篇博客再做记录~，可参考 <a href="https://zhuanlan.zhihu.com/p/97045053" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/97045053</a></p>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
          <category> GAN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural Networks </tag>
            
            <tag> PyTorch </tag>
            
            <tag> grad </tag>
            
            <tag> 梯度 </tag>
            
            <tag> 高阶导数 </tag>
            
            <tag> 二次求导 </tag>
            
            <tag> 复杂网络 </tag>
            
            <tag> higher order derivative </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch之requires_grad</title>
      <link href="/2020/10/27/PyTorch%E4%B9%8Brequires-grad/"/>
      <url>/2020/10/27/PyTorch%E4%B9%8Brequires-grad/</url>
      
        <content type="html"><![CDATA[<p>requires_grad是Pytorch中通用数据结构Tensor的一个属性，用于说明当前量是否需要在计算中保留对应的梯度信息，以线性回归为例，容易知道权重w和偏差b为需要训练的对象，为了得到最合适的参数值，我们需要设置一个相关的损失函数，根据梯度回传的思路进行训练。</p><p>官方文档中的说明如下：</p><blockquote><p>If there’s a single input to an operation that requires gradient, its output will also require gradient.</p></blockquote><p>只要某一个输入需要相关梯度值，则输出也需要保存相关梯度信息，这样就保证了这个输入的梯度回传。<br>而反之，若所有的输入都不需要保存梯度，那么输出的requires_grad会自动设置为False。既然没有了相关的梯度值，自然进行反向传播时会将这部分子图从计算中剔除。</p><p>对于那些要求梯度的tensor，PyTorch会存储他们相关梯度信息和产生他们的操作，这产生额外内存消耗，为了优化内存使用，默认产生的tensor是不需要梯度的。<br>而我们在使用神经网络时，这些<strong>全连接层卷积层等结构的参数都是默认需要梯度的。</strong><a id="more"></a></p><p><br></p><p>而打印参数名和参数值，使用</p><ul><li>state_dict():打印model所有参数名</li><li>named_parameters():打印model所有参数名和参数值（包括从父类继承的参数）返回的是迭代器</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">    print(d[<span class="number">0</span>], d[<span class="number">1</span>].requires_grad)</span><br></pre></td></tr></table></figure><p><br></p><p><div align="center"> <img src="/images/blog/2020/parameters.png" alt></div></p><center>模型参数打印结果</center>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
            <tag> requires_grad </tag>
            
            <tag> parameters </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch实战 计算Wasserstein距离</title>
      <link href="/2020/10/26/Pytorch%E5%AE%9E%E6%88%98-%E8%AE%A1%E7%AE%97Wasserstein%E8%B7%9D%E7%A6%BB/"/>
      <url>/2020/10/26/Pytorch%E5%AE%9E%E6%88%98-%E8%AE%A1%E7%AE%97Wasserstein%E8%B7%9D%E7%A6%BB/</url>
      
        <content type="html"><![CDATA[<blockquote><p>编译：机器之心， 作者： Daniel Daza</p><p>最优传输理论及 Wasserstein 距离是很多读者都希望了解的基础，本文主要通过简单案例展示了它们的基本思想，并通过 PyTorch 介绍如何实战 W 距离。</p></blockquote><p><br></p><p>机器学习中的许多问题都涉及到令两个分布尽可能接近的思想，例如在 GAN 中令生成器分布接近判别器分布就能伪造出逼真的图像。但是 KL 散度等分布的度量方法有很多局限性，本文则介绍了 Wasserstein 距离及 Sinkhorn 迭代方法，它们在 GAN 及众多任务上都展示了杰出的性能。</p><p>在简单的情况下，我们假设从未知数据分布 p(x) 中观测到一些随机变量 x（例如，猫的图片），我们想要找到一个模型 q(x|θ)（例如一个神经网络）能作为 p(x) 的一个很好的近似。如果 p 和 q 的分布很相近，那么就表明我们的模型已经学习到如何识别猫。</p><p>因为 KL 散度可以度量两个分布的距离，所以只需要最小化 KL(q‖p) 就可以了。可以证明，最小化 KL(q‖p) 等价于最小化一个负对数似然，这样的做法在我们训练一个分类器时很常见。例如，对于变分自编码器来说，我们希望后验分布能够接近于某种先验分布，这也是我们通过最小化它们之间的 KL 散度来实现的。</p><p>尽管 KL 散度有很广泛的应用，在某些情况下，KL 散度则会失效。不妨考虑一下如下图所示的离散分布：</p><p><img src="/images/blog/2020/wasserstein/distribution.png" alt></p><p><br></p><p>KL 散度假设这两个分布共享相同的支撑集（也就是说，它们被定义在同一个点集上）。因此，我们不能为上面的例子计算 KL 散度。由于这一个限制和其他计算方面的因素促使研究人员寻找一种更适合于计算两个分布之间差异的方法。</p><p>在本文中，作者将：</p><ul><li>简单介绍最优传输问题</li><li>将 Sinkhorn 迭代描述为对解求近似</li><li>使用 PyTorch 计算 Sinkhorn 距离</li><li>描述用于计算 mini-batch 之间的距离的对该实现的扩展<a id="more"></a></li></ul><p><br></p><p><center><font color="red">移动概率质量函数</font></center><br>我们不妨把离散的概率分布想象成空间中分散的点的质量。我们可以观测这些带质量的点从一个分布移动到另一个分布需要做多少功，如下图所示：</p><p><img src="/images/blog/2020/wasserstein/move.png" alt></p><p>接着，我们可以定义另一个度量标准，用以衡量移动所有点所需要做的功。要想将这个直观的概念形式化定义下来，首先，我们可以通过引入一个<font color="orange"><strong>耦合矩阵</strong></font> P（coupling matrix），它表示要从 p(x) 支撑集中的一个点上到 q(x) 支撑集中的一个点需要分配多少概率质量。对于均匀分布，我们规定每个点都具有 1/4 的概率质量。如果我们将本例支撑集中的点从左到右排列，我们可以将上述的耦合矩阵写作：</p><script type="math/tex; mode=display">P = \left [\begin{matrix} 0 & 0 & 0 & \frac{1}{4} \\0 & 0 & \frac{1}{4} & 0 \\0 & \frac{1}{4} & 0 & 0 \\\frac{1}{4} & 0 & 0 & 0 \\\end{matrix} \right ]</script><p>也就是说，p(x) 支撑集中点 1 的质量被分配给了 q(x) 支撑集中的点 4，p(x) 支撑集中点 2 的质量被分配给了 q(x) 支撑集中的点 3，以此类推。</p><p>为了算出质量分配的过程需要做多少功，我们将引入第二个矩阵：<font color="orange"><strong>距离矩阵</strong></font>。该矩阵中的每个元素 C_ij 表示将 p(x) 支撑集中的点移动到 q(x) 支撑集中的点上的成本。点与点之间的欧几里得距离是定义这种成本的一种方式，它也被称为「ground distance」。如果我们假设 p(x) 的支撑集和 q(x) 的支撑集分别为 {1,2,3,4} 和 {5,6,7,8}，成本矩阵即为：</p><script type="math/tex; mode=display">C = \left [\begin{matrix} 4 & 5 & 6 & 7 \\3 & 4 & 5 & 6 \\2 & 3 & 4 & 5 \\1 & 2 & 3 & 4 \\\end{matrix} \right ]</script><p>根据上述定义，总的成本可以通过 P 和 C 之间的 Frobenius 内积来计算：</p><script type="math/tex; mode=display"><C, P> \quad = \quad \sum_{ij}C_{ij}P_{ij}</script><p>你可能已经注意到了，实际上有很多种方法可以把点从一个支撑集移动到另一个支撑集中，每一种方式都会得到不同的成本。上面给出的只是一个示例，但是我们感兴趣的是最终能够让成本较小的分配方式。这就是两个离散分布之间的「最优传输」问题，该问题的解是所有耦合矩阵上的最低成本 L_C。</p><p>由于不是所有矩阵都是有效的耦合矩阵，最后一个条件会引入了一个约束。对于一个耦合矩阵来说，其所有列都必须要加到带有 q(x) 概率质量的向量中。在本例中，该向量包含 4 个值为 1/4 的元素。更一般地，我们可以将两个向量分别记为 a 和 b，因此最优运输问题可以被写作：</p><script type="math/tex; mode=display">L_C = \min_P \quad < C, P> \\s.t. P_1 = a \\\quad P^T_1 = b</script><p>当距离矩阵基于一个有效的距离函数构建时，最小成本即为我们所说的「Wasserstein 距离」。</p><font color="blue">关于该问题的解以及将其扩展到连续概率分布中还有大量问题需要解决。如果想要获取更正式、更容易理解的解释，读者可以参阅 Gabriel Peyré 和 Marco Cuturi 编写的「Computational Optimal Transport」一书，此书也是本文写作的主要参考来源之一。</font><p>这里的基本设定是，<strong>我们已经把求两个分布之间距离的问题定义为求最优耦合矩阵的问题</strong>。事实证明，我们可以通过一个小的修改让我们以迭代和可微分的方式解决这个问题，这将让我们可以很好地使用深度学习自动微分机制完成该工作。</p><p><center><font color="red">熵正则化和 Sinkhorn 迭代</font></center><br>首先，我们将一个矩阵的熵定义如下：</p><script type="math/tex; mode=display">H(P) = -\sum_{ij} P_{ij} \log P_{ij}</script><p>正如信息论中概率分布的熵一样，一个熵较低的矩阵将会更稀疏，它的大部分非零值集中在几个点周围。相反，一个具有高熵的矩阵将会更平滑，其最大熵是在均匀分布的情况下获得的。我们可以将正则化系数 ε 引入最优传输问题，从而得到更平滑的耦合矩阵：</p><script type="math/tex; mode=display">L_C = \min_P \quad < C, P>  - \epsilon H(P)\\s.t. P_1 = a \\\quad P^T_1 = b</script><p>通过增大 ε，最终得到的耦合矩阵将会变得更加平滑；而当 ε 趋近于零时，耦合矩阵会更加稀疏，同时最终的解会更加趋近于原始最优运输问题。</p><p>通过引入这种熵正则化，该问题变成了一个凸优化问题，并且可 以通过使用「Sinkhorn iteration」求解。解可以被写作 P=diag(u)Kdiag(v)，在迭代过程中交替更新 u 和 v：</p><script type="math/tex; mode=display">u^{(k+1)} = \frac{a}{Kv^{(k)}} \\v ^{(k+1)} = \frac{b}{K^Tu^{(k+1)}}</script><p>其中 K 是一个用 C 计算的核矩阵（kernel matrix）。由于这些迭代过程是在对原始问题的正则化版本求解，因此对应产生的 Wasserstein 距离有时被称为 Sinkhorn 距离。该迭代过程会形成一个线性操作的序列，因此对于深度学习模型，通过这些迭代进行反向传播是非常简单的。</p><p><br></p><p><center><font color="red">通过pytorch实现Sinkhorn迭代</font></center><br>为了提升 Sinkhorn 迭代的收敛性和稳定性，还可以加入其它的步骤。我们可以在 GitHub 上找到 Gabriel Peyre 完成的详细实现。</p><p>项目链接：<a href="https://github.com/gpeyre/SinkhornAutoDiff。" target="_blank" rel="noopener">https://github.com/gpeyre/SinkhornAutoDiff。</a></p><p>让我们先用一个简单的例子来测试一下，现在我们将研究二维空间（而不是上面的一维空间）中的离散均匀分布。在这种情况下，我们将在平面上移动概率质量。让我们首先定义两个简单的分布：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">n_points = <span class="number">5</span></span><br><span class="line">a = np.array([[i, <span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(n_points)])</span><br><span class="line">b = np.array([[i, <span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(n_points)])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br><span class="line">plt.scatter(a[:, <span class="number">0</span>], a[:, <span class="number">1</span>], label=<span class="string">'supp($p(x)$)'</span>)</span><br><span class="line">plt.scatter(b[:, <span class="number">0</span>], b[:, <span class="number">1</span>], label=<span class="string">'supp($q(x)$)'</span>)</span><br><span class="line">plt.legend();</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2020/wasserstein/pic.png" alt></p><p>我们很容易看出，最优传输对应于将 p(x) 支撑集中的每个点分配到 q(x) 支撑集上的点。对于所有的点来说，距离都是 1，同时由于分布是均匀的，每点移动的概率质量是 1/5。因此，Wasserstein 距离是 5×1/5= 1。现在我们用 Sinkhorn 迭代来计算这个距离：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> layers <span class="keyword">import</span> SinkhornDistance</span><br><span class="line"></span><br><span class="line">x = torch.tensor(a, dtype=torch.float)</span><br><span class="line">y = torch.tensor(b, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">sinkhorn = SinkhornDistance(eps=<span class="number">0.1</span>, max_iter=<span class="number">100</span>, reduction=<span class="literal">None</span>)</span><br><span class="line">dist, P, C = sinkhorn(x, y)</span><br><span class="line">print(<span class="string">"Sinkhorn distance: &#123;:.3f&#125;"</span>.format(dist.item()))</span><br><span class="line"></span><br><span class="line">————————————————————————————————————————————————</span><br><span class="line">Sinkhorn distance: <span class="number">1.000</span></span><br></pre></td></tr></table></figure><p>结果正如我们所计算的那样，距离为 1。现在，让我们查看一下「Sinkhorn( )」方法返回的矩阵，其中 P 是计算出的耦合矩阵，C 是距离矩阵。距离矩阵如下图所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(C)</span><br><span class="line">plt.title(<span class="string">'Distance matrix'</span>)</span><br><span class="line">plt.colorbar();</span><br><span class="line">plt.imshow(C)plt.title(<span class="string">'Distance matrix'</span>)plt.colorbar();</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2020/wasserstein/matrix.png" alt></p><p>元素C[0, 0]说明了将（0,0）点的质量移动到（0,1）所需要的成本 1 是如何产生的。在该行的另一端，元素C[0, 4]包含了将点（0,0）的质量移动到点（4,1）所需要的成本，这个成本是整个矩阵中最大的：</p><p>由于我们为距离矩阵使用的是平方后的 ℓ2 范数，计算结果如上所示。现在，让我们看看计算出的耦合矩阵吧：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(P)</span><br><span class="line">plt.title(<span class="string">'Coupling matrix'</span>);</span><br><span class="line">plt.imshow(P)plt.title(<span class="string">'Coupling matrix'</span>);</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2020/wasserstein/coupling.png" alt></p><p>该图很好地向我们展示了算法是如何有效地发现最优耦合，它与我们前面确定的耦合矩阵是相同的。到目前为止，我们使用了 0.1 的正则化系数。如果将该值增加到 1 会怎样？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sinkhorn = SinkhornDistance(eps=<span class="number">1</span>, max_iter=<span class="number">100</span>, reduction=<span class="literal">None</span>)</span><br><span class="line">dist, P, C = sinkhorn(x, y)</span><br><span class="line">print(<span class="string">"Sinkhorn distance: &#123;:.3f&#125;"</span>.format(dist.item()))</span><br><span class="line">plt.imshow(P);</span><br><span class="line"></span><br><span class="line">————————————————————————————————————————————————</span><br><span class="line">Sinkhorn distance: <span class="number">1.408</span></span><br></pre></td></tr></table></figure><p><img src="/images/blog/2020/wasserstein/coupling1.png" alt></p><p>正如我们前面讨论过的，加大 ε 有增大耦合矩阵熵的作用。接下来，我们看看 P 是如何变得更加平滑的。但是，这样做也会为计算出的距离带来一个不好的影响，导致对 Wasserstein 距离的近似效果变差。</p><p><br></p><p>可视化支撑集的空间分配也很有意思：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_assignments</span><span class="params">(a, b, P)</span>:</span>    </span><br><span class="line">    norm_P = P/P.max()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(a.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(b.shape[<span class="number">0</span>]):</span><br><span class="line">            plt.arrow(a[i, <span class="number">0</span>], a[i, <span class="number">1</span>], b[j, <span class="number">0</span>]-a[i, <span class="number">0</span>], b[j, <span class="number">1</span>]-a[i, <span class="number">1</span>],</span><br><span class="line">                     alpha=norm_P[i,j].item())</span><br><span class="line">    plt.title(<span class="string">'Assignments'</span>)</span><br><span class="line">    plt.scatter(a[:, <span class="number">0</span>], a[:, <span class="number">1</span>])</span><br><span class="line">    plt.scatter(b[:, <span class="number">0</span>], b[:, <span class="number">1</span>])</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line"></span><br><span class="line">show_assignments(a, b, P)</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2020/wasserstein/support.png" alt></p><p>让我们在一个更有趣的分布（Moons 数据集）上完成这项工作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"></span><br><span class="line">X, Y = make_moons(n_samples = <span class="number">30</span>)</span><br><span class="line">a = X[Y==<span class="number">0</span>]</span><br><span class="line">b = X[Y==<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">x = torch.tensor(a, dtype=torch.float)</span><br><span class="line">y = torch.tensor(b, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">sinkhorn = SinkhornDistance(eps=<span class="number">0.1</span>, max_iter=<span class="number">100</span>, reduction=<span class="literal">None</span>)</span><br><span class="line">dist, P, C = sinkhorn(x, y)</span><br><span class="line">print(<span class="string">"Sinkhorn distance: &#123;:.3f&#125;"</span>.format(dist.item()))</span><br><span class="line">show_assignments(a, b, P)</span><br><span class="line"></span><br><span class="line">——————————————————————————————————————————</span><br><span class="line">Sinkhorn distance: <span class="number">1.714</span></span><br></pre></td></tr></table></figure><p><img src="/images/blog/2020/wasserstein/moon.png" alt></p><p><br></p><p>在深度学习中，我们通常对使用 mini-batch 来加速计算十分感兴趣。我们也可以通过使用额外的批处理维度修改 Sinkhorn 迭代来满足该设定。将此更改添加到具体实现中后，我们可以在一个 mini-batch 中计算多个分布的 Sinkhorn 距离。下面我们将通过另一个容易被验证的例子说明这一点。</p><p>代码：<a href="https://github.com/dfdazac/wassdistance/blob/master/layers.py" target="_blank" rel="noopener">https://github.com/dfdazac/wassdistance/blob/master/layers.py</a></p><p>我们将计算包含 5 个支撑点的 4 对均匀分布的 Sinkhorn 距离，它们垂直地被 1（如上所示）、2、3 和 4 个单元分隔开。这样，它们之间的 Wasserstein 距离将分别为 1、4、9 和 16。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">5</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">a = np.array([[[i, <span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)] <span class="keyword">for</span> b <span class="keyword">in</span> range(batch_size)])</span><br><span class="line">b = np.array([[[i, b + <span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)] <span class="keyword">for</span> b <span class="keyword">in</span> range(batch_size)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Wrap with torch tensors</span></span><br><span class="line">x = torch.tensor(a, dtype=torch.float)</span><br><span class="line">y = torch.tensor(b, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">sinkhorn = SinkhornDistance(eps=<span class="number">0.1</span>, max_iter=<span class="number">100</span>, reduction=<span class="literal">None</span>)</span><br><span class="line">dist, P, C = sinkhorn(x, y)</span><br><span class="line">print(<span class="string">"Sinkhorn distances: "</span>, dist)</span><br><span class="line"></span><br><span class="line">——————————————————————————————————————————</span><br><span class="line">Sinkhorn distances:  tensor([ <span class="number">1.0001</span>,  <span class="number">4.0001</span>,  <span class="number">9.0000</span>, <span class="number">16.0000</span>])</span><br></pre></td></tr></table></figure><p>这样做确实有效！同时，也请注意，现在 P 和 C 为 3 维张量，它包含 mini-batch 中每对分布的耦合矩阵和距离矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'P.shape = &#123;&#125;'</span>.format(P.shape))</span><br><span class="line">print(<span class="string">'C.shape = &#123;&#125;'</span>.format(C.shape))</span><br><span class="line"></span><br><span class="line">——————————————————————————————————————————</span><br><span class="line">P.shape = torch.Size([<span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">C.shape = torch.Size([<span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure><p><center><font color="red">结语</font></center><br>分布之间的 Wasserstein 距离及其通过 Sinkhorn 迭代实现的计算方法为我们带来了许多可能性。该框架不仅提供了对 KL 散度等距离的替代方法，而且在建模过程中提供了更大的灵活性，我们不再被迫要选择特定的参数分布。这些迭代过程可以在 GPU 上高效地执行，并且是完全可微分的，这使得它对于深度学习来说是一个很好的选择。这些优点在机器学习领域的最新研究中得到了充分的利用（如自编码器和距离嵌入），使其在该领域的应用前景更加广阔。</p><p><em>原文链接：<a href="https://dfdazac.github.io/sinkhorn.html" target="_blank" rel="noopener">https://dfdazac.github.io/sinkhorn.html</a></em></p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> pytorch </tag>
            
            <tag> Wasserstein </tag>
            
            <tag> 概率分布 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ReLU家族</title>
      <link href="/2020/10/25/ReLU%E5%AE%B6%E6%97%8F/"/>
      <url>/2020/10/25/ReLU%E5%AE%B6%E6%97%8F/</url>
      
        <content type="html"><![CDATA[<p>ReLU是常用的激活函数，也有很多变种，比如：<a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU" target="_blank" rel="noopener">relu</a>, <a href="https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU" target="_blank" rel="noopener">leaky_relu</a>, <a href="https://pytorch.org/docs/stable/nn.functional.html#gelu" target="_blank" rel="noopener">gelu</a>, <a href="https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#torch.nn.CELU" target="_blank" rel="noopener">celu</a>, 等等，下面选取几种典型的、常用的介绍。</p><h2 id="1-ReLU"><a href="#1-ReLU" class="headerlink" title="1. ReLU"></a>1. ReLU</h2><script type="math/tex; mode=display">ReLU(x) = \max (0, x)</script><p><img src="/images/blog/2020/relu/relu.png" alt></p><h2 id="2-LeakyReLU"><a href="#2-LeakyReLU" class="headerlink" title="2. LeakyReLU"></a>2. LeakyReLU</h2><script type="math/tex; mode=display">LeakyReLU(x) = \max(0, x) + \gamma * \min (0,x)</script><p>其中$\gamma$为负斜率，下图可以看出，但是负斜率太小了，不太明显。</p><p><img src="/images/blog/2020/relu/leakyrelu.png" alt></p><p>在我个人的PCNN实验中，使用leaky_relu作为激活函数，速度比tanh快了近三倍。<a id="more"></a></p><h2 id="3-Gelu"><a href="#3-Gelu" class="headerlink" title="3. Gelu"></a>3. Gelu</h2><p>Applies element-wise function GELU(x) = x * $\phi$(x)</p><p>$\phi$(x) is the Cumulative Distribution Function for Gaussian Distribution.</p><p><a href="https://pytorch.org/docs/stable/nn.functional.html#gelu" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.functional.html#gelu</a></p><p>在PCNN实验中，使用Gelu效果非常稳定，达到最优F1值后，能够始终稳定在最优值附近。使用其他激活函数，如ReLU，tanh很快都会因为过拟合f1迅速下降。</p><h2 id="4-Celu"><a href="#4-Celu" class="headerlink" title="4. Celu"></a>4. Celu</h2><script type="math/tex; mode=display">CELU(x) = \max(0, x) + \min (0,  \alpha * (\exp(\frac{x}{\alpha}) - 1))</script><p><img src="/images/blog/2020/relu/celu.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ReLU </tag>
            
            <tag> gelu </tag>
            
            <tag> celu </tag>
            
            <tag> leaky_relu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Glove word embedding添加特殊字符</title>
      <link href="/2020/10/22/Glove-word-embedding%E6%B7%BB%E5%8A%A0%E7%89%B9%E6%AE%8A%E5%AD%97%E7%AC%A6/"/>
      <url>/2020/10/22/Glove-word-embedding%E6%B7%BB%E5%8A%A0%E7%89%B9%E6%AE%8A%E5%AD%97%E7%AC%A6/</url>
      
        <content type="html"><![CDATA[<h2 id="1-问题引入"><a href="#1-问题引入" class="headerlink" title="1. 问题引入"></a>1. 问题引入</h2><p>对于一些特殊字符，例如[CLS], [SEP], [PAD], [UNK]等，在GLOVE词向量中没有对应的值。另外，在进行词嵌入时，难免会有OOV (out of vocabulary) 的现象，需要特殊处理。</p><p>在阅读<a href="https://github.com/INK-USC/NERO" target="_blank" rel="noopener">NERO</a>源码时，注意到作者在对未登录词处理时，并没有按照其他词的分布来生成，通过统计，GLOVE词向量均值应该为0附近，<strong>但是作者采用的是$N(-1., 1.)$的正态分布，均值是-1</strong>，不明白这样做是出于什么特殊的原因吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">token2id</span><span class="params">(config, counter, embedding_dict)</span>:</span></span><br><span class="line">    <span class="string">"""添加 &lt;PAD&gt;, &lt;UNK&gt;, 实体遮罩 的词嵌入"""</span></span><br><span class="line">    vec_size = len(list(embedding_dict.values())[<span class="number">0</span>])</span><br><span class="line">    masks = entity_masks()</span><br><span class="line">    token2idx_dict = &#123;&#125;</span><br><span class="line">    token2idx_dict[constant.PAD_TOKEN] = constant.PAD_ID</span><br><span class="line">    token2idx_dict[constant.UNK_TOKEN] = constant.UNK_ID</span><br><span class="line">    embedding_dict[constant.PAD_TOKEN] = [<span class="number">0.</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(vec_size)]  <span class="comment"># 初始化为零向量</span></span><br><span class="line">    embedding_dict[constant.UNK_TOKEN] = [<span class="number">0.</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(vec_size)]</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> list(embedding_dict.keys()) + masks:</span><br><span class="line">        <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> token2idx_dict:</span><br><span class="line">            token2idx_dict[token] = len(token2idx_dict)</span><br><span class="line">        <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> embedding_dict:</span><br><span class="line">            embedding_dict[token] = [np.random.normal(<span class="number">-1.</span>, <span class="number">1.</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(vec_size)]  <span class="comment"># <span class="doctag">TODO:</span> why loc = -1.</span></span><br><span class="line">    idx2emb_dict = &#123;idx: embedding_dict[token] <span class="keyword">for</span> token, idx <span class="keyword">in</span> token2idx_dict.items()&#125;</span><br><span class="line">    word_emb = np.array([idx2emb_dict[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> range(len(token2idx_dict))], dtype=np.float32)</span><br><span class="line">    <span class="keyword">return</span> token2idx_dict, word_emb</span><br></pre></td></tr></table></figure><h2 id="2-阅读源码"><a href="#2-阅读源码" class="headerlink" title="2. 阅读源码"></a>2. 阅读源码</h2><p>pytorch中nn.Embedding.weight初始化分布是满足$N(0., 1.)$的，参考<a href="https://blog.csdn.net/weixin_30788619/article/details/99645247" target="_blank" rel="noopener">https://blog.csdn.net/weixin_30788619/article/details/99645247</a> ，pytorch源码：<a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## class Embedding具体实现（在此只展示部分代码）</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter </span><br><span class="line"><span class="keyword">from</span> .module <span class="keyword">import</span> Module </span><br><span class="line"><span class="keyword">from</span> .. <span class="keyword">import</span> functional <span class="keyword">as</span> F </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embedding</span><span class="params">(Module)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_embeddings, embedding_dim, padding_idx=None, max_norm=None,                   norm_type=<span class="number">2</span>, scale_grad_by_freq=False, sparse=False, _weight=None)</span>:</span> </span><br><span class="line">        <span class="keyword">if</span> _weight <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">            self.weight = Parameter(torch.Tensor(num_embeddings, embedding_dim)) </span><br><span class="line">            self.reset_parameters()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> list(_weight.shape) == [num_embeddings, embedding_dim]</span><br><span class="line">            self.weight = Parameter(_weight) </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.weight.data.normal_(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># N(0., 1.) 正态分布</span></span><br><span class="line">        <span class="keyword">if</span> self.padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">            self.weight.data[self.padding_idx].fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="3-查询博客"><a href="#3-查询博客" class="headerlink" title="3. 查询博客"></a>3. 查询博客</h2><p>因此对作者的操作更加不解，去<font color="blue">G</font><font color="red">o</font><font color="#ffee00">o</font><font color="blue">g</font><font color="#33ff00">l</font><font color="red">e</font>一下后，在知乎上有比较好的解释，感谢<a href="https://www.zhihu.com/people/tsxiyao" target="_blank" rel="noopener">夕小瑶</a>的回答，</p><p><br></p><blockquote><p>其实大部分NLP场景对OOV都不会太敏感，一般能带来的稳定收益很小。</p><p>至于使用零向量还是随机采样初始化，一般在数据集level上看不出明显的区别（除非训练时把它fix住了），当然，如果把OOV和PAD参数共享的话，<strong>有时候影响就会让人怀疑人生了</strong>，极力不建议这俩功能性token合并。总之，打出来OOV之后，扫一眼，只要不是跟任务特别相关，就别太纠结了。先把其他事处理好回头再照顾一下OOV就好。</p></blockquote><p><br></p><p><br></p><blockquote><p>Word Embedding 如何处理未登录词？：</p><p>​    <a href="https://www.zhihu.com/question/308543084" target="_blank" rel="noopener">https://www.zhihu.com/question/308543084</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> word embedding </tag>
            
            <tag> OOV </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Piecewise CNN</title>
      <link href="/2020/10/21/Piecewise-CNN/"/>
      <url>/2020/10/21/Piecewise-CNN/</url>
      
        <content type="html"><![CDATA[<p>PCNN是<a href="https://www.aclweb.org/anthology/D15-1203.pdf" target="_blank" rel="noopener">Zeng et al. 2015</a>提出的算法，理解起来很容易，但是实现却有问题。</p><p>首先给出自己的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_piecewise_pool</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :param x: (batch_size, cur_max_len, filters)</span></span><br><span class="line"><span class="string">    :param mask: (batch_size, cur_max_len)</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    mask_part1 = (mask == <span class="number">1</span>).float().unsqueeze(<span class="number">2</span>)  <span class="comment"># (batch_size, cur_max_len, 1)</span></span><br><span class="line">    mask_part2 = (mask == <span class="number">2</span>).float().unsqueeze(<span class="number">2</span>)</span><br><span class="line">    mask_part3 = (mask == <span class="number">3</span>).float().unsqueeze(<span class="number">2</span>)</span><br><span class="line">    x1 = (x * mask_part1).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (batch_size, filters, cur_max_len)</span></span><br><span class="line">    x2 = (x * mask_part2).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    x3 = (x * mask_part3).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    x1 = self.pool(x1)  <span class="comment"># (batch_size, filters, 1)</span></span><br><span class="line">    x2 = self.pool(x2)</span><br><span class="line">    x3 = self.pool(x3)</span><br><span class="line">    x = torch.cat((x1, x2, x3), dim=<span class="number">1</span>).squeeze()</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>这里的mask按照句子的不同位置分为[1, 1, 1, 2, 2, 3, 3, 3, 0, 0, 0]的向量。<a id="more"></a></p><p>但是在实践中，效果不如fast_piecewise，不明白原因是什么。而且fast_piecewise很难想到啊！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fast_piecewise_pool</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :param x: (batch_size, cur_max_len, filters)</span></span><br><span class="line"><span class="string">    :param mask: (batch_size, cur_max_len)</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x = x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    x = x.unsqueeze(<span class="number">-1</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    masks = self.mask_embedding(mask).unsqueeze(<span class="number">-2</span>) * <span class="number">100</span></span><br><span class="line">    x = masks.float() + x</span><br><span class="line">    x = torch.max(x, <span class="number">1</span>)[<span class="number">0</span>] - torch.tensor([<span class="number">100</span>]).cuda().float()</span><br><span class="line">    x = x.reshape(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># mask_embedding</span></span><br><span class="line">        self.mask_embedding = nn.Embedding.from_pretrained(</span><br><span class="line">        torch.tensor([</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">        ]).float()</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>自己写代码的功力还是不够，多借鉴别人的源码，不断学习才是真理 😭</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PCNN </tag>
            
            <tag> CNN </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Text Adversarial Training</title>
      <link href="/2020/10/20/Text-Adversarial-Training/"/>
      <url>/2020/10/20/Text-Adversarial-Training/</url>
      
        <content type="html"><![CDATA[<p>本文是对一个好人😁 (Ian Goodfellow) ，大名鼎鼎的GAN的提出者，的论文 <a href="https://arxiv.org/abs/1605.07725" target="_blank" rel="noopener">ADVERSARIAL TRAINING METHODS FOR SEMI-SUPERVISED TEXT CLASSIFICATION</a> 核心部分的理解以及实现。</p><h2 id="1-论文核心思想"><a href="#1-论文核心思想" class="headerlink" title="1. 论文核心思想"></a>1. 论文核心思想</h2><p>对抗训练是一种新颖的正则化方法，用以提高分类器对微小扰动的鲁棒性。令$\mathbf{x}$代表输入，$\mathbf{\theta}$代表分类器的参数，对抗训练是在损失函数上添加一项：</p><script type="math/tex; mode=display">-\log p(y| \mathbf{x + r_{adv}}; \mathbf{\theta}) \quad where \quad \mathbf{r_{adv}} = \arg  \min _ {\mathbf{r}, \mathbf{||r||} \leq \epsilon } \log p(y | \mathbf{x + r}; \mathbf{\hat{\theta}})</script><p>即添加当前状态下使损失最大的扰动项。但是，在实际操作中，寻找最优项是NP难问题；于是作者提出了一种线性近似方法，即：</p><script type="math/tex; mode=display">\mathbf{r_{adv}} = -\epsilon \mathbf{g} / ||\mathbf{g}||_2 \quad where \quad \mathbf{g} = \nabla _ \mathbf{x} \log p(y|\mathbf{x};\hat{\mathbf{\theta}})</script><p>如何理解上式呢？<font color="orange"><strong>对当前损失函数求导后取负数</strong></font>，是使得损失增大的最快方向。通过梯度反向传播很容易计算。$\epsilon$用于控制扰动项的范数，将扰动的大小限制在合理范围内。</p><h2 id="2-实现"><a href="#2-实现" class="headerlink" title="2. 实现"></a>2. 实现</h2><p>使用pytorch进行实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_add_perturbation</span><span class="params">(self, embedded, loss)</span>:</span></span><br><span class="line">    <span class="string">"""Adds gradient to embedding and recomputes classification loss."""</span></span><br><span class="line">    grad, = tf.gradients(</span><br><span class="line">        loss,</span><br><span class="line">        embedded,</span><br><span class="line">        aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)</span><br><span class="line">    grad = tf.stop_gradient(grad)</span><br><span class="line">    <span class="comment"># L2 norm constraint</span></span><br><span class="line">    perturb = scale_l2(grad, self.epsilon)</span><br><span class="line">    <span class="keyword">return</span> embedded + perturb</span><br></pre></td></tr></table></figure><p>scale_l2用于$L_2$ norm constraint。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scale_l2</span><span class="params">(x, norm_length)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    L2 normalization</span></span><br><span class="line"><span class="string">    :param x:</span></span><br><span class="line"><span class="string">    :param norm_length: 范数长度</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># shape(x) = (batch, num_timesteps, d)</span></span><br><span class="line">    <span class="comment"># Divide x by max(abs(x)) for a numerically stable L2 norm.</span></span><br><span class="line">    <span class="comment"># 2norm(x) = a * 2norm(x/a)</span></span><br><span class="line">    <span class="comment"># Scale over the full sequence, dims (1, 2)</span></span><br><span class="line">    alpha = tf.reduce_max(tf.abs(x), (<span class="number">1</span>, <span class="number">2</span>), keepdims=<span class="literal">True</span>) + <span class="number">1e-12</span></span><br><span class="line">    l2_norm = alpha * tf.sqrt(</span><br><span class="line">        tf.reduce_sum(tf.pow(x / alpha, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>), keepdims=<span class="literal">True</span>) + <span class="number">1e-6</span>)</span><br><span class="line">    x_unit = x / l2_norm</span><br><span class="line">    <span class="keyword">return</span> norm_length * x_unit</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Normalization </tag>
            
            <tag> Adversarial Training </tag>
            
            <tag> Text Classification </tag>
            
            <tag> Data Augmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《百面深度学习》笔记3 - 图神经网络</title>
      <link href="/2020/10/19/%E3%80%8A%E7%99%BE%E9%9D%A2%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B03-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/10/19/%E3%80%8A%E7%99%BE%E9%9D%A2%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B03-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="1-什么是图谱和傅里叶变换？"><a href="#1-什么是图谱和傅里叶变换？" class="headerlink" title="1. 什么是图谱和傅里叶变换？"></a>1. 什么是图谱和傅里叶变换？</h2><p>图谱是图的拉普拉斯矩阵的特征值。具体来说，给定图G(V, E)，点集大小为n，边集大小为m。图G的邻接矩阵A为一个n×n的矩阵，如果i, j之间有边相连，则$A_{ij}$为1，否则为0。图的度矩阵为$D=diag (d_1, d_2, …, d_n)$，其中$d_i$是第i个节点在图中的度数。定义<font color="orange">图的拉普拉斯矩阵为L=D-A</font>，对其进行特征值分解：</p><script type="math/tex; mode=display">L = U \Lambda U^T</script><p>其中，$\Lambda = diag(\lambda _1, \lambda _2, …, \lambda _n)$是按照特征值<strong>从小到大</strong>的顺序排列的，$U = [\mathbf{u_1}, u_2, …, u_n]$是对应的特征向量组成的正交矩阵。上述特征值集合$\Lambda$ 即为图G的图谱。尽管A会随着G节点编号的改变而改变，但是$\Lambda$不会，它只与图的抽象结构有关。</p><p>图神经网络有两个派系，一个是基于频谱域，例如GCN；一个是基于空间域，例如GAT和GraphSAGE。不过两者殊途同归，在实际的使用中，图神经网络更常见的框架是：</p><script type="math/tex; mode=display">\mathbf{Y} = \sigma(\mathbf{\hat{D}^{-1}\hat{A}XW})</script><p>其中，$\mathbf{X}$和$\mathbf{Y}$分别是输入和输出矩阵，$\mathbf{\hat{A}}$是添加了自环边的图邻接矩阵，$\mathbf{\hat{D}}$是对应的度数矩阵，$\mathbf{W}$是待训练的参数矩阵。将框架分解来看：</p><p>(1) $\mathbf{XW}$将节点的特征向量进行线性变化。</p><p>(2) $\mathbf{\hat{A}XW}$将变换后的节点特征传播到邻居节点（包括自身）。</p><p>(3) $\mathbf{\hat{D}^{-1}\hat{A}XW}$将每个节点收到的特征（来自邻居节点和自身）进行归一化。</p><p>(4) $\sigma(\mathbf{\hat{D}^{-1}\hat{A}XW})$将归一化后的特征通过非线性激活单元。<a id="more"></a></p><h2 id="2-图神经网络的推理能力"><a href="#2-图神经网络的推理能力" class="headerlink" title="2. 图神经网络的推理能力"></a>2. 图神经网络的推理能力</h2><p>图神经网络可以看作符号主义和数值主义的结合，其将规则、知识引入神经网络，使得神经网络具备了可解释性和推理能力。</p><font color="purple">没有对复杂关系的准确刻画，神经网络的组合泛化能力几乎为零。事实上，迁移学习、元学习、少次学习等任务关注的都是神经网络的组合泛化能力。</font><p>在书中，作者提出了一个想法，我觉得非常有趣也非常有创见。作者将自注意力机制和基于度量的元学习统一到了图神经网络的框架下。自注意力机制 $\iff$ 完全图。基于度量的元学习 $\iff$ 二部图。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 图神经网络 </tag>
            
            <tag> 百面深度学习 </tag>
            
            <tag> 图谱 </tag>
            
            <tag> GCN </tag>
            
            <tag> GAT </tag>
            
            <tag> GraphSAGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《百面深度学习》笔记2 - 循环神经网络</title>
      <link href="/2020/10/17/%E3%80%8A%E7%99%BE%E9%9D%A2%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B02-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/10/17/%E3%80%8A%E7%99%BE%E9%9D%A2%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B02-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="1-公式推导"><a href="#1-公式推导" class="headerlink" title="1. 公式推导"></a>1. 公式推导</h2><p>记网络的输入为$x_1, x_2, … ,x_n$，则网络展开后可以看作一个$n$层的前馈神经网络，第$t$层对应着$t$时刻的状态 ($t = 1,2, …, n$) 。记第$t$层（时刻）的输入状态、隐藏状态、输出状态分别为$x_t$、$h_t$、$o_t$，训练时的目标输出值为$y_t$，则有：</p><ul><li>$h<em>t = \sigma (Ux_t + Wh</em>{t-1} + b)$，$\sigma$通常采用Tanh函数</li><li><p>$o_t = g(Vh_t + c)$</p></li><li><p>在训练时，网络在整个序列上的损失可以定义为不同时刻的损失之和：$L = \sum _t L_t = \sum _t Loss(o_t, y_t)$</p></li></ul><p>在循环神经网络中，<font color="pink">所有循环或重复的结构都共享参数。</font><a id="more"></a></p><p>循环神经网络在训练时，也是利用梯度下降法和反向传播算法进行一轮轮的迭代。将循环神经网络按照时间展开，则有如下公式：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial U} = \sum_t \frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial U}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial W} = \sum_t \frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial W}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial V} = \sum_t \frac{\partial L}{\partial o_t} \frac{\partial o_t}{\partial V}</script><p>其中，符号$\frac{\partial y}{\partial x}$表示雅可比矩阵（Jacobian matrix），它的尺寸为$d_y \times d_x$，矩阵的第$i$行第$j$列元素为$\frac{\partial y_i}{\partial x_j}$。</p><p>在上述公式中，$\frac{\partial h_t}{\partial U}$、$\frac{\partial h_t}{\partial W}$、$\frac{\partial o_t}{\partial V}$、$\frac{\partial L}{\partial o_t}$、都可以直接计算出来，只有$\frac{\partial L}{\partial h_t}$的计算相对复杂，公式为：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial h_t} = \frac{\partial L}{\partial h_{t+1}} \frac{\partial h_{t+1}}{\partial h_{t}} + \frac{\partial L}{\partial o_t} \frac{\partial o_t}{\partial h_{t}}</script><font color="purple">上式其实是一个递推公式，</font>先计算$\frac{\partial L}{\partial h_t}$，再计算$\frac{\partial L}{\partial h_{t-1}}$，依次递推。上述方法就是基于时间的反向传播算法。## 2. 长短期记忆网络 (LSTM)<font color="orange">想法：可以将遗忘门$f_t (t=1,2,...,n)$状态可视化，增强神经网络的解释性？</font>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 百面深度学习 </tag>
            
            <tag> RNN </tag>
            
            <tag> LSTM </tag>
            
            <tag> GRU </tag>
            
            <tag> 循环神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《百面深度学习》笔记1 - 卷积神经网络的基础模块</title>
      <link href="/2020/10/17/%E3%80%8A%E7%99%BE%E9%9D%A2%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B01-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9D%97/"/>
      <url>/2020/10/17/%E3%80%8A%E7%99%BE%E9%9D%A2%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B01-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h2 id="1-批归一化在网络中的位置"><a href="#1-批归一化在网络中的位置" class="headerlink" title="1. 批归一化在网络中的位置"></a>1. 批归一化在网络中的位置</h2><p>批归一化在网络中的位置，从直觉上看无论是放在激活层前还是激活层后都有一定道理：</p><ul><li>把批归一化放在激活层之前，可以有效避免批归一化破坏非线性特征的分布；另外，批归一化还可以使数据点尽量不落入激活函数的饱和区域，缓解梯度消失问题。</li><li>由于现在常用的激活函数是ReLU，它没有Sigmoid、Tanh函数的那些问题，因此也可以把批归一化放在激活层之后，避免数据在激活层之前被转入相似的模式从而使得非线性特征分布趋于同化。</li></ul><p>在具体实践中，原始论文是将批归一化放在激活层之前的，但学术界和工业界也有不少人表示倾向于将批归一化放在激活层之后。批归一化究竟应该放在什么位置，仍是一个存在争议的问题。</p><h2 id="2-分类层"><a href="#2-分类层" class="headerlink" title="2. 分类层"></a>2. 分类层</h2><p>最近几年，分类网络在卷积层之后，最后一层之前通常采用全局平均池化，它与全连接网络有着相似的效果（可以提取全局信息），并且具有如下优点：</p><ul><li>参数量和计算量大大降低。</li><li>具有较好的可解释性，比如，我们可以知道特征图上哪些点对最后的分类贡献最大。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> 卷积神经网络 </tag>
            
            <tag> 百面深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Softmax温度系数</title>
      <link href="/2020/10/15/Softmax%E6%B8%A9%E5%BA%A6%E7%B3%BB%E6%95%B0/"/>
      <url>/2020/10/15/Softmax%E6%B8%A9%E5%BA%A6%E7%B3%BB%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>softmax temperature是在softmax层上加了一个超参数，一般softmax计算过程如下：</p><script type="math/tex; mode=display">P_t(w) = \frac{\exp (s_w)}{\sum_{w^{'} \in V} \exp(s_{w^{'}})}</script><p>加入温度系数计算过程为：</p><script type="math/tex; mode=display">P_t(w) = \frac{\exp (\sigma s_w)}{\sum_{w^{'} \in V} \exp(\sigma s_{w^{'}})}</script><p>$\sigma$越小概率分布就越接近于uniform，概率值在整个词表上做了平滑，可以生成更加diverse的句子；$\sigma$越大概率密度就越集中在某几个token上，可以生成具有更小diverse的句子。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> softmax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NERO A Neural Rule Grounding Framework for Label-Efficient Relation Extraction</title>
      <link href="/2020/10/15/NERO-A-Neural-Rule-Grounding-Framework-for-Label-Efficient-Relation-Extraction/"/>
      <url>/2020/10/15/NERO-A-Neural-Rule-Grounding-Framework-for-Label-Efficient-Relation-Extraction/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/blog/2020/NERO/幻灯片1.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片2.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片3.PNG" alt></p><a id="more"></a><p><img src="/images/blog/2020/NERO/幻灯片4.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片5.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片6.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片7.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片8.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片9.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片10.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片11.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片12.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片13.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片14.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片15.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片16.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片17.PNG" alt></p><p><img src="/images/blog/2020/NERO/幻灯片18.PNG" alt></p>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
          <category> Distant Supervision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
            <tag> WWW </tag>
            
            <tag> Relation Extraction </tag>
            
            <tag> Low Resource </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>乐理 - 五度相生律、纯律与十二平均律</title>
      <link href="/2020/10/11/%E4%B9%90%E7%90%86-%E4%BA%94%E5%BA%A6%E7%9B%B8%E7%94%9F%E5%BE%8B%E3%80%81%E7%BA%AF%E5%BE%8B%E4%B8%8E%E5%8D%81%E4%BA%8C%E5%B9%B3%E5%9D%87%E5%BE%8B/"/>
      <url>/2020/10/11/%E4%B9%90%E7%90%86-%E4%BA%94%E5%BA%A6%E7%9B%B8%E7%94%9F%E5%BE%8B%E3%80%81%E7%BA%AF%E5%BE%8B%E4%B8%8E%E5%8D%81%E4%BA%8C%E5%B9%B3%E5%9D%87%E5%BE%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于搞不明白“大二度”与“减三度”的区别和关系，从网上找了一些相关乐理教材，终于弄懂了！😊</p><p>随着年龄增长，越来越觉得学无止境~而且，更多的是要学会学习、主动学习，哈哈哈不感叹了，开始吧~</p><h2 id="五度相生律"><a href="#五度相生律" class="headerlink" title="五度相生律"></a>五度相生律</h2><p>首先应该明白什么是“五度循环圈”，如果不明白，可以参考知乎上的很好的乐理专栏：</p><p><a href="https://zhuanlan.zhihu.com/p/23125945" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/23125945</a></p><p><img src="/images/blog/2020/music/五度圈.jpg" alt></p><p><br></p><a id="more"></a><p>五度相生律就是利用泛音生成，其中只有两个频率比，八度为$\frac{1}{2}$，五度为$\frac{3}{2}$。</p><p>按照五度圈，<font color="orange">顺时针旋转时，频率乘以$\frac{3}{2}$；逆时针旋转时，频率除以$\frac{3}{2}$，</font>这样就可以得到所有音的频率值。</p><p>以中央C频率为1，尝试推导其他音：</p><script type="math/tex; mode=display">G = C \times \frac{3}{2} = \frac{3}{2}</script><p><br></p><script type="math/tex; mode=display">D = G \times \frac{3}{2} = \frac{9}{4}</script><p>由于D比G低，所以要降八度，再乘$\frac{1}{2}$，即：</p><script type="math/tex; mode=display">D = \frac{9}{4} \times \frac{1}{2} = \frac{9}{8}</script><p><br></p><p>以此类推，可以得到表格：</p><div class="table-container"><table><thead><tr><th style="text-align:center">C</th><th style="text-align:center">D</th><th style="text-align:center">E</th><th style="text-align:center">F</th><th style="text-align:center">G</th><th style="text-align:center">A</th><th style="text-align:center">B</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">$\frac{9}{8}$</td><td style="text-align:center">$\frac{81}{64}$</td><td style="text-align:center">$\frac{4}{3}$</td><td style="text-align:center">$\frac {3}{2}$</td><td style="text-align:center">$\frac{27}{16}$</td><td style="text-align:center">$\frac{243}{128}$</td></tr><tr><td style="text-align:center"><strong>bD</strong></td><td style="text-align:center"><strong>bE</strong></td><td style="text-align:center"><strong>#F</strong></td><td style="text-align:center"><strong>bG</strong></td><td style="text-align:center"><strong>bA</strong></td><td style="text-align:center"><strong>bB</strong></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">$\frac{256}{243}$</td><td style="text-align:center">$\frac{32}{27}$</td><td style="text-align:center">$\frac{729}{512}$</td><td style="text-align:center">$\frac{1024}{729}$</td><td style="text-align:center">$\frac{128}{81}$</td><td style="text-align:center">$\frac{16}{9}$</td></tr></tbody></table></div><p><br></p><p>但是存在一个问题，在循环圈里转一圈后<strong>不能得到两个相同的音</strong>，例如对于#F和bG两个音：</p><script type="math/tex; mode=display">\frac{\# F}{bG} = \frac{729/ 512}{1024 / 729} = \frac{531441}{524288}</script><p>这说明在五度相生律中<strong>升F不等于降G</strong>！</p><p>这一点如果想明白了，就能理解为什么音程要分<strong>增四度</strong>和<strong>减五度</strong>了，因为C到升F就是增四度，C到降G就是减五度，而这两个音程是不一样的。</p><blockquote><p>参考：</p><p><a href="https://oeis.org/DUNNE/TEMPERAMENT.HTML" target="_blank" rel="noopener">https://oeis.org/DUNNE/TEMPERAMENT.HTML</a></p><p><a href="https://zhuanlan.zhihu.com/p/27028014" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27028014</a></p></blockquote><p>由于和声的应用以及复调音乐的兴起，由五度相生律得到的三和弦中的三音非常不协和，比值数字达到了$\frac{81}{64}$，为了和声效果更好，许多多声部音乐开始使用<strong>纯律</strong>。</p><p><br></p><h2 id="纯律"><a href="#纯律" class="headerlink" title="纯律"></a>纯律</h2><p><strong>纯律</strong>是一种由<strong>五度相生律</strong>改进而来的律制，它以大三和弦为基础，为了解决和声中和弦三音听起来不“协和”的问题。<strong>纯律</strong>其实就是在<strong>五度相生律</strong>的基础上，插入一个<strong>大三度</strong>形成的一种律制。</p><p><img src="/images/blog/2020/music/change.PNG" alt></p><p><br></p><p>但是纯律也暴露出了许多缺点：</p><ol><li><p><strong>全音</strong>有大、小之分</p></li><li><p>纯律中小二度比五度相生律要大，称为<strong>大半音</strong></p></li><li>纯律中存在<strong>狭五度</strong></li></ol><p><strong>五度相生律</strong>和<strong>纯律</strong>还有一个共同的缺点，就是<strong>无法转调</strong>，当一首乐曲要转到其它调上演奏时，听起来居然和原调有很大差异，这也使得无法写作转调的乐曲。</p><p><br></p><blockquote><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/27150865" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27150865</a></p></blockquote><p><br></p><h2 id="十二平均律"><a href="#十二平均律" class="headerlink" title="十二平均律"></a>十二平均律</h2><p>简单粗暴，直接上图：</p><p><img src="/images/blog/2020/music/十二平均律.jpg" alt></p><p><br></p><p>在十二平均律里<strong>黑键</strong>上的音不管是用<strong>升号</strong>还是<strong>降号</strong>表示，他们的频率是一样的。<font color="#00d000">这也是很多音乐理论积累不够的初学者容易对<strong>大二度、减三度</strong>产生迷惑的原因。</font></p><p>从此开始，以<strong>十二音</strong>为基础的音乐开始普及，经历了<strong>巴洛克</strong>时期、<strong>古典</strong>时期、<strong>浪漫</strong>时期、<strong>印象</strong>时期、<strong>现代</strong>时期，古典音乐蓬勃发展，这些时代的作曲家给我们留下了宝贵的音乐财富，换句话说，没有十二平均律，就没有今天我们听到的这么多音乐。</p><blockquote><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/27647079" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27647079</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
          <category> Music </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 乐理 </tag>
            
            <tag> 十二平均律 </tag>
            
            <tag> 五度相生律 </tag>
            
            <tag> 纯律 </tag>
            
            <tag> music </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Conditional GAN</title>
      <link href="/2020/10/10/Conditional-GAN/"/>
      <url>/2020/10/10/Conditional-GAN/</url>
      
        <content type="html"><![CDATA[<p>Conditional GAN可以控制生成的图像类别，是非常实用的一种模型。废话不多说，先上效果图：</p><p><img src="/images/blog/2020/CGAN/101.png" alt></p><p>使用MNIST数据集，生成的均为数字7类别。</p><h2 id="TensorFlow代码"><a href="#TensorFlow代码" class="headerlink" title="TensorFlow代码"></a>TensorFlow代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">'Agg'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'../../MNIST_data'</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">mb_size = <span class="number">64</span></span><br><span class="line">Z_dim = <span class="number">100</span></span><br><span class="line">X_dim = mnist.train.images.shape[<span class="number">1</span>]</span><br><span class="line">y_dim = mnist.train.labels.shape[<span class="number">1</span>]</span><br><span class="line">h_dim = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xavier_init</span><span class="params">(size)</span>:</span></span><br><span class="line">    in_dim = size[<span class="number">0</span>]</span><br><span class="line">    xavier_stddev = <span class="number">1.</span> / tf.sqrt(in_dim / <span class="number">2.</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.random_normal(shape=size, stddev=xavier_stddev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">""" Discriminator Net model """</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, y_dim])</span><br><span class="line"></span><br><span class="line">D_W1 = tf.Variable(xavier_init([X_dim + y_dim, h_dim]))</span><br><span class="line">D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class="line"></span><br><span class="line">D_W2 = tf.Variable(xavier_init([h_dim, <span class="number">1</span>]))</span><br><span class="line">D_b2 = tf.Variable(tf.zeros(shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">theta_D = [D_W1, D_W2, D_b1, D_b2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    inputs = tf.concat(axis=<span class="number">1</span>, values=[x, y])  <span class="comment"># 1. D 添加监督信息</span></span><br><span class="line">    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)</span><br><span class="line">    D_logit = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class="line">    D_prob = tf.nn.sigmoid(D_logit)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> D_prob, D_logit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">""" Generator Net model """</span></span><br><span class="line">Z = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, Z_dim])</span><br><span class="line"></span><br><span class="line">G_W1 = tf.Variable(xavier_init([Z_dim + y_dim, h_dim]))</span><br><span class="line">G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class="line"></span><br><span class="line">G_W2 = tf.Variable(xavier_init([h_dim, X_dim]))</span><br><span class="line">G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))</span><br><span class="line"></span><br><span class="line">theta_G = [G_W1, G_W2, G_b1, G_b2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(z, y)</span>:</span></span><br><span class="line">    inputs = tf.concat(axis=<span class="number">1</span>, values=[z, y])  <span class="comment"># 2. G 添加监督信息</span></span><br><span class="line">    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)</span><br><span class="line">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br><span class="line">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> G_prob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_Z</span><span class="params">(m, n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[m, n])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(samples)</span>:</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">    gs = gridspec.GridSpec(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">    gs.update(wspace=<span class="number">0.05</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">        ax = plt.subplot(gs[i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        ax.set_xticklabels([])</span><br><span class="line">        ax.set_yticklabels([])</span><br><span class="line">        ax.set_aspect(<span class="string">'equal'</span>)</span><br><span class="line">        plt.imshow(sample.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">'Greys_r'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">G_sample = generator(Z, y)</span><br><span class="line">D_real, D_logit_real = discriminator(X, y)</span><br><span class="line">D_fake, D_logit_fake = discriminator(G_sample, y)</span><br><span class="line"></span><br><span class="line">D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))</span><br><span class="line">D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))</span><br><span class="line">D_loss = D_loss_real + D_loss_fake</span><br><span class="line">G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))</span><br><span class="line"></span><br><span class="line">D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)</span><br><span class="line">G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'out/'</span>):</span><br><span class="line">    os.makedirs(<span class="string">'out/'</span>)</span><br><span class="line"></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        n_sample = <span class="number">16</span></span><br><span class="line"></span><br><span class="line">        Z_sample = sample_Z(n_sample, Z_dim)</span><br><span class="line">        y_sample = np.zeros(shape=[n_sample, y_dim])</span><br><span class="line">        y_sample[:, <span class="number">7</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        samples = sess.run(G_sample, feed_dict=&#123;Z: Z_sample, y:y_sample&#125;)</span><br><span class="line"></span><br><span class="line">        fig = plot(samples)</span><br><span class="line">        plt.savefig(<span class="string">'out/&#123;&#125;.png'</span>.format(str(i).zfill(<span class="number">3</span>)), bbox_inches=<span class="string">'tight'</span>)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        plt.close(fig)</span><br><span class="line"></span><br><span class="line">    X_mb, y_mb = mnist.train.next_batch(mb_size)</span><br><span class="line"></span><br><span class="line">    Z_sample = sample_Z(mb_size, Z_dim)</span><br><span class="line">    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict=&#123;X: X_mb, Z: Z_sample, y:y_mb&#125;)</span><br><span class="line">    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict=&#123;Z: Z_sample, y:y_mb&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Iter: &#123;&#125;'</span>.format(it))</span><br><span class="line">        print(<span class="string">'D loss: &#123;:.4&#125;'</span>. format(D_loss_curr))</span><br><span class="line">        print(<span class="string">'G_loss: &#123;:.4&#125;'</span>.format(G_loss_curr))</span><br><span class="line">        print()</span><br></pre></td></tr></table></figure><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p><img src="/images/blog/2020/CGAN/000.png" alt></p><p>0个step</p><p><img src="/images/blog/2020/CGAN/010.png" alt></p><p>10 * 1000个step</p><p><img src="/images/blog/2020/CGAN/028.png" alt></p><p>28 * 1000个step</p><p><img src="/images/blog/2020/CGAN/999.png" alt></p><p>999 * 1000个step</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Conditional GAN的思想以及实现都是很简单的，值得借鉴。往往经典的模型都是简单优雅的。</p>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
          <category> GAN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Conditional GAN </tag>
            
            <tag> GAN </tag>
            
            <tag> CGAN </tag>
            
            <tag> 生成对抗网络 </tag>
            
            <tag> 生成模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WGAN使用Pytorch简单实现</title>
      <link href="/2020/10/09/WGAN%E4%BD%BF%E7%94%A8Pytorch%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/"/>
      <url>/2020/10/09/WGAN%E4%BD%BF%E7%94%A8Pytorch%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<p>上一篇文章介绍了 WGAN 的原理，这篇文章给出使用Pytorch的简单实现：</p><h2 id="环境配置："><a href="#环境配置：" class="headerlink" title="环境配置："></a>环境配置：</h2><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensorflow: &gt;= 2.0.0</span><br><span class="line">pytorch: &gt;= 1.5.0</span><br><span class="line">numpy</span><br><span class="line">matplotlib</span><br></pre></td></tr></table></figure><h2 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">'Agg'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> tensorflow</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mnist, _ = tensorflow.keras.datasets.mnist.load_data(path=<span class="string">'mnist.npz'</span>)</span><br><span class="line">mb_size = <span class="number">32</span></span><br><span class="line">z_dim = <span class="number">10</span></span><br><span class="line">X_dim = int(pow(mnist[<span class="number">0</span>][<span class="number">0</span>].shape[<span class="number">1</span>], <span class="number">2</span>))</span><br><span class="line">y_dim = <span class="number">10</span>  <span class="comment"># 手写数字 0~9</span></span><br><span class="line">h_dim = <span class="number">128</span></span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line">lr = <span class="number">1e-4</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">G = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(z_dim, h_dim),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(h_dim, X_dim),</span><br><span class="line">    torch.nn.Sigmoid()</span><br><span class="line">).float()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">D = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(X_dim, h_dim),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(h_dim, <span class="number">1</span>),  <span class="comment"># 1. 不用 sigmoid</span></span><br><span class="line">).float()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_grad</span><span class="params">()</span>:</span></span><br><span class="line">    G.zero_grad()</span><br><span class="line">    D.zero_grad()</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">G_solver = optim.RMSprop(G.parameters(), lr=lr)  <span class="comment"># 2. 采用 RMSprop 优化器</span></span><br><span class="line">D_solver = optim.RMSprop(D.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_batch</span><span class="params">(mb_size)</span>:</span></span><br><span class="line">    rand_indexes = np.random.randint(<span class="number">0</span>, mnist[<span class="number">0</span>].shape[<span class="number">0</span>], mb_size)</span><br><span class="line">    <span class="keyword">return</span> mnist[<span class="number">0</span>][rand_indexes]</span><br></pre></td></tr></table></figure><a id="more"></a><p><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):  <span class="comment"># 5. 训练 D 更多次</span></span><br><span class="line">        <span class="comment"># Sample data</span></span><br><span class="line">        z = Variable(torch.randn(mb_size, z_dim))</span><br><span class="line">        X = sample_batch(mb_size)</span><br><span class="line">        X = Variable(torch.from_numpy(X)).float()</span><br><span class="line">        X = X.reshape(mb_size, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        G_sample = G(z)</span><br><span class="line">        D_real = D(X)</span><br><span class="line">        D_fake = D(G_sample)</span><br><span class="line"></span><br><span class="line">        D_loss = -(torch.mean(D_real) - torch.mean(D_fake))  <span class="comment"># 3. 损失函数 - 搬土距离</span></span><br><span class="line"></span><br><span class="line">        D_loss.backward()</span><br><span class="line">        D_solver.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Weight clipping</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> D.parameters():</span><br><span class="line">            p.data.clamp_(<span class="number">-0.01</span>, <span class="number">0.01</span>)  <span class="comment"># 4. weight clipping or gradient penalty</span></span><br><span class="line"></span><br><span class="line">        reset_grad()</span><br><span class="line"></span><br><span class="line">    X = sample_batch(mb_size)</span><br><span class="line">    X = Variable(torch.from_numpy(X)).float()</span><br><span class="line">    X = X.reshape(mb_size, <span class="number">-1</span>)</span><br><span class="line">    z = Variable(torch.randn(mb_size, z_dim))</span><br><span class="line"></span><br><span class="line">    G_sample = G(z)</span><br><span class="line">    D_fake = D(G_sample)</span><br><span class="line"></span><br><span class="line">    G_loss = -torch.mean(D_fake)</span><br><span class="line"></span><br><span class="line">    G_loss.backward()</span><br><span class="line">    G_solver.step()</span><br><span class="line"></span><br><span class="line">    reset_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Iter-&#123;&#125;; D_loss: &#123;&#125;; G_loss: &#123;&#125;'</span></span><br><span class="line">              .format(it, D_loss.data.numpy(), G_loss.data.numpy()))</span><br><span class="line"></span><br><span class="line">        samples = G(z).data.numpy()[:<span class="number">16</span>]</span><br><span class="line"></span><br><span class="line">        fig = plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">        gs = gridspec.GridSpec(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">        gs.update(wspace=<span class="number">0.05</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">            ax = plt.subplot(gs[i])</span><br><span class="line">            plt.axis(<span class="string">'off'</span>)</span><br><span class="line">            ax.set_xticklabels([])</span><br><span class="line">            ax.set_yticklabels([])</span><br><span class="line">            ax.set_aspect(<span class="string">'equal'</span>)</span><br><span class="line">            plt.imshow(sample.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">'Greys_r'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'out/'</span>):</span><br><span class="line">            os.makedirs(<span class="string">'out/'</span>)</span><br><span class="line"></span><br><span class="line">        plt.savefig(<span class="string">'out/&#123;&#125;.png'</span>.format(str(cnt).zfill(<span class="number">3</span>)), bbox_inches=<span class="string">'tight'</span>)</span><br><span class="line">        cnt += <span class="number">1</span></span><br><span class="line">        plt.close(fig)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
          <category> GAN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Wasserstein </tag>
            
            <tag> WGAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Wasserstein GAN implementation in TensorFlow and Pytorch</title>
      <link href="/2020/10/08/Wasserstein-GAN-implementation-in-TensorFlow-and-Pytorch/"/>
      <url>/2020/10/08/Wasserstein-GAN-implementation-in-TensorFlow-and-Pytorch/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文译自：<a href="https://wiseodd.github.io/techblog/2017/02/04/wasserstein-gan/" target="_blank" rel="noopener">https://wiseodd.github.io/techblog/2017/02/04/wasserstein-gan/</a></p></blockquote><p>GAN现在是机器学习中非常受欢迎的研究主题。 GAN有两种类型的研究，一种将GAN应用到有趣的问题中，另一种试图稳定训练。</p><p>确实，稳定GAN培训在该领域非常重要。原始GAN遇到许多困难，例如模式崩溃，即生成器陷入非常狭窄的分布，仅涵盖数据分布中的单个模式。模式崩溃的含义是，生成器只能生成非常相似的样本（例如MNIST中的一位数字），即生成的样本区别不大。这个问题当然违反了GAN的精神。</p><p>GAN中的另一个问题是，没有度量标准可以告诉我们有关收敛的信息。生成器和鉴别器的损失并没有告诉我们任何有关此的信息。当然，我们可以不时查看发生器生成的数据来监视培训进度。但是，这是一个严格的手动过程。因此，拥有一个可解释的指标来告诉我们有关培训进度的信息将非常棒。</p><p>Note, code could be found here: <a href="https://github.com/wiseodd/generative-models" target="_blank" rel="noopener">https://github.com/wiseodd/generative-models</a></p><h2 id="Wasserstein-GAN"><a href="#Wasserstein-GAN" class="headerlink" title="Wasserstein GAN"></a>Wasserstein GAN</h2><p>Wasserstein GAN (WGAN) 是新提出的 GAN 算法旨在解决上面提到的两个问题。</p><p>For the intuition and theoritical background behind WGAN, redirect to <a href="https://paper.dropbox.com/doc/Wasserstein-GAN-GvU0p2V9ThzdwY3BbhoP7" target="_blank" rel="noopener">this excellent summary</a> (credits to the author).</p><p>算法如下：</p><p><img src="/images/blog/2020/WGAN.PNG" alt></p><p>可以看到这个算法非常接近原始的 GAN。但是，值得注意下面几点：</p><ol><li>Loss 函数中没有 log，$D$ (判别器) 的输出不再是概率，因此在输出层不使用 sigmoid 函数</li><li>Clip the weight of $D$</li><li>Train $D$ more than $G$</li><li>Use RMSProp instead of ADAM</li><li>Lower learning rate, the paper uses $\alpha = 0.00005$<a id="more"></a></li></ol><p><br></p><h2 id="WGAN-TensorFlow-implementation"><a href="#WGAN-TensorFlow-implementation" class="headerlink" title="WGAN TensorFlow implementation"></a>WGAN TensorFlow implementation</h2><p>The base implementation of GAN could be found in <a href="https://wiseodd.github.io/techblog/2016/09/17/gan-tensorflow/" target="_blank" rel="noopener">the past post</a>. We need only to modify traditional GAN with respect to those items above. So first, let’s update our $D$:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" Vanilla GAN """</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(x)</span>:</span></span><br><span class="line">    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)</span><br><span class="line">    out = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class="line">    <span class="keyword">return</span> tf.nn.sigmoid(out)</span><br><span class="line"></span><br><span class="line"><span class="string">""" WGAN """</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(x)</span>:</span></span><br><span class="line">    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)</span><br><span class="line">    out = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>Next, we modify our loss by simply removing the <strong>log</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" Vanilla GAN """</span></span><br><span class="line">D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(<span class="number">1.</span> - D_fake))</span><br><span class="line">G_loss = -tf.reduce_mean(tf.log(D_fake))</span><br><span class="line"></span><br><span class="line"><span class="string">""" WGAN """</span></span><br><span class="line">D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)</span><br><span class="line">G_loss = -tf.reduce_mean(D_fake)</span><br></pre></td></tr></table></figure><p>We then clip the weight of $D$ after each gradient descent update:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># theta_D is list of D's params</span></span><br><span class="line">clip_D = [p.assign(tf.clip_by_value(p, <span class="number">-0.01</span>, <span class="number">0.01</span>)) <span class="keyword">for</span> p <span class="keyword">in</span> theta_D]</span><br></pre></td></tr></table></figure><p>Lastly, we train $D$ more:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">D_solver = (tf.train.RMSPropOptimizer(learning_rate=<span class="number">5e-5</span>)</span><br><span class="line">            .minimize(-D_loss, var_list=theta_D))</span><br><span class="line">G_solver = (tf.train.RMSPropOptimizer(learning_rate=<span class="number">5e-5</span>)</span><br><span class="line">            .minimize(G_loss, var_list=theta_G))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        X_mb, _ = mnist.train.next_batch(mb_size)</span><br><span class="line"></span><br><span class="line">        _, D_loss_curr, _ = sess.run(</span><br><span class="line">            [D_solver, D_loss, clip_D],</span><br><span class="line">            feed_dict=&#123;X: X_mb, z: sample_z(mb_size, z_dim)&#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    _, G_loss_curr = sess.run(</span><br><span class="line">        [G_solver, G_loss],</span><br><span class="line">        feed_dict=&#123;z: sample_z(mb_size, z_dim)&#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>And that is it.</p><p><br></p><h2 id="WGAN-Pytorch-implementation"><a href="#WGAN-Pytorch-implementation" class="headerlink" title="WGAN Pytorch implementation"></a>WGAN Pytorch implementation</h2><p>The base implementation of original GAN could be found in <a href="https://wiseodd.github.io/techblog/2017/01/20/gan-pytorch/" target="_blank" rel="noopener">the past post</a>. Similar to the TensorFlow version, the modifications are quite straight forward. Note the codes below are inside each training iteration.</p><p>First, update $D$:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" Vanilla GAN """</span></span><br><span class="line">D = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(X_dim, h_dim),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(h_dim, <span class="number">1</span>),</span><br><span class="line">    torch.nn.Sigmoid()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="string">""" WGAN """</span></span><br><span class="line">D = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(X_dim, h_dim),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(h_dim, <span class="number">1</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Modifying loss:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" Vanilla GAN """</span></span><br><span class="line"><span class="comment"># During discriminator forward-backward-update</span></span><br><span class="line">D_loss = torch.mean(torch.log(D_real) - torch.log(<span class="number">1</span>- D_fake))</span><br><span class="line"><span class="comment"># During generator forward-backward-update</span></span><br><span class="line">G_loss = -torch.mean(torch.log(D_fake))</span><br><span class="line"></span><br><span class="line"><span class="string">""" WGAN """</span></span><br><span class="line"><span class="comment"># During discriminator forward-backward-update</span></span><br><span class="line">D_loss = -(torch.mean(D_real) - torch.mean(D_fake))</span><br><span class="line"><span class="comment"># During generator forward-backward-update</span></span><br><span class="line">G_loss = -torch.mean(D_fake)</span><br></pre></td></tr></table></figure><p>Weight clipping:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">D_loss.backward()</span><br><span class="line">D_solver.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> D.parameters():</span><br><span class="line">    p.data.clamp_(<span class="number">-0.01</span>, <span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>Train $D$ more:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">G_solver = optim.RMSprop(G.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line">D_solver = optim.RMSprop(D.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="string">""" Dicriminator forward-loss-backward-update """</span></span><br><span class="line"></span><br><span class="line">    <span class="string">""" Generator forward-loss-backward-update """</span></span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="noopener">https://arxiv.org/abs/1701.07875</a></li><li><a href="https://paper.dropbox.com/doc/Wasserstein-GAN-GvU0p2V9ThzdwY3BbhoP7" target="_blank" rel="noopener">https://paper.dropbox.com/doc/Wasserstein-GAN-GvU0p2V9ThzdwY3BbhoP7</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
          <category> GAN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Loss Function </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> WGAN </tag>
            
            <tag> Wasserstein Loss </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DCGAN的实现及效果</title>
      <link href="/2020/10/06/DCGAN%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8F%8A%E6%95%88%E6%9E%9C/"/>
      <url>/2020/10/06/DCGAN%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8F%8A%E6%95%88%E6%9E%9C/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">DCGAN (Deep Convolutional Generative Adversarial Networks)</a> 是最简单的GAN模型了，也是在GAN之后最早出现的变种，只是把GAN中的MLP换作了卷积层以及转置卷积层 (Tranposed Convolution)，一份简洁的，使用<a href="https://github.com/tensorlayer/tensorlayer" target="_blank" rel="noopener">TensorLayer</a> 实现的网络结构为：</p><h3 id="1-代码"><a href="#1-代码" class="headerlink" title="1. 代码"></a>1. 代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorlayer <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> tensorlayer.layers <span class="keyword">import</span> Input, Dense, DeConv2d, Reshape, BatchNorm2d, Conv2d, Flatten</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_generator</span><span class="params">(shape, gf_dim=<span class="number">64</span>)</span>:</span> <span class="comment"># Dimension of gen filters in first conv layer. [64]</span></span><br><span class="line">    image_size = <span class="number">64</span></span><br><span class="line">    s16 = image_size // <span class="number">16</span></span><br><span class="line">    <span class="comment"># w_init = tf.glorot_normal_initializer()</span></span><br><span class="line">    w_init = tf.random_normal_initializer(stddev=<span class="number">0.02</span>)  <span class="comment"># 正态初始化</span></span><br><span class="line">    gamma_init = tf.random_normal_initializer(<span class="number">1.</span>, <span class="number">0.02</span>)</span><br><span class="line">    </span><br><span class="line">    ni = Input(shape)</span><br><span class="line">    nn = Dense(n_units=(gf_dim * <span class="number">8</span> * s16 * s16), W_init=w_init, b_init=<span class="literal">None</span>)(ni)</span><br><span class="line">    nn = Reshape(shape=[<span class="number">-1</span>, s16, s16, gf_dim*<span class="number">8</span>])(nn)</span><br><span class="line">    nn = BatchNorm2d(decay=<span class="number">0.9</span>, act=tf.nn.relu, gamma_init=gamma_init, name=<span class="literal">None</span>)(nn)</span><br><span class="line">    <span class="comment"># n_filter, filter_size, strides</span></span><br><span class="line">    nn = DeConv2d(gf_dim * <span class="number">4</span>, (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">2</span>), W_init=w_init, b_init=<span class="literal">None</span>)(nn)</span><br><span class="line">    nn = BatchNorm2d(decay=<span class="number">0.9</span>, act=tf.nn.relu, gamma_init=gamma_init)(nn)</span><br><span class="line">    nn = DeConv2d(gf_dim * <span class="number">2</span>, (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">2</span>), W_init=w_init, b_init=<span class="literal">None</span>)(nn)</span><br><span class="line">    nn = BatchNorm2d(decay=<span class="number">0.9</span>, act=tf.nn.relu, gamma_init=gamma_init)(nn)</span><br><span class="line">    nn = DeConv2d(gf_dim, (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">2</span>), W_init=w_init, b_init=<span class="literal">None</span>)(nn)</span><br><span class="line">    nn = BatchNorm2d(decay=<span class="number">0.9</span>, act=tf.nn.relu, gamma_init=gamma_init)(nn)</span><br><span class="line">    nn = DeConv2d(<span class="number">3</span>, (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">2</span>), act=tf.nn.tanh, W_init=w_init)(nn)</span><br><span class="line">    <span class="keyword">return</span> tl.models.Model(inputs=ni, outputs=nn, name=<span class="string">'generator'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_discriminator</span><span class="params">(shape, df_dim=<span class="number">64</span>)</span>:</span> <span class="comment"># Dimension of discrim filters in first conv layer. [64]</span></span><br><span class="line">    <span class="comment"># w_init = tf.glorot_normal_initializer()</span></span><br><span class="line">    w_init = tf.random_normal_initializer(stddev=<span class="number">0.02</span>)</span><br><span class="line">    gamma_init = tf.random_normal_initializer(<span class="number">1.</span>, <span class="number">0.02</span>)</span><br><span class="line">    lrelu = <span class="keyword">lambda</span> x : tf.nn.leaky_relu(x, <span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    ni = Input(shape)</span><br><span class="line">    nn = Conv2d(df_dim, (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">2</span>), act=lrelu, W_init=w_init)(ni)</span><br><span class="line">    nn = Conv2d(df_dim*<span class="number">2</span>, (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">2</span>), W_init=w_init, b_init=<span class="literal">None</span>)(nn)</span><br><span class="line">    nn = BatchNorm2d(decay=<span class="number">0.9</span>, act=lrelu, gamma_init=gamma_init)(nn)</span><br><span class="line">    nn = Conv2d(df_dim*<span class="number">4</span>, (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">2</span>), W_init=w_init, b_init=<span class="literal">None</span>)(nn)</span><br><span class="line">    nn = BatchNorm2d(decay=<span class="number">0.9</span>, act=lrelu, gamma_init=gamma_init)(nn)</span><br><span class="line">    nn = Conv2d(df_dim*<span class="number">8</span>, (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">2</span>), W_init=w_init, b_init=<span class="literal">None</span>)(nn)</span><br><span class="line">    nn = BatchNorm2d(decay=<span class="number">0.9</span>, act=lrelu, gamma_init=gamma_init)(nn)</span><br><span class="line">    nn = Flatten()(nn)</span><br><span class="line">    nn = Dense(n_units=<span class="number">1</span>, act=tf.identity, W_init=w_init)(nn)  </span><br><span class="line">    <span class="keyword">return</span> tl.models.Model(inputs=ni, outputs=nn, name=<span class="string">'discriminator'</span>)</span><br></pre></td></tr></table></figure><h3 id="2-实验设置"><a href="#2-实验设置" class="headerlink" title="2. 实验设置"></a>2. 实验设置</h3><p>使用celeba数据集，是名人 (celebrity) 人脸图片数据集，可以在Google Driver中下载：</p><p><a href="https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8" target="_blank" rel="noopener">https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8</a></p><p>使用Nvidia TITAN RTX显卡，显存为24G，batch_size设为64；每个epoch有3165 steps，共训练25个epoch，每个epoch耗时284秒，训练共耗时2h 3min。<a id="more"></a></p><h3 id="3-实验效果"><a href="#3-实验效果" class="headerlink" title="3. 实验效果"></a>3. 实验效果</h3><p>随机初始化网络参数，未经训练效果为：</p><p><img src="/images/blog/2020/DCGAN/train_00.png" alt></p><p>训练一个epoch后：</p><p><img src="/images/blog/2020/DCGAN/train_01.png" alt></p><p>训练五个epoch后：</p><p><img src="/images/blog/2020/DCGAN/train_05.png" alt></p><p>训练结束时的效果：</p><p><img src="/images/blog/2020/DCGAN/train_24.png" alt></p><h3 id="4-结论"><a href="#4-结论" class="headerlink" title="4. 结论"></a>4. 结论</h3><p>训练是有效的，每个epoch人脸逐渐变得清晰。但最后的效果看起来也不是很好😓我个人觉得，可能还是网络比较早期，相比于2019年的styleGAN差的不是一点点，DCGAN是2015年提出的。不过，还是有所收获的！</p>]]></content>
      
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> 生成模型 </tag>
            
            <tag> DCGAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务器CuDNN版本升级</title>
      <link href="/2020/10/05/%E6%9C%8D%E5%8A%A1%E5%99%A8CuDNN%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7/"/>
      <url>/2020/10/05/%E6%9C%8D%E5%8A%A1%E5%99%A8CuDNN%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7/</url>
      
        <content type="html"><![CDATA[<p>最近在用到tensorlayer跑代码时，遇到了CuDNN版本问题，编译时的版本与运行时版本不同，导致报错。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-12-24 05:34:15.862520: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.1 but source was compiled with: 7.6.0. CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library. If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.</span><br></pre></td></tr></table></figure><p>在服务器上，<strong>没有root权限</strong>，也可以升级CuDNN版本，具体操作下面给出。</p><h3 id="1-下载CuDNN"><a href="#1-下载CuDNN" class="headerlink" title="1. 下载CuDNN"></a>1. 下载CuDNN</h3><p>网址为：<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">https://developer.nvidia.com/cudnn</a></p><p>需要先注册，之后选择<font color="#4fff60">cnDNN Library for Linux</font></p><h3 id="2-解压安装"><a href="#2-解压安装" class="headerlink" title="2. 解压安装"></a>2. 解压安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xzvf cudnn*</span><br></pre></td></tr></table></figure><p>得到一个名为cuda的文件夹</p><h3 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3. 配置环境变量"></a>3. 配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure><p>在末尾加上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/home/xxx/cuda/lib64"</span><br></pre></td></tr></table></figure><p>最后激活：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><p>到这里就大功告成了！</p><h3 id="4-验证"><a href="#4-验证" class="headerlink" title="4. 验证"></a>4. 验证</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><p>输出中包含<code>/home/xxx/cuda/lib64</code>，证明激活成功。</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NVIDIA </tag>
            
            <tag> cuda </tag>
            
            <tag> cudnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解Batch Normalization</title>
      <link href="/2020/10/03/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Batch-Normalization/"/>
      <url>/2020/10/03/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Batch-Normalization/</url>
      
        <content type="html"><![CDATA[<p>Batch Normalization具有可以增强神经网络的稳定性，加速神经网络训练等优点，但一直没有深入详细地了解过其原理与实现。通过<a href>coursera</a>的<font color="#0077dd">Generative Adversarial Networks (GANs)</font>专项课程，较为深入的学习了Batch Normalization，在此记录。</p><p>&lt;/br&gt;</p><h3 id="1-原理"><a href="#1-原理" class="headerlink" title="1. 原理"></a>1. 原理</h3><p>假设有一个很简单的神经网络，输入为向量$[x_1, x_2]$，输出为一只猫的概率。</p><p><img src="/images/blog/2020/BN/network.PNG" alt></p><p>而$x_1,x_2$两个维度的分布是不同的，假设两者均服从正态分布，$x_2$的$\mu$值更大，$\sigma$更小：</p><p><img src="/images/blog/2020/BN/x1.PNG" alt></p><p>这导致损失函数被拉长，</p><blockquote><p>So that changes to the weights relating to each of the inputs will have kind of </p><p>a different effect of varying impact on this cost function.</p></blockquote><p>并且导致训练变得困难，网络收敛更慢而且更依赖参数初始化的效果。</p><p><img src="/images/blog/2020/BN/costfunction.PNG" alt></p><hr><p>另外，由于测试集与训练集可能存在数据分布的不一致，即：</p><p><img src="/images/blog/2020/BN/shift.PNG" alt></p><p>会导致损失函数的变化：</p><p><img src="/images/blog/2020/BN/costshift.PNG" alt></p><p>这种现象叫做<strong>covariate shift</strong>。<font color="orange">如果对$[x_1, x_2]$进行规范化，那么数据分布会较为平衡，并且损失函数更顺滑，训练神经网络会更简单、更快速。</font><a id="more"></a></p><p><img src="/images/blog/2020/BN/norm.PNG" alt></p><p>&lt;/br&gt;</p><h3 id="2-Training-Procedure"><a href="#2-Training-Procedure" class="headerlink" title="2. Training Procedure"></a>2. Training Procedure</h3><font color="#ff0077">Batch Normalization是针对**特征**进行的。</font><p><img src="/images/blog/2020/BN/training.PNG" alt></p><p>计算出平均值$\mu$和标准差$\sigma$，按照以下公式规范化：</p><script type="math/tex; mode=display">\hat{z}_i^{[l]} = \frac{z_i^{[l]} - \mu _ {z_i^{[l]}}} {\sqrt{\sigma _ {z_i^{[l]}}^{2} + \epsilon}}</script><blockquote><p>After you get the normalized value z-hat, you have learned perimeters </p><p>in the batch normalization layer. What that means is that you will </p><p>have a value called Beta, which will be the shift factor and Gamma, </p><p>which will be the scale factor. These parameters are learned during training to ensure that </p><p>the distribution to which you’re transforming z is the optimal one for your task.</p></blockquote><script type="math/tex; mode=display">y_i^{[l]} = \gamma \hat{z}_i ^{[l]} + \beta</script><p><strong>这是普通的规范化与Batch Normalization的根本区别。</strong></p><p>&lt;/br&gt;</p><h3 id="3-Testing-Procedure"><a href="#3-Testing-Procedure" class="headerlink" title="3. Testing Procedure"></a>3. Testing Procedure</h3><p>在测试时，按照同样的方法计算平均值$\mu$和标准差$\sigma$，<strong>但是$\gamma$和$\beta$是固定的</strong>，是训练时的最优值。</p>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural Networks </tag>
            
            <tag> 规范化 </tag>
            
            <tag> Batch Normalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模拟退火算法</title>
      <link href="/2020/10/03/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/"/>
      <url>/2020/10/03/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>总是在一些深度学习相关的博客、文章中看到<strong>模拟退火</strong>算法，这个名字让人感觉云里雾里，不明白是什么意思，今天查询了一下，做一个记录，以备以后用到。<font color="orange">简单粗暴，直接给出伪代码</font>：</p><hr><p>寻找能量$E(s)$最低的状态$s$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">s = s0, e = E(s)  <span class="comment"># 设定目前状态为s0，能量为E(s0)</span></span><br><span class="line">k = <span class="number">0</span>  <span class="comment"># 评估次数k</span></span><br><span class="line"><span class="keyword">while</span> k &lt; kmax <span class="keyword">and</span> e &gt; emin:  <span class="comment"># 若还有时间 (k &gt; kmax) 而且结果还不够好 (e &gt; emin) 则：</span></span><br><span class="line">    sn = neighbour(s)  <span class="comment"># 随机选取一临近状态sn</span></span><br><span class="line">    en = E(sn)  <span class="comment"># sn的能量为E(sn)</span></span><br><span class="line">    <span class="keyword">if</span> random() &lt; P(e, en, temp(k / kmax)):  <span class="comment"># 决定是否移至临近状态sn</span></span><br><span class="line">        s = sn  <span class="comment"># 移至临近状态sn</span></span><br><span class="line">        e = en</span><br><span class="line">    k = k + <span class="number">1</span>  <span class="comment"># 评估完成，次数k加一</span></span><br><span class="line"><span class="keyword">return</span> s  <span class="comment"># 回传状态s</span></span><br></pre></td></tr></table></figure><hr><p>重点应该是$neighbor()$函数、$temp()$函数以及$P(e, en , temp(k / kmax))$概率分布函数。</p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithms </tag>
            
            <tag> 模拟退火 </tag>
            
            <tag> 算法 </tag>
            
            <tag> Simulated Annealing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用Freebase远程监督-SPARQL查询</title>
      <link href="/2020/09/26/%E5%88%A9%E7%94%A8Freebase%E8%BF%9C%E7%A8%8B%E7%9B%91%E7%9D%A3-SPARQL%E6%9F%A5%E8%AF%A2/"/>
      <url>/2020/09/26/%E5%88%A9%E7%94%A8Freebase%E8%BF%9C%E7%A8%8B%E7%9B%91%E7%9D%A3-SPARQL%E6%9F%A5%E8%AF%A2/</url>
      
        <content type="html"><![CDATA[<p>在DisFeb实验中，需要利用远程监督生成数据，因此记录一下<a href>Freebase</a>的查询方法：</p><p>在个人实验中，采用下面5种关系作为demo：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/business/company/place_founded</span><br><span class="line">/location/administrative_division/country</span><br><span class="line">/location/country/capital</span><br><span class="line">/people/deceased_person/place_of_death</span><br><span class="line">/people/person/children</span><br></pre></td></tr></table></figure><p>以<code>/location/country/capital</code>为例，介绍查询过程：</p><h4 id="1-查询头尾实体"><a href="#1-查询头尾实体" class="headerlink" title="1. 查询头尾实体"></a>1. 查询头尾实体</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select *</span><br><span class="line">where</span><br><span class="line">&#123;</span><br><span class="line">?s &lt;http://rdf.freebase.com/ns/location.country.capital&gt; ?o</span><br><span class="line">&#125;</span><br><span class="line">limit 100</span><br></pre></td></tr></table></figure><p>得到结果：</p><p><img src="/images/blog/2020/head_tail.PNG" alt></p><a id="more"></a><h4 id="2-查询字面量"><a href="#2-查询字面量" class="headerlink" title="2. 查询字面量"></a>2. 查询字面量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select *</span><br><span class="line">where</span><br><span class="line">&#123;</span><br><span class="line">&lt;http://rdf.freebase.com/ns/m.02psqkz&gt; </span><br><span class="line">&lt;http://rdf.freebase.com/ns/type.object.name&gt; </span><br><span class="line">?o</span><br><span class="line">&#125;</span><br><span class="line">limit 100</span><br></pre></td></tr></table></figure><p>查询结果为：</p><p><img src="/images/blog/2020/head_name.jpg" alt></p><p>头实体对应的中文名称为“意大利王国”。尾实体字面量的查询与头实体同理，查询结果为：</p><p><img src="/images/blog/2020/tail_name.PNG" alt></p><p>得到了一个正确的三元组$(意大利，/location/country/capital，佛罗伦萨)$，接下来可以用 <em>意大利</em> 和 <em>佛罗伦萨</em> 做远程监督了。</p><h4 id="附表"><a href="#附表" class="headerlink" title="附表"></a>附表</h4><p>在NYT10数据集中，利用的远程监督<font color="green">关系类别</font>及<font color="green">数量统计</font>为：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">('/broadcast/content/location', 8)</span><br><span class="line">('/broadcast/producer/location', 71)</span><br><span class="line">('/business/business_location/parent_company', 19)</span><br><span class="line">('/business/company/advisors', 9)</span><br><span class="line">('/business/company/founders', 901)</span><br><span class="line">('/business/company/locations', 19)</span><br><span class="line">('/business/company/major_shareholders', 328)</span><br><span class="line">('/business/company/place_founded', 648)</span><br><span class="line">('/business/company_advisor/companies_advised', 2)</span><br><span class="line">('/business/person/company', 7336)</span><br><span class="line">('/business/shopping_center/owner', 1)</span><br><span class="line">('/business/shopping_center_owner/shopping_centers_owned', 1)</span><br><span class="line">('/film/film/featured_film_locations', 18)</span><br><span class="line">('/film/film_festival/location', 4)</span><br><span class="line">('/film/film_location/featured_in_films', 18)</span><br><span class="line">('/location/administrative_division/country', 7286)</span><br><span class="line">('/location/br_state/capital', 4)</span><br><span class="line">('/location/cn_province/capital', 2)</span><br><span class="line">('/location/country/administrative_divisions', 7286)</span><br><span class="line">('/location/country/capital', 8883)</span><br><span class="line">('/location/de_state/capital', 7)</span><br><span class="line">('/location/fr_region/capital', 1)</span><br><span class="line">('/location/in_state/administrative_capital', 4)</span><br><span class="line">('/location/in_state/judicial_capital', 3)</span><br><span class="line">('/location/in_state/legislative_capital', 4)</span><br><span class="line">('/location/it_region/capital', 22)</span><br><span class="line">('/location/jp_prefecture/capital', 2)</span><br><span class="line">('/location/location/contains', 66721)</span><br><span class="line">('/location/mx_state/capital', 1)</span><br><span class="line">('/location/neighborhood/neighborhood_of', 9275)</span><br><span class="line">('/location/province/capital', 39)</span><br><span class="line">('/location/us_county/county_seat', 110)</span><br><span class="line">('/location/us_state/capital', 798)</span><br><span class="line">('/people/deceased_person/place_of_burial', 24)</span><br><span class="line">('/people/deceased_person/place_of_death', 2422)</span><br><span class="line">('/people/ethnicity/geographic_distribution', 86)</span><br><span class="line">('/people/ethnicity/included_in_group', 7)</span><br><span class="line">('/people/family/country', 6)</span><br><span class="line">('/people/family/members', 4)</span><br><span class="line">('/people/person/children', 622)</span><br><span class="line">('/people/person/ethnicity', 148)</span><br><span class="line">('/people/person/nationality', 9733)</span><br><span class="line">('/people/person/place_lived', 8907)</span><br><span class="line">('/people/person/place_of_birth', 4053)</span><br><span class="line">('/people/person/profession', 10)</span><br><span class="line">('/people/person/religion', 202)</span><br><span class="line">('/people/place_of_interment/interred_here', 24)</span><br><span class="line">('/people/profession/people_with_this_profession', 2)</span><br><span class="line">('/sports/sports_team/location', 294)</span><br><span class="line">('/time/event/locations', 4)</span><br><span class="line">('NA', 385664)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Freebase </tag>
            
            <tag> SPARQL </tag>
            
            <tag> Relation Extraction </tag>
            
            <tag> Distant Supervision </tag>
            
            <tag> 关系抽取 </tag>
            
            <tag> 远程监督 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生成对抗网络(GAN)初探</title>
      <link href="/2020/09/23/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-GAN-%E5%88%9D%E6%8E%A2/"/>
      <url>/2020/09/23/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-GAN-%E5%88%9D%E6%8E%A2/</url>
      
        <content type="html"><![CDATA[<p>一直对GAN即生成对抗网络很感兴趣，随着科研的进展，近期可能会用到GAN来做文本数据的增强和去噪，所以简单尝试了一下，真的很有意思！😄</p><p>推荐两个GAN的GitHub代码库：</p><p><a href="https://github.com/eriklindernoren/PyTorch-GAN" target="_blank" rel="noopener">https://github.com/eriklindernoren/PyTorch-GAN</a></p><p><a href="https://github.com/eriklindernoren/Keras-GAN" target="_blank" rel="noopener">https://github.com/eriklindernoren/Keras-GAN</a></p><p>下面用<strong>MNIST</strong>来做实验，使用的是generator和discriminator均为多层感知机的最简单的GAN网络，代码非常易读，就不过多解释了：<a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Reshape, Flatten, Dropout</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> BatchNormalization, Activation, ZeroPadding2D</span><br><span class="line"><span class="keyword">from</span> keras.layers.advanced_activations <span class="keyword">import</span> LeakyReLU</span><br><span class="line"><span class="keyword">from</span> keras.layers.convolutional <span class="keyword">import</span> UpSampling2D, Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential, Model</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GAN</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.img_rows = <span class="number">28</span></span><br><span class="line">        self.img_cols = <span class="number">28</span></span><br><span class="line">        self.channels = <span class="number">1</span></span><br><span class="line">        self.img_shape = (self.img_rows, self.img_cols, self.channels)</span><br><span class="line">        self.latent_dim = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">        optimizer = Adam(<span class="number">0.0002</span>, <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build and compile the discriminator</span></span><br><span class="line">        self.discriminator = self.build_discriminator()</span><br><span class="line">        self.discriminator.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            optimizer=optimizer,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build the generator</span></span><br><span class="line">        self.generator = self.build_generator()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The generator takes noise as input and generates imgs</span></span><br><span class="line">        z = Input(shape=(self.latent_dim,))</span><br><span class="line">        img = self.generator(z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For the combined model we will only train the generator</span></span><br><span class="line">        self.discriminator.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># The discriminator takes generated images as input and determines validity</span></span><br><span class="line">        validity = self.discriminator(img)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The combined model  (stacked generator and discriminator)</span></span><br><span class="line">        <span class="comment"># Trains the generator to fool the discriminator</span></span><br><span class="line">        self.combined = Model(z, validity)</span><br><span class="line">        self.combined.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=optimizer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_generator</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        model = Sequential()</span><br><span class="line"></span><br><span class="line">        model.add(Dense(<span class="number">256</span>, input_dim=self.latent_dim))</span><br><span class="line">        model.add(LeakyReLU(alpha=<span class="number">0.2</span>))</span><br><span class="line">        model.add(BatchNormalization(momentum=<span class="number">0.8</span>))</span><br><span class="line">        model.add(Dense(<span class="number">512</span>))</span><br><span class="line">        model.add(LeakyReLU(alpha=<span class="number">0.2</span>))</span><br><span class="line">        model.add(BatchNormalization(momentum=<span class="number">0.8</span>))</span><br><span class="line">        model.add(Dense(<span class="number">1024</span>))</span><br><span class="line">        model.add(LeakyReLU(alpha=<span class="number">0.2</span>))</span><br><span class="line">        model.add(BatchNormalization(momentum=<span class="number">0.8</span>))</span><br><span class="line">        model.add(Dense(np.prod(self.img_shape), activation=<span class="string">'tanh'</span>))</span><br><span class="line">        model.add(Reshape(self.img_shape))</span><br><span class="line"></span><br><span class="line">        model.summary()</span><br><span class="line"></span><br><span class="line">        noise = Input(shape=(self.latent_dim,))</span><br><span class="line">        img = model(noise)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Model(noise, img)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_discriminator</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        model = Sequential()</span><br><span class="line"></span><br><span class="line">        model.add(Flatten(input_shape=self.img_shape))</span><br><span class="line">        model.add(Dense(<span class="number">512</span>))</span><br><span class="line">        model.add(LeakyReLU(alpha=<span class="number">0.2</span>))</span><br><span class="line">        model.add(Dense(<span class="number">256</span>))</span><br><span class="line">        model.add(LeakyReLU(alpha=<span class="number">0.2</span>))</span><br><span class="line">        model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">        model.summary()</span><br><span class="line"></span><br><span class="line">        img = Input(shape=self.img_shape)</span><br><span class="line">        validity = model(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Model(img, validity)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, epochs, batch_size=<span class="number">128</span>, sample_interval=<span class="number">50</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Load the dataset</span></span><br><span class="line">        (X_train, _), (_, _) = mnist.load_data()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Rescale -1 to 1</span></span><br><span class="line">        X_train = X_train / <span class="number">127.5</span> - <span class="number">1.</span></span><br><span class="line">        X_train = np.expand_dims(X_train, axis=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Adversarial ground truths</span></span><br><span class="line">        valid = np.ones((batch_size, <span class="number">1</span>))</span><br><span class="line">        fake = np.zeros((batch_size, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ---------------------</span></span><br><span class="line">            <span class="comment">#  Train Discriminator</span></span><br><span class="line">            <span class="comment"># ---------------------</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Select a random batch of images</span></span><br><span class="line">            idx = np.random.randint(<span class="number">0</span>, X_train.shape[<span class="number">0</span>], batch_size)</span><br><span class="line">            imgs = X_train[idx]</span><br><span class="line"></span><br><span class="line">            noise = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (batch_size, self.latent_dim))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Generate a batch of new images</span></span><br><span class="line">            gen_imgs = self.generator.predict(noise)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Train the discriminator</span></span><br><span class="line">            d_loss_real = self.discriminator.train_on_batch(imgs, valid)</span><br><span class="line">            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)</span><br><span class="line">            d_loss = <span class="number">0.5</span> * np.add(d_loss_real, d_loss_fake)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ---------------------</span></span><br><span class="line">            <span class="comment">#  Train Generator</span></span><br><span class="line">            <span class="comment"># ---------------------</span></span><br><span class="line"></span><br><span class="line">            noise = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (batch_size, self.latent_dim))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Train the generator (to have the discriminator label samples as valid)</span></span><br><span class="line">            g_loss = self.combined.train_on_batch(noise, valid)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Plot the progress</span></span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]"</span> % (epoch, d_loss[<span class="number">0</span>], <span class="number">100</span>*d_loss[<span class="number">1</span>], g_loss))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># If at save interval =&gt; save generated image samples</span></span><br><span class="line">            <span class="keyword">if</span> epoch % sample_interval == <span class="number">0</span>:</span><br><span class="line">                self.sample_images(epoch)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample_images</span><span class="params">(self, epoch)</span>:</span></span><br><span class="line">        r, c = <span class="number">5</span>, <span class="number">5</span></span><br><span class="line">        noise = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (r * c, self.latent_dim))</span><br><span class="line">        gen_imgs = self.generator.predict(noise)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Rescale images 0 - 1</span></span><br><span class="line">        gen_imgs = <span class="number">0.5</span> * gen_imgs + <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        fig, axs = plt.subplots(r, c)</span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(r):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(c):</span><br><span class="line">                axs[i,j].imshow(gen_imgs[cnt, :,:,<span class="number">0</span>], cmap=<span class="string">'gray'</span>)</span><br><span class="line">                axs[i,j].axis(<span class="string">'off'</span>)</span><br><span class="line">                cnt += <span class="number">1</span></span><br><span class="line">        fig.savefig(<span class="string">"images/%d.png"</span> % epoch)</span><br><span class="line">        plt.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    gan = GAN()</span><br><span class="line">    gan.train(epochs=<span class="number">30000</span>, batch_size=<span class="number">32</span>, sample_interval=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><p>GAN可以利用输入的高斯分布的噪声数据生成图片，想想都是很神奇的事情~</p><font color="orange">实验效果很明显：</font><p>没有训练时生成的图片，可以看到是一片噪声，像是电视没信号时的雪花斑点：</p><p><img src="/images/blog/2020/0.png" alt></p><p>由于MNIST数据集很小，我在cpu上训练，平均<code>35.7 steps/s</code></p><p>经过1000个step后，生成的图片：</p><p><img src="/images/blog/2020/1000.png" alt></p><font color="green">经过3000个step后，生成的图片，已经可以看出数字了：</font><p><img src="/images/blog/2020/3000.png" alt></p><p>经过10000个step后，生成的图片：</p><p><img src="/images/blog/2020/10000.png" alt></p><p>经过20000个step后，生成的图片：</p><p><img src="/images/blog/2020/20000.png" alt></p><font color="orange">经过30000个step后，生成的图片，已经是非常接近real image了，😀效果很好：</font><p><img src="/images/blog/2020/29800.png" alt></p><p>由于图像可以直接用数字矩阵表示，而文本数据需要首先转化为向量，然后再进行计算，所以存在一个天然的gap，还需要仔细思考如何将GAN如此强大的利器用到NLP中~</p>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
          <category> GAN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> 生成对抗网络 </tag>
            
            <tag> 生成模型 </tag>
            
            <tag> Generative Adversarial Networks </tag>
            
            <tag> MNIST </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorboard projector 不显示问题</title>
      <link href="/2020/09/21/tensorboard-projector-%E4%B8%8D%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/"/>
      <url>/2020/09/21/tensorboard-projector-%E4%B8%8D%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="1-问题"><a href="#1-问题" class="headerlink" title="1. 问题"></a>1. 问题</h2><p>使用Pytorch的过程中，想要将得到的句子向量可视化显示，查看特征向量在特征空间中的分布情况，但是在使用tensorboard可视化过程中，却显示空白。</p><h2 id="2-解决方案"><a href="#2-解决方案" class="headerlink" title="2. 解决方案"></a>2. 解决方案</h2><h3 id="2-1-版本问题"><a href="#2-1-版本问题" class="headerlink" title="2.1 版本问题"></a>2.1 版本问题</h3><p>可能是因为tensorboard版本问题，可以参考我之前写的一篇文章<a href="https://haokailong.top/2020/09/05/tensorboard%E6%98%BE%E7%A4%BA%E7%A9%BA%E7%99%BD%E9%97%AE%E9%A2%98/">tensorboard显示空白</a>，尝试将tensorboard降级为2.0.0版本。</p><h3 id="2-2-Windows注册表问题"><a href="#2-2-Windows注册表问题" class="headerlink" title="2.2 Windows注册表问题"></a>2.2 Windows注册表问题</h3><p>在Linux下，没有问题；但在Windows下，一些其他程序会修改注册表，导致tensorboard显示异常。</p><p>在Github中有相关issue：<a href="https://github.com/tensorflow/tensorboard/issues/3077" target="_blank" rel="noopener">https://github.com/tensorflow/tensorboard/issues/3077</a></p><p>但直接看我的文章可以更快速地解决问题😄</p><h4 id="2-2-1-修改注册表"><a href="#2-2-1-修改注册表" class="headerlink" title="2.2.1 修改注册表"></a>2.2.1 修改注册表</h4><p>首先，打开注册表编辑器</p><p><code>win + R</code> 然后输入<code>regedit</code>，依次点击<code>HKEY_CLASSES_ROOT\.js</code>，如果注册表被修改过 ，可以看到<font color="green">Content Type</font>对应“数据”被改成了<font color="red">text/plain</font> </p><p>点击<font color="green">Content Type</font>，并将对应数据修改为<font color="blue">application/javascript</font>，修改后注册表为：</p><p><img src="/images/blog/2020/regedit.PNG" alt></p><a id="more"></a><h4 id="2-2-2-重启tensorboard"><a href="#2-2-2-重启tensorboard" class="headerlink" title="2.2.2 重启tensorboard"></a>2.2.2 重启tensorboard</h4><p><code>Ctrl + C</code>终止tensorboard并重新运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir runs</span><br></pre></td></tr></table></figure><p>显示正常：😊</p><p><img src="/images/blog/2020/projector.PNG" alt></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorboard </tag>
            
            <tag> 可视化 </tag>
            
            <tag> projector </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EMNLP-2020 审稿指南</title>
      <link href="/2020/09/21/EMNLP-2020-%E5%AE%A1%E7%A8%BF%E6%8C%87%E5%8D%97/"/>
      <url>/2020/09/21/EMNLP-2020-%E5%AE%A1%E7%A8%BF%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p>对于审稿标准，<a href="https://2020.emnlp.org/blog/2020-05-17-write-good-reviews" target="_blank" rel="noopener">EMNLP-2020</a>给出了审稿指南，要求除SOTA以外，要更注重创新性、实在贡献等方面。对于我们开展科研工作也有一定的指导，下面是官方说明：</p><p>The intention for this post is to provide some advice to reviewers, such that we can identify the best research to be presented in the conference, and provide constructive feedback in order for authors to further improve their papers. We recognize and appreciate the amount of efforts reviewers have contributed, and hope to make that more beneficial. We want all the authors to feel the delight when they read the peer reviews for their papers.</p><p>This is not the first attempt to educate reviewers. Many major conferences have included advice to reviewers, in NLP and in other fields, and there’s also plentiful advice relating to journal reviewing. Within the field of NLP, we would highlight:</p><ul><li><a href="https://acl2017.wordpress.com/2017/02/23/last-minute-reviewing-advice/" target="_blank" rel="noopener">Discursive advice</a> in ACL 2017 from leading lights in the field: Mirella Lapata, Marco Baroni, Yoav Artzi, Emily Bender, Joel Tetreault, Ani Nenkova, and Tim Baldwin</li><li><a href="https://naacl2018.wordpress.com/2018/01/20/a-review-form-faq/" target="_blank" rel="noopener">Two example good reviews</a> from NAACL 2018 presented in their reviewing form</li><li><a href="https://soundcloud.com/nlp-highlights/77-on-writing-quality-peer-reviews-with-noah-a-smith" target="_blank" rel="noopener">A podcast by Noah Smith</a> about peer reviews</li></ul><p>Please take the time to look through these excellent resources.</p><p>We hope reiterating some dos and don’ts here can help reviewers as well as authors.</p><p>First, evaluate the paper’s contributions. This is where you will use your NLP domain knowledge. We advise that you should not accept papers just because their reported results are better, or that they appear to be mathematically sophisticated. These are not sufficient or necessary to constitute contributions. And we advise that you should not reject papers just because their results are not better than state-of-the-art. In the previous <em>ACL conferences, some reviewers placed too much emphasis on SOTA performance, giving low scores to any systems that failed to reach that. While we aim to publish the very best work, a more constructive question to ask is “<em>*state of which art?</em></em>“. As discussed in <a href="https://hackingsemantics.xyz/2020/reviewing-models/" target="_blank" rel="noopener">this blog post</a>, a paper could offer a step forward in terms of efficiency, generalizability, interpretability, and many other criteria. A convincing contribution of any kind should not be rejected only for not topping the leaderboards.</p><p>Regarding different kinds of contributions, here’s what Prof. Philip Resnik at University of Maryland says:</p><blockquote><p>I think there would be significant value in encouraging reviewers to think explicitly about the nature of the contribution, and what questions then need to be asked. As a first pass for consideration/discussion:</p><ul><li>Is this research making a <strong>scientific</strong> contribution? If so:<ul><li>What is the phenomenon in the world that the authors are seeking to improve our understanding of?</li><li>What do we now know about this phenomenon that we did not know before?</li></ul></li><li>Is this research making an <strong>engineering</strong> contribution? If so:<ul><li>What is the real-world problem (or set of problems) that this work is making progress on solving?</li><li>Alternatively, if it’s not targeting a current real-world problem, what real-world problem(s) will this work help <em>enable</em> solutions of?</li></ul></li><li>Is this research making a <strong>theoretical</strong> (e.g. mathematical) contribution? If so:<ul><li>What do we know now that we did not know before?</li><li>How does this theoretical or mathematical advance connect to either scientific or engineering goals? (See above.)</li></ul></li></ul><p>Work in computational linguistics might include a mixture of scientific, engineering, and theoretical contributions, rather than just one. But, I am suggesting, if a paper does not make a contribution in <em>any</em> of those three categories, with the sub-bullets having understandable answers, one should seriously consider whether it belongs at the conference.</p></blockquote><a id="more"></a><p>Second, consider these other important points when reading the paper and writing your review:</p><ul><li>Check what the paper’s claims are, and how the content of the paper supports that claim. If the paper claims X and there is a performance increase, is that really because of X?</li><li>Be specific in your comments. For example, if you think the authors have neglected to cite key papers, then provide these references in your review. It might be obvious to you, but it’s often less clear to the authors. Being specific will help the authors to formulate a cogent response to the review, and to fix these problems in their paper. And it is worth noting that the authors are <strong>not</strong> obliged to cite or draw comparisons with contemporaneous work (i.e. appearing within 3 months of submission), especially if it is not published in a peer-reviewed venue.</li><li>Be constructive in your advice. Stating that some aspect of the paper is done badly can be helpful in the gatekeeping aspect of review (providing grounds for rejection), but it tends to be less helpful to the authors. Some suggestions of how the authors might improve these problematic aspects can allow them to develop the work into something considerably better.</li><li>Be kind in your language, even when being critical. It’s easy to get carried away, and write something nasty that you would never say to someone’s face. Try to be polite in your feedback.</li></ul><p>Finally, it’s becoming more common for people to share reviews on social media, especially when the reviews reject the work on spurious grounds. These lead us to advise that the following are often invalid bases for rejecting a paper:</p><ul><li>The paper’s language or writing style. Please focus on the paper’s substance. We understand that there may be times when the language or writing style is so poor that reviewers can not understand the paper’s content and substance. In that case, it is fine to reject the paper, however you should only do so after making a concerted effort to understand the paper.</li><li>The paper’s work is on a language other than English. We care about NLP for any language.</li><li>The paper’s results are not better than SOTA. Please look at the paper’s contributions and findings, as discussed above and in <a href="https://hackingsemantics.xyz/2020/reviewing-models/" target="_blank" rel="noopener">this blog post</a>.</li><li>The paper does not use a particular method (e.g., deep learning). No one particular method is a requirement for good work. Please justify why that method is needed. Think about what the paper’s contributions are, and bear in mind that having a diversity of methods used is not a bad thing.</li><li>The paper’s method is too simple. Our goal is not to design the most complex method. Again, think what the paper’s contributions and findings are. Often the papers with the simplest methods are the most cited. If a simple method outperforms more complex methods from prior work, then this is often an important finding.</li><li>The paper’s topic is narrow or outdated. Please be open minded. We do not want the whole community to chase a trendy topic. Look at the paper’s contributions and consider what impact it may have on our community.</li><li>The paper’s topic is completely new, such that there’s no prior art or all the prior art has been done in another field. We are interested in papers that tread new ground.</li><li>The paper is a resource paper. In a field that relies on supervised machine learning as much as NLP, development of datasets is as important as modeling work. This <a href="https://hackingsemantics.xyz/2020/reviewing-data/" target="_blank" rel="noopener">blog post</a> discusses what can and cannot be grounds for dismissing a resource paper.</li></ul><p>Please refrain from using the reasons above as primary grounds for rejection when writing your reviews. We will ensure authors are aware of these guidelines and can reference them during the author rebuttal period. ACs will be checking reviews carefully based on the above criteria, and may ask that you revise your review, or that you provide objective reasons to justify your positions.</p><p>We hope these tips are helpful to reviewers, and hope there will be more authors that appreciate the insightful feedback they get from the reviews, and fewer frustrating authors that complain about review quality.</p><p>Thank you all for helping to review papers, and let’s make a great EMNLP 2020 together!</p><h2 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h2><p>To read more about the general advice on reviewing, we recommend the following resources:</p><ul><li>NeurIPS not only instructs reviewers as to what to include in their reviews, but also gives examples of useful reviewer comments in their <a href="https://nips.cc/Conferences/2019/PaperInformation/ReviewerGuidelines" target="_blank" rel="noopener">Reviewer Guidelines</a>, organized by evaluation criterion, such as “<em>Contributions of the submission</em>“, “<em>Quality of the submission</em>“, “<em>Clarity</em>“, “<em>Originality</em>“, and “<em>Significance</em>“. This will be particularly useful for new reviewers, and also for authors.</li><li>ICML gives some examples of good reviews. Please refer to the Part 2 of their <a href="https://icml.cc/Conferences/2020/ReviewerGuidelines" target="_blank" rel="noopener">Reviewer Guidelines</a>.</li><li>A blog <a href="https://patthomson.net/2019/10/14/reviewing-your-first-paper/" target="_blank" rel="noopener">article</a> by Pat Thomson about journal review, which shares a broadly similar procedure to conference review. The article is split into three parts, covering how to read the paper critically, how to decide on revisions required and recommendations to the PC, and how to write constructive feedback.</li><li>Wiley Publishing has a cute video with <a href="https://players.brightcove.net/3806881048001/rFXiCa5uY_default/index.html?videoId=4518165477001" target="_blank" rel="noopener">10 tips for first-time reviewers</a>. A list of those tips can be found in the <a href="https://authorservices.wiley.com/asset/photos/reviewers.html/journal-reviewers.html/Top_Tips_for_Peer_Review.pdf" target="_blank" rel="noopener">pdf document</a>. One tip that is useful for EVERYONE is tip 8: “Look at the Conclusion First”. Wiley advocates doing so because the Conclusion will give you a good idea whether the research is an exciting development within its own field. But another reason for doing so is to see what the paper is claiming to have done: This often differs from the Abstract and Introduction, which may make more impressive claims than the work actually supports.</li><li>Elsevier’s “Researcher Academy” has produced a video about “<a href="https://researcheracademy.elsevier.com/navigating-peer-review/certified-peer-reviewer-course/31-write-helpful-peer-review-report" target="_blank" rel="noopener">How to write a helpful peer review report</a>“, presented by Zoë Mullan, Founding editor and Editor-in-chief of the open access journal “The Lancet Global Health”. To watch the video, you have to sign in to join the academy (which is free).</li><li>Some interesting findings about paper reviews can be found in an early article published in the Journal of the American Medical Association (JAMA), “<a href="https://jamanetwork.com/journals/jama/fullarticle/187762" target="_blank" rel="noopener">What Makes a Good Reviewer and a Good Review for a General Medical Journal?</a>”.</li></ul><p>What any of the articles and videos indicated above will give you is CONFIDENCE that you’re doing the right thing. (Early Career reviewers will be happy to hear that one of the conclusions of the JAMA paper is that <em>“Younger age also was an independent predictor for editors’ quality assessments</em>“)</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EMNLP </tag>
            
            <tag> NLP </tag>
            
            <tag> 审稿 </tag>
            
            <tag> 科研 </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SemEval-2020自由文本关系抽取冠军方案解读</title>
      <link href="/2020/09/20/SemEval-2020%E8%87%AA%E7%94%B1%E6%96%87%E6%9C%AC%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E5%86%A0%E5%86%9B%E6%96%B9%E6%A1%88%E8%A7%A3%E8%AF%BB/"/>
      <url>/2020/09/20/SemEval-2020%E8%87%AA%E7%94%B1%E6%96%87%E6%9C%AC%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E5%86%A0%E5%86%9B%E6%96%B9%E6%A1%88%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<blockquote><p>以下文章来自于平安寿险PAI，作者谢舒翼</p></blockquote><h2 id="1-全文框架概览"><a href="#1-全文框架概览" class="headerlink" title="1. 全文框架概览"></a>1. 全文框架概览</h2><p><img src="/images/blog/2020/SemEval-2020/overview.webp" alt></p><h2 id="2-赛题介绍"><a href="#2-赛题介绍" class="headerlink" title="2. 赛题介绍"></a>2. 赛题介绍</h2><p><strong>SemEval</strong> 由 ACL（国际计算语言学协会）主办，是全球范围影响力最强、规模最大、参赛人数最多的词汇与语义计算领域权威赛事，迄今已举办 14 届，历届吸引了卡内基梅隆大学、TCS Research、百度、美团、科大讯飞等国内外一流高校、顶级科研机构和知名企业参与。</p><h3 id="2-1-任务简介"><a href="#2-1-任务简介" class="headerlink" title="2.1 任务简介"></a>2.1 任务简介</h3><p>本次我们参赛任务为DeftEval: Extracting term-definition pairs in free text（自由文本定义抽取）。在该任务中，我们团队以满分成绩夺得了关系抽取赛道冠军。 </p><p><img src="/images/blog/2020/SemEval-2020/2.1.webp" alt></p><p>主办方提供了两万多条从专业工具书筛选的句子，覆盖生物、历史、物理、心理学、金融、社会、政治等多个专业领域。该任务包含以下三个子任务：</p><ul><li><strong>Subtask1：</strong>给定一个句子，判断该句子里是否包含定义</li><li><strong>Subtask2：</strong>在给定tag schema下的词粒度BIO标签预测</li><li><strong>Subtask3：</strong>在给定relation schema下的关系抽取</li></ul><p>其中，Subtask2的tag schema包含以下六种：</p><ol><li><strong>Term:</strong> A primary term.</li><li><strong>Alias Term:</strong> A secondary or less common name for the primary term. Links to a term tag.</li><li><strong>Referential Term:</strong> A noun phrase(NP) reference to a previously mentioned term tag. Typically this/that/these + NP following a sentence boundary.</li><li><strong>Definition:</strong> A primary definition of a term. May not exist without a matching term.</li><li><strong>Referential Definition:</strong> NP reference to a previously mentioned definition tag. See Referential Term.</li><li><strong>Qualifier:</strong> A specific date, location, or other condition under which the definition holds true. Typically seen at the clause level.<a id="more"></a></li></ol><p>Subtask3的relation schema包含以下五种：</p><ol><li><strong>Direct-defines:</strong> Links definition to term.</li><li><strong>Indirect-defines:</strong> Links definition to referential term or term to referential definition.</li><li><strong>Refers-to:</strong> Links referential term to term or referential definition to definition.</li><li><strong>AKA:</strong> Links alias term to term.</li><li><strong>Supplements :</strong> Links qualifier to term.</li></ol><h3 id="2-2-数据集格式"><a href="#2-2-数据集格式" class="headerlink" title="2.2 数据集格式"></a>2.2 数据集格式</h3><p>训练数据集如下图所示有8列，用tab分隔，句子之间有空行，段落用2个空行分隔。</p><ul><li>TOKEN是句子里的单词；</li><li>SOURCE标识当前句子来源于哪篇文章；</li><li>START/END是单词在文章中的起始位置；</li><li>TAG则是来自于前面所述tag schema里的标签，符合BIO标注格式；</li><li>TAG_ID是TAG标签的一个唯一标识，如果O标签则为-1；</li><li>ROOT_ID是指在关系中当前TAG_ID所关联的TAG_ID，下面表格中T10是对T9的一个定义，所以T10的ROOT_ID为T9；</li><li>RELATION则是上文relation schema里介绍的关系，表中的例子为Direct-Defines关系。</li></ul><p><img src="/images/blog/2020/SemEval-2020/2.2.webp" alt></p><p>Subtask1判断一句话是否包含定义，Subtask2，已知前四列信息，预测第五列Tag，Subtask3，已知前六列信息，预测ROOT_ID和关系。</p><h3 id="2-3-评估方法"><a href="#2-3-评估方法" class="headerlink" title="2.3 评估方法"></a>2.3 评估方法</h3><p>以下是本次比赛三个任务的评测方案。</p><ul><li><p><strong>Subtask 1:</strong> 句子分类官方将对正负样本的准确率，召回率和F1值进行评估，但官方排名只看正样本的F1值。</p></li><li><p><strong>Subtask 2:</strong> 序列标注官方将评估每个标签类别的准确率，召回率和F1值，以及所有类别的整体macro- and micro-averaged F1 。但官方排名只看所有评估类别的macro-averaged F1。被评估的类别包括：Term, Alias-Term, Referential-Term, Definition, Referential-Definition, and Qualifier。</p></li><li><p><strong>Subtask 3:</strong> 关系抽取官方将评估每个关系类别的准确率，召回率和F1值，以及所有关系类别的整体macro- and micro-averaged F1 。但官方排名只看所有评估关系类别的macro-averaged F1。被评估的关系类别包括：Direct-defines, Indirect-defines, Refers-to, AKA, and Supplements。</p><p>也即任务1只看正样本的F1值，任务2，3则以所有类别的Macro-F1为评价指标，其中Macro-F1是指分别计算每个类别的F1，然后做平均（各类别F1的权重相同），<strong>此种评价标准对小样本数据类别的准确率有较高的要求。</strong></p></li></ul><h2 id="3-任务分析"><a href="#3-任务分析" class="headerlink" title="3. 任务分析"></a>3. 任务分析</h2><h3 id="3-1-EDA-Exploratory-Data-Analysis"><a href="#3-1-EDA-Exploratory-Data-Analysis" class="headerlink" title="3.1 EDA (Exploratory Data Analysis)"></a>3.1 EDA (Exploratory Data Analysis)</h3><p>通过统计的方法，分析训练样本句式，各类标签占比。我们发现40%左右的常见定义句式都是Term be Definition和Definition be called Term这两种格式。</p><p>比如：</p><ol><li><p>Term is/are Definition 在definition句子中占比约为27.31%</p><p>示例：The small intestine is the organ where the digestion of protein , fats , and carbohydrates is completed.</p></li><li><p>Definition be called Term 在definition句子中占比约为13.12%</p><p>示例：The process by which capital ages and loses value is called depreciation.</p></li></ol><p><img src="/images/blog/2020/SemEval-2020/3.1.webp" alt></p><p>经过统计发现，包含定义的句子在所有句子里占比23%左右，也即任务1的正样本数量不到负样本的三分之一，这对我们后面模型损失函数的超参数设计有一定指导作用。关系标签里占比最多的是直接定义，占到85-89%的比例，其他的关系标签都属于小样本标签，这些都是后续数据增强重点关注类别。</p><p>另外我们还对句子长度，跨句关联比例，段落句子个数，噪音文本等特征进行了统计。其中两个关联标签分布在相邻两个句子的情况，是本次赛题的难点。</p><p>对train/dev/test统计分析如下：</p><ol><li>train/dev中跨距关联语句占比约为0.02，大部分都是本句关联的</li><li>train/dev/test语句长度约在26个单词左右</li><li>train/dev/test中每个段落大约由4个句子组成</li><li>训练集里段落开头的数字单句要清洗</li></ol><p><img src="/images/blog/2020/SemEval-2020/3.1.2.webp" alt></p><p>针对不同领域中不同标签占比进行统计分析发现，训练集/验证集/测试集在不同领域内的不同标签分布基本相同，同时统计每个领域内每个标签的占比可知，government领域中只有13.16%的数据带有标签，而biology领域中大约有33.79%的数据带有标签，远大于government。且biology领域内Qualifier标签远大于其他领域，占比约为0.73%，其他领域均小于0.2%。这些是符合常理认知的。</p><p>通过统计分析，我们还发现句子之间的一些关联性</p><ul><li>一个句子如果包含定义，则一定含有Definition标签</li><li>各种实体之间的关系都是围绕定义或者实体本身</li><li>实体和定义可能不在同一句话里</li><li>指代定义通常是两个相邻的句子</li></ul><h3 id="3-2-数据增强"><a href="#3-2-数据增强" class="headerlink" title="3.2 数据增强"></a>3.2 数据增强</h3><p>通常来说，英文文本的数据增强可以采用单复数、缩写、上下位词的替换，还可以通过随机增加、删除或者改写来生成新的样本。回译法也是常用的方法，就是先把英文翻译成其他语言再回译成英文。英文里使用nltk，wordnet可以获取单词的同义词。</p><p>这里我们用到了Crafting Text Adversarial Samples的思想：<strong>一个单词有高贡献是指去掉它后文本将被分为当前类的概率大幅减小。</strong>生成扩充语料的一个重要方面就是要保持样本的语义 。</p><p>在我们的任务中定义专有名词不能替换，句子要保持通顺。</p><p>Subtask1可以用回译法，Subtask2只能用同义词替换保持句子单词数一致。下图中新句子里红色的单词是我们经过替换产生的。</p><p><img src="/images/blog/2020/SemEval-2020/3.2.webp" alt></p><h3 id="3-3-定义抽取关系抽取传统方法"><a href="#3-3-定义抽取关系抽取传统方法" class="headerlink" title="3.3 定义抽取关系抽取传统方法"></a>3.3 定义抽取关系抽取传统方法</h3><p>传统的定义抽取方法一般有规则模版方法和基于特征工程的机器学习方法。我们基于观察和统计，会发现很多定义都是有定义连接动词的，但是有定义连接动词的pattern匹配到的也未必是定义。另一方面，用规则模版，特征工程的方法并不能在保持准确率的同时具有良好的泛化性。</p><p><img src="/images/blog/2020/SemEval-2020/3.3.webp" alt></p><p>而传统的关系抽取一般分两步走，先识别出实体，再判断出实体之间的关系。实体识别通常有双指针方法以及深度学习模型+CRF等方法，关系抽取可以转化为分类问题。</p><p><img src="/images/blog/2020/SemEval-2020/3.3.2.webp" alt></p><h2 id="4-模型构建"><a href="#4-模型构建" class="headerlink" title="4. 模型构建"></a>4. 模型构建</h2><h3 id="4-1-多任务学习"><a href="#4-1-多任务学习" class="headerlink" title="4.1 多任务学习"></a>4.1 多任务学习</h3><p>本赛题多个任务存在较强的关联性，Subtask2中如果预测出一些单词为Definition，则对应句子在Subtask1中应属于正样本。为解决传统方法的累积误差，我们引入多任务联合学习框架。多任务学习有以下三个好处：</p><ul><li><strong>隐式数据增强：</strong>有效地增加了我们用于训练模型的样本量。由于所有数据都存在噪声，因此在对某个任务A进行模型训练时，我们的目标是学习任务A的良好表示形式，理想情况下可以忽略与数据相关的噪声并很好地进行概括。由于不同的任务具有不同的噪声模式，仅学习任务A可能会过拟合任务A，而学习A和B则可以使模型通过平均噪声模式获得更好的表示。</li><li><strong>更通用的文本表达：</strong>联合训练模型倾向于学到每个子任务都能学到的表达，这也将有助于该模型将来推广到新任务，因为对于足够多的训练任务而言表现良好的假设空间，只要它们来自同一环境，对于学习新颖任务也将表现良好。</li><li><strong>特征选择双重检验：</strong>如果一个feature对多个任务都非常重要，则这个feature很有可能对通用文本表达非常重要。</li></ul><p>通常来说，多任务学习分为hard parameter sharing和soft parameter sharing, 其中soft parameter sharing每个任务有自己的参数，最后<strong>通过对不同任务的参数之间的差异加约束</strong>，表达相似性。</p><p><img src="/images/blog/2020/SemEval-2020/4.1.webp" alt></p><font color="orange">我们在建模过程中采用hard parameter sharing的方式，定义抽取任务和序列标注任务共享一个Shared Layer用以学习句子单词通用的表达，然后不同的任务各自有自己的Task Specific Layer。</font><p><img src="/images/blog/2020/SemEval-2020/4.1.2.webp" alt></p><p>关于损失函数的设计，我们定义：</p><script type="math/tex; mode=display">S(X, y_{tag}) = \sum_{i=0}^{n} P_{i, y_i} + \sum_{i=0}^{n} A_{y_{i-1},y_i}</script><p>其中$P<em>{i, y_i}$是第i个单词标签为$y_i$的概率，$A</em>{y<em>{i-1},y_i}$是CRF里转移矩阵标签$y</em>{i-1}$到$y_{i}$ 的概率，序列标注的损失函数为最大路径得分除以所有路径得分再取log的负数。 </p><p><img src="/images/blog/2020/SemEval-2020/4.1.3.webp" alt></p><p>分类任务的损失函数为</p><p><img src="/images/blog/2020/SemEval-2020/4.1.4.webp" alt></p><p>其中y是正确标签，是预测概率，我们在交叉熵的基础上增加了两个超参数，<strong>用来控制正负样本的权重和容易分类样本对整体损失贡献的权重</strong>。当样本为正的时候，标签y=1，损失函数第二项为0，alpha的大小控制着正样本的权重，alpha越大，把正样本分错产生的损失也越大。当一个正样本比较容易分类，预测值$p_t$会非常接近1，当gamma越大的时候，容易分类样本贡献的损失也会越小，难分类样本贡献的损失会越大。</p><p><img src="/images/blog/2020/SemEval-2020/4.1.5.webp" alt></p><p>训练过程我们采用先训练Subtask2，再训练Subtask1，交替训练的方法。为解决训练数据不足，<strong>我们引入Pseudo-Label的技巧，但控制数量占比不超过有标签数据的十分之一</strong>。所谓Pseudo-Label，就是先用有标签数据训练一个模型，然后预测无标签数据，将概率较高的无标签数据打上伪标签，然后加入到训练数据中重新训练模型。</p><p><img src="/images/blog/2020/SemEval-2020/4.1.6.webp" alt></p><h3 id="4-2-关系抽取模型"><a href="#4-2-关系抽取模型" class="headerlink" title="4.2 关系抽取模型"></a><font color="orange">4.2 关系抽取模型</font></h3><p>我们提出Enhancement Inference BERT模型对关系进行抽取 。模型输入两个实体，其中词向量取BERT最后四层hidden states的token embedding求平均。计算两个实体之间单词的Attention，然后通过TextB里词向量乘以一个归一化参数得到TextA的交互表达$\widetilde{a}$，同样的方式得到TextB的交互表达$\widetilde{b}$，最后通过差积，点积等操作，经过MLP，Softmax层输出关系label。</p><p><img src="/images/blog/2020/SemEval-2020/4.2.webp" alt></p><p>模型融合方面，我们采用五个预训练模型，五折交叉验证得到25个预测结果，然后采用hard voting的方式输出结果。其中XLM-Roberta是一个多语言训练模型，不需要传入额外的参数来指定当前输入的语言，模型可以通过input_id自己识别。</p><p><img src="/images/blog/2020/SemEval-2020/4.2.1.webp" alt></p><p>模型融合之后，我们会辅助一些实体纠正规则，用来解决一些嵌套NER，一词多tag的情况。</p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><h3 id="5-1-比赛总结"><a href="#5-1-比赛总结" class="headerlink" title="5.1 比赛总结"></a>5.1 比赛总结</h3><p>本次比赛我们使用多任务联合训练框架，对定义抽取和词粒度BIO标注两项任务进行联合建模。提出Enhancement Inference BERT对实体关系进行分类，加上规则辅助对关系抽取任务进行预测。最终在关系抽取任务中获得第一。</p><p>数据集存在多重定义以及长距离的指代关系，同一句话存在不同的实体标注和关系，当前讨论的实体在前文或后文中描述过，需要结合上下文信息才能预测出结果，这些都对准确识别提出了很高的要求。光靠模型很难学习到一句多义的情况。因此本次比赛采用实体规则对模型预测实体结果进行纠正，实验结果如下所示：</p><p><img src="/images/blog/2020/SemEval-2020/5.1.webp" alt></p><h3 id="5-2-NLP竞赛常用解题技巧总结"><a href="#5-2-NLP竞赛常用解题技巧总结" class="headerlink" title="5.2 NLP竞赛常用解题技巧总结"></a>5.2 NLP竞赛常用解题技巧总结</h3><p>拿到赛题，我们首先需要理解题目和评测目标，浏览训练数据格式，分析数据各个类别占比，数据集的大小，会影响 Epoch 的数量，而很多学习率衰减策略是直接与 Epoch 相关的。异常点检测有3σ原则，Isolation Forest等方法 。1-2万的数据量，经验规则预训练模型Epoch 10-20次。</p><p>先保证baseline的合理性，后期再加入数据增强。效果第一，用大模型（中文BERT，ERNIE，NEZHA，英文BERT，Roberta，XLNET等）去拟合小数据，正则化，Batch不用太大。固定随机 seed，始终使用固定的随机 seed 能保证很多属性，例如在我们两次运行相同代码时能得到相同的输出。这能消除变化因子，进行合理的判断。</p><p>构建合理的Local CV至关重要，在test数据集放出来之前，我们通过五折交叉验证拆分训练集和验证集，把dev数据集作为test数据集。本地评估体系是验证新模型，新方案的有效手段。</p><p><img src="/images/blog/2020/SemEval-2020/5.2.webp" alt></p><p>过拟合，典型的表现为训练集损失远远小于验证集损失。除了常规的正则化，early stop等方法外，这里介绍一种<strong>Adversarial Training</strong>的方法，一般用于test数据集发布之后，训练一个分类器用于区分训练集和测试集，选取最像测试数据的前k个训练数据作为最终模型的验证数据。</p><p>模型输入也有许多优化方法，BPE（Byte-Pair Encoding）是其中一种，这种方法可以扩大词汇表，使unknown出现的少。对于字向量与词向量如果要采用相加的形式组合在一起，可以把词向量重复n次，n表示的是当前词有多少个字。</p><p>Ensemble里单模型的Diversity越大，最终Model的Bias就越低。在实际中很有可能表现相近的Model只有寥寥几个而且它们之间相关性还不低，但是实践告诉我们即使在这种情况下Ensemble还是能大幅提高成绩。</p><p><img src="/images/blog/2020/SemEval-2020/5.2.1.webp" alt></p><p>打 NLP 比赛像是搜索解空间，不断寻找下一个更优解，直到比赛结束。这样看来，快速构建一个 baseline，建立可靠的 Local CV 系统，然后不断尝试并验证新想法，收获好成绩似乎也有迹可循。</p><h2 id="6-应用价值"><a href="#6-应用价值" class="headerlink" title="6. 应用价值"></a><font color="grey">6. 应用价值</font></h2><p>此次参赛所应用到的创新技术，在实际业务场景中，可支持保险信息抽取、保险实体识别、文本挖掘等技术应用，对搭建寿险垂直领域的知识图谱起到重要推动作用，能大幅提升对话式机器人的响应效率和服务体验。</p><p>目前对话式机器人作为平安人寿智能转型的利器之一，在代理人赋能和客户服务两大业务体系中已大规模落地，覆盖招聘、培训、销售支持、客服等业务场景，并将持续发挥价值。</p>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Relation Extraction </tag>
            
            <tag> 关系抽取 </tag>
            
            <tag> SemEval </tag>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>局部敏感哈希(LSH)</title>
      <link href="/2020/09/19/%E5%B1%80%E9%83%A8%E6%95%8F%E6%84%9F%E5%93%88%E5%B8%8C-LSH/"/>
      <url>/2020/09/19/%E5%B1%80%E9%83%A8%E6%95%8F%E6%84%9F%E5%93%88%E5%B8%8C-LSH/</url>
      
        <content type="html"><![CDATA[<h2 id="1-引入"><a href="#1-引入" class="headerlink" title="1. 引入"></a>1. 引入</h2><p>​        在做微博文本挖掘的时候，会发现很多微博是高度相似的，因为大量的微博都是转发其他人的微博，并且没有添加评论，导致很多数据是重复或者高度相似的。这给我们进行数据处理带来很大的困扰，我们得想办法把找出这些相似的微博，再对其进行去重处理。</p><p>　　如果只是要找到重复的微博，我们可以用两两比较所有的微博，对相同的微博值保留一条即可；但这只能在数据量很小的情况下才有可能，当我们有1000万条微博时，需要两两比较的微博有10^6亿（n*(n-1)/2）对，这个计算量是惊人的，即便你用map-reduce，拥有强大的集群，那也顶不住数据再增加一两个数量级。一种稍微好一点的办法是对所以微博进行一次hash，再对桶内的微博进行比较，利用hash可以过滤了绝大多数不相同的微博，避免了无谓的比较，时间复杂度O(n)，显著提高效率。Hash的一个基本特性就是随机性，它将一个字符串随机的hash到一个桶中，对于相同的两个字符串，hash值总是相同的，但两个hash值只要相差一点，hash值就就可能大相径庭，可谓差之毫厘谬以千里：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; s1 = <span class="string">"我是一个字符串"</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; s2 = <span class="string">"我是一个字符串"</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">hash</span>(s1)</span></span><br><span class="line">2006838971</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">hash</span>(s2)</span></span><br><span class="line">2006838971</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; s3 = <span class="string">"我就是一个字符串"</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">hash</span>(s3)</span></span><br><span class="line">-1823451294</span><br></pre></td></tr></table></figure><p>　    如果能有一种hash算法，能将相似的字符串hash得到相似的hash值，那就能在接近线性的时间解决海量微博中的去重问题。幸好，这样的hash已经有大牛发明了，它叫局部敏感哈希（Locality Sensitive Hashing，简称LSH）。接下来我们通过海量微博文本相似项发现的例子来探讨这个神奇的hash。<a id="more"></a></p><h2 id="2-文本相似度计算"><a href="#2-文本相似度计算" class="headerlink" title="2. 文本相似度计算"></a>2. 文本相似度计算</h2><p>​        按照文本处理的术语，我们认为一条微博是一篇文档，度量两篇文档相似度有多种方法，有欧式距离、编辑距离、余弦距离、Jaccard距离，我们这里使用Jaccard距离。这里注意一下，距离和相似度是不同的概念，距离越近相似度应该越高，距离越远相似度应该越低，因此similar = 1-distace。</p><p>　　集合S和T的Jaccard相识度：</p><script type="math/tex; mode=display">SIM(S, T) = \frac{|S \cap T|}{ |S \cup T|}</script><p>​        不考虑微博中重复出现的词时，一条微博就可以看成一个集合，集合的元素是一个个的词：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s1 = '''从 决心 减肥 的 这 一刻 起 请 做 如下 小 改变 你 做 得 到 么'''</span><br><span class="line">s2 = '''从 决心 减肥 的 这 一刻 起 请 做 如下 小 改变'''</span><br></pre></td></tr></table></figure><p>​        sim(s1,s2)=11/16=0.69.</p><h2 id="3-文档的Shingling"><a href="#3-文档的Shingling" class="headerlink" title="3. 文档的Shingling"></a>3. 文档的Shingling</h2><p>​        为了字面上相似的文档，将文档表示成集合最有效的方法是构建文档中的短字符串集合，一个常用的方法时Shingling（不知道翻译成什么，囧），看定义很简单的：文档的k-shingle定义为其中长度为k的子串。对于上面的字符串s2，选择k=2，这s2中的所有2-single组成的集合为:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;"从 决心","决心 减肥","减肥 的","的 这","这 一刻","一刻 起","起 请","请 做","做 如下","如下 小","小 改变"&#125;</span><br></pre></td></tr></table></figure><p>　    k值的选取具有一定的技巧，k越大越能找到真正相似的文档，而k越小就能召回更多的文档，但他们可能相似度不高，比如k=1，就变成了基本词的比较了。我这里词作为shingle的基本单位，在英文处理中，是以字母为基本单位，原因在于汉字有上万个，而英文字母只有27个，以汉字为单位将造成shingle集合巨大。</p><p>　　遍历所用文档，就得到了shingle全集。</p><h2 id="4-保持相似度矩阵表示"><a href="#4-保持相似度矩阵表示" class="headerlink" title="4. 保持相似度矩阵表示"></a>4. 保持相似度矩阵表示</h2><p>1、集合的矩阵表示</p><p>　　假设我们有这样4篇文档（分词后）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">s1 = "我 减肥"</span><br><span class="line">s2 = "要"</span><br><span class="line">s3 = "他 减肥 成功"</span><br><span class="line">s4 = "我 要 减肥"</span><br></pre></td></tr></table></figure><p>​        为方便叙述，我们取k=1，这时shingle全集为{我，他，要，减肥，成功}，将文档表示成特征矩阵，行代表shingle元素，列代表文档，只有文档j出现元素i时，矩阵M[i][j]=1，否则M[i][j] = 0.</p><p><img src="/images/blog/2020/hash1.PNG" alt></p><p>​        实际上，真正计算的过程中矩阵不是这样表示的，因为数据很稀疏。得到矩阵表示后，我们来看最小hash的定义。</p><h2 id="5-最小哈希（min-hashing"><a href="#5-最小哈希（min-hashing" class="headerlink" title="5. 最小哈希（min-hashing)"></a>5. 最小哈希（min-hashing)</h2><p>​        最小hash定义为：特征矩阵按行进行一个随机的排列后，第一个列值为1的行的行号。举例说明如下，假设之前的特征矩阵按行进行的一个随机排列如下：</p><p><img src="/images/blog/2020/hash2.PNG" alt></p><p>最小哈希值：h(S1)=3，h(S2)=5，h(S3)=1，h(S4)=2.</p><p>　　为什么定义最小hash？事实上，两列的最小hash值就是这两列的Jaccard相似度的一个估计，换句话说，两列最小hash值同等的概率与其相似度相等，即P(h(Si)=h(Sj)) = sim(Si,Sj)。为什么会相等？我们考虑Si和Sj这两列，它们所在的行的所有可能结果可以分成如下三类：</p><p>　　（1）A类：两列的值都为1；</p><p>　　（2）B类：其中一列的值为0，另一列的值为1；</p><p>　　（3）C类：两列的值都为0.</p><p>　　特征矩阵相当稀疏，导致大部分的行都属于C类，但只有A、B类行的决定sim(Si,Sj)，假定A类行有a个，B类行有b个，那么sim(si,sj)=a/(a+b)。现在我们只需要证明对矩阵行进行随机排列，两个的最小hash值相等的概率P(h(Si)=h(Sj))=a/(a+b)，如果我们把C类行都删掉，那么第一行不是A类行就是B类行，如果第一行是A类行那么h(Si)=h(Sj)，因此P(h(Si)=h(Sj))=P(删掉C类行后，第一行为A类)=A类行的数目/所有行的数目=a/(a+b)，这就是最小hash的神奇之处。</p><h2 id="6-最小hash签名"><a href="#6-最小hash签名" class="headerlink" title="6. 最小hash签名"></a>6. 最小hash签名</h2><p>​        有了最小hash还不够，一次最小hash只是一次独立的随即事件，中心极限定理告诉我们，只有多次重复随机事件才能造就必然。选择n个随机排列作用于特征矩阵，得到n个最小hash值，h1,h2,…,hn，这n个最小hash值组成一个n维向量，即为最小签名，两列的最小签名的相似度即为两列的Jaccard相似度的估计。</p><p>　　现在就看如何计算最小签名了。对一个很大的矩阵按行进行随机排列，首先需要得到一个随机排列，然后按这个随机排列去排序，这将耗费很长的时间。我们需要一种方法来模拟随机排列的过程。又有一个神奇的办法是：找一个哈希函数h，将第r行放在排列转换后的第h(r)的位置上。我们不再是选择n个随机排列，而是选择n个hash，h1,h2,…,hn作用于行，这样我们就可以得到签名矩阵。令SIG(i,c)为第i个哈希函数在第c列上的元素。一开始对所有的SIG(i,c)初始化为无穷大，然后从上往下对列c进行处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">　　<span class="keyword">if</span> 特征矩阵中的c列r行值为<span class="number">1</span>:</span><br><span class="line">　　　　<span class="keyword">for</span> i <span class="keyword">in</span> xrange(n):<span class="comment">#对每个hash函数</span></span><br><span class="line">　　　　SIG(i,c)=min(SIG(i,C),hi(r))</span><br><span class="line">　　<span class="keyword">else</span>:</span><br><span class="line">　　　　<span class="keyword">continue</span></span><br></pre></td></tr></table></figure><p>　　按上面的方法处理每一列，即得到最小签名矩阵。</p><h2 id="7-基于最小hash的局部敏感哈希"><a href="#7-基于最小hash的局部敏感哈希" class="headerlink" title="7. 基于最小hash的局部敏感哈希"></a>7. 基于最小hash的局部敏感哈希</h2><p>​        前面我们辛辛苦苦的工作貌似只是将一个文档的集合转换为一个最小签名，虽然这个签名大大压缩了集合的空间，但要计算两列的相似度还是需要两两比较签名矩阵两列的相似度，如果有n篇文档，两个比较的次数是n*(n-1)/2。接下来就看局部敏感hash怎么个敏感法。</p><p>　　我们对签名矩阵按行进行分组，将矩阵分成b组，每组由r行组成，下面的实列将一个签名矩阵分成6组，每组由3行组成。 </p><div class="table-container"><table><thead><tr><th>组1</th><th>…</th><th>1 0 0 0 2<br>3 2 1 2 2<br>0 1 3 1 1</th><th>…</th></tr></thead><tbody><tr><td>组2</td><td></td><td></td><td></td></tr><tr><td>组3</td><td></td><td></td><td></td></tr><tr><td>组4</td><td></td><td></td><td></td></tr><tr><td>组5</td><td></td><td></td><td></td></tr><tr><td>组6</td><td></td><td></td></tr></tbody></table></div><p>​    分组之后，我们对最小签名向量的每一组进行hash，各个组设置不同的桶空间。只要两列有一组的最小签名部分相同，那么这两列就会hash到同一个桶而成为候选相似项。签名的分析我们知道，对于某个具体的行，两个签名相同的概率p =两列的相似度= sim(S1,S2)，然后：</p><p>　　（1）在某个组中所有行的两个签名值都相等概率是$p^r$;</p><p>　　（2）在某个组中至少有一对签名不相等的概率是$1-p^r$;</p><p>　　（3）在每一组中至少有一对签名不相等的概率是$(1-p^r)^b$;</p><p>　　（4）至少有一个组的所有对的签名相等的概率是$1 - (1-p^r)^b$;</p><p>　　于是两列成为候选相似对的概率是$1 - (1-p^r)^b$，它曲线如下：</p><p><img src="/images/blog/2020/hash3.PNG" alt></p><p>​        当两篇文档的相似度为0.8时，它们hash到同一个桶而成为候选对的概率是0.9996，而当它们的相似度只有0.3时，它们成为候选对的概率只有0.0475，因此局部敏感hash解决了让相似的对以较高的概率hash到同一个桶，而不相似的项hash到不同的桶的问题。</p><p>　　对于每个桶内的文档，会有一部分是不相似的，因为hash有冲突，需要进行桶内检验。基于最小签名的LSH很容易转化为Map-reduce来计算。</p><p>　　下面是我对微博进行局部敏感hash的一个结果（分词并删掉标点）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1. "外表 活泼 内心 孤僻 的 人 会 做 的 事 1 手机 不 离 身 2 对待 不同 的 人 有 不同 的 性格 3 从 小 懂得 很多 道理 4 有 时候 很 神经 有时候 很 镇静 5 会 因为 别人 一 句 话 伤心 但 不 会 被 发现 6 安慰 很多 人 但 自己 却 没 人 安慰 7 会 怀念 从前 讨厌 现在 8 有时候 会 笑 的 没 心 没 肺 有 时 却 很 沉默"</span><br><span class="line">2. "外表 活泼 内心 孤僻 的 人 会 做 的 事 1. 手机 不 离 身 2. 对待 不同 的 人 有 不同 的 性格 3. 从小 懂得 很多 道理 4. 有时候 很 神经 有时候 很 镇静 5. 会 因为 别人 一 句 话 伤心 但 不 会 被 发现 6. 安慰 很多 人 但 自己 却 没 人 安慰 7. 会 怀念 从前 讨厌 现在 8. 有时候 会 笑 的 没 心 没 肺 有时 却 很 沉默"</span><br><span class="line">3. "射手 座 的 特征 1 手机 不 离 身 2 对待 不同 的 人 有 不同 的 性格 3 从 小 懂得 很多 道理 4 有 时候 很 神经 有时候 很 镇静 5 会 因为 别人 一 句 话 伤心 但 不 会 被 发现 6 安慰 很多 人 但 自己 却 没 人 安慰"</span><br><span class="line">4. "外表 活泼 内心 孤僻 的 人 会 做 的 事 1 手机 不 离 身 2 对待 不同 的 人 有 不同 的 性格 3 从 小 懂得 很多 道理 4 有 时候 很 神经 有时候 很 镇静 5 会 因为 别人 一 句 话 伤心 但 不 会 被 发现 6 安慰 很多 人 但 自己 却 没 人 安慰 7 会 怀念 从前 讨厌 现在 8 有时候 会 笑 的 没 心 没 肺 有 时 却 很 沉默 你 是 这样 吗"</span><br><span class="line">5. "金牛座 特征 1 手机 不 离 身 2 对待 不同 的 人 有 不同 的 性格 3 从 小 懂得 很多 道理 4 有 时候 很 神经 有时候 很 镇静 5 会 因为 别人 一 句 话 伤心 但 不 会 被 发现 6 安慰 很多 人 但 自己 却 没 人 安慰 会 怀念 从前 讨厌 现在 8 有时候 会 笑 的 没 心 没 肺 有时 却 很 沉默"</span><br><span class="line">6. "外表 活泼 内心 孤僻 的 人 会 做 的 事 1 手机 不 离 身 2 对待 不同 的 人 有 不同 的 性格 3 从 小 懂得 很多 道理 4 有 时候 很 神经 有时候 很 镇静 5 会 因为 别人 一 句 话 伤心 但 不 会 被 发现 6 安慰 很多 人 但 自己 却 没 人 安慰 7 会 怀念 从前 讨厌 现在 8 有时候 会 笑 的 没 心 没 肺 有 时 却 很 沉默 你 是 这样 吗"</span><br><span class="line">7. "天蝎座 特征 1 手机 不 离 身 2 对待 不同 的 人 有 不同 的 性格 3 从 小 懂得 很多 道理 4 有 时候 很 神经 有时候 很 镇静 5 会 因为 别人 一 句 话 伤心 但 不 会 被 发现 6 安慰 很多 人 但 自己 却 没 人 安慰 7 会 怀念 从前 讨厌 现在 8 有时候 会 笑 的 没 心 没 肺 有时 却 很 沉默"</span><br><span class="line">8. "双子座 的 你 是 这样 的 吗 1 手机 不 离 身 睡觉 不 关机 2 对待 不同 的 人 有 不同 的 性格 3 从 小 懂得 很多 道理 但 知 行 往往 难以 合 一 4 有 时候 很 神经 有时候 很 镇静 5 会 因为 别人 一 句 话 伤心 但 不 会 被 发现 6 很 会 安慰 别人 却 不 会 安慰 自己 7 会 经常 怀念 从 前"</span><br><span class="line">9. "外表 活泼 内心 孤僻 的 人 会 做 的 事 1 手机 不 离 身 2 对待 不同 的 人 有 不同 的 性格 3 从 小 懂得 很多 道理 4 有 时候 很 神经 有时候 很 镇静 5 会 因为 别人 一 句 话 伤心 但 不 会 被 发现 6 安慰 很多 人 但 自己 却 没 人 安慰 7 会 怀念 从前 讨厌 现在 8 有时候 会 笑 的 没 心 没 肺 有时 却 很 沉默 你 是 这样 吗"</span><br><span class="line">10. "外表 活泼 内心 孤僻 的 人 会 做 的 事 1 手机 不 离 身 2 对待 不同 的 人 有 不同 的 性格 3 从 小 懂得 很多 道理 4 有 时候 很 神经 有时候 很 镇静 5 会 因为 别人 一 句 话 伤心 但 不 会 被 发现 6 安慰 很多 人 但 自己 却 没 人 安慰 7 会 怀念 从前 讨厌 现在 8 有时候 会 笑 的 没 心 没 肺 有 时 却 很 沉默 你 是 这样 吗 转"</span><br><span class="line">11. "外表 活泼 内心 孤僻 的 人 会 做 的 事 1. 手机 不 离 身 2. 对待 不同 的 人 有 不同 的 性格 3. 从小 懂得 很多 道理 4. 有时候 很 神经 有时候 很 镇静 5. 会 因为 别人 一 句 话 伤心 但 不 会 被 发现 6. 安慰 很多 人 但 自己 却 没 人 安慰 7. 会 怀念 从前 讨厌 现在 8. 有时候 会 笑 的 没 心 没 肺 有时 却 很 沉默"</span><br></pre></td></tr></table></figure><p>​        看了这个，你还相信星座吗？</p><h2 id="8-局部敏感哈希的一般定义"><a href="#8-局部敏感哈希的一般定义" class="headerlink" title="8. 局部敏感哈希的一般定义"></a>8. 局部敏感哈希的一般定义</h2><p>　　局部敏感hash实质上是满足一定条件的函数簇，上面介绍只是一个基于Jaccard的例子，实际上还有面向其他距离的LSH。</p><p>　　令d1&lt;d2是定义在距离测定d下得两个距离值，如果一个函数族的每一个函数f满足：</p><p>　　（1）如果d(x,y)&lt;=d1,则f(x)=f(y)的概率至少为p1，即P(f(x)=f(y)) &gt;= p1;</p><p>　　（2）如果d(x,y)&gt;=d2,则f(x)=f(y)的概率至多为p2，即p(f(x)=f(y)) &lt;= p2.</p><p>　　那么称F为(d1,d2,p1,p2)-敏感的函数族。实际上我们之前的最小hash函数族是(d1,d2,1-d1,1-d2)-敏感的。</p><p>　　局部敏感hash族还可以进行放大处理，已获得更高的准确率和召回率，当然也有面向其他距离的LSH。本文的东西全部源自参考文献的课本，有兴趣可以好好读一下这本书。</p><h2 id="9-参考文献"><a href="#9-参考文献" class="headerlink" title="9. 参考文献"></a>9. 参考文献</h2><blockquote><p>本文来自<a href="http://www.cnblogs.com/fengfenggirl" target="_blank" rel="noopener">http://www.cnblogs.com/fengfenggirl</a></p></blockquote><p>参考资料：</p><p><a href="http://www.stanford.edu/class/cs246/" target="_blank" rel="noopener">《Mining of Massive Dataset》</a></p><p>《互联网大规模数据挖掘与分布式处理》</p>]]></content>
      
      
      <categories>
          
          <category> Data Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> 局部敏感哈希 </tag>
            
            <tag> Local Sensitive Hashing </tag>
            
            <tag> LSH </tag>
            
            <tag> Data Mining </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Freebase Search Cookbook</title>
      <link href="/2020/09/15/Freebase-Search-Cookbook/"/>
      <url>/2020/09/15/Freebase-Search-Cookbook/</url>
      
        <content type="html"><![CDATA[<blockquote><p>内容来自于：<a href="https://developers.google.com/freebase/v1/search-cookbook" target="_blank" rel="noopener">https://developers.google.com/freebase/v1/search-cookbook</a></p></blockquote><h1 id="Search-Cookbook"><a href="#Search-Cookbook" class="headerlink" title="Search Cookbook"></a>Search Cookbook</h1><p>This page contains a list of recipes for different ways to constrain search queries using the <a href="https://developers.google.com/freebase/v1/search-overview" target="_blank" rel="noopener">Search Service</a>.</p><h2 id="Textual-constraints"><a href="#Textual-constraints" class="headerlink" title="Textual constraints"></a>Textual constraints</h2><p>Textual data for a Freebase entity comes first from its name and its aliases, then from its keys and other textual properties, and finally from its Wikipedia anchor data if it was reconciled with a language-specific Wikipedia topic.</p><p>Textual constraints are language-specific; currently, 18 languages are supported. English has by far the most coverage and is the default language.</p><p>For a list of all currently supported language codes, visit the following:</p><p><a href="https://www.googleapis.com/freebase/v1/search?help=langs&amp;indent=true" target="_blank" rel="noopener">https://www.googleapis.com/freebase/v1/search?help=langs&amp;indent=true</a></p><p>A textual constraint is specified with the <code>query</code> parameter. Its language is specified with the <code>lang</code> parameter. For example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;gore&quot;</span><br><span class="line">query: &quot;gore&quot; lang: &quot;fr&quot;</span><br><span class="line">query: &quot;gore&quot; lang: &quot;de&quot;</span><br></pre></td></tr></table></figure><p>During indexing, <strong>textual data is normalized</strong> in a language-specific way. For example, in English, text is converted to lowercase and accents are removed. At query time, the same language-specific normalization is performed on the query text. For example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;beyoncé&quot;</span><br><span class="line">query: &quot;beyonce&quot;</span><br></pre></td></tr></table></figure><p>Several parameters control how a textual constraint is matched. Setting the <code>prefixed</code> parameter to <code>true</code> triggers a prefix match on name and aliases only (and a regular match on other textual data). For example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;bob dy&quot; prefixed: true</span><br></pre></td></tr></table></figure><p>Setting <code>stemmed</code> to <code>true</code> triggers a stemmed match on name and aliases only (and a regular match on other textual data). Stemmed matches may be used to paste over language-specific suffix differences introduced by plurals or other grammatical forms. For example:<a id="more"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;potatos&quot; stemmed: true</span><br></pre></td></tr></table></figure><p>Surrounding the query text with double quotation marks (“”) triggers a phrase match. The text tokens in the query must appear next to each other in the matching entity’s textual data. For example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;\&quot;to be or not to be\&quot;&quot;</span><br></pre></td></tr></table></figure><p>Use the <code>name</code>, <code>alias</code>, and <code>type</code> parameters to match against people and person entities whose name contains the word you want to match. For example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter: &quot;(all name:gore alias:gore type:/people/person)&quot;</span><br></pre></td></tr></table></figure><p>Similarly, the following example matches <code>/people/person</code> entities with a matching <code>alias</code> only and <em>not</em> their <code>name</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter: &quot;(all (not name:gore) alias:gore type:/people/person)&quot;</span><br></pre></td></tr></table></figure><p>In addition to specifying what text fields should be matched, it is also possible to specify how the match should occur by inserting one of the following modifiers between the operand and the text field:</p><ul><li><code>{word}</code>: require that the words in the string match words in the corresponding text field in the document. <em>(default)</em></li><li><code>{phrase}</code>: require that the words occur next to each other in the same order in the corresponding text field in the document.</li><li><code>{full}</code>: like <code>{phrase}</code> but also require that the phrase completely match the text field, not just within the text field. In other words, a full match.</li></ul><p>For example, to find the musical single called Home by Marc Broussard, use a filter like the following:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter: &quot;(all type:/music/single name&#123;full&#125;:home /music/track/artist:&quot;Marc Broussard&quot;)&quot;</span><br></pre></td></tr></table></figure><h2 id="Language-constraints"><a href="#Language-constraints" class="headerlink" title="Language constraints"></a>Language constraints</h2><p>As described with textual constraints, the <code>lang</code> parameter is used to specify what language normalization rules to use to transform text into query tokens. The language of the query also conditions result ranking as freebase-search gets a language-specific relevance signal from the corresponding language Wikipedia.</p><p>Currently, nine languages are supported: English (en), Spanish (es), French (fr), German (de), Italian (it), Portuguese (pt), Chinese (zh), Japanese (ja) and Korean (ko). English has by far the most coverage and is the default language.</p><h3 id="Searching-with-multiple-languages-at-the-same-time"><a href="#Searching-with-multiple-languages-at-the-same-time" class="headerlink" title="Searching with multiple languages at the same time"></a>Searching with multiple languages at the same time</h3><p>The <code>lang</code> parameter accepts a comma-separated list of language codes that cause the search to be done in all the languages specified and the results to be ranked in the first language listed and displayed in the first language of the list that has a name for the entity.</p><p>For example:</p><p>The following searches for the German word “Sonnenblume” in German and French. It ranks and displays the results in French:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;Sonnenblume&quot; lang: &quot;fr,de&quot;</span><br></pre></td></tr></table></figure><p>The following searches in English for movies whose language is Korean and displays their Korean name. The English part of the query is the word “korean” in the <code>expressed_by</code> constraint:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter: &quot;(all expressed_by:korean type:/film/film)&quot; lang: &quot;ko,en&quot;</span><br></pre></td></tr></table></figure><h2 id="Schema-constraints"><a href="#Schema-constraints" class="headerlink" title="Schema constraints"></a>Schema constraints</h2><p>Schema constraints are specified with the <code>type</code> and the <code>domain</code> parameters. <code>type</code> corresponds to the <code>/type/object/type</code> property values of an entity.</p><p>For example, the following restricts a search to people only:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;gore&quot; type: &quot;/people/person&quot;</span><br></pre></td></tr></table></figure><p><code>domain</code> corresponds to the <code>/type/type/domain</code> values of all <code>/type/object/type</code> values of an entity.</p><p>For example, the following restricts a search to entities in French in the <code>/film</code> domain only:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;babar&quot; domain: &quot;/film&quot; lang: &quot;fr&quot;</span><br></pre></td></tr></table></figure><p>You can also use individual Freebase properties to filter a query. For example, the following restricts a search to people who are from Canada:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query &quot;john&quot; filter: &quot;(all type:/people/person /people/person/nationality:&quot;Canada&quot;)&quot;</span><br></pre></td></tr></table></figure><h2 id="Metaschema-constraints"><a href="#Metaschema-constraints" class="headerlink" title="Metaschema constraints"></a>Metaschema constraints</h2><p><a href="https://developers.google.com/freebase/v1/search-metaschema" target="_blank" rel="noopener">Metaschema</a> constraints filter entities by semantic predicates. These predicates are higher level concepts built from collections of Freebase properties describing similar semantic relationships.</p><p>Metaschema constraints are specified using the <code>filter</code> parameter operands combined with an entity name or MID constraint.</p><h3 id="Supported-Metaschema-filter-operands"><a href="#Supported-Metaschema-filter-operands" class="headerlink" title="Supported Metaschema filter operands"></a>Supported Metaschema filter operands</h3><p>The following Metaschema filter operands are supported by Freebase Search. Try out each example by clicking on the link in the table, or by using the <a href="http://api-examples.freebaseapps.com/search" target="_blank" rel="noopener">Freebase Search Example App</a>.</p><div class="table-container"><table><thead><tr><th style="text-align:left">Operand Name</th><th style="text-align:left">Example(s)</th></tr></thead><tbody><tr><td style="text-align:left"><code>abstraction</code></td><td style="text-align:left"><em>“fettuccine dishes”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;filter=(all+abstraction%3Afettuccine" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all abstraction:fettuccine)&quot;</code></td></tr><tr><td style="text-align:left"><code>abstraction_of</code></td><td style="text-align:left"><em>“class of the Western Bulwark locomotive”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+abstraction_of%3A&quot;Western+Bulwark&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all abstraction_of:&quot;Western Bulwark&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>adaptation</code></td><td style="text-align:left"><em>“Works La Traviata is an adaptation of”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+adaptation_of%3A&quot;La+Traviata&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all adaptation_of:&quot;La Traviata&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>administered_by</code></td><td style="text-align:left"><em>“Cannes awards”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Aawards+administered_by%3Acannes" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:awards administered_by:cannes)&quot;</code></td></tr><tr><td style="text-align:left"><code>administers</code></td><td style="text-align:left"><em>“Who runs the Synapse newspaper?”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+administers%3Asynapse" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all administers:synapse)&quot;</code></td></tr><tr><td style="text-align:left"><code>appears_in</code></td><td style="text-align:left"><em>“characters in the Magic Flute”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+appears_in%3A&quot;magic+flute&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all appears_in:&quot;magic flute&quot;)&quot;</code><em>“Figuren in der Zauberflöte”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+appears_in%3A&quot;Die+Zauberflöte&quot;" target="_blank" rel="noopener">Try it.</a>“+lang%3A+”de”&amp;indent=true))<code>filter: &quot;(all appears_in:&quot;Die Zauberflöte&quot;)&quot; lang: &quot;de&quot;</code></td></tr><tr><td style="text-align:left"><code>broader_than</code></td><td style="text-align:left"><em>“line of aircraft that the Airbus 319 belongs to”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+broader_than%3A&quot;Airbus+A319&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all broader_than:&quot;Airbus A319&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>category</code></td><td style="text-align:left"><em>“french actresses”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+category%3Afemale+origin%3Afrance+notable%3Aactor" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all category:female origin:france notable:actor)&quot;</code><em>“french actresses” (variant)</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+category%3Afemale+origin%3Afrance+practitioner_of%3Aactor" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all category:female origin:france practitioner_of:actor)&quot;</code><em>“california or french volcanos”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+category%3Avolcano+(any+part_of%3Acalifornia+part_of%3Afrance" target="_blank" rel="noopener">Try it.</a>)&amp;indent=true))<code>filter: &quot;(all category:volcano (any part_of:california part_of:france))&quot;</code><em>“pasta dishes”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+category%3Apasta" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all category:pasta)&quot;</code></td></tr><tr><td style="text-align:left"><code>center</code></td><td style="text-align:left"><em>“airlines with a hub in San Francisco”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Aairline+center%3A&quot;San+Francisco&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:airline center:&quot;San Francisco&quot;)&quot;</code><em>“airlines with hubs in San Francisco and Atlanta”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Aairline+center%3A&quot;San+Francisco&quot;+center%3Aatlanta" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:airline center:&quot;San Francisco&quot; center:atlanta)&quot;</code><em>“newspapers centered in San Francisco”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3A%2Fbook%2Fnewspaper+center%3A%2Fm%2F0d6lp" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:/book/newspaper center:/m/0d6lp)&quot;</code></td></tr><tr><td style="text-align:left"><code>center_for</code></td><td style="text-align:left"><em>“sports facilities for the San Francisco 49ers”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+center_for%3A&quot;san+francisco+49ers&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all center_for:&quot;san francisco 49ers&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>certification</code></td><td style="text-align:left"><em>“R-rated movies by Wim Wenders”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3A%2Ffilm%2Ffilm+contributor%3Awenders+certification%3Ar" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:/film/film contributor:wenders certification:r)&quot;</code></td></tr><tr><td style="text-align:left"><code>character</code></td><td style="text-align:left"><em>“works which have Papageno as character”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+character%3Apapageno" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all character:papageno)&quot;</code></td></tr><tr><td style="text-align:left"><code>child</code></td><td style="text-align:left"><em>“parents of Bill Clinton”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+child%3A&quot;bill+clinton&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all child:&quot;bill clinton&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>contributed_to</code></td><td style="text-align:left"><em>“Who contributed to Blade Runner?”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+contributed_to%3A&quot;Blade+Runner&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all contributed_to:&quot;Blade Runner&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>contributor</code></td><td style="text-align:left"><em>“movies by Steven Spielberg”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3A%2Ffilm%2Ffilm+contributor%3A&quot;Steven+Spielberg&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:/film/film contributor:&quot;Steven Spielberg&quot;)&quot;</code><em>“movies by Steven Spielberg”</em> using the MID instead of the contributor’s name. (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3A%2Ffilm%2Ffilm+contributor%3A%2Fm%2F06pj8" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:/film/film contributor:/m/06pj8)&quot;</code><em>“movies with Harrison Ford”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3A%2Ffilm%2Ffilm+contributor%3A&quot;Harrison+Ford&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:/film/film contributor:&quot;Harrison Ford&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>created</code></td><td style="text-align:left"><em>“who created ‘for whom the bell tolls’”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+created%3A&quot;for+whom+the+bell+tolls&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all created:&quot;for whom the bell tolls&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>created_by</code></td><td style="text-align:left"><em>“software by Google”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+notable%3Asoftware+created_by%3Agoogle" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all notable:software created_by:google)&quot;</code></td></tr><tr><td style="text-align:left"><code>discovered</code></td><td style="text-align:left"><em>“discoverers of radium”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+discovered%3Aradium" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all discovered:radium)&quot;</code></td></tr><tr><td style="text-align:left"><code>discovered_by</code></td><td style="text-align:left"><em>“discoveries by Curie”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+discovered_by%3Acurie" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all discovered_by:curie)&quot;</code></td></tr><tr><td style="text-align:left"><code>distributed_by</code></td><td style="text-align:left"><em>“NPR shows”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Ashow+distributed_by%3Anpr" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:show distributed_by:npr)&quot;</code></td></tr><tr><td style="text-align:left"><code>exhibited</code></td><td style="text-align:left"><em>“where was ‘down by law’ presented ?”</em> (<a href="https://developers.google.com/freebase/v1/&amp;indent=true" target="_blank" rel="noopener">Try it.</a>)<code>filter: &quot;(all exhibited:&quot;down by law&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>exhibited_at</code></td><td style="text-align:left"><em>“nominated works shown at the 2010 Cannes Film Festival”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3A&quot;nominated+work&quot;+exhibited_at%3A&quot;2010+Cannes+Film+festival&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:&quot;nominated work&quot; exhibited_at:&quot;2010 Cannes Film festival&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>expressed_by</code></td><td style="text-align:left"><em>“books in esperanto”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Abook+expressed_by%3Aesperanto" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:book expressed_by:esperanto)&quot;</code></td></tr><tr><td style="text-align:left"><code>fictional_link</code></td><td style="text-align:left"><em>“fictional characters related to Mickey Mouse”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3A%2Ffictional_universe%2Ffictional_character+fiction_link%3A&quot;mickey+mouse&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:/fictional_universe/fictional_character fiction_link:&quot;mickey mouse&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>genre</code></td><td style="text-align:left"><em>“gothic cathedrals”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+category%3Acathedral+genre%3Agothic" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all category:cathedral genre:gothic)&quot;</code><em>“gothic cathedrals by Viollet-le-duc”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+category%3Acathedral+genre%3Agothic+created_by%3Aviollet" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all category:cathedral genre:gothic created_by:viollet)&quot;</code></td></tr><tr><td style="text-align:left"><code>identifies</code></td><td style="text-align:left"><em>“What identifies Southwest Airlines?”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+identifies%3A&quot;Southwest+Airlines&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all identifies:&quot;Southwest Airlines&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>leader</code></td><td style="text-align:left"><em>“Mitch Kapor companies”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Acompany+leader%3Akapor" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:company leader:kapor)&quot;</code></td></tr><tr><td style="text-align:left"><code>leader_of</code></td><td style="text-align:left"><em>“Paris mayors”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+title%3Amayor+leader_of%3Aparis" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all title:mayor leader_of:paris)&quot;</code></td></tr><tr><td style="text-align:left"><code>made_of</code></td><td style="text-align:left"><em>“wax paintings”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Apainting+made_of%3Awax" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:painting made_of:wax)&quot;</code></td></tr><tr><td style="text-align:left"><code>means_of_demise</code></td><td style="text-align:left"><em>“executed politicians”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Apolitician+means_of_demise%3A&quot;capital+punishment&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:politician means_of_demise:&quot;capital punishment&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>member_of</code></td><td style="text-align:left"><em>“african monarchs”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Amonarch+member_of%3Aafrica" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:monarch member_of:africa)&quot;</code><em>“Democratic politicians and notable actors”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Apolitician+member_of%3Ademocratic+notable%3Aactor" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:politician member_of:democratic notable:actor)&quot;</code></td></tr><tr><td style="text-align:left"><code>narrower_than</code></td><td style="text-align:left"><em>“examples of v8 engines”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Aengine+narrower_than%3A&quot;v8+engine&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:engine narrower_than:&quot;v8 engine&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>occurs_in</code></td><td style="text-align:left"><em>“languages spoken in Romania”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Alanguage+occurs_in%3Aromania" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:language occurs_in:romania)&quot;</code></td></tr><tr><td style="text-align:left"><code>origin</code></td><td style="text-align:left"><em>“Republican governors from Austria”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+title%3Agovernor+member_of%3Arepublican+origin%3Aaustria" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all title:governor member_of:republican origin:austria)&quot;</code></td></tr><tr><td style="text-align:left"><code>owner</code></td><td style="text-align:left"><em>“makes owned by Ford”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Amake+owner%3Aford" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:make owner:ford)&quot;</code></td></tr><tr><td style="text-align:left"><code>owns</code></td><td style="text-align:left"><em>“Who owns the Mavericks?”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+owns%3Amavericks" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all owns:mavericks)&quot;</code></td></tr><tr><td style="text-align:left"><code>parent</code></td><td style="text-align:left"><em>“Al Gore’s children”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+parent%3A&quot;al+gore&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all parent:&quot;al gore&quot;)&quot;</code><em>“descendants of the Lisp programming language”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3A%2Fcomputer%2Fprogramming_language+parent%3Alisp" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:/computer/programming_language parent:lisp)&quot;</code></td></tr><tr><td style="text-align:left"><code>part_of</code></td><td style="text-align:left"><em>“swedish lakes”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Alake+part_of%3Asweden" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:lake part_of:sweden)&quot;</code><em>“competitions at the 2008 summer olympics”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Acompetition+part_of%3A&quot;2008+summer+olympics&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:competition part_of:&quot;2008 summer olympics&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>participant</code></td><td style="text-align:left"><em>“Bowie concerts”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+participant%3Abowie+type%3Aconcert" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all participant:bowie type:concert)&quot;</code></td></tr><tr><td style="text-align:left"><code>participated_in</code></td><td style="text-align:left"><em>“Notable austrian skiers who participated in Olympics”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+notable%3Askier+member_of%3Aaustria+participated_in%3Aolympics" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all notable:skier member_of:austria participated_in:olympics)&quot;&#39;</code></td></tr><tr><td style="text-align:left"><code>peer_of</code></td><td style="text-align:left"><em>“politicians peers of Al Gore”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+notable%3Apolitician+peer_of%3Agore" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all notable:politician peer_of:gore)&quot;</code></td></tr><tr><td style="text-align:left"><code>permits_use_of</code></td><td style="text-align:left"><em>“Diesel engines”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+permits_use_of%3Adiesel" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all permits_use_of:diesel)&quot;</code></td></tr><tr><td style="text-align:left"><code>portrayed</code></td><td style="text-align:left"><em>“actors who portrayed John Lennon”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+notable%3Aactor+portrayed%3A&quot;john+lennon&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all notable:actor portrayed:&quot;john lennon&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>portrayed_by</code></td><td style="text-align:left"><em>“characters portrayed by Harrison Ford”</em> (<a href="https://developers.google.com/freebase/v1/&amp;indent=true" target="_blank" rel="noopener">Try it.</a>)<code>filter: &quot;(all portrayed_by:&quot;Harrison Ford&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>practitioner_of</code></td><td style="text-align:left"><em>“female african american lawyers”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+category%3Afemale+category%3A&quot;african+american&quot;+practitioner_of%3Alawyer" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all category:female category:&quot;african american&quot; practitioner_of:lawyer)&quot;</code></td></tr><tr><td style="text-align:left"><code>preceeding</code></td><td style="text-align:left"><em>“sequels to The Lord of the Rings, the two Towers”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3A%2Ffilm%2Ffilm+preceeding%3A&quot;The+Lord+of+the+Rings%2C+the+two+Towers&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:/film/film preceeding:&quot;The Lord of the Rings, the two Towers&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>produced_by</code></td><td style="text-align:left"><em>“Apple computers”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Acomputers+produced_by%3Aapple" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:computers produced_by:apple)&quot;</code></td></tr><tr><td style="text-align:left"><code>publication</code></td><td style="text-align:left"><em>“which book has /m/0clw238 as first edition ?”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+publication%3A%2Fm%2F0clw238" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all publication:/m/0clw238)&quot;</code></td></tr><tr><td style="text-align:left"><code>publication_of</code></td><td style="text-align:left"><em>“releases of La Traviata”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+publication_of%3A&quot;La+Traviata&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all publication_of:&quot;La Traviata&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>service_area</code></td><td style="text-align:left"><em>“California broadcasters”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Abroadcaster+service_area%3Acalifornia" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:broadcaster service_area:california)&quot;</code></td></tr><tr><td style="text-align:left"><code>status</code></td><td style="text-align:left"><em>“retreating swiss glaciers”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Aglacier+status%3Aretreating+part_of%3Aswitzerland" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:glacier status:retreating part_of:switzerland)&quot;</code></td></tr><tr><td style="text-align:left"><code>subclass_of</code></td><td style="text-align:left"><em>“kinds of swimwear”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+subclass_of%3Aswimwear" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all subclass_of:swimwear)&quot;</code></td></tr><tr><td style="text-align:left"><code>subject</code></td><td style="text-align:left"><em>“movies about the Holocaust”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Afilm+subject%3Aholocaust" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:film subject:holocaust)&quot;</code><em>“books about mathematics”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Abook+subject%3Amathematics" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:book subject:mathematics)&quot;</code></td></tr><tr><td style="text-align:left"><code>subsequent</code></td><td style="text-align:left"><em>“prequels to The Lord of the Rings, the two Towers”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3A%2Ffilm%2Ffilm+subsequent%3A&quot;The+Lord+of+the+Rings%2C+the+two+Towers&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:/film/film subsequent:&quot;The Lord of the Rings, the two Towers&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>succeeded_by</code></td><td style="text-align:left"><em>“Which automotive platform was succeeded by the Ford B3 platform ?”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+succeeded_by%3A&quot;ford+b3+platform&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all succeeded_by:&quot;ford b3 platform&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>succeeds</code></td><td style="text-align:left"><em>“Who succeeded the House of Stuart ?”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+succeeds%3Astuart" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all succeeds:stuart)&quot;</code></td></tr><tr><td style="text-align:left"><code>superclass_of</code></td><td style="text-align:left"><em>“Classes coronary heart disease belongs to”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+superclass_of%3A&quot;coronary+heart+disease&quot;" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all superclass_of:&quot;coronary heart disease&quot;)&quot;</code></td></tr><tr><td style="text-align:left"><code>title</code></td><td style="text-align:left"><em>“Google engineers”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+title%3Aengineer+member_of%3Agoogle" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all title:engineer member_of:google)&quot;</code></td></tr><tr><td style="text-align:left"><code>tookplace_at</code></td><td style="text-align:left"><em>“battles that took place at Marengo”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3Abattles+tookplace_at%3Amarengo" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:battles tookplace_at:marengo)&quot;</code></td></tr><tr><td style="text-align:left"><code>use_permitted_by</code></td><td style="text-align:left"><em>“File formats supported on an iPhone”</em> (<a href="https://www.googleapis.com/freebase/v1/search?query=&amp;limit=10&amp;filter=(all+type%3A&quot;file+format&quot;+use_permitted_by%3Aiphone" target="_blank" rel="noopener">Try it.</a>&amp;indent=true))<code>filter: &quot;(all type:&quot;file format&quot; use_permitted_by:iphone)&quot;</code></td></tr></tbody></table></div><h2 id="Scoring-and-ranking"><a href="#Scoring-and-ranking" class="headerlink" title="Scoring and ranking"></a>Scoring and ranking</h2><p>Freebase entities have an inherent relevance score (ranking) computed during indexing that is function of its inbound and outbound link counts in Freebase and Wikipedia. Some popular Freebase entities also have a popularity score computed by Google. By default, both scores are combined together during queries.</p><p>When a textual constraint is present, a textual match score is computed from the number of hits returned by the search index and is combined with the relevance score.</p><p>FreebaseSearch results are always sorted by the final score, highest score first.</p><p>The scoring parameter makes it possible to control what relevance score components are used to compute the final score:</p><p><strong>freebase</strong>: Use only the Freebase relevance score.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;beyoncé&quot; scoring: freebase</span><br></pre></td></tr></table></figure><p><strong>entity</strong>: Use both relevance scores, which replaces any missing Google scores to 1.0. This is the default.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;beyoncé&quot; scoring: entity</span><br></pre></td></tr></table></figure><p><strong>schema</strong>: Use when looking for schema entities like types, properties or domains. The link counts of schema entities is computed differently.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;performance&quot; scoring: schema</span><br></pre></td></tr></table></figure><h2 id="Other-constraints"><a href="#Other-constraints" class="headerlink" title="Other constraints"></a>Other constraints</h2><p>Entities can be filtered by index tag using the with or without parameters. Entities are tagged during indexing, each tag corresponding to one or several Freebase queries that would be too expensive to run during search:</p><p><code>commons</code> is a tag that can be used to restrict a schema search to returning only Freebase Commons schema. For example, Freebase Commons types matching the word “color”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;color&quot; type: &quot;/type/type&quot; with: &quot;commons&quot;</span><br></pre></td></tr></table></figure><p><code>gg</code> is a tag that can be used to restrict a search to entities for which there is or isn’t a Google popularity score.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;1923&quot; type: &quot;/people/person&quot; with: &quot;gg&quot;</span><br><span class="line">query: &quot;1923&quot; type: &quot;/people/person&quot; without: &quot;gg&quot;</span><br></pre></td></tr></table></figure><p>The <code>without</code> parameter is equivalent to the a negated with in a filter expression.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: &quot;color&quot; limit: 5 type: &quot;/type/type&quot; without: &quot;commons&quot; query: &quot;color&quot; limit: 5 type: &quot;/type/type&quot; filter: &quot;(not with:commons)&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Knowledge Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Freebase </tag>
            
            <tag> Knowledge Graph </tag>
            
            <tag> 知识图谱 </tag>
            
            <tag> SPARQL </tag>
            
            <tag> search </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Freebase介绍</title>
      <link href="/2020/09/15/Freebase%E4%BB%8B%E7%BB%8D/"/>
      <url>/2020/09/15/Freebase%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<blockquote><p>内容来自于Freebase官方文档：<a href="https://developers.google.com/freebase/guide/basic_concepts" target="_blank" rel="noopener">https://developers.google.com/freebase/guide/basic_concepts</a></p></blockquote><h1 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h1><p>If you are new to Freebase, this section covers the basic terminology and concepts required to understand how Freebase works.</p><h2 id="Graphs"><a href="#Graphs" class="headerlink" title="Graphs"></a>Graphs</h2><p>Freebase data is stored in a data structure called a <a href="http://en.wikipedia.org/wiki/Graph_(abstract_data_type" target="_blank" rel="noopener">graph</a>). A graph is composed on nodes connected by edges. In Freebase, the nodes are defined using <a href="http://www.freebase.com/type/object" target="_blank" rel="noopener">/type/object</a> and edges are defined using <a href="http://www.freebase.com/type/link" target="_blank" rel="noopener">/type/link</a>. By storing the data as a graph, Freebase can quickly traverse arbitrary connections between topics and easily add new schema without having to change structure of the data.</p><h2 id="Topics"><a href="#Topics" class="headerlink" title="Topics"></a>Topics</h2><p>Freebase has over 39 million topics about real-world entities like people, places and things. Since Freebase data is represented a graph, these topics correspond to the nodes in the graph. However, not every node is a topic. See the section on <a href="https://developers.google.com/freebase/guide/basic_concepts#cvts" target="_blank" rel="noopener">CVTs</a> to as an example of nodes that are not topics.</p><p>Examples of the types of topics found in Freebase:</p><ul><li>Physical entities, e.g., <a href="http://www.freebase.com/en/bob_dylan" target="_blank" rel="noopener">Bob Dylan</a>, <a href="http://www.freebase.com/en/louvre" target="_blank" rel="noopener">the Louvre Museum</a>, the <a href="http://www.freebase.com/en/saturn" target="_blank" rel="noopener">Saturn planet</a>, to</li><li>Artistic/media creations, e.g., <a href="http://www.freebase.com/en/the_dark_knight" target="_blank" rel="noopener">The Dark Knight (film)</a>, <a href="http://http//www.freebase.com/guid/9202a8c04000641f80000000002e46af" target="_blank" rel="noopener">Hotel California (song)</a>, to</li><li>Classifications, e.g., <a href="http://www.freebase.com/en/noble_gas" target="_blank" rel="noopener">noble gas</a>, <a href="http://www.freebase.com/en/chordate" target="_blank" rel="noopener">Chordate</a>, to</li><li>Abstract concepts, e.g., <a href="http://www.freebase.com/en/love" target="_blank" rel="noopener">love</a>, to</li><li>Schools of thoughts or artistic movements, e.g., <a href="http://www.freebase.com/en/impressionism" target="_blank" rel="noopener">Impressionism</a>.</li></ul><p>Some topics are notable because they hold a lot of data (e.g., <a href="http://www.freebase.com/en/wal-mart" target="_blank" rel="noopener">Wal-Mart</a>), and some are notable because they link to many other topics, potentially in different domains of information. For example, abstract topics like love, poverty, chivalry, etc. don’t have many properties associated with them but they appear often as book subjects, poetry subjects, film subjects, etc. making them more notable.</p><h2 id="Types-and-Properties"><a href="#Types-and-Properties" class="headerlink" title="Types and Properties"></a>Types and Properties</h2><p>Any given topic can be seen for many different perspectives for example:</p><ul><li>Bob Dylan was a song writer, singer, performer, book author, and film actor;</li><li>Leonardo da Vinci was a painter, a sculptor, an anatomist, an architect, an engineer, …;</li><li>Love is a book subject, film subject, play subject, poetry subject, …;</li><li>Any city is a location, potentially a tourist destination, and an employer of civil servants.<a id="more"></a></li></ul><p>In order to capture this multi-faceted nature of many topics, we introduce the concept of <em>types</em> in Freebase. Topics in Freebase can have any number of types assigned to them. The topic about Bob Dylan is assigned several types: the song writer type, the music composer type, the music artist (singer) type, the book author type, etc. Each type carries a different set of <em>properties</em> germane to that type. For example,</p><ul><li>The music artist type contains a property that lists all the albums that Bob Dylan has produced as well as all the music instruments he was known to play;</li><li>The book author type contains a property that lists all the books Bob Dylan has written or edited, as well as his writing school of thoughts or movement;</li><li>The company type contains many property for listing a company’s founders, board members, parent company, divisions, employees, products, year-by-year revenue and profit records, etc.</li></ul><p>Thus, a type can be thought of as a conceptual container of properties that are most commonly needed for describing a particular aspect of information. (You can think of a type as analogous to a relational table, and each “type” table has a foreign key into the one “identity” table that uniquely defines each topic.)</p><h2 id="Domains-and-IDs"><a href="#Domains-and-IDs" class="headerlink" title="Domains and IDs"></a>Domains and IDs</h2><p>Just as properties are grouped into types, types themselves are grouped into <em>domains</em>. Think of domains as the sections in your favorite newspaper: Business, Life Style, Arts and Entertainment, Politics, Economics, etc. Each domain is given an ID (identifier), e.g.,</p><ul><li><code>/business</code> is the ID of the Business domain</li><li><code>/music</code> - the Music domain</li><li><code>/film</code> - the Film domain</li><li><code>/medicine</code> - the Medicine domain</li></ul><p>The ID of a domain looks like a file path, or a path in a web address.</p><p>Each type is also given an ID, and its ID is based on the domain in which it belongs. For example, the Company type belongs in the Business domain, and it’s given the ID <code>/business/company</code>. Here are some other examples:</p><ul><li><code>/music/album</code> is the ID of the (Music) Album type, belonging in the Music domain</li><li><code>/film/actor</code> - the Actor type in the Film domain</li><li><code>/medicine/disease</code> - the Disease type in the Medicine domain</li></ul><p>Just as a type inherits the beginning of its ID from its domain, a property also inherits the beginning of its ID from the type it belongs to. For example, the Industry property of the Company type (used for specifying which industry a company is in) is given the ID <code>/business/company/industry</code>. Here are some other examples:</p><ul><li><code>/automotive/engine/horsepower</code> is the ID of the Horsepower property of the (Automotive) Engine type</li><li><code>/astronomy/star/planet_s</code> is the ID of the Planets property of the Star type (used for listing planets around a star)</li><li><code>/language/human_language/writing_system</code> is the ID of the Writing System property of the Human Language type</li></ul><p>Thus, even though <strong>types are not arranged into hierarchies in Freebase</strong>; domains, types, and properties are given IDs conceptually arranged in a file directory-like hierarchy.</p><h2 id="Compound-Value-Types"><a href="#Compound-Value-Types" class="headerlink" title="Compound Value Types"></a>Compound Value Types</h2><p>A Compound Value Type is a Type within Freebase which is used to represent data where each entry consists of multiple fields. Compound value types, or CVT’s are used in Freebase to represent complex data. It may be a little confusing at first, but CVT’s are a very important part of the Freebase schema and allow it to more accurately model complex relationships between topics.</p><p><strong>Think about the following example: Population for a city is something that changes over time.</strong> That means, whenever you query Freebase for population, you are at least implicitly asking for a population at a certain date. Two Values are involved, a number of people, and the date. Here’s a situation where a CVT becomes extremely useful. Without one, to model population data, you would need to make a topic, and name it something like “Vancouver’s population in 1997”, and submit the information over there.</p><p><strong>A CVT can be thought of as a topic that does not require you to make a display name.</strong> CVT’s, like normal topics, have a GUID that can be referenced independently. However, the Freebase client treats them much differently than topics. In most cases, every property of the CVT should be a disambiguation property.</p><h2 id="Topic-MIDs"><a href="#Topic-MIDs" class="headerlink" title="Topic MIDs"></a>Topic MIDs</h2><p>While a topic might or might not be identifiable by namespace/key IDs, it can always be identified with a MID — a Machine Identifier, which consist <code>/m/</code> followed by a base-32 unique identifier. MIDs are assigned to topics at creation time, and are managed throughout the topic’s lifetime. They play a critical role when topics are merged or split, allowing external applications to track the logical topic even though the physical Freebase identity (the topic’s GUID) may change. Machine-generated ids differ from other human-readable Freebase ids (returned by the “id” property) in that they are:</p><ul><li>Guaranteed to exist</li><li>Machine-generated</li><li>Designed to support offline comparison</li><li>Not designed to convey meaning to humans</li><li>Short (possibly fixed length)</li><li>Ideal for quick exchange of keys between external systems and components (external, exchange)</li></ul><p>MIDs are the recommended identifier to use to address topics in Freebase</p><h2 id="Namespaces-Keys-and-Topic-IDs"><a href="#Namespaces-Keys-and-Topic-IDs" class="headerlink" title="Namespaces, Keys, and Topic IDs"></a>Namespaces, Keys, and Topic IDs</h2><p>The <strong>file directory-like hierarchy of domain, type, and property IDs</strong> is just one application of a more general concept: <em>namespaces</em> and <em>keys</em>. A namespace is like a file directory, and a key is like a file name. Just as all file names within a particular file directory must be unique among themselves, all keys within a particular namespace must also be unique among themselves.</p><p>As a more specific example, <code>/business</code> is the namespace corresponding to the Business domain. Within it, Business-related types are given keys (e.g., <code>company</code>) that are unique among themselves. Each type’s ID is formed by appending its key to the namespace’s ID (e.g., <code>/business/company</code>).</p><p>There are several kinds of namespaces beside namespaces that correspond to domains and types. Most <strong>important and frequently encountered is the <code>/en</code> namespace</strong>. This is the English namespace in which most well-known topics can be given unique keys to form human-readable English IDs. For example, the prolific Bob Dylan is so well-known that his topic in Freebase is given the key <code>bob_dylan</code> in the <code>/en</code> namespace, and so the topic’s ID is <code>/en/bob_dylan</code>. This ID allows you to access his topic in the web client with the simple URL</p><h2 id="More-on-Properties"><a href="#More-on-Properties" class="headerlink" title="More on Properties"></a>More on Properties</h2><p>The last basic concept to discuss involves a major difference between Freebase properties and their analogy in relational database technologies, namely relational table columns. For each row, a relational table column can only hold one value. For example, consider a typical “book” relational table with a column named “author”. For each row in the “book” table, the “author” column can only hold one foreign key to an “author” table. If a book happens to have several authors, then this simple relational schema design does not work, and we would have to make a new table to model the authorships. That is, we would need one “book” table, one “author” table, and one “authorship” table to store <strong>the n-to-n relationships between books and authors</strong>. And the way you retrieve data changes quite radically as you switch from one schema design to the other.</p><p>In contrast with conventional database technologies, <strong>Freebase considers multi-value properties to be so desirable in modeling real-life data that it supports multi-value properties by default</strong>. That is, when the <code>/book/written_work/author</code> property was created, it was assumed to allow for multiple authors per book, and you can query for a multi-value property and for a single-value property in exactly the same way. There is no need to think if you need to join with a third table that models the n-to-n relationship.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul><li>A type is a conceptual container of related properties commonly needed to describe a certain aspect of a topic.</li><li>A topic can be assigned one or more types (the default type being <code>/common/topic</code>)</li><li>As properties are grouped into types, types are grouped into domains.</li><li>Domains, types, and properties are given IDs in a namespace/key hierarchy.</li><li>Common well-known topics are given IDs in the <code>/en</code> namespace, which are human-readable English strings.</li><li>Topics are uniquely identified within Freebase by GUIDs.</li><li>Properties are multi-value by default, and multi-value properties and single-value properties can be queried in the same way.</li></ul><h1 id="Search-Overview"><a href="#Search-Overview" class="headerlink" title="Search Overview"></a>Search Overview</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="https://developers.google.com/freebase/images/Search-Service.png" alt="img"></p><p>The Freebase Search API provides access to Freebase data given a free text query. The results of the query are ordered and have a numerical relevancy score.</p><p>Developers can apply filters to constrain the search results to certain types of data. See the <a href="https://developers.google.com/freebase/v1/search-cookbook" target="_blank" rel="noopener">Search Cookbook</a> for more information on how to construct detailed search queries.</p><p>Some examples of how developers may want to use the Search API include:</p><ul><li>Autosuggesting entities (e.g. <a href="https://developers.google.com/freebase/v1/suggest" target="_blank" rel="noopener">Freebase Suggest Widget</a>)</li><li>Getting a ranked list of the most notable entities with a given name.</li><li>Finding entities using <a href="https://developers.google.com/freebase/v1/search-metaschema" target="_blank" rel="noopener">Search Metaschema</a>.</li></ul><p>The following code in Python show how to perform a search for a musical artist that matches the text “Cee Lo Green”. An additional constraint is that they created something called “The Lady Killer”.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">api_key = open(<span class="string">".api_key"</span>).read()</span><br><span class="line">query = <span class="string">'blue bottle'</span></span><br><span class="line">service_url = <span class="string">'https://www.googleapis.com/freebase/v1/search'</span></span><br><span class="line">params = &#123; </span><br><span class="line">    <span class="string">'query'</span>: query, </span><br><span class="line">    <span class="string">'key'</span>: api_key</span><br><span class="line">&#125;</span><br><span class="line">url = service_url + <span class="string">'?'</span> + urllib.urlencode(params)</span><br><span class="line">response = json.loads(urllib.urlopen(url).read())</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> response[<span class="string">'result'</span>]:  </span><br><span class="line">    print(result[<span class="string">'name'</span>] + <span class="string">' ('</span> + str(result[<span class="string">'score'</span>]) + <span class="string">')'</span>)</span><br></pre></td></tr></table></figure><p>Also see the <a href="https://developers.google.com/freebase/v1/libraries" target="_blank" rel="noopener">Client Libraries</a> page to see if your favorite language is supported.</p><h3 id="Search-API-documentation"><a href="#Search-API-documentation" class="headerlink" title="Search API documentation"></a>Search API documentation</h3><p>Also see the following documentation:</p><ul><li>See the <a href="https://developers.google.com/freebase/v1/search" target="_blank" rel="noopener">Search reference documents</a> for details on how to use the API.</li><li>See the <a href="https://developers.google.com/freebase/v1/search-cookbook" target="_blank" rel="noopener">Search Cookbook</a> for more information on how to construct detailed search queries.</li><li>See the <a href="https://developers.google.com/freebase/v1/search-metaschema" target="_blank" rel="noopener">Search Metaschema document</a> for information on how the relationships between entities and properties are described.</li></ul><h2 id="Security-considerations"><a href="#Security-considerations" class="headerlink" title="Security considerations"></a>Security considerations</h2><p>The Search API indexes and searches user generated content stored in the Freebase graph. This means that you cannot directly use the content on a web page without safely escaping it first.</p><p>See <a href="https://developers.google.com/freebase/v1/getting-started#security" target="_blank" rel="noopener">Getting Started: Security</a> for more information.</p><h2 id="Advanced-filtering"><a href="#Advanced-filtering" class="headerlink" title="Advanced filtering"></a>Advanced filtering</h2><p>The Search API supports a large number of filter constraints to better aim the search at the correct entities.</p><p>For example, using a “type” filter constraint, we can show a list of the most notable people in Freebase.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter=(any type:/people/person)</span><br></pre></td></tr></table></figure><p>Filter constraints accept a variety of inputs:</p><ul><li>Human readable IDs for schema entities or users, for example:<ul><li><code>/people/person</code> for a type constraint</li><li><code>/film</code> for a domain constraint</li></ul></li><li>Freebase MIDs, for example:<ul><li><code>/m/04kr</code> for the same <code>/people/person</code> type constraint</li><li><code>/m/010s</code> for the above <code>/film</code> domain constraint</li></ul></li><li>Entity names, for example:<ul><li><code>&quot;person&quot;</code> for a less precise <code>/people/person</code> type constraint</li><li><code>&quot;film&quot;</code> for a less precise <code>/film</code> domain constraint</li></ul></li></ul><p>Filter constraints can be classified into a few categories. See the <a href="https://developers.google.com/freebase/v1/search-cookbook" target="_blank" rel="noopener">Search Cookbook</a> for more details.</p><p>Filter constraints can be freely combined and repeated in the <code>SearchRequest</code> directly. Repeated filter constraint parameters are combined into an OR query. Different filter constraint parameters or groups are combined into an AND query.</p><p><strong>For example:</strong></p><p>To search for “people or cities named <em>Gore</em>“, try:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">query=gore</span><br><span class="line">&amp;filter=(any type:/people/person type:/location/citytown)</span><br></pre></td></tr></table></figure><p>This combining behavior can be overriden and better controlled with the filter parameter which offers a richer interface to combining constraints. It is an s-expression, possibly arbitrarily nested, where the operator is one of:</p><ul><li><code>any</code>, logically an OR</li><li><code>all</code>, logically an AND</li><li><code>not</code></li><li><code>should</code>, which can only be used at the top level and which denotes that the constraint is optional. During scoring, matches that don’t match optional constraints have their score divided in half for each optional constraint they don’t match.</li></ul><p><strong>For example:</strong></p><p>To match on the <code>/people/person</code> type or the <code>/film</code> domain, try:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query=gore &amp;filter=(any type:/people/person domain:/film)</span><br></pre></td></tr></table></figure><p>下一篇文章继续介绍Search Cookbook~</p>]]></content>
      
      
      <categories>
          
          <category> Knowledge Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Freebase </tag>
            
            <tag> 知识图谱 </tag>
            
            <tag> SPARQL </tag>
            
            <tag> Knowledeg Graph </tag>
            
            <tag> ontology </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Focal Loss</title>
      <link href="/2020/09/07/Focal-Loss/"/>
      <url>/2020/09/07/Focal-Loss/</url>
      
        <content type="html"><![CDATA[<h2 id="1-总述"><a href="#1-总述" class="headerlink" title="1. 总述"></a>1. 总述</h2><p>Focal Loss是由何恺明大神在论文<a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">Focal Loss for Dense Object Detection</a>中提出的。</p><p>Focal Loss主要是为了解决one-stage目标检测中正负样本比例严重失衡的问题。该损失函数降低了大量负样本在训练中所占的权重，也可以理解为一种困难样本挖掘。</p><h2 id="2-损失函数形式"><a href="#2-损失函数形式" class="headerlink" title="2. 损失函数形式"></a>2. 损失函数形式</h2><p>Focal Loss是在交叉熵损失函数基础上进行的修改，首先回顾二分类交叉熵损失：</p><script type="math/tex; mode=display">Loss = - [y\log y^{'} + (1-y) \log (1-y^{'})] = \left\{\begin{aligned}-\log y^{'}\quad \quad y=1 \\-\log (1-y^{'}) \quad y=0\end{aligned}\right.</script><p>$y^{‘}$是经过激活函数的输出，介于0-1之间。可见普通的交叉熵对于正样本而言，输出概率越大损失越小。对于负样本而言，输出概率越小则损失越小。此时的损失函数在大量简单样本的迭代过程中收敛比较缓慢而且可能无法优化到最优。那么Focal Loss是怎么改进的呢？</p><script type="math/tex; mode=display">Loss_{focal} = \left\{\begin{aligned}-(1-y^{'})^\gamma \log y^{'}  & \quad & y=1\\-y^{'\gamma} \log (1-y^{'}) & \quad & y=0\end{aligned}\right.</script><p>首先在原有的基础上添加了$(1-y^{‘})$和$y^{‘}$项，可以看作当前样本在损失函数中所占的权重。当$y=1$时，预测结果$y^{‘}$越接近$1$，说明当前样本置信度越高、越容易分类，对应的权重$(1-y^{‘})$就越低。$y=0$时类似。</p><p>而$\gamma$用于控制惩罚力度，$\gamma$越大，惩罚力度越小：</p><p><img src="/images/blog/2020/focalloss.png" alt></p><h2 id="3-补充"><a href="#3-补充" class="headerlink" title="3. 补充"></a>3. 补充</h2><p>对于正负样本类别不平衡，可以在上式基础上继续添加平衡因子$\alpha$，形式变为：</p><script type="math/tex; mode=display">Loss_{focal} = \left\{\begin{aligned}-\alpha(1-y^{'})^\gamma \log y^{'} \quad & \quad & y=1\\-(1-\alpha)y^{'\gamma} \log (1-y^{'}) & \quad & y=0\end{aligned}\right.</script><p>在文中负样本比重较大，论文中取$\alpha=0.25$</p><p>在论文中实验发现$\gamma$取$2.0$效果最优</p><p>对于不同的数据集，最优参数不同，可以通过grid-search的方法选择适合自己实验的参数。<a id="more"></a></p><h2 id="4-自定义损失函数"><a href="#4-自定义损失函数" class="headerlink" title="4. 自定义损失函数"></a>4. 自定义损失函数</h2><p>在tensorflow中自定义损失函数，方法很简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">custom_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.square(y_true - y_pred))</span><br></pre></td></tr></table></figure><p>MSE损失就可以用上面的形式定义。函数写好后，就可以交给模型编译：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=custom_loss)</span><br></pre></td></tr></table></figure><p>对于Focal Loss，我用tensorflow自己实现了一版，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(logits, label, gamma, alpha)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    gamma: if gamma is 0, then same as BCELoss;</span></span><br><span class="line"><span class="string">    alpha: pos sample weight; if alpha is 0.5, then same as BCELoss;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    epsilon = <span class="number">1e-9</span></span><br><span class="line">    BCELoss = -tf.math.log(tf.minimum(tf.abs(<span class="number">1</span> - label - logits) + epsilon, <span class="number">1.0</span>))</span><br><span class="line">    weight = tf.math.pow(tf.abs(label - logits), tf.constant([gamma], dtype=tf.float32))</span><br><span class="line">    balance = tf.abs(<span class="number">1</span> - label - alpha)</span><br><span class="line">    focal_loss = tf.math.multiply(weight, BCELoss)</span><br><span class="line">    balanced_focal_loss = tf.math.multiply(balance, focal_loss)</span><br><span class="line">    <span class="keyword">return</span> balanced_focal_loss</span><br></pre></td></tr></table></figure><p>实验效果不错👍</p><p><img src="/images/blog/2020/casrel.PNG" alt></p>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Loss </tag>
            
            <tag> 损失函数 </tag>
            
            <tag> 正负样本平衡 </tag>
            
            <tag> 调参 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorboard显示空白问题</title>
      <link href="/2020/09/05/tensorboard%E6%98%BE%E7%A4%BA%E7%A9%BA%E7%99%BD%E9%97%AE%E9%A2%98/"/>
      <url>/2020/09/05/tensorboard%E6%98%BE%E7%A4%BA%E7%A9%BA%E7%99%BD%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在服务器上执行pytorch代码训练深度神经网络，用<a href="https://github.com/huggingface/transformers/tree/master/examples/language-modeling" target="_blank" rel="noopener">run_language_modeling.py</a>在<code>bert-base-uncased</code>基础上基于Amazon情感分析语料进行fine-tuning，服务器上版本为：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python==3.6.10</span><br><span class="line">tensorboard==2.1.1</span><br><span class="line">pytorch==1.5.1</span><br></pre></td></tr></table></figure><p>得到结果记录保存在runs文件夹中，在个人电脑上，执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir runs</span><br></pre></td></tr></table></figure><p>然后打开本地服务器<a href="http://localhost:6006，结果页面显示空白。" target="_blank" rel="noopener">http://localhost:6006，结果页面显示空白。</a></p><p>在网上查询了相关资料，最后问题定位为tensorboard版本问题，由于tensorboard升级后对某些原有的API不再支持，所以使用pytorch的SummaryWriter旧API写得的记录文件不能直接显示。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>将tensorboard降级为2.0.0版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboard==2.0.0</span><br></pre></td></tr></table></figure><p>再次尝试，看到页面可以正常显示了。</p><p><img src="/images/blog/2020/scalars.PNG" alt></p><a id="more"></a><p><img src="/images/blog/2020/text.PNG" alt></p><p><img src="/images/blog/2020/hparams.PNG" alt></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorboard </tag>
            
            <tag> visualisation </tag>
            
            <tag> 可视化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch多GPU训练</title>
      <link href="/2020/09/04/Pytorch%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/"/>
      <url>/2020/09/04/Pytorch%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<p>在数据越来越多的时代，随着模型参数规模增多，以及数据量不断提升，使用多GPU训练是不可避免的事情。Pytorch在0.4.0及以后的版本中提供了多GPU的训练方式，本文简单讲解使用Pytorch多GPU训练的方式以及一些注意的地方。</p><p>在此我们主要讨论单机多GPU方式训练，而不是多机多卡分布式训练，在实际的任务中，两者存在部分交集。</p><h2 id="1-使用方式"><a href="#1-使用方式" class="headerlink" title="1. 使用方式"></a>1. 使用方式</h2><p>在设备存在多个显卡的情况下，最简单的方法是使用<strong>torch.nn.DataParallel</strong>将模型包装一下即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.DataParallel(model)</span><br></pre></td></tr></table></figure><p>这时，默认所有存在的显卡都会被使用，如果想要指定某几张显卡，可以：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.DataParallel(model, device_ids=[<span class="number">0</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><p>就指定了0号和2号显卡作为训练使用。</p><h2 id="2-进阶方式"><a href="#2-进阶方式" class="headerlink" title="2. 进阶方式"></a>2. 进阶方式</h2><p>另一种方法，使用<strong>torch.nn.parallel.DistributedDataParallel</strong>，虽然用于分布式训练，但是单机训练同样可以使用。只不过使用起来相对麻烦一些。</p><p>在<a href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel" target="_blank" rel="noopener">Pytorch官方文档</a>中，有一段值得注意的话：</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/Pytorch%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/warning.PNG" alt></p><p>Pytorch推荐使用<strong>DistributedDataParallel</strong>代替<strong>DataParallel</strong>。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/Pytorch%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/instead.PNG" alt></p><p>概括来说，就是<strong>DistributedDataParallel</strong>使用多进程，每个GPU单独占有一个进程；<strong>DataParallel</strong>使用多线程，会受到Python解释器中GIL的影响，效率较低。</p><p>下面介绍一下具体的使用方法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=你的GPU数量 YOUR_TRAINING_SCRIPT.py --arg1 --arg2 and all other arguments of your training script</span><br></pre></td></tr></table></figure><p>上述命令和我们平常的命令稍有区别，用到了torch.distributed.launch这个模块，我们选择的运行方式变为<code>python -m</code>，相当于使用torch.distributed.launch.py去运行我们的YOUR_TRAINING_SCRIPT.py，其中torch.distributed.launch会向我们的运行程序传递一些变量。<a id="more"></a></p><p>为此，我们的YOUR_TRAINING_SCRIPT.py也就是训练代码应该对应修改为（只保留核心部分）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="comment"># 由torch.distributed.launch传递过来，我们设置参数来接收，local_rank代表当前进程使用的GPU标号</span></span><br><span class="line">parser.add_argument(<span class="string">'--local_rank'</span>, type=int, default=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">synchronize</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Helper function to synchronize (barrier) among all processes when using distributed</span></span><br><span class="line"><span class="string">    training</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> dist.is_available():</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> dist.is_initialized():</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    world_size = dist.get_world_size()</span><br><span class="line">    <span class="keyword">if</span> world_size == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    dist.barrier()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># WORLD_SIZE由torch.distributed.launch.py产生，具体数值为nproc_per_node * node，单机node值为1</span></span><br><span class="line">num_gpus = int(os.environ[<span class="string">'WORLD_SIZE'</span>]) <span class="keyword">if</span> <span class="string">'WORLD_SIZE'</span> <span class="keyword">in</span> os.environ <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">is_distributed = num_gpus &gt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_distributed:</span><br><span class="line">    torch.cuda.set_device(args.local_rank)</span><br><span class="line">    torch.distributed.init_process_group(</span><br><span class="line">    backend=<span class="string">'nccl'</span>, init_method=<span class="string">'env://'</span></span><br><span class="line">    )</span><br><span class="line">    synchronize()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 将模型迁移到DistributedDataParallel中，此时就可以进行训练了</span></span><br><span class="line"><span class="keyword">if</span> is_distributed:</span><br><span class="line">    model = torch.nn.parallel.DistributedDataParallel(</span><br><span class="line">    model, device_ids=[args.local_rank], output_device=args.local_rank,</span><br><span class="line">        <span class="comment"># this should be removed if we update BatchNorm stats</span></span><br><span class="line">        broadcast_buffers=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意，在测试的时候需要执行model = model.module</span></span><br></pre></td></tr></table></figure><p>我们要注意，上述代码在运行的过程中产生了很多个进程，具体多少个取决于你的GPU数量，也就是为什么上面需要torch.cuda.set_device(args.local_rank)设定默认的GPU，因为torch.distributed.launch为我们触发了n个YOUR_TRAINING_SCRIPT.py，n就是我们将要使用的GPU数量。</p><h2 id="遇到的坑"><a href="#遇到的坑" class="headerlink" title="遇到的坑"></a>遇到的坑</h2><h3 id="1-双卡效果不显著"><a href="#1-双卡效果不显著" class="headerlink" title="1. 双卡效果不显著"></a>1. 双卡效果不显著</h3><p>来自<a href="https://www.jianshu.com/p/bb28669018b3" target="_blank" rel="noopener">https://www.jianshu.com/p/bb28669018b3</a></p><p>最近两天训练一个魔改的mobilenetv2+yolov3，同样的优化方法同样的学习率衰减率，所有的参数都相同的情况下，发现单显卡训练的方式竟然比多显卡训练的方式收敛更快。</p><p>配置为两张1080Ti，使用Pytorch的版本为1.0.0。下图<strong>红线为使用一张1080Ti训练的情况，蓝线为使用两张1080Ti训练的情况</strong>，batchsize每张显卡设置为10，也就是说，使用两张显卡训练时的batchsize为单张显卡的两倍，同一个step时，双卡走的步数为单卡步数的两倍。这里使用的多卡训练方式为DataParallel。</p><p>但是下图可以看到，在双卡相同step的情况下，虽然红色曲线的损失相较蓝色下降的稍微慢一些，但是到了一定时候，两者的损失值会相交(此时未达到最低损失点),<strong>也就是说使用双卡和单卡训练时候loss损失收敛的速度是一样的。</strong></p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/Pytorch%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/loss1.webp" alt></p><p>更奇怪的是，下图中，在验证集中，单显卡虽然没有双显卡的准确度曲线增长迅速，但是到了某一点，单显卡的曲线会超过双显卡训练的精度，<strong>也就是说，单卡训练在前期没有双卡训练效果显著，但是到了训练中期效果就会优于双卡。</strong><br><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/Pytorch%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/loss2.webp" alt></p><h3 id="2-BatchNorm官方实现存在问题"><a href="#2-BatchNorm官方实现存在问题" class="headerlink" title="2. BatchNorm官方实现存在问题"></a>2. BatchNorm官方实现存在问题</h3><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/Pytorch%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/batchnorm.PNG" alt></p><p>当使用多卡训练时，pytorch官方实现中的BatchNorm只使用单卡上的数据计算normalization，猜测可能是为了加速和减少通信开支，但同时可能带来不稳定、不准确的问题。</p><p>可以使用<a href="https://github.com/vacancy/Synchronized-BatchNorm-PyTorch" target="_blank" rel="noopener">Synchronized-BatchNorm-PyTorch</a>开源代码。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><p>[知乎]  <a href="https://zhuanlan.zhihu.com/p/86441879" target="_blank" rel="noopener">pytorch多gpu并行训练</a></p></li><li><p>[知乎]  <a href="https://zhuanlan.zhihu.com/p/95700549" target="_blank" rel="noopener">和nn.DataParallel说再见</a></p></li><li><p>[pytorch]  <a href="https://github.com/pytorch/vision/tree/master/references/detection" target="_blank" rel="noopener">pytorch官方example</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> 并行 </tag>
            
            <tag> Parallel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch优化器传入两个模型的参数</title>
      <link href="/2020/09/01/pytorch%E4%BC%98%E5%8C%96%E5%99%A8%E4%BC%A0%E5%85%A5%E4%B8%A4%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0/"/>
      <url>/2020/09/01/pytorch%E4%BC%98%E5%8C%96%E5%99%A8%E4%BC%A0%E5%85%A5%E4%B8%A4%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="1-传入两个模型参数"><a href="#1-传入两个模型参数" class="headerlink" title="1. 传入两个模型参数"></a>1. 传入两个模型参数</h2><p>举例说明：model_one只是一个提取特征的网络，得到features，model_two对features进行处理，求loss，而且model_two有需要学习的参数。平时我们一般使用如下的方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr)</span><br></pre></td></tr></table></figure><p>传入两个模型的参数为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD([</span><br><span class="line">&#123;<span class="string">'params'</span>: model.parameters()&#125;,</span><br><span class="line">&#123;<span class="string">'params'</span>: lossnet.parameters(), <span class="string">'lr'</span>: <span class="number">1e-4</span>&#125;</span><br><span class="line">], lr, momentum=<span class="number">0.99</span>)</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="2-保存-载入两个模型的参数"><a href="#2-保存-载入两个模型的参数" class="headerlink" title="2. 保存/载入两个模型的参数"></a>2. 保存/载入两个模型的参数</h2><ul><li>保存</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">filename = <span class="string">'./models/vgg16/ocr.pth'</span></span><br><span class="line">state = &#123;</span><br><span class="line"><span class="string">'model'</span>: model.state_dict(),</span><br><span class="line"><span class="string">'lossnet'</span>: lossnet.state_dict()</span><br><span class="line">&#125;</span><br><span class="line">torch.save(state, filename)</span><br></pre></td></tr></table></figure><ul><li>载入，使用torch.load()，并使用字典进行索引</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">load_name = <span class="string">'./models/vgg16/ocr.pth'</span></span><br><span class="line">checkpoint = torch.load(load_name)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">'model'</span>])</span><br><span class="line">lossnet.load_state_dict(checkpoint[<span class="string">'lossnet'</span>])</span><br></pre></td></tr></table></figure><blockquote><p>参考 <a href="https://blog.csdn.net/u011622208/article/details/90698688" target="_blank" rel="noopener">https://blog.csdn.net/u011622208/article/details/90698688</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> pytorch </tag>
            
            <tag> 优化 </tag>
            
            <tag> optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>优化方法总结：SGD, Momentum, AdaGrad, RMSProp, Adam</title>
      <link href="/2020/08/14/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%EF%BC%9ASGD-Momentum-AdaGrad-RMSProp-Adam/"/>
      <url>/2020/08/14/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%EF%BC%9ASGD-Momentum-AdaGrad-RMSProp-Adam/</url>
      
        <content type="html"><![CDATA[<h2 id="1-SGD"><a href="#1-SGD" class="headerlink" title="1. SGD"></a>1. SGD</h2><h3 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a>Batch Gradient Descent</h3><p>在每一轮的训练过程中，Batch Gradient Descent算法用整个训练集的数据计算cost function的梯度，并用该梯度对模型参数进行更新：</p><script type="math/tex; mode=display">\Theta = \Theta - \alpha \cdot \nabla _\Theta J(\Theta)</script><p><strong>优点:</strong></p><ul><li>cost fuction若为凸函数，能够保证收敛到全局最优值；若为非凸函数，能够收敛到局部最优值</li></ul><p><strong>缺点:</strong></p><ul><li>由于每轮迭代都需要在整个数据集上计算一次，所以批量梯度下降可能非常慢</li><li>训练数较多时，需要较大内存</li><li>批量梯度下降不允许在线更新模型，例如新增实例。<a id="more"></a></li></ul><h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a><strong>Stochastic Gradient Descent</strong></h3><p>和批梯度下降算法相反，Stochastic gradient descent 算法每读入一个数据，便立刻计算cost fuction的梯度来更新参数：</p><script type="math/tex; mode=display">\Theta = \Theta - \alpha \cdot \nabla _\Theta J(\Theta;x^{(i)},y^{(i)})</script><p><strong>优点:</strong></p><ul><li>算法收敛速度快(在Batch Gradient Descent算法中, 每轮会计算很多相似样本的梯度, 这部分是冗余的)</li><li>可以在线更新</li><li>有几率跳出一个比较差的局部最优而收敛到一个更好的局部最优甚至是全局最优</li></ul><p><strong>缺点:</strong></p><ul><li>容易收敛到局部最优，并且容易被困在鞍点</li></ul><h3 id="Mini-batch-Gradient-Descent"><a href="#Mini-batch-Gradient-Descent" class="headerlink" title="Mini-batch Gradient Descent"></a><strong>Mini-batch Gradient Descent</strong></h3><p>mini-batch Gradient Descent的方法是在上述两个方法中取折衷, 每次从所有训练数据中取一个子集（mini-batch） 用于计算梯度：</p><script type="math/tex; mode=display">\Theta = \Theta - \alpha \cdot \nabla _\Theta J(\Theta;x^{(i:i+n)},y^{(i:i+n)})</script><p>Mini-batch Gradient Descent在每轮迭代中仅仅计算一个mini-batch的梯度，不仅计算效率高，而且收敛较为稳定。该方法是目前深度学训练中的主流方法</p><p>上述三个方法面临的主要<strong>挑战</strong>如下：</p><ul><li>选择适当的学习率$\alpha$较为困难。太小的学习率会导致收敛缓慢，而学习速度太块会造成较大波动，妨碍收敛。</li><li>目前可采用的方法是在训练过程中调整学习率大小，例如模拟退火算法：预先定义一个迭代次数m，每执行完m次训练便减小学习率，或者当cost function的值低于一个阈值时减小学习率。然而迭代次数和阈值必须事先定义，因此无法适应数据集的特点。</li><li>上述方法中, 每个参数的 learning rate 都是相同的，这种做法是不合理的：如果训练数据是稀疏的，并且不同特征的出现频率差异较大，那么比较合理的做法是对于出现频率低的特征设置较大的学习速率，对于出现频率较大的特征数据设置较小的学习速率。</li><li>近期的的研究表明，深层神经网络之所以比较难训练，并不是因为容易进入local minimum。相反，由于网络结构非常复杂，在绝大多数情况下即使是 local minimum 也可以得到非常好的结果。而之所以难训练是因为学习过程容易陷入到马鞍面中，即在坡面上，一部分点是上升的，一部分点是下降的。而这种情况比较容易出现在平坦区域，在这种区域中，所有方向的梯度值都几乎是 0。</li></ul><h2 id="2-Momentum"><a href="#2-Momentum" class="headerlink" title="2. Momentum"></a>2. Momentum</h2><p>SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力：</p><script type="math/tex; mode=display">v_t = \gamma \cdot v_{t-1} + \alpha \cdot \nabla_{\Theta}J(\Theta)</script><script type="math/tex; mode=display">\Theta = \Theta - v_t</script><p>Momentum算法会观察历史梯度$v_{t-1}$，若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯方向不一致，则梯度会衰减。<strong>一种形象的解释是：</strong>我们把一个球推下山，球在下坡时积聚动量，在途中变得越来越快，γ可视为空气阻力，若球的方向发生变化，则动量会衰减。</p><h2 id="3-Nesterov-Momentum"><a href="#3-Nesterov-Momentum" class="headerlink" title="3. Nesterov Momentum"></a>3. Nesterov Momentum</h2><p>在小球向下滚动的过程中，我们希望小球能够提前知道在哪些地方坡面会上升，这样在遇到上升坡面之前，小球就开始减速。这方法就是Nesterov Momentum，其在凸优化中有较强的理论保证收敛。并且，在实践中Nesterov Momentum也比单纯的 Momentum 的效果好：</p><script type="math/tex; mode=display">v_t = \gamma \cdot v_{t-1} + \alpha \cdot \nabla_{\Theta}J(\Theta - \gamma \cdot v_{t-1})</script><script type="math/tex; mode=display">\Theta = \Theta - v_t</script><p>其核心思想是：注意到 momentum 方法，如果只看 γ <em> v 项，那么当前的 θ经过 momentum 的作用会变成 θ-γ </em> v。因此可以把 θ-γ <em> v这个位置看做是当前优化的一个”展望”位置。所以，可以在 θ-γ </em> v求导, 而不是原始的θ。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/优化方法总结/Nesterov.PNG" alt></p><h2 id="4-Adagrad"><a href="#4-Adagrad" class="headerlink" title="4. Adagrad"></a>4. Adagrad</h2><p>上述方法中，对于每一个参数$\theta_i$的训练都使用了相同的学习率α。Adagrad算法能够在训练中自动的对learning rate进行调整，对于出现频率较低参数采用较大的α更新；相反，对于出现频率较高的参数采用较小的α更新。因此，Adagrad非常适合处理稀疏数据。</p><p>我们设$g<em>{t,i}$为第t轮第i个参数的梯度，即$g</em>{t,i} = \nabla _{\Theta}J(\Theta_i)$。因此，SGD中参数更新的过程可写为：</p><script type="math/tex; mode=display">\Theta _{t+1, i} = \Theta _{t,i} - \alpha \cdot g_{t,i}</script><p>Adagrad在每轮训练中对每个参数$\theta _i$的学习率进行更新，参数更新公式如下：</p><script type="math/tex; mode=display">\Theta _{t+1, i} = \Theta _{t,i} - \frac{\alpha}{\sqrt{G_{t,ii} + \epsilon}} \cdot g_{t,i}</script><p>其中，$G_t \in \mathbb{R} ^{d \times d}$为对角矩阵，每个对角线位置$i,i$为对应参数$\theta_i$从第1轮到第t轮梯度的平方和。ϵ是平滑项，用于避免分母为0，一般取值1e−8。Adagrad的缺点是在训练的中后期，分母上梯度平方的累加将会越来越大，从而梯度趋近于0，使得训练提前结束。</p><h2 id="5-RMSprop"><a href="#5-RMSprop" class="headerlink" title="5. RMSprop"></a>5. RMSprop</h2><p>RMSprop是Geoff Hinton提出的一种自适应学习率方法。Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题。</p><script type="math/tex; mode=display">E[g^2]_t = 0.9E[g^2]_{t-1} + 0.1g_t^2</script><script type="math/tex; mode=display">\Theta_{t+1} = \Theta_t - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} \cdot g_t</script><h2 id="6-Adam"><a href="#6-Adam" class="headerlink" title="6. Adam"></a>6. Adam</h2><p>Adam(Adaptive Moment Estimation)是另一种自适应学习率的方法。它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下：</p><script type="math/tex; mode=display">m_t = \beta_1m_{t-1} + (1-\beta_1)g_t</script><script type="math/tex; mode=display">v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2</script><script type="math/tex; mode=display">\hat{m}_t = \frac{m_t}{1-\beta_1^t}</script><script type="math/tex; mode=display">\hat{v}_t = \frac{v_t}{1-\beta_2^t}</script><script type="math/tex; mode=display">\Theta_{t+1} = \Theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t</script><p>其中，$m_t, v_t$分别是对梯度的一阶矩估计和二阶矩估计，可以看作对期望$E[g_t], E[g_t^2]$的近似；$\hat{m}_t,\hat{v}_t$是对$m_t, v_t$的矫正，这样可以近似为对期望的无偏估计。Adam算法的提出者建议$\beta_1$的默认值为0.9，$\beta_2$的默认值为0.999，$\epsilon$的默认值为1e-8。另外，在数据比较稀疏的时候，adaptive的方法能得到更好的效果，例如Adagrad，RMSprop， Adam等。Adam方法比RMSprop方法收敛的结果好一些，所以在实际应用中，Adam是最为常用的算法，可以比较快的得到一个预估结果。</p><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><p>最后两张动图从直观上展现了算法的优化过程。第一张图为不同算法在损失平面等高线上随时间的变化情况，第二张图为不同算法在鞍点处的行为比较。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/优化方法总结/contours_evaluation_optimizers.gif" alt></p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/优化方法总结/saddle_point_evaluation_optimizers.gif" alt></p><blockquote><p>参考资料：</p><p><a href="https://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural Networks </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> optimization </tag>
            
            <tag> SGD </tag>
            
            <tag> Adam </tag>
            
            <tag> Momentum </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习debug沉思录</title>
      <link href="/2020/08/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0debug%E6%B2%89%E6%80%9D%E5%BD%95/"/>
      <url>/2020/08/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0debug%E6%B2%89%E6%80%9D%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a><strong>前言</strong></h2><p>接触深度学习也有一两年了，一直没有将一些实战经验整理一下形成文字。本文打算用来记录一些<strong>在深度学习实践中的调试过程</strong>，记录一些经验之谈。因为目前深度学习业界的理论基础尚且薄弱，很多工程实践中的问题没法用理论解释得很好，这里的只是实践中的一些经验之谈，以供参考以及排错。本文将持续更新。</p><p>需要强调的是，本文的很多单纯只是经验，在尽可能列出参考文献的同时却并无严格理论验证，希望大家见谅。欢迎大家集思广益，共同维护这个经验集，为整个社区贡献微弱力量。<a id="more"></a></p><h2 id="1、在分类问题中，损失函数及其快速得下降为0-0000"><a href="#1、在分类问题中，损失函数及其快速得下降为0-0000" class="headerlink" title="1、在分类问题中，损失函数及其快速得下降为0.0000"></a><strong>1、在分类问题中，损失函数及其快速得下降为0.0000</strong></h2><p>在分类问题中，我们一般采用的是交叉熵[1]作为损失函数，如式(1.1)所示</p><script type="math/tex; mode=display">L_{cls} = - \sum _{i=1} ^ n y_i\log \hat{y_i} = -\boldsymbol{y}^T\log \hat{\boldsymbol{y}}</script><p>其中和是预测结果，以概率分布的形式表达，如[0.2, 0.3, 0.3]等，一般是通过softmax层实现，和是样本真实标签，在单分类问题中，采用的是独热编码[2]，只有一个分量是为1的，如[0, 1, 0]。（公式第二行是向量化表达）</p><p><strong>我们发现，交叉熵损失的下确界是0，但是永远都不可能达到0</strong>，因为要达到0，那么所有的预测向量分布就必须完全和真实标签一致，退化为独热编码。但是实际上在神经网络中，经过了softmax层之后，是不可能使得除了目标分量的其他所有分量为0的（这个这里只是抛出了结论，讨论需要比较长的篇幅。），因此永远不可能达到0的，正是因为如此，交叉熵损失可以一直优化，这也是其比MSE损失优的一个点之一。</p><p>既然注意到了不可能为0，我们就可以分析，这肯定是自己程序问题，我们将经过softmax之前的logit打印出，如：</p><script type="math/tex; mode=display">[1023, -201, 1021, 124]</script><p>发现了没有，这些值都很大，而softmax函数为:</p><script type="math/tex; mode=display">P(x_i) = \frac{\exp (x_i)}{\sum _{i=1}^{n} \exp(x_i)}</script><p>我们会发现，过大或者过小的指数项，比如1023，会涉及到计算，这个数值在TensorFlow或者大部分框架中是溢出的，显示为inf，因此就会把该分量拉成1，而其他变成了0。这种操作是会导致严重的过拟合的。因此，一般来说，logit值不能太大，否则将会出现数值计算问题。</p><p><strong>那么如何解决？</strong> 出现这种问题的情况很多时候是因为<strong>参数初始化</strong>导致的数值计算问题，比如都采用了方差过小的高斯分布进行初始化，那么就会把网络的输出的范围拉的特别大，导致以上的问题。因此在参数初始化中，确保每一层的初始化都是在一定范围内的，可以考虑采用Xavier初始化，Kaiming初始化等。（这个初始化的影响我们将会以后讨论，这是一个新的话题。）</p><h2 id="2、在正则化的过程中对神经网络的偏置也进行了正则"><a href="#2、在正则化的过程中对神经网络的偏置也进行了正则" class="headerlink" title="2、在正则化的过程中对神经网络的偏置也进行了正则"></a><strong>2、在正则化的过程中对神经网络的偏置也进行了正则</strong></h2><p>一般来说，我们常用的是二范数正则，也即是岭回归，如式子(2.1)</p><script type="math/tex; mode=display">L = \gamma _w(y, \hat{y})  + \frac{1}{2} \boldsymbol{w}^T\boldsymbol{w}</script><p>一般来说，我们只会对神经网络的<strong>权值</strong>进行正则操作，使得权值具有一定的稀疏性[21]或者控制其尺寸，使得其不至于幅度太大[3]，减少模型的容量以减少过拟合的风险。同时，我们注意到神经网络中每一层的权值的作用是<strong>调节每一层超平面的方向</strong>（因为就是其法向量），因此只要比例一致，不会影响超平面的形状的。但是，我们必须注意到，每一层中的偏置是<strong>调节每一层超平面的平移长度的</strong>，如果你对偏置进行了正则，那么我们的可能就会变得很小，或者很稀疏，这样就导致你的每一层的超平面只能局限于很小的一个范围内，使得模型的容量大大减少，一般会导致欠拟合[7]的现象。</p><p><strong>因此，一般我们不会对偏置进行正则的，注意了。</strong></p><p>在Pytorch框架中，常用的方法为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">param_optimizer = list(self.model.named_parameters())</span><br><span class="line">no_decay = [<span class="string">'bias'</span>, <span class="string">'LayerNorm.bias'</span>, <span class="string">'LayerNorm.weight'</span>]</span><br><span class="line">optimizer_grouped_parameters = [</span><br><span class="line">&#123;<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimizer <span class="keyword">if</span> <span class="keyword">not</span> any(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: config.weight_decay&#125;,</span><br><span class="line">   &#123;<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimizer <span class="keyword">if</span> any(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: <span class="number">0.0</span>&#125;</span><br><span class="line">]</span><br><span class="line">self.optimizer = AdamW(optimizer_grouped_parameters, config.lr)</span><br></pre></td></tr></table></figure><h2 id="3、学习率太大导致不收敛"><a href="#3、学习率太大导致不收敛" class="headerlink" title="3、学习率太大导致不收敛"></a><strong>3、学习率太大导致不收敛</strong></h2><p>不收敛是个范围很大的问题，有很多可能性，其中有一种是和网络结构无关的原因，就是学习率设置的太大了，如下图所示，太大的学习率将会导致严重的抖动，使得无法收敛，<strong>甚至在某些情况下可能使得损失变得越来越大直到无穷</strong>。这个时候请调整你的学习率，尝试是否可以收敛。当然，这里的“太大”目前没有理论可以衡量，不过我喜欢从的Adam优化器[4]开始进行尝试优化。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/深度学习debug沉思录/3.1.PNG" alt="img"></p><p>下图展示了过大过小的学习率对模型性能的影响曲线图：</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/深度学习debug沉思录/3.2.PNG" alt="img"></p><h2 id="4、别在softmax层前面的输入施加了激活函数"><a href="#4、别在softmax层前面的输入施加了激活函数" class="headerlink" title="4、别在softmax层前面的输入施加了激活函数"></a><strong>4、别在softmax层前面的输入施加了激活函数</strong></h2><p>假设我们的网络提取出来的最后的特征向量是，如果我们最后的分类的类别有类，那么我们会用一个全连接层将其映射到对应维度的空间里面，如式(4.2)。</p><script type="math/tex; mode=display">\boldsymbol{y} = g_w(\widetilde{\boldsymbol{y}})</script><p>那么，这个全连接层虽然说可以看成是分类器，但是我们最好把它看成是上一层的“近线性可分特征”的一个维度转换（有点绕，意思是我们这里只是一个维度的转换，而不涉及到kernel），不管怎么说，这个时候，我们的输出是不能有激活函数的，如下式是<strong>不可以的</strong>：</p><script type="math/tex; mode=display">\boldsymbol{y} = \sigma( g_w(\widetilde{\boldsymbol{y}}))</script><p>这时候的输出，具有和分类类别相同的维度，在很多框架中被称之为<strong>logits</strong>值，这个值一般是在实数范围内的，一般不会太大，参考笔记第一点的情况。</p><h2 id="5、检查原数据输入的值范围"><a href="#5、检查原数据输入的值范围" class="headerlink" title="5、检查原数据输入的值范围"></a><strong>5、检查原数据输入的值范围</strong></h2><p>原始数据输入可能千奇百怪，每个特征维的值范围可能有着数量级上的差别，这个时候如果我们不对数据进行预处理，将会大大增大设计网络的负担。一般来说我们希望输入的数据是中心对齐的，也即是<strong>0均值</strong>的[5]，可以加速网络收敛的速度。同时，我们希望不同维度上的数值范围是一致的，可以采用一些归一化[6]的手段进行处理（这个时候假设每个维度重要性是一样的，比如我们图片的三个通道等）。</p><h2 id="6、别忘了对你的训练数据进行打乱"><a href="#6、别忘了对你的训练数据进行打乱" class="headerlink" title="6、别忘了对你的训练数据进行打乱"></a><strong>6、别忘了对你的训练数据进行打乱</strong></h2><p>经常，你的训练过程非常完美，能够很好地拟合训练数据，但是在测试过程中确实一塌糊涂，是的，你的模型这个时候过拟合[7]了。这个时候你会检查模型的有效性，不过在进行这一步之前，不妨先检查下你的<strong>数据加载器</strong>(Data Loader)是否是正常设计的。</p><p>一般来说，我们的训练数据在训练过程中，每一个epoch[8]中，都是需要进行<strong>打乱</strong>(shuffle)的，很多框架的数据加载器参数列表中都会有这项选项，比如pytorch的DataLoader类[9]。为什么需要打乱呢？那是因为如果不打乱我们的训练数据，我们的模型就有可能学习到训练数据的个体与个体之间特定的排列顺序，而这种排列顺序，在很多情况下是无用的，会导致过拟合的糟糕现象。因此，我们在训练过程中，在每一个epoch训练中都对训练集进行打乱，以确保模型不能“记忆”样本之间的特定排序。这其实也是<strong>正则</strong>的一种手段。</p><p>在训练中，大概如：</p><script type="math/tex; mode=display">epoch1 -> shuffle -> eposh2 -> shuffle ...</script><h2 id="7、一个batch中，label不要全部相同"><a href="#7、一个batch中，label不要全部相同" class="headerlink" title="7、一个batch中，label不要全部相同"></a><strong>7、一个batch中，label不要全部相同</strong></h2><p>这个情况有点类似与笔记的第六点，我们需要尽量给训练过程中人为引入不确定性，这是很多正则手段，包括dropout，stochastic depth等的思路，这样能够有效地减少过拟合的风险。因此，一个batch中，尽量确保你的样本是来自于各个类的（针对分类问题而言），这样你的模型会减少执着与某个类别的概率，减少过拟合风险，同时也会加快收敛速度。</p><h2 id="8、少用vanilla-SGD优化器"><a href="#8、少用vanilla-SGD优化器" class="headerlink" title="8、少用vanilla SGD优化器"></a><strong>8、少用vanilla SGD优化器</strong></h2><p>在高维度情况下的优化，其优化平面会出现很多鞍点（既是梯度为0，但却不是极点），通常，鞍点会比局部极值更容易出现（直观感受就是，因为高维度情况下，一个点周围有很多维度，如果是极值点，那么就需要其他所有维度都是朝向同一个方向“弯曲”的，而这个要比鞍点的各个方向“弯曲”的情况可能要小），因此这个时候我们更担心陷于鞍点，而不是局部极小值点（当然局部极小值点也是一个大麻烦，不过鞍点更麻烦）。如果采用普通的SGD优化器，那么就会陷于任何一个梯度为0的点，也就是说，极有可能会陷于鞍点。如果要使用SGD方法，建议使用带有momentum的SGD方法，可以有效避免陷入鞍点的风险。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/深度学习debug沉思录/8.1.PNG" alt="img"></p><p>下图是某个函数的三维曲线图和等高线图，我们可以看到有若干个局部最优点和鞍点，这些点对于vanilla SGD来说是不容易处理的。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/深度学习debug沉思录/8.2.PNG" alt="img"></p><h2 id="9、检查各层梯度，对梯度爆炸进行截断"><a href="#9、检查各层梯度，对梯度爆炸进行截断" class="headerlink" title="9、检查各层梯度，对梯度爆炸进行截断"></a><strong>9、检查各层梯度，对梯度爆炸进行截断</strong></h2><p>有些时候，你会发现在训练过程中，你的损失突然变得特别大，或者特别小，这个时候不妨检查下每一层的梯度（用tensorboard的distribution可以很好地检查），很可能是发生了<strong>梯度爆炸</strong>(gradient explosion)的情况，特别是在存在LSTM等时序的网络中，很容易出现这种情况。因此，这个时候我们会用梯度截断进行处理，操作很简单粗暴，就是设置一个阈值，把超过这个阈值的梯度全部拉到这个阈值，如下图所示：</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/深度学习debug沉思录/9.1.PNG" alt="img"></p><p>在tensorflow中也提供了相应的API供梯度截断使用[10]，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.clip_by_value(    </span><br><span class="line">    t,    </span><br><span class="line">clip_value_min, <span class="comment"># 指定截断最小值    </span></span><br><span class="line">clip_value_max, <span class="comment"># 指定截断最大值    </span></span><br><span class="line">name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>具体使用见[11]，在应用梯度之前，对梯度截断进行处理。</p><h2 id="10、检查你的样本label"><a href="#10、检查你的样本label" class="headerlink" title="10、检查你的样本label"></a><strong>10、检查你的样本label</strong></h2><p>有些时候，你的训练过程可以很好地收敛，当使用MSE损失[12]的时候甚至可能达到0.0000的情况。但是，当你把模型拿到测试集中评估的时候，却发现性能极差，仿佛没有训练一样。这是过拟合吗？显然是的，但是这可能并不是你的模型的问题，请检查你的数据加载中训练集的样本标签是否正确对应。</p><p>这个问题很白痴，但是却真的很容易在数据加载过程中因为种种原因把label信息和对应样本给混掉。根据文献[13]中的实验，用MSE损失的情况下，就算是你的label完全随机的，和样本一点关系都没有，也可以通过基于SGD的优化算法达到0.0000损失的。<strong>因此，请务必确保你的样本label是正确的。</strong></p><h2 id="11、分类问题中的分类置信度问题"><a href="#11、分类问题中的分类置信度问题" class="headerlink" title="11、分类问题中的分类置信度问题"></a><strong>11、分类问题中的分类置信度问题</strong></h2><p>在分类问题中我们一般都是采用的是交叉熵损失，如式子(1.1)所示，在一些实验中，如果我们绘制出训练损失和分类准确度的曲线图，我们可能会有下图这种情况[14]：</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/深度学习debug沉思录/11.1.PNG" alt="img"></p><p>其中上图为分类损失，紫色为训练损失，蓝色为测试损失，下图为分类准确度，绿色为训练准确度，蓝色为测试准确度。<strong>我们不难发现一个比较有意思的现象，就是当测试损失开始到最低点，开始向上反弹的时候，其测试准确度却还是上升的，而不是下降。</strong> 这是为什么呢？为什么分类准确度不会顺着分类损失的增大而减少呢？</p><p>这个涉及到了分类过程中对某个类的“置信程度”的多少，比如：</p><script type="math/tex; mode=display">[0.9, 0.01, 0.01, 0.07]</script><p>模型是对第一类相当确信的，但是在第二种情况：</p><script type="math/tex; mode=display">[0.5, 0.4, 0.05, 0.05]</script><p>这对第一类的置信程度就很低了，虽然按照贝叶斯决策，还是会选择第一类作为决策结果。因此这就是导致以上现象的原因，<strong>在那个拐点后面，这个模型对于分类的置信程度其实已经变得很差了，虽然对于准确度而言，其还能分类正确。</strong> 但是这其实正是过拟合的一种表现，模型已经对自己的分类结果不确信了。</p><h2 id="12、少在太小的批次中使用BatchNorm层"><a href="#12、少在太小的批次中使用BatchNorm层" class="headerlink" title="12、少在太小的批次中使用BatchNorm层"></a><strong>12、少在太小的批次中使用BatchNorm层</strong></h2><p>Batch Normalization[15]，中文译作<strong>批规范化</strong>，在深度学习中是一种加快收敛速度，提高性能的一个利器，其本质和我们对输入的原数据进行0均值单位方差规范化差不多，是以batch为单位，对中间层的输出进行规范化，可以缓和<strong>内部协方差偏移</strong>(Internal Covariate Shift)的现象。其基本公式很简单，如下：</p><script type="math/tex; mode=display">\widetilde{x} = \frac{x_i - \mu}{\sigma_i}</script><script type="math/tex; mode=display">x_i^{norm} = \gamma_i \cdot \widetilde{x} + \beta_i</script><p>不过这里并不打算对BN进行详细讲解，只是想告诉大家，因为BN操作在训练过程中是对每个batch进行处理的，从每个batch中求得均值和方差才能进行操作。如果你的batch特别小（比如是受限于硬件条件或者网络要求小batch），那么BN层的batch均值和方差可能就会不能很好符合整个训练集的统计特征，导致差的性能。实际上，实验[16]说明了这个关系，当batch小于16时，性能大幅度下降。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/深度学习debug沉思录/12.1.PNG" alt="img"></p><p><strong>因此，少在太小的batch中使用BN层，如果实在要使用，在发生性能问题时优先检查BN层。</strong></p><h2 id="13、数值计算问题，出现Nan"><a href="#13、数值计算问题，出现Nan" class="headerlink" title="13、数值计算问题，出现Nan"></a><strong>13、数值计算问题，出现Nan</strong></h2><p>Nan(Not An Number)是一个在数值计算中容易出现的问题，在深度学习中因为涉及到很多损失函数，有些损失函数的定义域并不是整个实数，比如常用的对数，因此一不小心就会出现Nan。在深度学习中，如果某一层出现了Nan，那么是具有传递性的，后面的层也会出现Nan，因此可以通过<strong>二分法</strong>对此进行排错。</p><p>一般来说，在深度学习中出现Nan是由于除0异常或者是因为损失函数中的（比如交叉熵，KL散度）对数操作中，输入小于或者等于0了，一般等于0的情况比较多，因此通常会：</p><script type="math/tex; mode=display">\log (x + \epsilon) \thickapprox \log(x)</script><p>这里的是个很小的值，一般取即可，可以防止因为对数操作中输入0导致的Nan异常。</p><p><strong>需要注意的是，有些时候因为参数初始化或者学习率太大也会导致数值计算溢出，这也是会出现Nan的，一般这样会出现在较前面的层里面。</strong></p><h2 id="14、BN层放置的位置问题"><a href="#14、BN层放置的位置问题" class="headerlink" title="14、BN层放置的位置问题"></a><strong>14、BN层放置的位置问题</strong></h2><p>BN层有两种常见的放置位置，如下图所示：<strong>第一个是放在激活函数之前：</strong></p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/深度学习debug沉思录/14.1.PNG" alt="img"></p><p><strong>第二个是放在激活函数之后：</strong></p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/深度学习debug沉思录/14.2.PNG" alt="img"></p><p>在原始BN的论文[15]中，Batch Norm(BN)层是位于激活层之前的，因为是对原始的，未经过激活的logit数据进行数据分布的重整。然而，不少实验证实似乎BN层放在<strong>激活层之后效果会更好</strong>，这个原因目前不明。<strong>Update 2020/5/18</strong>: 在新的文献[28]中，作者尝试解释了以下BN用法的原因，有兴趣的读者可以移步去细读下。</p><p><strong>传统用法:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR   </span><br><span class="line">weights --&gt; BatchNorm   </span><br><span class="line">BatchNorm --&gt; ReLU</span><br></pre></td></tr></table></figure><p><strong>[28]的作者提出的用法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR   </span><br><span class="line">ReLU --&gt; BatchNorm+dropout </span><br><span class="line">BatchNorm+dropout --&gt; weights</span><br></pre></td></tr></table></figure><h2 id="15、dropout层应用在卷积层中可能导致更差的性能"><a href="#15、dropout层应用在卷积层中可能导致更差的性能" class="headerlink" title="15、dropout层应用在卷积层中可能导致更差的性能"></a><strong>15、dropout层应用在卷积层中可能导致更差的性能</strong></h2><p>dropout[19]是hinton大神与2012年提出的一种神经网络正则手段，其可以简单解释为<strong>在训练过程中，按一定概率让神经网络中的某些神经元输出为0</strong>，其原因可以有几个解释，一个是作为一种集成模型进行解释，另一个可以看成是在特征提取学习过程中给数据加入噪声，可以看成是一种数据增强的正则手段。</p><p>在原始论文中，dropout被应用于全连接层中，而没有应用在卷积层中，Hinton的解释是因为卷积层参数并不多，过拟合风险较小不适合采用dropout这种大杀器的正则手段。有人也认为因为卷积网络是局部感知的，用dropout正则对于其在后层中对于全局信息的获取可能具有负作用[20]。</p><p>不过在一些工作中，也有人将dropout层应用在卷积层中的[17-18]，其层次安排为:$CONV -&gt; RELU -&gt; DROP$，不过其丢弃率都是选择的较小数如0.1, 0.2等，个人觉得这里的作用大概是对<strong>中间数据进行加入噪声，以便于数据增强的正则手段。</strong></p><p><strong>个人建议是可以尝试在卷积层中使用少量的dropout，用较小的丢弃率，但是最后别忘了扔掉这些dropout再进行一些探索，也许可以具有更好的效果。</strong></p><h2 id="16、较小的batch-size可以提供较好的泛化"><a href="#16、较小的batch-size可以提供较好的泛化" class="headerlink" title="16、较小的batch size可以提供较好的泛化"></a><strong>16、较小的batch size可以提供较好的泛化</strong></h2><p>现代的深度学习优化器基本上都是基于SGD算法进行修改而成的，在每一次训练中都是以一个batch size为单位进行训练的，在这个过程中相当于在统计这个batch中样本的一些统计特性，因此batch size是会影响模型的超曲线形状的。</p><p>一般来说较大的batch size比如128，256会和整个训练集的统计性质更相近，从而使得具有较少的多样性，而较小的batch size 比如16，32，因为batch size较小，不同batch之间的差异性较大，这种差异性可以看成是正则手段，有机会提高模型的泛化性能。（不过有些文章似乎不同意这个观点，认为较大batch size有较好性能，<strong>个人建议是大batch size和小batch size都可以跑跑，有可能能提升性能。</strong>）</p><h2 id="17、初始化权值不能初始化为全0"><a href="#17、初始化权值不能初始化为全0" class="headerlink" title="17、初始化权值不能初始化为全0"></a><strong>17、初始化权值不能初始化为全0</strong></h2><p>这个应该是老生常谈了，但是初学者经常会出现这种错误，在初始化权值的时候将权值全部初始化为了0，在反向传播的过程中，对于某个权值的更新公式为[22]：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial w_{i,j}^l} = a_k^{l-1} * \delta_j^l = a_k^{l-1} * \sum (\delta_k^{l+1} * w_{k,j}^{l+1} * \sigma^{'}(z_j^l))</script><p>这个公式推导具体参见[22]，这里不累述了，我们可以发现，当初始化权值参数全部为0的时候，我们的将全部为0，这个时候对于某个权值的梯度也就变为了0，<strong>因此整个网络的任何参数都得不到更新，将会导致训练无法进行。</strong></p><p>而<strong>对于偏置的初始化不同</strong>，对于偏置的更新公式如[22]：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial b_j^l} = \delta _j^l * 1 = \delta _j ^l</script><p>我们可以发现，对于偏置的更新而言，不依赖与初始值，因此<strong>偏置的初始化可以初始化为全0</strong>。</p><h2 id="18、别忘了你的偏置"><a href="#18、别忘了你的偏置" class="headerlink" title="18、别忘了你的偏置"></a><strong>18、别忘了你的偏置</strong></h2><p>这个也是初学者很容易犯的错误，就是忘记给每一层添加偏置。我们在笔记第二点中提到了神经网络中偏置的作用，总的来说就是对超平面进行平移的，因此一般来说，我们的神经网络都是需要添加偏置的，不然你的超平面就只能是通过原点的了，这样大大减少了模型的容量，经常会使得模型欠拟合。</p><h2 id="19、验证准确率远大于测试准确率"><a href="#19、验证准确率远大于测试准确率" class="headerlink" title="19、验证准确率远大于测试准确率"></a><strong>19、验证准确率远大于测试准确率</strong></h2><p>有些时候，你发现你的验证集准确率远大于测试集的准确率，在排除了代码问题和操作问题之后，其实也可能是因为训练集和测试集划分的问题。一般来说，你的验证集是从训练集中划分出来的[23]，因此你的验证集和训练集可以视为是同分布的，但是并不能确保你的训练集和测试集是同分布的，如果训练集和测试集的分布差的比较大，就可能出现这种情况。这个时候，可以考虑迁移学习中的一些方法。</p><h2 id="20、KL散度出现负数"><a href="#20、KL散度出现负数" class="headerlink" title="20、KL散度出现负数"></a><strong>20、KL散度出现负数</strong></h2><p>Kullback–Leibler散度，简称KL散度[24]，也称为相对熵，是一种用于<strong>度量两个分布之间相似性</strong>的常用手段，公式如(20.1)，其中第二行形式的变形描述了相对熵的特性。</p><script type="math/tex; mode=display">D_{KL} (P||Q) = \sum _i P(i)\log(\frac{P(i)}{Q(i)}) = \sum_i\{P(i)\log P(i) - P(i)\log Q(i)\}</script><p>我们注意到KL散度是不可能为负数的，其中P, Q是定义在同一个概率空间[25]里面的同型的分布，维度相同。从相对熵的定义来看，这个公式描述了用分布去近似所造成的不一致性的程度。在深度学习和机器学习中，一般是用来描述两个维度相同的<strong>概率分布</strong>之间的相似度。注意到，这里的P, Q都是概率分布，因此是需要经过<code>softmax</code>层的，才能保证概率和为1，不然可能会出现KL散度为负数的笑话。</p><p>而且，在一些框架如Pytorch中，其输入值需要是<code>log_softmax</code>而目标值需要是<code>softmax</code>值，也就说输入值需要进行对数操作后再转变为概率分布[27]。</p><p><strong>参考资料</strong></p><p>[1] Janocha K, Czarnecki W M. On loss functions for deep neural networks in classification[J]. arXiv preprint arXiv:1702.05659, 2017.(Overview about loss function used in DNN)</p><p>[2] tf.one_hot()进行独热编码</p><p>[3] 曲线拟合问题与L2正则</p><p>[4] Kinga D, Adam J B. A method for stochastic optimization[C]//International Conference on Learning Representations (ICLR). 2015, 5.</p><p>[5] 深度学习系列：深度学习中激活函数的选择</p><p>[6] 机器学习之特征归一化（normalization）</p><p>[7] 机器学习模型的容量，过拟合与欠拟合</p><p>[8] 在机器学习中epoch, iteration, batch_size的区别</p><p>[9] Pytorch Dataloader doc</p><p>[10] tf.clip_by_value</p><p>[11] 梯度截断的tensorflow实现</p><p>[12] 均方误差(MSE)和均方根误差(RMSE)和平均绝对误差(MAE)</p><p>[13]. Du S S, Zhai X, Poczos B, et al. Gradient Descent Provably Optimizes Over-parameterized Neural Networks[J]. arXiv preprint arXiv:1810.02054, 2018.</p><p>[14] The inconsistency between the loss curve and metric curve?</p><p>[15] Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift[C]// International Conference on International Conference on Machine Learning. JMLR.org, 2015:448-456.</p><p>[16] Wu Y, He K. Group normalization[J]. arXiv preprint arXiv:1803.08494, 2018.</p><p>[17] Park S, Kwak N. Analysis on the dropout effect in convolutional neural networks[C]//Asian Conference on Computer Vision. Springer, Cham, 2016: 189-204.</p><p>[18] Where should I place dropout layers in a neural network?</p><p>[19] Hinton G E, Srivastava N, Krizhevsky A, et al. Improving neural networks by preventing co-adaptation of feature detectors[J]. arXiv preprint arXiv:1207.0580, 2012.</p><p>[20] Why do I never see dropout applied in convolutional layers?</p><p>[21] L1正则</p><p>[22] 深度学习系列：反向传播算法的公式推导</p><p>[23] 训练集、测试集、检验集的区别与交叉检验</p><p>[24] Kullback–Leibler divergence</p><p>[25] Probability space</p><p>[26] 如何理解K-L散度（相对熵）</p><p>[27] KL Divergence produces negative values</p><p>[28] Chen G, Chen P, Shi Y, et al. Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks[J]. arXiv preprint arXiv:1905.05928, 2019.</p><p>[29] <a href="https://forums.fast.ai/t/rethinking-batchnorm-dropout-combine-together-for-independent-component-layer/46232" target="_blank" rel="noopener">https://forums.fast.ai/t/rethinking-batchnorm-dropout-combine-together-for-independent-component-layer/46232</a></p><p>作者丨土豆@知乎</p><p>来源丨<a href="https://zhuanlan.zhihu.com/p/158739701" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/158739701</a></p>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 调参 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 炼丹 </tag>
            
            <tag> 经验 </tag>
            
            <tag> 技巧 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP数据增强技术</title>
      <link href="/2020/07/05/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/"/>
      <url>/2020/07/05/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<p>对于CV领域，数据增强技术已经成为一种标准操作被广泛使用，例如旋转、镜像、添加高斯白噪声等。</p><p>但是，对于NLP领域，数据增强技术却不是那么多见，很多CV领域的技术不能直接用于文本数据。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/CVvsNLP.PNG" alt></p><p>因此，本文对常见的NLP领域数据增强技术进行汇总。<a id="more"></a></p><h2 id="1-词法层级替换"><a href="#1-词法层级替换" class="headerlink" title="1. 词法层级替换"></a>1. 词法层级替换</h2><h3 id="1-1-同义词替换"><a href="#1-1-同义词替换" class="headerlink" title="1.1 同义词替换"></a>1.1 同义词替换</h3><p>在文本中随机抽取一个单词，将其替换为同义词。例如，利用WordNet数据库，将[awesome]替换为[amazing]</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/wordnet.PNG" alt></p><p>这个技术比较常见，在以往的很多论文中都用到了这个技术，例如：</p><p>Zhang et al. [Character-level Convolutional Networks for Text Classification]</p><blockquote><p><a href="https://arxiv.org/abs/1509.01626" target="_blank" rel="noopener">https://arxiv.org/abs/1509.01626</a></p></blockquote><p>Wei et al. 「EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks」</p><blockquote><p>论文链接：<a href="https://arxiv.org/abs/1901.11196" target="_blank" rel="noopener">https://arxiv.org/abs/1901.11196</a></p></blockquote><p>另外，有一个<a href="http://paraphrase.org/#/download" target="_blank" rel="noopener">PPDB数据库</a>，是一个包含着数百万个单词的词库，但遗憾的是不支持中文。</p><h3 id="1-2-词嵌入替换"><a href="#1-2-词嵌入替换" class="headerlink" title="1.2 词嵌入替换"></a>1.2 词嵌入替换</h3><p>这种方法是，采取已经预训练好的单词嵌入，如Word2Vec、GloVe、FastText、Sent2Vec等，并将嵌入空间中最近的邻接词作为句子中某些单词的替换。</p><p>比如：</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/embedding1.PNG" alt></p><p>这样，就可以将单词替换成临近的3个单词，获得文本的3种变体形式。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/embedd.PNG" alt></p><h3 id="1-3-掩码语言模型（MLM）"><a href="#1-3-掩码语言模型（MLM）" class="headerlink" title="1.3 掩码语言模型（MLM）"></a>1.3 掩码语言模型（MLM）</h3><p>类似于BERT、ROBERTA、ALBERT，Transformer模型已经在大量的文本训练过，使用掩码语言模型的前置任务。在这个任务中，模型必须依照上下文来预测掩码的单词。此外，还可以利用这一点，对文本进行扩容。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/bert1.PNG" alt></p><p>跟之前的方法相比，生成的文本在语法上会更加连贯。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/bert2.PNG" alt></p><p>但是，需要注意的是，决定掩盖哪一个单词并非易事，它决定了效果的最终呈现。</p><h3 id="1-4-基于TF-IDF的单词替换"><a href="#1-4-基于TF-IDF的单词替换" class="headerlink" title="1.4 基于TF-IDF的单词替换"></a>1.4 基于TF-IDF的单词替换</h3><p>这一方法最初是出现在Xie et al.「Unsupervised Data Augmentation for Consistency Training」。</p><blockquote><p>论文链接：<a href="https://arxiv.org/abs/1904.12848" target="_blank" rel="noopener">https://arxiv.org/abs/1904.12848</a></p></blockquote><p>基本思路在于TF-IDF得分低的单词是没有信息量的的词，因此可以替换，而不影响句子的原本含义。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/tfidf.PNG" alt></p><p>通过计算整个文档中单词的 TF - IDF得分并取最低得分来选择替换原始单词的单词。</p><h2 id="2-反向翻译"><a href="#2-反向翻译" class="headerlink" title="2. 反向翻译"></a>2. 反向翻译</h2><p>反向翻译，就是先将句子翻译成另一种语言，比如，英语翻译成法语。</p><p>然后再翻译回原来的语言，也就是将法语翻译回英语。</p><p>检查两个句子之间的不同之处，由此将新的句子作为增强文本。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/machinetrans1.PNG" alt></p><p>还可以一次使用多种语言进行反向翻译，产生更多的变体。</p><p>比如，除了法语以外，再将其翻译为汉语和意大利语。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/mt2.PNG" alt></p><p>要实现反向翻译，可以使用TextBlob。另外，还可以使用Google Sheets，说明书已附文末。</p><h2 id="3-文本形式转换"><a href="#3-文本形式转换" class="headerlink" title="3. 文本形式转换"></a>3. 文本形式转换</h2><p>这一方法主要是利用正则表达式应用的的简单模式匹配转换，在Claude Coulombe的论文「Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs」中有详细介绍。</p><blockquote><p>论文链接：<a href="https://arxiv.org/abs/1812.04718" target="_blank" rel="noopener">https://arxiv.org/abs/1812.04718</a></p></blockquote><p>举个简单的例子，将原本形式转换为缩写，反之亦然。但是将缩写扩充，容易带来错误，例如she’s可以是she has，也可以是she is，引入歧义。所以一般的只对文本进行缩写，不做扩充操作。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/textSurfaceTransformation.PNG" alt="img"></p><p>Python的文本形式收缩库已附文末。</p><h2 id="3-随机噪声注入"><a href="#3-随机噪声注入" class="headerlink" title="3. 随机噪声注入"></a>3. 随机噪声注入</h2><p>顾名思义，也就是在文本中注入噪声，来训练模型对扰动的鲁棒性。</p><p>比如，拼写错误。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/noise1.PNG" alt="img"></p><p>句子改组。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/noise2.PNG" alt="img"></p><p>空白噪声。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/noise3.PNG" alt="img"></p><p>随机插入。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/noise4.PNG" alt="img"></p><p>随机交换。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/noise5.PNG" alt="img"></p><p>随机删除。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/noise6.PNG" alt="img"></p><h2 id="4-语法树"><a href="#4-语法树" class="headerlink" title="4. 语法树"></a>4. 语法树</h2><p>这一方法也出现在了Claude Coulombe的论文「Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs」中。</p><blockquote><p>论文链接：<a href="https://arxiv.org/abs/1812.04718" target="_blank" rel="noopener">https://arxiv.org/abs/1812.04718</a></p></blockquote><p>其思路是解析并生成原句的从属树，利用规则进行转换，生成新句子。</p><p>比如，将句子的主动语气转换为被动语气，反之亦然。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/tree.PNG" alt="img"></p><h2 id="5-文本混合"><a href="#5-文本混合" class="headerlink" title="5. 文本混合"></a>5. 文本混合</h2><p>这项技术的想法源于一项名为“Mixup”的图像增强技术。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/mix1.PNG" alt="img"></p><p>Guo et al.在此基础上进行了修改，将其应用到NLP。</p><p>「Augmenting Data with Mixup for Sentence Classification: An Empirical Study」</p><blockquote><p>论文链接：<a href="https://arxiv.org/abs/1905.08941" target="_blank" rel="noopener">https://arxiv.org/abs/1905.08941</a></p></blockquote><p>主要有两种方法。</p><h3 id="5-1-wordMixup"><a href="#5-1-wordMixup" class="headerlink" title="5.1 wordMixup"></a>5.1 wordMixup</h3><p>这个方法在于，抽取两个随机的句子，将它们进行零填充，使其长度相同。然后，按一定比例组合在一起。</p><p>所得到的单词嵌入通过CNN/LSTM编码器传递到句子嵌入中，随后计算交叉熵损失。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/mix2.PNG" alt="img"></p><h3 id="5-2-sentenceMixup"><a href="#5-2-sentenceMixup" class="headerlink" title="5.2 sentenceMixup"></a>5.2 sentenceMixup</h3><p><img src="https://raw.githubusercontent.com/Hao-Kailong/blog-image/master/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF/mix3.PNG" alt="img"></p><p>可以看到这一方法，与上述方法类似，只不过在具体步骤上有所调整。</p><h2 id="传送门"><a href="#传送门" class="headerlink" title="传送门"></a>传送门</h2><p><strong>原文博客地址：</strong><br><a href="https://amitness.com/2020/05/data-augmentation-for-nlp/" target="_blank" rel="noopener">https://amitness.com/2020/05/data-augmentation-for-nlp/</a></p><p>WordNet数据集：<br><a href="https://www.nltk.org/howto/wordnet.html" target="_blank" rel="noopener">https://www.nltk.org/howto/wordnet.html</a></p><p>TextBlob API：<br><a href="https://textblob.readthedocs.io/en/dev/quickstart.html#wordnet-integration" target="_blank" rel="noopener">https://textblob.readthedocs.io/en/dev/quickstart.html#wordnet-integration</a></p><p>PPDB数据集：<br><a href="http://paraphrase.org/#/download" target="_blank" rel="noopener">http://paraphrase.org/#/download</a></p><p>TF-IDF代码：<br><a href="https://github.com/google-research/uda/blob/master/text/augmentation/word_level_augment.py" target="_blank" rel="noopener">https://github.com/google-research/uda/blob/master/text/augmentation/word_level_augment.py</a></p><p>使用Google Sheets实现反向翻译：<br><a href="https://amitness.com/2020/02/back-translation-in-google-sheets/" target="_blank" rel="noopener">https://amitness.com/2020/02/back-translation-in-google-sheets/</a></p><p>Python收缩库：<br><a href="https://github.com/kootenpv/contractions" target="_blank" rel="noopener">https://github.com/kootenpv/contractions</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> data augmentation </tag>
            
            <tag> Mechine Learning </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 数据增强 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Normalization神经网络中的正则化</title>
      <link href="/2020/04/20/Normalization%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/"/>
      <url>/2020/04/20/Normalization%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>在神经网络中经常需要用到正则化方法，来加速神经网络的训练以及减少震荡等</p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>对每个batch中同一维度的特征做normalization</p><h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>针对单个训练样本的所有维度做normalization。之前的BN是纵向规范化，而layer normalization是横向规范化</p><h2 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a>Instance Normalization</h2><p>与BN只有一点不同，BN是作用于一个batch，而IN是作用于单个样本</p><h2 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h2><p>划分区域，然后进行normalization<a id="more"></a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>用一张图概括如下为：</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/BlogImage/master/Normalization.webp" alt></p><p>N代表batch_size，C代表channels，H,W分别代表height, weight。对于NLP应用中，只需要将H,W作为seq_len理解即可。</p>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> 正则化 </tag>
            
            <tag> Normalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用CCproxy使实验室服务器联网</title>
      <link href="/2020/03/09/%E4%BD%BF%E7%94%A8CCproxy%E4%BD%BF%E5%AE%9E%E9%AA%8C%E5%AE%A4%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%81%94%E7%BD%91/"/>
      <url>/2020/03/09/%E4%BD%BF%E7%94%A8CCproxy%E4%BD%BF%E5%AE%9E%E9%AA%8C%E5%AE%A4%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%81%94%E7%BD%91/</url>
      
        <content type="html"><![CDATA[<p>实验室的服务器是不能联网的，但有时需要通过pip, conda命令下载一些包。经过师兄的建议，使用CCproxy软件，用自己的可以联网的windows台式机作为代理，成功的让服务器可以使用conda install 命令了！这里记录一下过程。</p><p>首先，下载并安装CCproxy软件，官网地址为：<a href="http://www.ccproxy.com/" target="_blank" rel="noopener">www.ccproxy.com</a><a id="more"></a></p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/BlogImage/master/CCproxy/界面.PNG" alt></p><p>然后，点击设置，进行协议端口的配置。我们实验室里的服务器开放了808端口，所以HTTP端口保持不变就可以。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/BlogImage/master/CCproxy/%E8%AE%BE%E7%BD%AE.PNG" alt></p><p><strong>注意自动检测那里</strong>，由于我的台式机上安装了docker, vmware，所以自动检测的ip地址是错误的，这让我苦恼了很长时间，最后才发现是这里出了bug！可以取消自动检测，手动选择正确的地址，可以通过ipconfig命令查看真正的ip地址。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/BlogImage/master/CCproxy/ipconfig.PNG" alt></p><p>配置完成后，在服务器端，使用export命令临时修改代理地址：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export http_proxy=http://114.212.87.108:808</span><br><span class="line">export https_proxy=http://114.212.87.108:808</span><br></pre></td></tr></table></figure><p>这样，就可以上网了，检测一下：</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/BlogImage/master/CCproxy/wget.PNG" alt></p><p>成功！这样就可以通过conda、pip方便的安装需要的包了。CCproxy还有一些高级的设置，但是我这里没有用到，可以参考官方的用户手册<a href="http://www.ccproxy.com/manual.htm" target="_blank" rel="noopener">http://www.ccproxy.com/manual.htm</a>进行配置修改</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代理 </tag>
            
            <tag> 服务器 </tag>
            
            <tag> 网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>测试MathJax显示数学公式</title>
      <link href="/2020/03/05/%E6%B5%8B%E8%AF%95MathJax%E6%98%BE%E7%A4%BA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
      <url>/2020/03/05/%E6%B5%8B%E8%AF%95MathJax%E6%98%BE%E7%A4%BA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<script type="math/tex; mode=display">y_t = softmax(W^{(a)}a_t)</script><script type="math/tex; mode=display">a_t = GRU([y_{t-1}, q],a_{t-1})</script><script type="math/tex; mode=display">o = \sum_ip_{h_i}A\theta_V(v_{h_i})</script><p>是否可以正常显示常用的数学符号？</p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>词形还原、中文分词和句法分析Demo</title>
      <link href="/2019/11/16/%E5%BD%A2%E6%80%81%E8%BF%98%E5%8E%9F%E3%80%81%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%92%8C%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90Demo/"/>
      <url>/2019/11/16/%E5%BD%A2%E6%80%81%E8%BF%98%E5%8E%9F%E3%80%81%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%92%8C%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90Demo/</url>
      
        <content type="html"><![CDATA[<p>在学校的自然语言处理课程中，需要完成几个小实验，采用传统方法，下面给出解释：</p><h2 id="词形还原"><a href="#词形还原" class="headerlink" title="词形还原"></a>词形还原</h2><p>使用<a href="http://nlp.nju.edu.cn/MT_Lecture/dic_ec.rar" target="_blank" rel="noopener">这个词典</a>，对输入的单词进行词形还原，也可以叫做提取词干</p><p>算法设计如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.输入一个单词</span><br><span class="line">2.词典中含有该词，输出其属性，转4</span><br><span class="line">3.如果含有该词的还原规则，并且词典中含有还原后的词，输出属性；否则，寻找该词的最近邻，输出属性</span><br><span class="line">4.输入中还有单词，转1；否则结束</span><br></pre></td></tr></table></figure><p>具体实现可以参照以下规则，采用正则表达式进行匹配：</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/BlogImage/master/nlp_traditional/形态还原规则.PNG" alt></p><p>更加具体详细的规则可以参考：<a href="https://wenku.baidu.com/view/4eec6eca326c1eb91a37f111f18583d048640f76.html" target="_blank" rel="noopener">百度文库-英语单词的形态变化</a><a id="more"></a></p><h2 id="中文分词"><a href="#中文分词" class="headerlink" title="中文分词"></a>中文分词</h2><p>采用基于词典的中文分词，词典地址为<a href="http://nlp.nju.edu.cn/MT_Lecture/dic_ce.rar" target="_blank" rel="noopener">这里</a></p><p>实现了正向最大匹配(FMM)和逆向最大匹配(RMM)算法</p><p><strong>正向最大匹配</strong>：从左到右，取最长的词作为切分</p><p><strong>逆向最大匹配</strong>：从右到左，取最长的词作为切分</p><p>例如，“讨论战争与和平等问题”，采用FMM，会被分为”讨论/战争/与/和平/等/问题”，采用RMM，结果为“讨论/战争/与/和/平等/问题”</p><p>算法较为简单，具体实现参见文末github链接</p><h2 id="句法分析"><a href="#句法分析" class="headerlink" title="句法分析"></a>句法分析</h2><p>采用自底向上，基于图的句法分析技术(Chart Parsing)</p><p>数据结构为：</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/BlogImage/master/nlp_traditional/句法分析数据结构.PNG" alt></p><p>具体的算法为：</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/BlogImage/master/nlp_traditional/句法分析算法.PNG" alt></p><h2 id="以上代码实现参见我的github链接nlp-traditional"><a href="#以上代码实现参见我的github链接nlp-traditional" class="headerlink" title="以上代码实现参见我的github链接nlp_traditional"></a><strong>以上代码实现参见我的github链接</strong><a href="https://github.com/Hao-Kailong/nlp_traditional" target="_blank" rel="noopener">nlp_traditional</a></h2><p>欢迎大家和我交流~</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 中文分词 </tag>
            
            <tag> 词形还原 </tag>
            
            <tag> 句法分析 </tag>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo Blog 迁移</title>
      <link href="/2019/10/22/Hexo-Blog-%E8%BF%81%E7%A7%BB/"/>
      <url>/2019/10/22/Hexo-Blog-%E8%BF%81%E7%A7%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="博客迁移"><a href="#博客迁移" class="headerlink" title="博客迁移"></a>博客迁移</h2><p>之前的笔记本不用了，但是hexo博客内容还在上面，要把它迁移到新的主机上来~</p><ol><li>拷贝原文件夹中以下几个目录：</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">_config.yml</span><br><span class="line">package.json</span><br><span class="line">scaffolds/</span><br><span class="line">sources/</span><br><span class="line">themes/</span><br></pre></td></tr></table></figure><ol><li><p>在新主机上安装hexo，可参考<a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">hexo官方文档</a></p></li><li><p>进入hexo目录，执行命令：</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line">npm install hexo-generator-feed --save</span><br><span class="line">npm install hexo-generator-sitemap --save</span><br></pre></td></tr></table></figure><a id="more"></a><ol><li>部署</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><ol><li>ERROR Deployer not found: leancloud_counter_security_sync</li></ol><p>由于leancloud插件出了些问题，leancloud插件主要用于统计文章的访问数，对我来说用处不大，索性直接在_config.yml中把插件禁用了……</p><p>如果想解决这个问题，推荐一篇博客：<a href="https://leaferx.online/2018/02/11/lc-security/" target="_blank" rel="noopener">Leancloud插件修复</a></p><p>leancloud插件对应的github地址为<a href="https://github.com/theme-next/hexo-leancloud-counter-security" target="_blank" rel="noopener">https://github.com/theme-next/hexo-leancloud-counter-security</a></p><p><em>参考：</em></p><p><em><a href="https://blog.csdn.net/eternity1118_/article/details/71194395?ref=myread&amp;tdsourcetag=s_pctim_aiomsg" target="_blank" rel="noopener">hexo: 更换电脑，如何继续写博客</a></em></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
            <tag> 网站 </tag>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用Scrapy爬取百度百科，互动百科</title>
      <link href="/2019/09/05/%E5%88%A9%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%8C%E4%BA%92%E5%8A%A8%E7%99%BE%E7%A7%91/"/>
      <url>/2019/09/05/%E5%88%A9%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%8C%E4%BA%92%E5%8A%A8%E7%99%BE%E7%A7%91/</url>
      
        <content type="html"><![CDATA[<h2 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h2><p>中文百科数据中蕴含了大量的知识，利用Scrapy爬取百科页面，为接下来的信息抽取等打下基础。</p><p>具体实践采用了Scrapy框架，这可能是python中最成熟的爬虫框架，利用代理ip池等技术，爬取全部的百科页面。</p><h2 id="Scrapy架构"><a href="#Scrapy架构" class="headerlink" title="Scrapy架构"></a>Scrapy架构</h2><p>scrapy整体的结构如下图所示：</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/BlogImage/master/Scrapy架构.jpg" alt></p><p>由Spider发送Request请求，Scheduler进行请求调度，Downloader执行具体的下载任务，并经过Pipeline写入数据库或磁盘等。用户可以自定义下载器中间件，比如在中间件中更换代理ip等，具有很大的简便性和灵活性。<a id="more"></a></p><h2 id="爬取逻辑"><a href="#爬取逻辑" class="headerlink" title="爬取逻辑"></a>爬取逻辑</h2><h3 id="百度百科"><a href="#百度百科" class="headerlink" title="百度百科"></a>百度百科</h3><h4 id="逻辑"><a href="#逻辑" class="headerlink" title="逻辑"></a>逻辑</h4><p>由于百度百科的所有页面都是连续编码的，具有以下的格式：</p><p><a href="http://baike.baidu.com/view/%s.html" target="_blank" rel="noopener">http://baike.baidu.com/view/%s.html</a></p><p>例如， <a href="http://baike.baidu.com/view/1.html" target="_blank" rel="noopener">http://baike.baidu.com/view/1.html</a> ，会被重定向为 <a href="https://baike.baidu.com/item/百度百科" target="_blank" rel="noopener">https://baike.baidu.com/item/百度百科</a> </p><p>所以，只需要连续的发送Request就可以爬取到全部的百科页面。</p><h4 id="代理池"><a href="#代理池" class="headerlink" title="代理池"></a>代理池</h4><p>百度百科具有反爬虫策略，大量的请求会导致ip被封，因此需要采用代理ip技术</p><p>使用<a href="https://www.kuaidaili.com/" target="_blank" rel="noopener">快代理</a>进行API调用，获取代理ip，并在下载器中间件中更换代理ip，代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        proxy = random.choice(util.proxies)</span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = <span class="string">"http://&#123;0&#125;"</span>.format(proxy)</span><br></pre></td></tr></table></figure><p>当下载出现异常时，对代理ip进行更换，并删除无效代理。这样，可以动态的保证代理ip池处于高质量的状态</p><h3 id="互动百科"><a href="#互动百科" class="headerlink" title="互动百科"></a>互动百科</h3><h4 id="逻辑-1"><a href="#逻辑-1" class="headerlink" title="逻辑"></a>逻辑</h4><p>互动百科没有像百度百科那样的连续编码（在网上没有找到），因此只能换一种思路。</p><p>利用 <a href="http://fenlei.baike.com" target="_blank" rel="noopener">http://fenlei.baike.com</a> 作为起始页面，利用beautifulsoup提取所有的超链接，按照超链发现的方式进行图遍历，如果遇到满足 <a href="http://www.baike.com/wiki/" target="_blank" rel="noopener">http://www.baike.com/wiki/</a> 的页面，则下载下来进入磁盘，否则继续遍历。</p><p><em>互动百科没有反爬虫策略，无需代理池</em></p><h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><h3 id="百度百科-1"><a href="#百度百科-1" class="headerlink" title="百度百科"></a>百度百科</h3><p>在centos服务器上，采用多进程同时爬取，服务器信息如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CPU: 8个 8核 Intel(R) Xeon(R) CPU E7- 4820  @ 2.00GHz</span><br><span class="line">内存: 174G</span><br><span class="line">硬盘: 2T</span><br></pre></td></tr></table></figure><p>4进程同时爬取，每天可以爬取1,500,000页面</p><h3 id="互动百科-1"><a href="#互动百科-1" class="headerlink" title="互动百科"></a>互动百科</h3><p>服务器信息同上，目前已经爬取了3,000,000页面，说明覆盖率还不错，不知道最终可以爬取到多少页面…</p><h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><p>具体的源代码可以在我的Github上找到</p><p><a href="https://github.com/Hao-Kailong/baiduBaikeSpider" target="_blank" rel="noopener">百度百科爬虫github</a></p><p><a href="https://github.com/Hao-Kailong/hudongBaikeSpider" target="_blank" rel="noopener">互动百科爬虫github</a></p><h2 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h2><p>在具体的实践中遇到了不少坑，比如多进程之间的协同、服务器的配置、代理逻辑等，如果大家遇到问题，欢迎和我留言交流~</p>]]></content>
      
      
      <categories>
          
          <category> Data Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Scrapy </tag>
            
            <tag> 知识获取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Relation Inductive Biases, Deep Learning, and Graph Networks</title>
      <link href="/2019/06/10/Relation-Inductive-Biases-Deep-Learning-and-Graph-Networks/"/>
      <url>/2019/06/10/Relation-Inductive-Biases-Deep-Learning-and-Graph-Networks/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>这是论文 <a href="https://arxiv.org/pdf/1806.01261.pdf" target="_blank" rel="noopener">Relation Inductive Biases, Deep Learning, and Graph Networks</a> 的阅读笔记。</p><p>Pearl指出当前的深度学习无法解决推理的问题，所做的不过是拟合曲线罢了。</p><p>机器学习主要有三个主要学派：</p><ul><li><p>符号主义 (Symbolicism)</p><ul><li><p>符号主义的起源，注重研究知识表达和逻辑推理。经过几十年的研究，目前这一学派的主要成果，一个是贝叶斯因果网络，另一个是知识图谱。</p><p>贝叶斯因果网络的旗手是 Judea Pearl 教授，2011年的图灵奖获得者。但是据说 2017年 NIPS 学术会议上，老爷子演讲时，听众寥寥。2018年，老爷子出版了一本新书，“The Book of Why”，为因果网络辩护，同时批判深度学习缺乏严谨的逻辑推理过程。而知识图谱主要由搜索引擎公司，包括谷歌、微软、百度推动，目标是把搜索引擎，由关键词匹配，推进到语义匹配。</p></li></ul></li><li><p>连接主义 (Connectionism)</p><ul><li>连接主义的起源是仿生学，用数学模型来模仿神经元。Marvin Minsky 教授因为对神经元研究的推动，获得了1969年图灵奖。把大量神经元拼装在一起，就形成了深度学习模型，深度学习的旗手是 Geoffrey Hinton 教授。深度学习模型最遭人诟病的缺陷，是不可解释。 </li></ul></li><li><p>行为主义 (Actionism)</p><ul><li>行为主义把控制论引入机器学习，最著名的成果是强化学习。强化学习的旗手是 Richard Sutton 教授。近年来Google DeepMind 研究员，把传统强化学习，与深度学习融合，实现了 AlphaGo，战胜当今世界所有人类围棋高手。 </li></ul></li></ul><p>这篇论文，提议把传统的贝叶斯因果网络和知识图谱，与深度强化学习融合，并梳理了与这个主题相关的研究进展。 </p><h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><h3 id="Relation-Inductive-Biases"><a href="#Relation-Inductive-Biases" class="headerlink" title="Relation Inductive Biases"></a>Relation Inductive Biases</h3><p>An <em>inductive bias</em> allows a learning algorithm to prioritize one solution (or interpretation) over another, independent of the observed data.</p><p>全连接层几乎没有归纳偏向，所有的输入单元直接影响到输出单元，并且独立。</p><p>卷积层的归纳偏向：局部性和Translation Invariance。Translation Invariance表现为对不同的局部采用同样的函数。</p><p>循环层的归纳偏向：局部性和Temporal Invariance，即时间不变性，对不同的step采用相同的函数。</p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> GNN </tag>
            
            <tag> 图神经网络 </tag>
            
            <tag> 关系推理 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>童年-吉他谱</title>
      <link href="/2019/06/06/%E7%AB%A5%E5%B9%B4-%E5%90%89%E4%BB%96%E8%B0%B1/"/>
      <url>/2019/06/06/%E7%AB%A5%E5%B9%B4-%E5%90%89%E4%BB%96%E8%B0%B1/</url>
      
        <content type="html"><![CDATA[<p>最近，其实已经有一段时间了，把“童年”这首歌的弹唱练得差不多了，刚用github建了一个图床，就用来测试一下效果。</p><p><img src="https://raw.githubusercontent.com/Hao-Kailong/BlogImage/master/guitar/TongNian/1.png" alt></p><a id="more"></a><p><img src="https://raw.githubusercontent.com/Hao-Kailong/BlogImage/master/guitar/TongNian/2.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吉他谱 </tag>
            
            <tag> 音乐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱-陈华钧-笔记</title>
      <link href="/2019/06/01/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1-%E9%99%88%E5%8D%8E%E9%92%A7-%E7%AC%94%E8%AE%B0/"/>
      <url>/2019/06/01/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1-%E9%99%88%E5%8D%8E%E9%92%A7-%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>本文主要记录在学习知识图谱课件时的一些笔记，课件来自于是浙江大学教授<a href="https://person.zju.edu.cn/huajun" target="_blank" rel="noopener">陈华钧</a></p><h2 id="知识图谱概览"><a href="#知识图谱概览" class="headerlink" title="知识图谱概览"></a>知识图谱概览</h2><p>知识图谱的技术体系可以总结为下图。</p><p><img src="/images/blog/201905/知识图谱的技术体系.PNG" alt></p><a id="more"></a><p>利用SPARQL可以查询知识图谱，举例为：</p><p><img src="/images/blog/201905/SPARQL查询.PNG" alt></p><p>其对应的结构为下面所示：</p><p><img src="/images/blog/201905/SPARQL查询结构图.PNG" alt></p><p>通过这个查询，可以找到出生在成立于1718年的城市，同时不是生于1976年的人的名字。</p><p>在OpenKG上存在类似于WordNet的中文词典，可以在项目中用于解决一些同义词的问题~</p><h2 id="历史中的知识表示方法"><a href="#历史中的知识表示方法" class="headerlink" title="历史中的知识表示方法"></a>历史中的知识表示方法</h2><p>在历史上，出现过多种知识的表示方式。</p><ul><li>一阶谓词逻辑</li><li>产生式系统</li><li>框架系统</li><li>语义网络</li><li>逻辑程序</li><li>缺省逻辑</li><li>模态逻辑</li></ul><p>三元组(Subject, Predicate, Object)，即主谓宾结构。</p><p>OWL是一种用来定义概念、描述客观事物以及这些事物之间关系（即本体）的语言，是一阶谓词逻辑的可判定子集。</p><p>描述逻辑可以为知识图谱的表示与建模提供理论基础。</p><p>schema.org是面向Web的语义数据标准。</p><p><a href="https://protege.stanford.edu" target="_blank" rel="noopener">Protege</a>是一种基于Java语言开发的本体编辑和本体开发工具。</p><p>domain和range类似于数学中的定义域和值域。</p><p>RDF作为⼀种知识图谱表示框架的参考标准，向上对接OWL等更丰富的语义表⽰和推理能力，向下对接简化后的属性图数据库以及图计算引擎。</p><h2 id="知识图谱的存储与关联查询"><a href="#知识图谱的存储与关联查询" class="headerlink" title="知识图谱的存储与关联查询"></a>知识图谱的存储与关联查询</h2><p>使用关系型数据库，在进行联合查询时，表之间的join操作会耗费大量的时间，效率低下。</p><h3 id="图数据库的文件存储结构"><a href="#图数据库的文件存储结构" class="headerlink" title="图数据库的文件存储结构"></a>图数据库的文件存储结构</h3><p><img src="/images/blog/201905/Node.PNG" alt></p><ul><li>节点存储于独立的”节点存储文件“，每个节点的存储空间固定，如14字节，便于直接通过ID编号计算获得访问地址，基于这种格式，节点存储成本为O(1)，而非O(N)</li><li>每个节点首字节标明是否inUse，接下来四个字节存储该节点的第一个关系的ID，而接下来四个字节存储该节点的第一个属性的ID，再接下来四个字节存储该节点的第一个Label ID。</li><li>节点的属性数据（如姓名、年龄等）是分开存储的，节点只存储其第一个属性的ID，这样的设计是为了保证节点遍历的高效性。</li></ul><p><img src="/images/blog/201905/Relationship.PNG" alt></p><ul><li>关系存储于独⽴的“关系存储⽂件”，每个关系的存储空间固定，如34字节，和节点⼀ 样，这种设计便于直接通过ID编号计算获得关系的访问地址。 </li><li>每个节点⾸字节标明是否inUse，接下来四个字节存储该关系的头节点ID，再接下来 四个字节存储该关系的尾节点ID，再接下来四个字节存储关系类型ID，再下⾯存储头节点和尾节点的上⼀个关系ID，以及头尾节点的下⼀个关系ID。</li></ul><h3 id="物流数据模型"><a href="#物流数据模型" class="headerlink" title="物流数据模型"></a>物流数据模型</h3><p>可以将物流数据建模为动态图谱，利用图谱查询操作，计算最短路径。</p><p>原⽣图存储在复杂关联查询和图计算方面有性能优势，⾮原⽣图存储兼容已有⼯具集通常学习和协调成本会低。  </p><p>图数据库有它弱处，假如你的应⽤场景不包含⼤量的关联查询，对于简单查询，传统关系模型目前在性能方面更加有优势。    </p><h2 id="知识图谱的获取与语义关系抽取"><a href="#知识图谱的获取与语义关系抽取" class="headerlink" title="知识图谱的获取与语义关系抽取"></a>知识图谱的获取与语义关系抽取</h2><p>知识图谱工程从不同来源、不同结构的数据中进行知识提取，形成知识存入到知识图谱。文本一般不作为知识图谱构建的初始来源，而多用来做知识图谱补全。</p><p><img src="/images/blog/201905/知识图谱工程.PNG" alt></p><p>Automatic Content Extraction (ACE)，自动内容抽取，主要包含五种任务：</p><ul><li>实体检测与识别</li><li>数值检测与识别</li><li>时间表达检测与识别</li><li>关系检测与识别</li><li>事件检测与识别</li></ul><h3 id="实体识别的常用方法"><a href="#实体识别的常用方法" class="headerlink" title="实体识别的常用方法"></a>实体识别的常用方法</h3><h4 id="基于模板和规则的方法"><a href="#基于模板和规则的方法" class="headerlink" title="基于模板和规则的方法"></a>基于模板和规则的方法</h4><h4 id="基于序列标注的机器学习方法"><a href="#基于序列标注的机器学习方法" class="headerlink" title="基于序列标注的机器学习方法"></a>基于序列标注的机器学习方法</h4><p>利用词本身的特征，如词性、依存关系；利用前后缀特征，如xx省；利用字本身的特征，如数字。</p><div class="table-container"><table><thead><tr><th>句子</th><th>由</th><th>浙</th><th>江</th><th>大</th><th>学</th><th>的</th><th>张</th><th>三</th><th>迎</th><th>战</th><th>清</th><th>华</th><th>大</th><th>学</th><th>的</th><th>李</th><th>四</th></tr></thead><tbody><tr><td>IOB标注</td><td>O</td><td>B-ORG</td><td>I-ORG</td><td>I-ORG</td><td>I-ORG</td><td>O</td><td>B-PER</td><td>I-PER</td><td>O</td><td>O</td><td>B-ORG</td><td>I-ORG</td><td>I-ORG</td><td>I-ORG</td><td>O</td><td>B-PER</td><td>I-PER</td></tr><tr><td>IO标注</td><td>O</td><td>I-ORG</td><td>I-ORG</td><td>I-ORG</td><td>I-ORG</td><td>O</td><td>I-PER</td><td>I-PER</td><td>O</td><td>O</td><td>I-ORG</td><td>I-ORG</td><td>I-ORG</td><td>I-ORG</td><td>O</td><td>I-PER</td><td>I-PER</td></tr></tbody></table></div><p><strong>HMM (隐马尔科夫模型)</strong></p><p>齐次马尔科夫链假设：任意时刻的隐藏状态只依赖于它前一个隐藏态</p><p>观测独立性假设：任意时刻的观察状态仅仅依赖于当前时刻的隐藏状态</p><p><em>鲍姆-韦尔奇算法-EM算法</em></p><p><em>维特比算法</em></p><p><strong>CRF (条件随机场)</strong></p><p>条件随机场是马尔科夫随机场的特例。</p><p>例如：实体识别任务要求对一句话中的十个词做实体类型标记，这十个词可以从可能实体类型标签中选择，这就形成了一个随机场。如果假设某个词的标签只与其相邻的词的标签有关，则形成马尔科夫随机场，同时由于这个随机场只有两种变量，令X为词，Y为实体类型标签，则形成一个条件随机场。</p><h4 id="基于深度学习"><a href="#基于深度学习" class="headerlink" title="基于深度学习"></a>基于深度学习</h4><p><strong>BiLSTM + CRF</strong></p><p><img src="/images/blog/201905/BiLSTM_CRF.PNG" alt></p><h3 id="在WordNet中的关系可以总结如下："><a href="#在WordNet中的关系可以总结如下：" class="headerlink" title="在WordNet中的关系可以总结如下："></a>在WordNet中的关系可以总结如下：</h3><div class="table-container"><table><thead><tr><th>Relation</th><th>Example</th></tr></thead><tbody><tr><td>Synonym 同义词</td><td>day (sense 2) / time</td></tr><tr><td>Antonym 反义词</td><td>day (sense 4) / night</td></tr><tr><td>Hypernym 上位词</td><td>berry (sense 2) / fruit</td></tr><tr><td>Hyponym 下位词</td><td>fruit (sense 1) / berry</td></tr><tr><td>Member-of Holonym</td><td>Germany / NATO</td></tr><tr><td>Has-member meronym</td><td>Germany / Sorbian</td></tr><tr><td>Part-of holonym</td><td>Germany / Europe</td></tr><tr><td>Has-part meronym</td><td>Germany / Mannheim</td></tr><tr><td>Substance-of holonym</td><td>wood(sense 1) / lumber</td></tr><tr><td>Has-substance meronym</td><td>lumber(sense 1) / wood</td></tr><tr><td>Domain - TOPIC</td><td>line (sense 7) / military</td></tr><tr><td>Domain - USAGE</td><td>line (sense 21) / channel</td></tr><tr><td>Domain member - TOPIC</td><td>ship / porthole</td></tr><tr><td></td><td></td></tr><tr><td>Attribute</td><td>speed (sense 2) / fast</td></tr><tr><td>Derived form</td><td>speed (sense 2) / quick</td></tr><tr><td>Derived form</td><td>speed (sense 2) / accelerate</td></tr></tbody></table></div><h3 id="实体关系抽取"><a href="#实体关系抽取" class="headerlink" title="实体关系抽取"></a>实体关系抽取</h3><p>实体关系抽取概览如下图：</p><p><img src="/images/blog/201905/实体关系抽取概览.PNG" alt></p><p>监督学习的特征可以分为轻量级特征，中等量级特征，重量级特征。轻量级特征有实体前后的词，实体的类型，实体之间的距离。中等量级特征如Chunk序列。重量级特征如实体间的依存关系路径，实体间树结构的距离，特定的结构信息。</p><p>在Bryan Rink et. al ACL 2016中，使用的监督学习的特征：</p><p><img src="/images/blog/201905/监督学习特征举例.PNG" alt></p><h4 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h4><p>最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。</p><h4 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h4><p>定义：给定一个对象空间X，核函数K: X * X  -&gt;  [0,$\infty$)表示一个二元函数，该函数可以将X中任意的两个对象x, y作为输入，并返回二者之间的相似度得分K(x, y)。</p><p>核函数的方法无需人工指定特征。</p><p>对应到关系分类任务，给定输入文本T中的两个实体e1和e2，核函数方法采用下述方法计算这两个实体间满足关系r的置信度：</p><ul><li>首先从标注数据中找到文本T’，且T’中包含满足关系r的e1’和e2’，然后基于核函数计算T和T’之间的相似度，作为e1和e2满足关系r的置信度。</li><li>该做法背后的思想是：如果两个实体对同时满足某个关系r，这两个实体对分别所在的上下文也应该相似，该相似通过核函数计算得到。</li></ul><ol><li>字符串核</li><li>基于Shallow Parsing核函数</li><li>句法树核函数</li><li>最短依赖路径树核函数</li><li>上下文相关最短路径依赖树核函数</li></ol><h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><h5 id="基于递归神经网络RNN"><a href="#基于递归神经网络RNN" class="headerlink" title="基于递归神经网络RNN"></a>基于递归神经网络RNN</h5><p>可以参考论文</p><p><a href="https://ai.stanford.edu/~ang/papers/emnlp12-SemanticCompositionalityRecursiveMatrixVectorSpaces.pdf" target="_blank" rel="noopener">Semantic Compositionality through Recursive Matrix-Vector Spaces</a></p><h5 id="基于CNN"><a href="#基于CNN" class="headerlink" title="基于CNN"></a>基于CNN</h5><p>可以参考论文</p><p><a href="https://www.aclweb.org/anthology/C14-1220" target="_blank" rel="noopener">Relation Classification via Convolutional Deep Neural Network</a></p><p>采用Piecewise CNN:</p><p><a href="http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP203.pdf" target="_blank" rel="noopener">Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks</a></p><p>采用Ranking CNN:</p><p><a href="https://www.aclweb.org/anthology/P15-1061" target="_blank" rel="noopener">Classifying Relations by Ranking with Convolutional Neural Networks</a></p><h5 id="Attention-BiLSTM"><a href="#Attention-BiLSTM" class="headerlink" title="Attention + BiLSTM"></a>Attention + BiLSTM</h5><p>参考论文</p><p><a href="https://www.aclweb.org/anthology/P16-2034" target="_blank" rel="noopener">Attention-Based Bidirectional Long Short Term Memory Networks for Relation Classification</a></p><h5 id="跨句推理"><a href="#跨句推理" class="headerlink" title="跨句推理"></a>跨句推理</h5><p>可以参考论文</p><p><a href="https://www.aclweb.org/anthology/D17-1186" target="_blank" rel="noopener">Incorporating Relation Paths in Neural Relation Extraction</a></p><h5 id="多元关系"><a href="#多元关系" class="headerlink" title="多元关系"></a>多元关系</h5><p>可以参考论文</p><p><a href="https://arxiv.org/pdf/1708.03743.pdf" target="_blank" rel="noopener">Cross-Sentence N-ary Relation Extraction with Graph LSTMs</a></p><h5 id="联合抽取"><a href="#联合抽取" class="headerlink" title="联合抽取"></a>联合抽取</h5><p>基于序列标注的端到端模型</p><p><a href="https://arxiv.org/pdf/1706.05075.pdf" target="_blank" rel="noopener">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme</a></p><p>基于序列和树结构的LSTM模型</p><p><a href="https://www.aclweb.org/anthology/P16-1105" target="_blank" rel="noopener">End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures</a></p><h5 id="远程监督降噪"><a href="#远程监督降噪" class="headerlink" title="远程监督降噪"></a>远程监督降噪</h5><p>给bag中的句子不同的attention</p><p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14491/14078" target="_blank" rel="noopener">Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions</a></p><p>强化学习训练句子选择器</p><p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/17151/16140" target="_blank" rel="noopener">Reinforcement Learning for Relation Classification from Noisy Data</a></p><h3 id="开放域关系抽取-Open-IE"><a href="#开放域关系抽取-Open-IE" class="headerlink" title="开放域关系抽取 Open IE"></a>开放域关系抽取 Open IE</h3><h4 id="基于模板（正则表达式）"><a href="#基于模板（正则表达式）" class="headerlink" title="基于模板（正则表达式）"></a>基于模板（正则表达式）</h4><h4 id="Bootstrapping"><a href="#Bootstrapping" class="headerlink" title="Bootstrapping"></a>Bootstrapping</h4><p>给定种子集合，如&lt;姚明，叶莉&gt;</p><p>从文档中抽取出包含种子实体的新闻，如：</p><div class="table-container"><table><thead><tr><th>句子</th><th>模板</th></tr></thead><tbody><tr><td>姚明老婆叶莉简历曝光</td><td>X老婆Y简历曝光</td></tr><tr><td>姚明与妻子叶莉外出赴约</td><td>X与妻子Y外出赴约</td></tr><tr><td>姚明携爱妻叶莉外出赴约</td><td>X携爱妻Y外出赴约</td></tr></tbody></table></div><p>将抽取出的Pattern与文档集匹配</p><ul><li>小猪与妻子伊万外出赴约</li></ul><p>根据Pattern抽取出的新文档加入种子库，迭代多轮直至不符合条件</p><h2 id="知识图谱与关系推理"><a href="#知识图谱与关系推理" class="headerlink" title="知识图谱与关系推理"></a>知识图谱与关系推理</h2><h3 id="Deductive-Reasoning-over-Konwledge-Graph"><a href="#Deductive-Reasoning-over-Konwledge-Graph" class="headerlink" title="Deductive Reasoning over Konwledge Graph"></a>Deductive Reasoning over Konwledge Graph</h3><p>RDFS: Simple Vocabulary and Schema</p><h4 id="OWL本体推理"><a href="#OWL本体推理" class="headerlink" title="OWL本体推理"></a>OWL本体推理</h4><p>基于Tableaux运算的方法</p><h4 id="基于逻辑编程改写的方法"><a href="#基于逻辑编程改写的方法" class="headerlink" title="基于逻辑编程改写的方法"></a>基于逻辑编程改写的方法</h4><p>Datalog语言，面向知识库和数据库设计的逻辑语言</p><h4 id="基于产生式规则的方法"><a href="#基于产生式规则的方法" class="headerlink" title="基于产生式规则的方法"></a>基于产生式规则的方法</h4><p>产生式系统由事实集，产生式集合，推理引擎构成。</p><h3 id="Inductive-Reasoning-over-Knowledge-Graph"><a href="#Inductive-Reasoning-over-Knowledge-Graph" class="headerlink" title="Inductive Reasoning over Knowledge Graph"></a>Inductive Reasoning over Knowledge Graph</h3><h4 id="词向量学习模型"><a href="#词向量学习模型" class="headerlink" title="词向量学习模型"></a>词向量学习模型</h4><p>词向量学习模型有CBoW，Skip-gram等</p><h4 id="知识图谱嵌入"><a href="#知识图谱嵌入" class="headerlink" title="知识图谱嵌入"></a>知识图谱嵌入</h4><p>为知识图谱中的每个实体和关系都学习一个向量表示，将离散的符号表示转化为连续的向量表示。</p><h5 id="嵌入模型：TransE"><a href="#嵌入模型：TransE" class="headerlink" title="嵌入模型：TransE"></a>嵌入模型：TransE</h5><p>h + r = t</p><p>参考论文</p><p><a href="https://www.utc.fr/~bordesan/dokuwiki/_media/en/transe_nips13.pdf" target="_blank" rel="noopener">Translating Embeddings for Modeling Multi-relational Data</a></p><h5 id="嵌入模型：DistMult"><a href="#嵌入模型：DistMult" class="headerlink" title="嵌入模型：DistMult"></a>嵌入模型：DistMult</h5><p>参考论文</p><p><a href="https://arxiv.org/pdf/1412.6575.pdf" target="_blank" rel="noopener">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</a></p><p>h * Mr = t</p><h5 id="嵌入模型：TransH"><a href="#嵌入模型：TransH" class="headerlink" title="嵌入模型：TransH"></a>嵌入模型：TransH</h5><p>参考论文</p><p><a href="https://pdfs.semanticscholar.org/2a3f/862199883ceff5e3c74126f0c80770653e05.pdf" target="_blank" rel="noopener">Knowledge Graph Embedding by Translating on Hyperplanes</a></p><h5 id="嵌入模型：TransR"><a href="#嵌入模型：TransR" class="headerlink" title="嵌入模型：TransR"></a>嵌入模型：TransR</h5><p>参考论文</p><p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9571/9523" target="_blank" rel="noopener">Learning Entity and Relation Embeddings for Knowledge Graph Completion</a></p><h5 id="嵌入模型：TransD"><a href="#嵌入模型：TransD" class="headerlink" title="嵌入模型：TransD"></a>嵌入模型：TransD</h5><p>参考论文</p><p><a href="https://www.aclweb.org/anthology/P15-1067" target="_blank" rel="noopener">Knowledge Graph Embedding via Dynamic Mapping Matrix</a></p><p>还有嵌入模型如PTransE，规则增强，多模态知识图谱，基于强化学习的DeepPath等等很多</p><h2 id="知识图谱与智能问答"><a href="#知识图谱与智能问答" class="headerlink" title="知识图谱与智能问答"></a>知识图谱与智能问答</h2><p>基本概念：问题类型</p><ul><li>事实性问题<ul><li>谓词型问题：Where is Taj Mahal?</li><li>列表型问题：Give me all cities in Germany.</li><li>最高级型问题：What is the highest mountain?</li><li>对错型问题：Was Margaret Thatcher a Chemist?</li></ul></li><li>观点型问题：What do most Americans think of gun control?</li><li>因果型问题：What is the most frequent cause for lung cancer?</li><li>方法型问题：How do I make a cheese cake?</li><li>解释型问题：Why did the revenue of IBM drop?</li><li>关联型问题：What is the connection between Barack Obama and Indonesia?</li><li>比较型问题：What is the difference between impressionism and expressionism?</li></ul><h3 id="知识图谱问答的主要方法"><a href="#知识图谱问答的主要方法" class="headerlink" title="知识图谱问答的主要方法"></a>知识图谱问答的主要方法</h3><h4 id="基于模板的方法"><a href="#基于模板的方法" class="headerlink" title="基于模板的方法"></a>基于模板的方法</h4><p>模板定义，模板生成，模板匹配</p><p>方法的两个重要步骤，一是模板生成，即根据问题选择相应的模板；二是模板实例化。</p><p>模板系统TBSL架构：</p><p><img src="/images/blog/201905/TBSL架构.PNG" alt></p><h4 id="基于语义解析的方法"><a href="#基于语义解析的方法" class="headerlink" title="基于语义解析的方法"></a>基于语义解析的方法</h4><p>两个关键问题：一是获得短语到资源的映射，即资源映射；二是如何解决文本的歧义，即逻辑表达式。</p><p>资源映射：将自然语言短语或单词节点映射到知识库的实体或实体关系。可以通过构造一个词汇表来完成这样的映射。简单映射可以通过字符串相似度匹配；复杂映射可以采用统计方法。</p><h4 id="基于深度学习的方法"><a href="#基于深度学习的方法" class="headerlink" title="基于深度学习的方法"></a>基于深度学习的方法</h4><p>KBQA与深度学习结合，利用深度学习对于传统问答方法进行改进，基于深度学习的End2End模型。</p><p>例如，Bordes et al. 2014</p><p><img src="/images/blog/201905/端到端KBQA.PNG" alt></p><p>深度学习目前只能处理简单问题和单边关系问题。</p><p>基于elasticsearch的KBQA实践还不错~</p>]]></content>
      
      
      <categories>
          
          <category> Knowledge Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 关系抽取 </tag>
            
            <tag> 笔记 </tag>
            
            <tag> KBQA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用Keras搭建多输入CNN</title>
      <link href="/2019/05/24/%E5%88%A9%E7%94%A8Keras%E6%90%AD%E5%BB%BA%E5%A4%9A%E8%BE%93%E5%85%A5CNN/"/>
      <url>/2019/05/24/%E5%88%A9%E7%94%A8Keras%E6%90%AD%E5%BB%BA%E5%A4%9A%E8%BE%93%E5%85%A5CNN/</url>
      
        <content type="html"><![CDATA[<p>Keras是一种比较高阶的封装，是一种深度学习框架，其可以架构在Tensorflow, Theano等多种框架上，便于快速的上手和开发。</p><p>在这里，利用Keras动手搭建一个CNN模型，Keras模型可以通过两种方式搭建，一是直接采用keras.model.sequential搭建模型，这样简单清晰；二是利用函数式API搭建，函数式API主要用于搭建复杂模型，具有更高的灵活性。</p><p>在我做的实验中，是要实现一个关系抽取的模型，有两个输入，一个主输入，一个辅输入，因此使用函数式API进行编程。</p><p>模型的结构图为：</p><p><img src="/images/blog/201905/model.png" alt></p><a id="more"></a><p>其对应的代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv1D, GlobalMaxPooling1D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Input, concatenate</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cnn_model</span><span class="params">()</span>:</span></span><br><span class="line">main_input = Input(shape=(<span class="literal">None</span>, <span class="number">300</span>))</span><br><span class="line"></span><br><span class="line">x = Conv1D(<span class="number">200</span>, kernel_size=<span class="number">3</span>)(main_input)  <span class="comment"># 卷积层</span></span><br><span class="line">x = GlobalMaxPooling1D()(x)  </span><br><span class="line">x = Dense(<span class="number">100</span>, activation=<span class="string">'tanh'</span>)(x)</span><br><span class="line">    </span><br><span class="line">addt_input = Input(shape=(<span class="number">300</span>, ))  <span class="comment"># 辅助输入</span></span><br><span class="line">x = concatenate([addt_input, x])</span><br><span class="line">output = Dense(<span class="number">19</span>, activation=<span class="string">'softmax'</span>)(x)</span><br><span class="line"></span><br><span class="line">model = Model(inputs=[main_input, addt_input], outputs=[output])  <span class="comment"># 多输入单输出</span></span><br><span class="line">model.summary()  <span class="comment"># 模型概览</span></span><br><span class="line"><span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">model = cnn_model()</span><br><span class="line">plot_model(model, <span class="string">'model.png'</span>)  <span class="comment"># 绘制模型结构图</span></span><br></pre></td></tr></table></figure><p>首先，输入的张量形式为(batchsize, None, 300)，这里None表示大小不定，是一个变量。在我做的关系抽取实验中，None表示变长的句子长度。</p><p>经过卷积操作，卷积核个数为200，张量的形式变为(batchsize, None - 2, 200)，由于卷积操作的进行，句子的长度变为None - 3 + 1，而每一个卷积核生成了一个特征向量，因此共200个。</p><p>然后对于每一个卷积核生成的向量，进行全局最大池化，这样，得到的张量就不再与句子长度有关。</p><p>……</p><p>通过model.summary()，可以直接看到模型的概述，如图：</p><p><img src="/images/blog/201905/summary.png" alt></p><p>每一层的张量形式均得到清晰地描述，而且看到共需训练的参数为207,919个。</p><p>这个CNN模型是对<a href="https://www.aclweb.org/anthology/C14-1220" target="_blank" rel="noopener">Relation Classification via Convolutional Deep Neural Network</a>的一种实现，利用这个模型，可以实现关系分类，进行一些关系抽取任务。</p>]]></content>
      
      
      <categories>
          
          <category> Neural Networks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> Keras </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Must-read papers on NRE</title>
      <link href="/2019/04/17/Must-read-papers-on-NRE/"/>
      <url>/2019/04/17/Must-read-papers-on-NRE/</url>
      
        <content type="html"><![CDATA[<p>NRE: Neural Relation Extraction，即关系抽取，这篇文章来自于：<a href="https://github.com/thunlp/NREPapers" target="_blank" rel="noopener">THU_NLP</a></p><h2 id="Survey-Papers"><a href="#Survey-Papers" class="headerlink" title="Survey Papers"></a>Survey Papers</h2><ol><li><p><strong>A Survey of Deep Learning Methods for Relation Extraction.</strong></p><p><em>Shantanu Kumar.</em><br>2017.<br><a href="https://arxiv.org/pdf/1705.03645.pdf" target="_blank" rel="noopener">paper</a></p></li><li><p><strong>Relation Extraction : A Survey.</strong><br><em>Sachin Pawara,b, Girish K. Palshikara, Pushpak Bhattacharyyab.</em><br>2017.<br><a href="https://arxiv.org/pdf/1712.05191.pdf" target="_blank" rel="noopener">paper</a></p></li></ol><a id="more"></a><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><h3 id="Supervised-Datasets"><a href="#Supervised-Datasets" class="headerlink" title="Supervised Datasets"></a>Supervised Datasets</h3><ol><li><strong>ACE 2005 Dataset</strong> <a href="https://catalog.ldc.upenn.edu/LDC2006T06" target="_blank" rel="noopener">link</a></li><li><strong>SemEval-2010 Task 8 Dataset</strong> <a href="http://semeval2.fbk.eu/semeval2.php?location=tasks#T11" target="_blank" rel="noopener">link</a></li></ol><h3 id="Distantly-Supervised-Datasets"><a href="#Distantly-Supervised-Datasets" class="headerlink" title="Distantly Supervised Datasets"></a>Distantly Supervised Datasets</h3><ol><li><strong>NYT Dataset</strong> <a href="http://iesl.cs.umass.edu/riedel/ecml/" target="_blank" rel="noopener">link</a></li></ol><h3 id="Few-shot-Datasets"><a href="#Few-shot-Datasets" class="headerlink" title="Few-shot Datasets"></a>Few-shot Datasets</h3><ol><li><p><strong>FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation</strong><br><em>Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, Maosong Sun</em><br>EMNLP 2018.<br><a href="http://aclweb.org/anthology/D18-1514" target="_blank" rel="noopener">paper</a></p><blockquote><p>We present a Few-Shot Relation Classification Dataset (FewRel), consisting of 70,000 sentences on 100 relations derived from Wikipedia and annotated by crowdworkers.</p></blockquote></li></ol><h2 id="Word-Vector-Tools"><a href="#Word-Vector-Tools" class="headerlink" title="Word Vector Tools"></a>Word Vector Tools</h2><ol><li><strong>Word2vec</strong> <a href="code.google.com/p/word2vec">link</a></li><li><strong>GloVe</strong> <a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">link</a></li></ol><h2 id="Journal-and-Conference-papers"><a href="#Journal-and-Conference-papers" class="headerlink" title="Journal and Conference papers:"></a>Journal and Conference papers:</h2><h3 id="Supervised-Datasets-1"><a href="#Supervised-Datasets-1" class="headerlink" title="Supervised Datasets"></a>Supervised Datasets</h3><ol><li><p><strong>SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals.</strong><br><em>Iris Hendrickx , Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O ́ Se ́aghdha, Sebastian Pado ́, Marco Pennacchiotti, Lorenza Romano, Stan Szpakowicz.</em><br>Workshop on Semantic Evaluations, ACL 2009<br><a href="http://delivery.acm.org/10.1145/1630000/1621986/p94-hendrickx.pdf?ip=133.130.111.179&amp;id=1621986&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;__acm__=1536636784_f0b60686e8866c0c08f63436f3ed81eb" target="_blank" rel="noopener">paper</a></p><blockquote><p>This leads us to introduce a new task, which will be part of SemEval-2010: multi-way classification of mutually exclusive semantic relations between pairs of common nominals.</p></blockquote></li></ol><h3 id="Distantly-Supervised-Datasets-and-Training-Methods"><a href="#Distantly-Supervised-Datasets-and-Training-Methods" class="headerlink" title="Distantly Supervised Datasets and Training Methods"></a>Distantly Supervised Datasets and Training Methods</h3><ol><li><p><strong>Learning to Extract Relations from the Web using Minimal Supervision.</strong><br><em>Razvan C. Bunescu, Department of Computer Sciences.</em><br>ACL 2007.<br><a href="http://www.aclweb.org/anthology/P07-1073" target="_blank" rel="noopener">paper</a></p><blockquote><p>We present a new approach to relation extraction that requires only a handful of training examples. Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web.</p></blockquote></li><li><p><strong>Distant Supervision for Relation Extraction without Labeled Data.</strong><br><em>Mike Mintz, Steven Bills, Rion Snow, Dan Jurafsky.</em><br>ACL-IJCNLP 2009.<br><a href="https://www.aclweb.org/anthology/P09-1113" target="_blank" rel="noopener">paper</a></p><blockquote><p>Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision.</p></blockquote></li><li><p><strong>Modeling Relations and Their Mentions without Labeled Text.</strong><br><em>Sebastian Riedel, Limin Yao, Andrew McCallum.</em><br>ECML 2010.<br><a href="https://link.springer.com/content/pdf/10.1007%2F978-3-642-15939-8_10.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p>We present a novel approach to distant supervision that can alleviate this problem based on the following two ideas: First, we use a factor graph to explicitly model the decision whether two entities are related, and the decision whether this relation is mentioned in a given sentence; second, we apply constraint-driven semi-supervision to train this model without any knowledge about which sentences express the relations in our training KB.</p></blockquote></li><li><p><strong>Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations.</strong><br><em>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, Daniel S. Weld.</em><br>ACL-HLT 2011.<br><a href="http://www.aclweb.org/anthology/P11-1055" target="_blank" rel="noopener">paper</a></p><blockquote><p>This paper presents a novel approach for multi-instance learning with overlapping re- lations that combines a sentence-level extrac- tion model with a simple, corpus-level compo- nent for aggregating the individual facts.</p></blockquote></li></ol><h3 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h3><ol><li><p><strong>Distributed Representations of Words and Phrases and their Compositionality.</strong><br><em>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean.</em><br>NIPS 2013.<br><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p>In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.</p></blockquote></li><li><p><strong>GloVe: Global Vectors for Word Representation.</strong><br><em>Jeffrey Pennington, Richard Socher, Christopher D. Manning.</em><br>EMNLP 2014.<br><a href="http://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">paper</a></p><blockquote><p>The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods.    </p></blockquote></li></ol><h3 id="Neural-Encoders"><a href="#Neural-Encoders" class="headerlink" title="Neural Encoders"></a>Neural Encoders</h3><ol><li><p><strong>Semantic Compositionality through Recursive Matrix-Vector Spaces.</strong><br><em>Richard Socher, Brody Huval, Christopher D. Manning, Andrew Y. Ng.</em><br>EMNLP-CoNLL 2012.<br><a href="https://ai.stanford.edu/~ang/papers/emnlp12-SemanticCompositionalityRecursiveMatrixVectorSpaces.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p>We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length.</p></blockquote></li><li><p><strong>Convolution Neural Network for Relation Extraction</strong><br><em>Chunyang Liu, Wenbo Sun, Wenhan Chao, Wanxiang Che.</em><br>ADMA 2013<br><a href="https://link.springer.com/chapter/10.1007/978-3-642-53917-6_21" target="_blank" rel="noopener">paper</a></p><blockquote><p>In this paper, we propose a novel convolution network, incorporating lexical features, applied to Relation Extraction. </p></blockquote></li><li><p><strong>Relation Classification via Convolutional Deep Neural Network.</strong><br><em>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, Jun Zhao.</em><br>COLING 2014.<br><a href="http://www.aclweb.org/anthology/C14-1220" target="_blank" rel="noopener">paper</a></p><blockquote><p>We exploit a convolutional deep neural network (DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as input without complicated pre-processing.</p></blockquote></li><li><p><strong>Classifying Relations by Ranking with Convolutional Neural Networks.</strong><br><em>C´ıcero Nogueira dos Santos, Bing Xiang, Bowen Zhou.</em><br>ACL 2015.<br><a href="https://www.aclweb.org/anthology/P15-1061" target="_blank" rel="noopener">paper</a></p><blockquote><p>In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN).</p></blockquote></li><li><p><strong>Relation Extraction: Perspective from Convolutional Neural Networks.</strong><br><em>Thien Huu Nguyen, Ralph Grishman.</em><br>NAACL-HLT 2015<br><a href="http://www.aclweb.org/anthology/W15-1506" target="_blank" rel="noopener">paper</a>    </p><blockquote><p>Our model takes advantages of multiple window sizes for filters and pre-trained word embeddings as an initializer on a non-static architecture to improve the performance. We emphasize the relation extraction problem with an unbalanced corpus.        </p></blockquote></li><li><p><strong>End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures.</strong><br><em>Makoto Miwa, Mohit Bansal.</em><br>ACL 2016.<br><a href="https://www.aclweb.org/anthology/P16-1105" target="_blank" rel="noopener">paper</a></p><blockquote><p>Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs… We further encourage detection of entities during training and use of entity information in relation extraction via entity pre-training and scheduled sampling.</p></blockquote></li><li><p><strong>A Walk-based Model on Entity Graphs for Relation Extraction.</strong><br><em>Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou.</em><br>ACL 2018.<br><a href="http://aclweb.org/anthology/P18-2014#page=6&amp;zoom=100,0,313" target="_blank" rel="noopener">paper</a></p><blockquote><p>We present a novel graph-based neural network model for relation extraction. Our model treats multiple pairs in a sentence simultaneously and considers interactions among them. All the entities in a sentence are placed as nodes in a fully-connected graph structure.</p></blockquote></li></ol><h3 id="Denoising-Methods"><a href="#Denoising-Methods" class="headerlink" title="Denoising Methods"></a>Denoising Methods</h3><ol><li><p><strong>Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks.</strong><br><em>Daojian Zeng, Kang Liu, Yubo Chen, Jun Zhao.</em><br>EMNLP 2015.<br><a href="http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP203.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p>We propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems.</p></blockquote></li><li><p><strong>Neural Relation Extraction with Selective Attention over Instances.</strong><br><em>Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, Maosong Sun.</em><br>ACL 2016.<br><a href="http://www.aclweb.org/anthology/P16-1200" target="_blank" rel="noopener">paper</a></p><blockquote><p>Distant supervision inevitably accompanies with the wrong labelling problem, and these noisy data will substantially hurt the performance of relation extraction. To alleviate this issue, we propose a sentence-level attention-based model for relation extraction.</p></blockquote></li><li><p><strong>Relation Extraction with Multi-instance Multi-label Convolutional Neural Networks.</strong><br><em>Xiaotian Jiang, Quan Wang, Peng Li, Bin Wang.</em><br>COLING 2016.<br><a href="http://www.aclweb.org/anthology/C16-1139" target="_blank" rel="noopener">paper</a></p><blockquote><p>In this paper, we propose a multi-instance multi-label convolutional neural network for distantly supervised RE. It first relaxes the expressed-at-least-once assumption, and employs cross-sentence max-pooling so as to enable information sharing across different sentences.        </p></blockquote></li><li><p><strong>Adversarial Training for Relation Extraction.</strong><br><em>Yi Wu, David Bamman, Stuart Russell.</em><br>EMNLP 2017.<br><a href="http://www.aclweb.org/anthology/D17-1187" target="_blank" rel="noopener">paper</a></p><blockquote><p>Adversarial training is a mean of regularizing classification algorithms by generating adversarial noise to the training data. We apply adversarial training in relation extraction within the multi-instance multi-label learning framework.</p></blockquote></li><li><p><strong>A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction.</strong><br><em>Tianyu Liu, Kexiang Wang, Baobao Chang, Zhifang Sui.</em><br>EMNLP 2017.<br><a href="http://www.aclweb.org/anthology/D17-1189" target="_blank" rel="noopener">paper</a></p><blockquote><p>We introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training.</p></blockquote></li><li><p><strong>DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction.</strong><br><em>Pengda Qin, Weiran Xu, William Yang Wang.</em><br><a href="https://arxiv.org/pdf/1805.09929.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p>We introduce an adversarial learning framework, which we named DSGAN, to learn a sentence-level true-positive generator. Inspired by Generative Adversarial Networks, we regard the positive samples generated by the generator as the negative samples to train the discriminator.</p></blockquote></li><li><p><strong>Reinforcement Learning for Relation Classification from Noisy Data.</strong><br><em>Jun Feng, Minlie Huang, Li Zhao, Yang Yang, Xiaoyan Zhu.</em><br>AAAI 2018.<br><a href="https://tianjun.me/static/essay_resources/RelationExtraction/Paper/AAAI2018Denoising.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p>We propose a novel model for relation classification at the sentence level from noisy data. The model has two modules: an instance selector and a relation classifier. The instance selector chooses high-quality sentences with reinforcement learning and feeds the selected sentences into the relation classifier, and the relation classifier makes sentence-level prediction and provides rewards to the instance selector.</p></blockquote></li><li><p><strong>Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning.</strong><br><em>Pengda Qin, Weiran Xu, William Yang Wang.</em><br>  2018.<br><a href="https://arxiv.org/pdf/1805.09927.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p>We explore a deep reinforcement learning strategy to generate the false-positive indicator, where we automatically recognize false positives for each relation type without any supervised information.</p></blockquote></li></ol><h3 id="Extensions"><a href="#Extensions" class="headerlink" title="Extensions"></a>Extensions</h3><ol><li><p><strong>Neural Knowledge Acquisition via Mutual Attention between Knowledge Graph and Text</strong><br><em>Xu Han, Zhiyuan Liu, Maosong Sun.</em><br>AAAI 2018.<br><a href="http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2018_jointnre.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p>We propose a general joint representation learning framework for knowledge acquisition (KA) on two tasks, knowledge graph completion (KGC) and relation extraction (RE) from text. We propose an effective mutual attention between KGs and text. The recip- rocal attention mechanism enables us to highlight important features and perform better KGC and RE.</p></blockquote></li><li><p><strong>Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention</strong><br><em>Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, Peng Li.</em><br>EMNLP 2018.<br><a href="http://www.aclweb.org/anthology/D18-1247" target="_blank" rel="noopener">paper</a></p><blockquote><p>We aim to incorporate the hierarchical information of relations for distantly supervised relation extraction and propose a novel hierarchical attention scheme. The multiple layers of our hierarchical attention scheme provide coarse-to-fine granularity to better identify valid instances, which is especially effective for extracting those long-tail relations.</p></blockquote></li><li><p><strong>Incorporating Relation Paths in Neural Relation Extraction</strong><br><em>Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, Maosong Sun.</em><br>EMNLP 2017.<br><a href="http://aclweb.org/anthology/D17-1186" target="_blank" rel="noopener">paper</a></p><blockquote><p>We build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. </p></blockquote></li><li><p><strong>RESIDE: Improving Distantly-Supervised Neural Relation Extractionusing Side Information</strong><br><em>Shikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga, Chiranjib Bhattacharyya, Partha Talukdar.</em><br>EMNLP 2018.<br><a href="https://aclweb.org/anthology/D18-1157" target="_blank" rel="noopener">paper</a></p><blockquote><p>In this paper, we propose RESIDE, a distantly-supervised neural relation extraction method which utilizes additional side  information from KBs for improved relation extraction. It uses entity type and relation alias information for imposing soft constraints  while predicting relations.</p></blockquote></li></ol>]]></content>
      
      
      <categories>
          
          <category> Information Extraction </category>
          
          <category> Relation Extraction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 知识图谱 </tag>
            
            <tag> 关系识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新的起点</title>
      <link href="/2019/04/14/%E6%96%B0%E7%9A%84%E8%B5%B7%E7%82%B9/"/>
      <url>/2019/04/14/%E6%96%B0%E7%9A%84%E8%B5%B7%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>花了两天时间，总算基本上做成这个blog的雏形了，撒花！</p><p>首先尝试了Material主题，但现在没有官方支持了…最后还是选择了NexT主题，简洁大方~</p><p>尝试一下代码功能：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Hello Blog"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>一些图片：</p><p><img src="/images/blog/balloon.png" alt></p><p><img src="/images/blog/xiamu.png" alt></p><h3 id="希望这个blog可以记录我将来学习研究道路上的一些收获，加油吧"><a href="#希望这个blog可以记录我将来学习研究道路上的一些收获，加油吧" class="headerlink" title="希望这个blog可以记录我将来学习研究道路上的一些收获，加油吧~"></a>希望这个blog可以记录我将来学习研究道路上的一些收获，加油吧~</h3>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 感悟 </tag>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
